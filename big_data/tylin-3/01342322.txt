Proceedings of the 5\224 World Congress on Intelligent Control and Automation June 15-19 2004 Hangzhou P.R China An Efficient Algorithm for Mining Frequent Closed Itemset Lei Wen Deparimeni of Economy and Managemeni North China Electric Power Universiij wenlei03 12@sohu.com Bao Ding HeBei Province China posi code 071003 Abstracf Association rules mining was an important fields of data mining research Discovering the potential frequent itemset was a key step of it The existed frequent itemset discovery algorithms 
could discover all the frequent itemset or maximal frequent itemset N Pasquier proposed a new task of mining frequent closed itemset The size of frequent closed itemset was much smaller than all the frequent itemset and did not lose any information In this paper a new frequent closed itemset algorithms base on the Directed Itemset Graph was given This algorithms can discover all the frequent closed itemset effciently by using depth first search strategy The experiment showed that it was eKicient for mining frequent closed itemsets Index Terms 
Association Rule Concept Lattice Frequent closed Itemset Directed Itemset Graph I INTRODUCTION Data mining which was also referred as knowledge discovery in databases, means a process of finding nontrivial extraction of implicit, pervious unknown and potential useful information from data in database I Among discovering many kinds of knowledge in database Association rules mining was a form of data mining to discover interesting relationships among attributes in those data The problem of finding association rules was introduced by  and has been attracted great intention in database research communities in recent years The discovered rules might help marketing decision making and business management 
An example of association rules was 90 of customers that purchase bread also purchase milk Mining association tules can be decomposed into two steps the first was Generate frequent itemsets The second was generated association rules Because the solution of second subproblem was rather straightforward so most research had focus on how to generate frequent itemsets The most famous algorithms was  The algorithms employed a breath-first and downward closure strategy and used \223any subset of a large itemsets must he large and any superset of a small itemsets must be small\224 to prune the search space 222 Most of the other algorithms were the 
variant of Apriori Such as Partiiion[S DHf[6 and DIc[7 etc They all need scan the database multiple times The Apriori-inspiries algorithm showed good performance with sparse dataset but was not suitable for dense dataset. Now more works focused on how to construct a special structure to replace the original database for mining frequent itemsets and Tree frojecfion[9 were samples ofthem They used a tree structure to save the information of original database construct a Directed Itemsets Graph to search the frequent itemsets They all had a good performance than Apriori-inspiries algorithms for mining dense datasets The algorithms above could find all the frequent itemsets and had a 
good performance with sparse dataset. But when we dealed with a dense dataset the size of maximal frequent itemsets was long the size of all frequent itemset would be very large So the performance of such algorithms decreased largely There was two solution to solve this problem: the first was finding maximal frequent itemset; the second was finding frequent closed itemset The maximal frequent itemset mining could reduce the result largely hut the result could not represent all the information of original database So the better choose was finding all the frequent close itemset In this paper a new algorithm named MFCIBD\(Mining Frequent Closed Itemset Based on DISG was introduced This algorithm stored 
all the related information of frequent itemset with a novelty data structure DISG\(Directed ItemSet Graph  based on DISG algorithm MFCIBD could discover all the frequent close itemset efficiently The paper organized as follow The second part introduced the problem of association rule mining and theory foundation of it the third part introduced the MFCIBD algorithm At last we performed an experiment on a real dataset to test the performance of MFCIBD The experiment showed that it was efficient for mining frequent closed itemsets 11 THEORY FOUNDATION AND THE PROBLEM REPRESENT A Theoryfoundation The formal concept analysis 
FCA  which was proposed by professor Wille was a method to discover, index and indicate concept knowledge 3 In FCA  A Formal Context K  0 A R consisted of two sets 0 and A and of a binary relation RGOXA The elements of 0 were called the objects those ofA the attributes of K O,A,R  oRa meaned object 0 has attribute a Galois connection let K  O A R was a Formal 4296 0-7803-8273-0/04/$20.00 02004 IEEE 


Context for oc0 and acA we defined two function f and g between the power set of 0 and the powerset of A f\(0 2  2 f\(0  a E AI Vo sO,\(o a E R g\(A 2 g\(A o,a f g called Galoisconnection Formal Concept if 0  g\(A  A  f\(0  we called O,,A fromP\(O A formal concept of K 0,was called extension of 0 A and A was called the intension of it Galois closure operation the operator h  f og in power set of A and h g 0 fin power set of Owere called Galois closure operator[2 For each Galois connection f g  the following property holded for all A A A and 0  0 U  h'\(0 1 A c h\(A 0 2 h\(h\(A h\(A h'\(O U 3 A rA,=h\(A A 0 c02*h'\(0 02 The set of all the formal concept in K was called CS\(K  in CS\(K if 0 c 0  then 0 A was a subconcept of O,,A  noted as O A I O,,A By using this relation we could received an order set CS\(K  CS\(K  This was a complete lattice called concept lattice CL  B The Problem ofAssociation Rule Mining The task of association rules mining was discover the relation between different item in database This problem was first introduced by R.Agrawalin 1993 4 I  i i2 _ in was a set of item given a transaction database D  each transaction tin D was a set of items such that t c I and had an unique mark Tid The transaction database D could be thought as a third tuple D  T I R  T and I was the tidlist and itemset R was a binary relation between tidset and itemset R i_ T x I we called D  T,I R as association rule mining context each\(t,i t c T,i L I means f was relation with i Support Give an itemset I I in D  T I R  defined thesupport of itemsetl as Support  1 g\(1 I/l TI  Confidence: Give an implication r  X  Y in association rule mining context D  T,I,R we defined the confidence of implication r as Conjdence\(X  Y I g\(X U Y I 1 I g\(X I Frequent itemset In association rule mining context D  T,I,R  if Support\(1 I I  was bigger than given minimum support threshold we called I frequent itemset the set of frequent itemset X,Y\200I XnY FIS  I c I 1 Support 2 s  Association rule in association rule mining contextD=\(T,I,R the association rule was defined as an implication with this fom X Y  X Y c I X n Y    S was the support of association rule satisfied S  I g\(X U Y l/l T I 2 s  C was the confidence of association rule C  g\(X U Y I  1 g\(X I Minconjdence  Closed itemset in association rule mining context D  T,I,R  for itemset I if there was not such itemset I satisfied g\(1  g\(1 and I c I I I c I  that means h\(I  I we called I closed itemset the set of closed itemset markedcl  I 5 11 h\(I  I Frequent closed itemset if the support of closed itemset I sup port t s we called I frequent closed itemset the set of all frequent closed itemset marked as FCI  I GI I h\(1  I g\(1 It s  Mining frequent closed itemset was to discover all the 111 THE ALGORITHMS OF MINING FREQUENT s.c closed itemset satisfied the support constrain CLOSED ITEMSET A Related Work In  author proposed an algorithm for mining frequent closed itemset named A-close. A-close discovered the frequent closed itemset as follow based on closed itemset properties it determined a set of generator that will give us all frequent closed itemsets by application of Galois closure operator h An itemset p was a generator of an closed itemset c if it was one of the smallest itemset that will determine c using galois closure operator h\(p  c  The i+l generators were created by i generators then the support of i  1 generators was counted According to their support and the i-subset in i-generators infrequent generators and generators that their support is same as one of their subsets are deleted Once all frequent useful generators were found by passing database their closure were determined All the frequent closed itemset were discovered This algorithms needed pass dataset n+ltimes n times was use to count the support of generators n is thesize of maximal frequent generator the other one pass was used to discover all the frequent closed itemset In this paper, we proposed a new mining frequent itemset algorithm MFCIBD based on DISG B Directed Itemset Graph DISG\(Directed itemset Graph R was a relation between frequent 1-itemset of association rule mining context D T,I,R  R  FI Flj 1 I g\(FI Flj 1 I2 sal g\(F1 I g\(Flj 1 lFIi j=1 1 FIj I I  We defined the relation graph of R as DISG The vertex set ofDISG V  Fl 11 FI I I the arc set was A  Each vertex V had three attribute the first 4297   


was Item  The second was Tidlist  The third was Adjacenfverfex of Vi  From the definition of DISG we can draw an conclusion that the number of vertex was equal to the number of frequent and each arc  Vi V  means that Vi V was a frequent 2-itemset In this paper we used vertical database to count the support of frequent itemset the support was counted by using the join of Tidlist of each item. This method can avoid passing database multiple and showed a better performance than the first An DISG was showed in figure 1 C\(O11011 D 110101 Figure1 DISG C Frequent closed itemset mining algorithm MFCIBD The step of algorithm MFCIBD was introduced with dataset in tablel there were 6 transaction in this dataset The DISG generated from this dataset was showed infigurel The minimum support was 0.5 which means the itemset which was supported by at least 3 transaction was frequent itemset The First step selected vertex B from DISG by support descending order selected unvisited vertex E from adjacentlist of B by support descending order from the property of DISG we know that itemset B E is a frequent 2-itemset store the transaction support itemset B E B,E}.Tid/ist  1,2,3,4,5 and Supporf{B,E 0.83 because Support\(B  Supporf{B,E so itemset E is a base of frequent closed itemset  add itemset B E into the candidate set of frequent closed itemset The Second step select unvisited vertex A from adjacentlist of E by support descending order count the support of itemset B E A S\(B E A  B E Tidlist n A.Tidlist I I T I 4/6 0.67>minsup itemset B,E A is a base of frequent closed itemset add itemset B,E,A into the candidate set of frequent closed itemset The Third step select unvisited vertex  D from adjacentlist of A by support descending order count the support of itemset B,E,A,DI S\(B,E A D 4 E A}.Tidlist nD.Tidlist 1 IT I 0.5=Minsup for there is not adjacentvertex of D  so add E A,D into the set of frequent closed itemset The Fourth step retum to last vertex A because there was not unvisited adjacentvertex of A so B,E,A was a frequent closed itemset add B, E A into the set of frequent closed itemset Delete B,E,A from the candidate set of frequent closed itemset  The fifth step selected unvisited vertex D from adjacentlist of E by support descending order counted the support of itemset B,E,D S\(B,E,D l/lTl  OS=minsup for there was not adjacentvertex of D check whether there were a super set of B,E,D in the set of frequent closed iyemset and with the same support. Because there were a super set B,E,A,D of B,E,D and S\(B E A,D  S{B E,D  itemset B E,D was not a frequent closed itemset Repeated 2-5 step above until discover all the frequent closed itemset with B then deleted B from DISG repeated above step to discover all the frequent close itemset Algorithms WCIBD V Vili=l,2,..,n is vertex set ofDISG List Vi is the adjacent table of Vi S V the support of the item V S\(Vi 5 the support ofthe itemsets  V  5 CFClS is the candidate set of frequent closed itemset FCIS is the set of all the frequent closed itemset Input DISG Output FCIS Begin While V   do Select V from V by support descending order FIS Vi S\(FIq=S V Add FIS into CFCIS Select unvisited 5 from List V Call Candidategmne FIS Vj  Call DFS\(4 end Procedure DFS\(5 Begin If List   do Select Vk with highest support fromList\(V IfS\(FIS,V 2 Minsup do Call Candidategtune FIS V  Call DFS V  Delete Vk from List\(V Call DFS V  Endif Else Else Closed-check\(itemset of CFCIS with V  Retum to its parent vertex v Call DFS v  Endif End 4298 


Procedure Candidategrune FIS Vi Begin If S\(F1S f S\(FIS,V then Add FISUV to CFCIS Else Delete FIS from CFCIS Endif FIS  FIS U Vi End Procedure Closed-check\(F1S Begin While FCISta do For each FCISj E FCIS If FIS c FCIS and S\(F1S  S\(FCIS then Else Endif Delete FIS Add FIS to FCIS End IV THE EXPERIMENT OF ALGORITHM To assess the performance of algorithm MFCIBD we performed some experiments on a PC with windows2000 All the programs were writed by Visual c 6.0 An real data  mushroom taken from the UC Imine Machine Lean Database Repository was used in this experiment The mil time was showed in figure 2 200 180 I t i 60 40 30 20 ID support  Figure 2 Run Time of Algorithm MFCIBD V.CONCLUSION In this paper we defined the task of frequent close itemset mining and introduce a new frequent closed itemset algorithm MFCIBD based on directed itemset graph An experiment was done with a real database and it show that this algorithm had a good performance to discover all the frequent closed itemset. It was important to help user to understand the mining result REFERENCES IIChen M S Han I and Yu P S Data Mining An Overview from Database Perspmtive IEEE Transactions an Knowledge and Data Engineering Volume 8 Number 6 December \(1996\,p866-883 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemets for association rules[C hoe 7th Int Cod Database Theory\(ICDT99\p 398416 Jerusalem Israel January 1999 wile R Reshllcluring lattice theory an approach based on hierarchies of in Rival I ed Ordered sets dordrecht Reidel 1982 445470 Agawal R Srikant R Fait algorithms for mining association rules In Proceedings of the 20th lntemational Conference on Very Large Databases Santiago Chile, September 1994.487-499 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules Proceedings of the 2lst intemational conference on very large databases, Zurich, Switzerland, Sept. 1995,432-444 1 S Park M S Chen, and P S Yu An effective hash-based algorithm for mining association rules Proceedings of ACM SIGMOD lntemational Conference on Management of Data, pages 175-186 San Jose CA May 1995  7 Brin S Mohvani R Ullman I D and Tsur S Dynamic Itemset Counting and implication NIW for Market Basket Data. Proceedings of the ACM SIGMOD, 1997. pages 255-264 8 J.Han,J.Pei,and Y.Yin.Mining frequent panems without candidate genera1ion.h Proc.2000 ACM-SIGMOD Int Conf Management of Data \(SIGMOD\222OOj,Dalas TX ZOOOj,pl-12 9 R.C.Agww.1 C.C.Agganual and V.V.V Prasad A tree projection algorithm for generation of frequent itemsets Parallel and Distributed Computing, March 2001 IO Lei Wen  Minqiang Li A New Association Rules Mining Algorithm Based on Directed Itemsect Graph Proceedings of RSFDGrC2003 Chongqing,China.\(2003\p664-668 2 3 4 SI 6 4299 


4 Conclusion I 11 Won I Song D Lee C Heo Y  Jang J A Machine Learning approach toward an environment-free network anomaly IDS  A primer report In Proc of 5th Intemational Conference on Advanced Comunication Technology 2o03 We have presented a series of experiment to set up a hybrid system of combining signature-based IDS SNORT with a machine learning based anomaly detector engine While many other systems have tried data mining techniques to reduce false alarms we have shown that if rich set of analyzed data is available memory based supervised leaming can be effectively used to improve the performance of signature based IDS The experiment shows that the reduction rate is though far from satisfactory This might be resulted by poor set of signature we have in SNORT in that too many alarms are triggered from the same attack set In that sense a careful alarm filter that does not omit valuable information could be helpful for this hybrid system At this point of time the implementation level of this hybrid system is just a taboratory style We are developing this hybrid system continuously and some other new ideas and test sets are ready to be included Acknowledgements This research is fully supported by Electronic and Telecommunication research Institute REFERENCES I Aha D  Kibler D Noise-tolerant instance-based learning algorithms Proceedings of the Eleventh International Joint Conference on Artificial Intelligence pp.794-799 1989 2 Julisch K Minin alarm clusters to improve alarm handling efficiency In 17 t Annual Computer Security Application Conference\(ACSAC pp 12-21,2000 3 Julisch K  Dacier M Mining Inmsion Detection Alarms for Actionable Knowledge In 8 ACM International Confemce on Knowledge Discovery and Data Mining 2002 Toth T Using decision trees to improve signaturebased detcction In 6th Symposium on Recent Advances in Intrusion Detection RAD Lecture Notes in Computer Science, Springer Verlag USA September 2003 5 Lippman R et Al Evaluation intrusion detection systems The 1998 DARPA Off-line intrusion detection evaluation Proc Of DARPA Information Survivability Conference and Exposition 6 Manganaris S Christensen, M Zerkle D  Henniz K A Data Mining Analysis of RTID Alarms In Znd Workshop on Recent Advances in intrusion Detection RAID99\1999 7 Patton S Yurcik W  Doss D An Achilles Heel in Signature-based IDS Squealing False Positives in SNORT Lecture Notes in Computer Science Springer Verlag USA 2003 SI SNORT http://w.snort.org 9 Stanfill C  Waltz D Toward memory-based reasoning IO 4 Kruegel C  pp 12-26,2000 Communications of the ACM 1986 Intrusion Detection Systems \(written in Korean Communication of the Korean Institute of Communication Sciences 19\(8 pp 41-51,2002 Won I Song D  Lee C The Architecture of Network   245  


                         


                      


            


100 34 33 34 DSD u u u                          u             Fig.6: Over Hiding problem of setting  1 in S No matter the left-hand or right-hand equation, the support of {1, 2} in D' is 0. That is, item 1 and item 2 never appear toge ther, and they are mutual exclusive! This situation almost never happens in the normal database. The attackers may interest in this situation and infer that {1, 2} is hidden deliberately. To hide the sensitive patterns, only need to make their supports smaller than minimum support and need not to decrease their support to 0. To solve the problem, we inject a probability ? which is called Distortion probability into this approach. Distortion probability is used only when the column j of the sanitization matrix S contains only one  1  i.e. Sjj = 1 0 1 d   m k k j i k  S D  m j n i j i  d d d d   1  1     D  i j  h a s   j probability to be set to 1 and 1  j probability to be set to 0 Lemma 1: Given a minimum support ?, and a level of confidence c. Let {i, j} be a pattern in Marked-Set, nij be the support count of {i, j}. ? is the Distortion probability of column j Without loss of generality, we assume that Sij  1. If ? satisfies    D n i j  u  u  V U   a n d    


   c x n xijnx D x ij t          u    1 0 UU V where D| is the number of transactions in D, we can say that we are c confident that {i, j} isn  t frequent in D Proof: If probability policies are not considered and Sij  1, that is, while a row \(transaction according to the matrix multiplication discussed in the previous section, D'tj will be set to 0. We can view the nij original jtiD jtjtjt ijn DDD 21     f o r  e a c h  r o w  t i  c o n t a i n s   i   j   i n  D  a s  realizations of nij independent identically distributed \(i.i.d om variables ijn X X X      2 1      e a c h  w i t h  t h e  s a m e  d i s t r i b u  tion as Bernoulli distribution, B \(1 X i    1  a n d  t h e  f a i l u r e  i s  d e f i n e d  a s  X i    0     i    1  t o  n i j   T h e  p r o b  ability ? is attached to the success outcome and 1?? is attached to the failure outcome. Let X be a random variable and could be viewed as the number of transactions which include {i, j} in D such that jin     2 1    T h u s   t h e  d i s t r i b u t i o n  o f  X is the same as Binomial distribution, X?B \(nij              otherwise nx x n XP ij xnxij x ij 0 1,0     U the mean of X = nij?, and the variance of X = nij?\(1 If we want to hide {i, j} in D', we must let its support in D' be smaller than ?. Therefore, the expected value of X must be s m a l l e r  t h a n     D u V    T h a t  i s     m u s t  s a t i s f y     D n i j  u  u  V U   Moreover, the probability of the number of success which is 


Moreover, the probability of the number of success which is equal to or smaller than     1     u  D V   s h o u l d  b e  h i g h e r  t h a n  c  Therefore, U must satisfy    c x n xijnx D x ij t           u    1 0 UU V  If ? satisfies the two equations, we can say that we are c confident that {i, j} isn  t frequent in D When nij &gt; 30, the Central Limit Theorem \(C.L.T used to reduce the complexity of the equation and speed up the execution time of the sanitization process Moreover, if several entries in column j of S are equal to  1 such as Sij  1, Skj  1, Smj  1  etc, several candidate Dist ortion probabilities such as iU , kU , mU , etc are gotten. The Dis tortion probability of column j, ? j, is set to the minimal candid ate Distortion probability to guarantee that all corresponding pair-subpatterns have at least c confident to be hidden in D 3.3. Sanitization Algorithm Fig.7: Sanitization Algorithm According to the probability policies discussed above, the sanitization algorithm D'n  m = Dn  m  Sm  m is showed in Fig.7 Notice that, if the sanitization process causes all the entries in row r of D' equal to 0, randomly choose an entry from some j Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE where Drj = 1, and set D'rj = 1. It is to guarantee that the size of the sanitized database equals the size of the original database 4. Performance Evaluation 4.1. Performance Quantifying There are three potential errors in the problem. First of all some sensitive patterns are hidden unsuccessfully. Secondly some non-sensitive patterns cannot be mined from D'. And the third, new artificial patterns may be produced. However, the third condition never happens in both of our approach and SWA Besides, we also introduce the other two criteria, dissimilarity and weakness. All of the criteria are discussed as follows Criterion 1: some sensitive patterns are frequent in D'. This condition is denoted as Hiding Failure [10], and measured by     DP DP HF H H , where XPH represents the number of patterns contained in PHwhich is mined from database X Criterion 2: some non-sensitive patterns are hidden in D'. It is also denoted as Misses Cost [10], and measured by      DP DPDP MC 


MC H H H      w h e r e        X P H  r e p r e s s ents the number of patterns contained in ~PH which is mined from database X Criterion 3: the dissimilarity between the original and the sanitized database is also concerned, and it is measured by      n i m j ij n i m j ijij D DD Dis 1 1 1 1    Criterion 4: according to the previous section, Forward Inference Attack is avoided while at least one pair-subpattern of a sensitive pattern is hidden. The attack is quantified by        DPDP DPairSDPDP Weakness HH HH    where DPairS is the set of sensitive patterns whose pair subsets can be completely mined from D 4.2. Experimental Results Table 1: Experiment factor The experiment is to compare the performance between our approach and SWA which has been compare with Algo2a [4 and IGA [10] and is so far the algorithm with best performances published and presented in [12] as we know. The IBM synthetic data generator is used to generate experimental data. The dataset contains 1000 different items, with 100K transactions where the average length of each transaction is 15 items. The Apriori algorithm with minimum support = 1% is used to mine the dataset and 52964 frequent patterns are gotten Several sensitive patterns are randomly selected from the frequent patterns with length two to three items to be seeds With the several seeds, all of their supersets are included into the sensitive patterns set since any pattern which contains sensitive patterns should also be sensitive 0 0.05 0.1 0.15 0.2 0.25 0.3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern H id 


id in g F ai lu re SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern W e ak n es s SP SWA Fig.8: Rel. bet. RS and HF. Fig.9: Rel. bet. RS and weakness Fig.8, Fig.9, Fig.10, Fig.11 and Fig.12 show the effect of RS by comparing our work to SWA. Refer to Fig.8, because the level of confidence in our sanitization process takes the minim um support into account, no matter how the distribution of the sensitive patterns, we still are C confident to avoid hiding failure problems. However, there is no correlation between the disclos ure threshold in SWA and the minimum support. Under the sa me disclosure threshold, if the frequencies of the sensitive patte rns are high, the hiding failure will get high too. In Fig.9, beca use our work is to hide the sensitive patterns by decreasing the supports of the pair-subpatterns of the sensitive patterns, the val ue of weakness is related to the level of confidence. However according to the disclosure threshold of SWA, when all the pair subpatterns of the sensitive patterns have large frequencies, it may cause serious F-I Attack problems. Hiding failure and wea kness of SWA change with the distributions of sensitive patterns 0 0.1 0.2 0.3 0.4 0.5 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern M is se s C o st SP SWA 0 0.005 0.01 0.015 0.02 0.025 0.03 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern D is si m 


m il ar it y SP SWA Fig.10: Rel. bet. RS and MC.  Fig.11: Rel. bet. RS and Dis In Fig.10 and Fig.11, ideally, the misses cost and dissimilar ity increase as the RS increases in our work and SWA. However if the sensitive pattern set is composed of too many seeds, a lot of victim pair-subpatterns will be contained in Marked-Set. And as a result, cause a higher misses cost, such as x = 0.04 in Fig.10 Moreover, refer to the turning points of SWA under x = 0.0855 to 0.1006 in Fig.10 and Fig.11. The reason of violation is that the result of the experiment has strong correlation with the distri bution of the sensitive patterns. Because the sensitive patterns Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE are chosen randomly, several variant factors of the sensitive patt erns are not under control such as the frequencies of the sensiti ve patterns and the overlap between the sensitive patterns if the overlap between the sensitive patterns is high, some sensitive patterns can be hidden by removing a common item in SWA Therefore, decrease the misses cost and the dissimilarity 0 0.5 1 1.5 2 2.5 3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern E x ec u ti o n T im e h  SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set H id in g F a il u re SP SWA 


SWA Fig.12: Rel. bet. RS and time. Fig.13: Rel. bet. RL2 and HF Fig.12 shows the execution time of SWA and our approach As shown in the result, the execution time of SWA increases as RS increases. On the other hand, our approach can be separated roughly into two parts, one is to get Marked-Set which is strong dependent on the data, the other is to set sanitization matrix and execute multiplication whose execution time is dependent on the numbers of transactions and items in the database. In the experi ment, because the items and transactions are fixed, therefore, the execution time of our approach is decided by the setting of Marked-Set which makes the execution time changing slightly 0 0.05 0.1 0.15 0.2 0.25 0.3 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set W ea k n es s SP SWA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set M is se s C o st SP SWA Fig.14: Rel. bet. RL2 and weakness. Fig.15: Rel. bet.RL2 and MC 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set D is si m il a ri ty SP SWA Fig.16: Rel. bet. RL2 and Dis Fig.13, Fig.14, Fig.15and Fig.16 show the effect of RL2 Refer to Fig.13 and Fig.14, our work outperforms SWA no matter what RL2 is. And our process is almost 0% hiding failure 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


