 000\003  Abstract The ability to mine large volumes of distributed datasets enables more precise decision making. However, privacy concerns should be carefully addressed when mining datasets distributed over autonomous site s. We propose a new PrivacyPreserving Protocol for Association Rule Mining \(P3ARM\over horizontally partitioned data. P3ARM is based on a distributed implementation of the Apriori algorithm. The key idea is to arbitrary assign polling sites to collect itemsets’ supports in encrypted forms using homomorphic encryption techniques. A 
pair of polling sites is assigned for each itemset. Polling sites are different for consecutive rounds of the protocol to reduce the potential for collusion. Our perf ormance analysis shows that P3ARM significantly outperforms a leading existing protocol Moreover, P3ARM is scalable in the number of sites and the volume of data  Index Terms Cryptography, Data Security, Privacy, Security I  I NTRODUCTION Data mining is the process of filtering through large amounts of raw data for useful information. This information is made up of meaningful patterns and trends that are already in the data but were previously unseen. Different data mining techniques help analysts recognize significant relationships 
trends and patterns in raw data in order to make better decisions. Distributed data mining algorithms apply data mining tasks on datasets distributed among different sites However, privacy concerns may prevent cooperative sites to provide their data for mining; a survey of Internet users attitudes towards privacy [1 o w ed th at 1 7 o f th e u s ers are extremely concerned about any use of their data and generally unwilling to provide their data, even when privacy protection measures were in place. For 56% of the users, these concerns are often significantly reduced by the presence of privacy protection measures. Indeed, ther e is an increasing need to develop privacy-preserving solu tions for different cooperative 
computation scenarios, including data mining This paper presents a new protocol for mining data that are distributed among mutually untrusting parties. We focus on  Manuscript received March 21, 2006 Iman Saleh and Mohamed Eltoweissy are with the Bradley Department of Electrical and Computer Engineering, Vi rginia Polytechnic Institute and State University, VA 22043, US A \(e-mail: iman.saleh, toweissy@vt.edu Alaa Mokhtar and Amin Shoukry are with the Computer and Systems Engineering Department, Alexandria University, Egypt \(e-mail: ahafez2001 shoukryamin @yahoo.com mining association rules on the union of datasets that are partitioned horizontally among au tonomous sites. The data 
schemes of all partitions are the same, i.e., their records represent transactions on the same set of items. The goal is to produce a set of global association rules, while limiting the information shared between site s. Examples of such cases include the consumer transactions recorded by different supermarkets and medical records collected by competitive medical insurance companies In this model of cooperation, it is not necessary to transfer the whole datasets to apply association mining tasks. Only aggregating information on each data set is sufficient to calculate the itemsets’ support counts and hence discovering association rules. However, ex changing aggregate information about the data may reveal sensitive rules. For example 
suppose a public agency would like to mine health records Insurance companies have data on patients’ diseases and prescriptions. Imagine a rule indicating a high rate of complications with a particular medical procedure. If this rule does not hold globally, the insurer would like to know this they can then try to pinpoint the problem with their policies and improve their care. If the f act that the insurer’s data supports this rule is revealed, the insurer could be exposed to significant public relations or liability issues [2 Th e proposed protocol mines the data without leaking any information about the sites’ private inputs The problem of privacy-preserving data mining is a special 
case of the Secure Multiparty Computation \(SMC\problem The SMC problem aims to compute some function of inputs that are scattered among different parties, without having these parties reveal their individual input Gol d rei c h provided a proof that for such function, there is a SMC solution. The approach used is to represent the function as a combinatorial circuit. The parties then run a short protocol for every gate in the circuit. In this approach, as in data mining the size of the protocol depends on the size of the circuit which can be highly inefficient for large inputs. Solutions should be developed to overcome the performance limitations of the general theoretical solution 
The main contribution of this paper is proposing P3ARM, a new protocol for privately mining association rules over data that is partitioned between three or more sites. P3ARM Privacy-Preserving Protocol for Association Rule Mining enables the participating sites to discover the correct and P3ARM: Privacy-Preserving Protocol for Association Rule Mining Iman Saleh Member IEEE Alaa Mokhtar, Amin Shoukry, and Mohamed Eltoweissy Senior Member IEEE  Proceedings of the 2006 IEEE Workshop on Information Assurance United States Military Academy, West Point, NY 1-4244-0130-5/06/$20.00 ©2006 IEEE 76 


 complete set of rules over the union of their databases without disclosing any information about their individual inputs. The proposed protocol achieves the required level of privacy while imposing minimal communication and computation overhead to the mining task. P3ARM is based on a distributed implementation of the Apriori algorithm W e propose a performance enhancement to the scheme privacy preserving functionality In P3ARM, each itemset is arbitrary assigned to a pair of polling sites. The polling sites securely decide whether the itemset is globally supported This organization minimizes the probability of collusion and provides a scalable solution for large number of sites. Hence P3ARM is a practical and appli cable approach for privately mining data that is distributed among competing or mutually untrusting parties The rest of this paper is organized as follows. Section 2 provides background on the data mining algorithms forming the basis of our solution. The proposed protocol is presented in Section 3. Section 4 gives a security analysis and proof of privacy level achieved. Section 5 presents performance analysis in terms of the computation, storage and communication costs. Section 6 presents further discussion on P3ARM and compares it with the protocol proposed in  Section 7 includes related work and finally Section 7 concludes the paper and outlines directions for future work II  B ACKGROUND  In this section we describe the Apriori algorithm and a distributed implementation of that algorithm As st at ed above, P3ARM builds on the distributed implementation of Apriori  A  Association rule mining and the Apriori algorithm The association rule mining problem was formally defined in [4 llo ws. Let I  i 1 i 2 i n be a set of items. Let DB  be a set of transactions, where each transaction T is an itemset such that T  000  I Given an itemset X  000  I a transaction T  contains X if and only if X  000  T An association rule is an implication of the form X 000 Y where X 000 I, Y 000 I and X  000 Y 000 The rule X 000 Y has support s in the transaction database DB if s of transactions in DB contain X 000 Y The association rule holds in the transaction database DB with confidence c if c of transactions in DB that contain X also contains Y An itemset X with k items called k itemset. The problem of mining association rules is to find all rules whose support and confidence are higher than certain user specified minimum support and confidence. In this respect, a transaction database DB can be seen as 0/1 matrix where each column is an item and each row is a transaction The Apriori algorithm has been developed for rule mining in large transaction databases by IBM's Quest project team   They  have decom posed t h e probl em of m i ni ng associ at i on rules into two parts  Find all combinations of items that have transaction support above minimum support. Call those combinations frequent itemsets, and  Use the frequent itemsets to generate the desired rules The general idea is that if, say, ABCD and AB are frequent itemsets, then we can determine if the rule AB CD holds by computing the ratio r support\(ABCD\support\(AB rule holds only if r minimum confidence. Note that the rule will have minimum support because ABCD is frequent The Apriori algorithm makes multiple passes over the database. In the first pass, it simply counts item occurrences to determine the frequent 1-itemsets. A subsequent pass, say pass k consists of two phases. First, the frequent itemsets L k-1 the set of all frequent k 1\temsets\n the k 1 th pass are used to generate the candidate itemsets C k using the apriorigen function. This function first joins L k-1  with L k-1 the joining condition being that the lexicographically ordered first k 2 items are the same. Next, it deletes all those itemsets from the join result that have some k 1\ that is not in L k-1  yielding C k  B  Distributed association rule mining Cheung et al. proposed a method for mining association rules when data is partitioned horizontally between sites [5 The problem can be defined as follows Let DB be a partitioned database located at n sites namely S 1 S 2 S n The database partitions at these sites are DB 1  DB 2 DB n respectively. Let the size of DB and the partition DB i be D and D i respectively. For a given itemset X  X has local support count of X.sup i at site S i if X.sup i  of the transactions in DB i contains X The global support count of X  is given as X.sup  000 000 n i i X 1 sup  For a given minimum support s  X is globally large if X.sup  000  s × D correspondingly X is locally large at site S i if X.sup i  000 s × D i   In the following, we will use L to denote all the globally large itemsets in DB and L k to denote all globally large k itemsets in L The problem of mining association rules in a distributed database DB can be reduced to the finding of all globally large itemsets. For a site S i if an itemset X is both locally large at site S i and globally large, then we say that X is heavy at site S i We will use HL i to denote the set of heavy itemsets at site S i and HL i k to denote the set of heavy k itemsets at site S i  In a straightforward adaptation of Apriori, in the k th  iteration, the set of candidate sets would be generated by applying Apriori-gen function on L k-1 We denote this set of candidate sets by CA k which stands for size-k candidate sets from Apriori CA k   Apriori-gen \(L k-1  At each site S i let CH i k be the set of candidates sets generated by applying Apriori-gen on HL i k-1 i.e CH i k Apriori-gen \(HL i k1   CH stands for candidate sets generated from heavy itemsets CH i k is generated from HL i k-1 which is only a subset of L k-1 For clarity, Table 1 lists the notations used in our discussion of the distributed association rule protocols Proceedings of the 2006 IEEE Workshop on Information Assurance United States Military Academy, West Point, NY 1-4244-0130-5/06/$20.00 ©2006 IEEE 77 


  The method can be summarized as follows 1. Candidate Sets Generation generate the candidate sets CH i k Apriori-gen\(HL i k-1  Each site generates candidates based on the intersection of globally large k 1\sets and locally large k 1\sets 2. Local Pruning: For each X 000 CH i k scan the database DB i at S i to compute X.sup i If X is locally large at S i it is included in the LL i k set. It is clear that if X is supported globally, it will be supported in one site 3. Support Count Exchange LL i k  are broadcast, and each site computes the local support for the items in i 000 LL i k  4. Broadcast Mining Results: Each site broadcasts the local support for itemsets in i 000 LL i k From this, each site is able to compute L k  The above method requires O\(n 2  messages for count exchange for each candidate set, where n is the number of  based on this m e thod To ensure that only O\(n messages are required for every candidate set, an optimization technique has been introduced A simple assignment function, which could be a hashing function, is used to determine a polling site for each candidate set. For each candidate set X its polling site is responsible for broadcasting the polling request, collecting the support counts and deciding whether X is large or not. Since there is only one polling site for each candidate set X the number of messages required for count exchange for X is O\(n The optimized method can be summarized as follows 1. Candidate Sets Generation generate the candidate sets CH i k Apriori-gen\(HL i k-1  as before 2- Candidates sent to Polling Sites S i acts as a home site of its candidate sets; for every polling site S j  S i finds all the candidate sets in LL i k whose polling site are S j and stores them in LL i,j k i.e., candidates are being divided into groups according to their polling sites the local support counts of the candidate sets are also stored in the corresponding set LL i,j k  sends each LL i,j k to the corresponding polling site S j  3- Polling Site sends Polling Requests S i acts as a polling site S i receives all LL i k sent to him from the other sites; for every candidate set X received S i finds the list of originating sites from which X is being sent S i then broadcasts the polling request to the other sites not on the list to collect the support counts 4- Remote Site replies Polling Requests S i acts as a remote site to reply polling requests; for every polling request LL p,j k  from polling site S p  S i sends the local support counts of the candidates in LL p,j k back to S p  5- Polling Site Computes Heavy Itemsets S i acts as a polling site to compute the heavy itemsets S i receives the support counts from the other sites; computes the global support counts for its candidates in LL i k  and finds the heavy itemsets; eventually S i broadcasts the heavy itemsets together with their global support counts to all sites III  P RIVACY P RESERVING P ROTOCOL FOR A SSOCIATION R ULE M INING P3ARM We will extend and enhance the distributed association rule algorithm in descri b ed above o achi e ve pri v acy  The goal is to construct a secure version of the algorithm by preventing the disclosure of any information beyond the mining results Our work is inspired, in pa rt, by the protocol recently proposed by the Kantarcioglu and Clifton Protocol \(KCP for short\ proposed in for pri v at el y  di scoveri ng associ at i on rules in horizontally partitioned data. Their protocol is based on cryptographic tools, namely the commutative encryption and secure comparison, to discover the frequent rules. We will discuss KCP in more detail in Section 6 along with a comparison with our P3ARM A  Problem definition Let n  000 3 be the number of sites. Each site i has a private transaction database DB i Given a support threshold s and confidence c as percentages, the goal is to discover all association rules satisfying the thresholds. To preserve privacy of data owned by participating sites, it is required to limit the information leakage so that each site – after the execution of the protocol – knows nothing that cannot be simulated knowing its own input data and the mining results We consider the following as private information and should not be revealed  The itemsets supported at each site  The local support count of an itemset at each site  The global support count of an itemset at each iteration k  Database size at each site Instead of using the actual support count to decide whether an itemsets is heavy or not we will use the excess support count of the itemset, i.e. by how much a support count at a site exceeds the threshold support s For an itemset to be globally heavy, the following inequality must be true [2 000 000 000 000 000t n j j n j j DB s X 1 1  sup  000 0  sup  1 000t 000\020 000 000 j n j j DB s X  We will denote X.sup j s * |DB j  by X.esup j  B  Assumptions  Public-Key Infrastructure We assume that a public-key infrastructure is available to all parties; Every party i have a public key P i known by all parties, and a private key Q i  TABLE  I N OTATION  D The number of transactions in database DB  D i  The number of transactions in the partition DB i  L k  The set of globally large k itemsets CA k  The set of candidate sets generated from L k  HL i k  The set of heavy k itemsets at site Si  CH i k  The set of candidate sets generated from HL i k-1  LL i k  The set of locally large k itemsets in CH i k  X.sup The global support count of an itemset X  X.sup i  The local support count of an itemset X at site S i   Proceedings of the 2006 IEEE Workshop on Information Assurance United States Military Academy, West Point, NY 1-4244-0130-5/06/$20.00 ©2006 IEEE 78 


 known only to party i The keys are generated from a cryptographic system homomorphic over addition, That is, the sum of two encrypted values is equal to the encrypted sum of the values E k x\ * E k y\ = E k x + y Homomorphic encryption is used in voting protocols to verify the tally of the ballots without revealing what those ballots are. Benaloh proposes in a hom om orphi c encryption system. The work in m a kes use of t h e homomorphism over the addition to construct cryptographic counters used to build a secure voting system  Semi-Honest Threat Model The participating parties are assumed to be semi-honest. A semi-honest party follows the rules of the protocol using its correct input, but after the protocol is free to use whatever it sees during execution of the protocol to compromise privacy   Collusion Initially, we assume no collusion between sites; in section 4.2, we w ill suggest a solution to protect against a potential collusion pr oblem that can reveal the support information of sensitive itemsets C  Proposed protocol P3ARM is based on the distributed data mining algorithm however, additional processing is required to limit the disclosure of information. This is achieved by communicating support counts in encrypted form and assigning a pair of polling sites to each itemset s: we will call them the polling site and the co-polling site. The polling site is responsible for collecting and summing encrypted support counts, the copolling site is responsible for decrypting the support count securely compare it with the threshold and broadcasting the result to all sites. We will assume that, for a polling site S i S i+1 mod n acts as its co-polling site. We will omit the mod n for simplicity.  The protocol can be summarized as follows 1. Candidate Sets Generation: Since globally large k 1 itemsets is known by all sites at each iteration k, each site can locally compute candidate sets CA k Apriori-gen\(L k-1 CA k is the same for all sites 2- Candidates sent to Polling Sites: For every polling site S j  the site finds all the candidate sets in CA k whose polling site are S j and stores them in CA j k The excess support count of the candidate sets is encrypted with the public key P j+1 of the co-polling site S j+1  and stored in the corresponding set CA j k  the site sends each CA j k to the corresponding polling site S j  Note that in the non-secure version of the algorithm, only CH i,j k the intersection of globally large k1\temsets and locally large k 1\sets - is sent to the polling site. In the secure version all candidate sets CA j k are sent instead. The reason of that is to avoid revealing the information of which itemsets are supported at site i However, this causes no overhead on local computation cost since all k itemset supports are already calculated during the local pruning as described in  B o t h  st eps 3 and 4 of t h e non-secure distributed data mining algorithm are no more needed in the secure version; since all sites send local supports for all k itemset in CA k to the corresponding polling site 3- Polling Site Computes Heavy Itemsets S i acts as a polling site to compute the heavy itemsets S i receives the support information from the other sites encrypted using the key of its neighboring site i 1 S i computes the global encrypted excess support counts for its candidates, so for a candidate k itemset X  S i computes E P i+1 X.esup\ = E P i+1  000 000 n j j e X 1 sup  E P i+1 X.esup 1  E P i+1 X.esup 2 E P i+1 X.esup n  S i generates a random number r that is used to conceal the global support count S i sends to S i+1 the value E\(r 000 000 n j 1 X.esup j   S i+1  decrypts the received value and obtains r 000 000 n j 1 X.esup j  To determine if X is globally heavy itemset S i and S i+1  engage in as secure protocol to test whether r 000 000 n j 1 X.esup j r where S i  knows r and S i+1  knows r   000 000 n j 1  X.esup j  This can be done using Yao method for secure comparison; an efficient algorithm for solving this problem is  S i+1  broadcasts the global heavy itemsets to all sites.  The protocol pseudo-code is given in Fig. 1 P3ARM contains the following message types submit\(k,esupp  sent by a site to the polling site at round k the message contains the encrypted excess support counts esupp of the itemsets assigned to that polling site collect\(k,sum  sent by a polling site to its co-polling site at round k the message contains the encrypted sum of the itemsets concealed by random values scompare\(k,r  sum   a set of messages exchanged between the polling and co-polling sites, at round k to securely compare the sum of supports with the random values result\(k,flag   broadcast by the co-polling sites to all other sites, the message contains a 1-bit flag per itemset indicating whether the itemset is globally supported or not. A typical message exchange sequence is depicted in Fig. 2  Proceedings of the 2006 IEEE Workshop on Information Assurance United States Military Academy, West Point, NY 1-4244-0130-5/06/$20.00 ©2006 IEEE 79 


  D 2 50 X.sup 2 10, Y.sup 2 5 X.esup 2 10-10%*50=5 Y.esup 2 5-10%*50=0 2 submit\(1,{ Ep 4 0 4 5 collect\(1,{Ep 4 0\+Ep 4 5\+Ep 4 15\Ep 4 7 Ep 4 5 4 0 4 4\Ep 4 2 D 4 50 X.sup 4 6, Y.sup 4 2 X.esup 4 6-10%*50=1 Y.esup 4 2-10%*50=-3 1 1 4 resul t  1  1  0   D 1 50 X.sup 1 5, Y.sup 1 10 X.esup 1 5-10%*50=0 Y.esu p 1 10-10%*50=5 submit\(1,{ Ep 4 5\ Ep 4 0 D 3 50 X.sup 3 20, Y.sup 3 1 Rx=7, Ry=2 X.esup 3 20-10%*50=15 Y.esup 3 1-10%*50=-4 3 scompare\(1,{7,2},{28,0 Site 3 Site 1 Site 4 Site 2 Co-Polling Site for the itemsets X, Y with Key Pair \(P 4  Q 4   Polling Site for the itemsets X, Y    Fig. 2. Exchange of Messages in P3ARM per Round per Polling Site  Securely comparing the confidence of a rule For a rule X 000 Y and under the restricted privacy constraints, the support counts are not revealed so no site knows the value of XY.sup or X.sup They only know whether a support count exceeds the minimum support threshold or not. We will use the transformati  to be able to use above protocol to check the confidence of a rule c X XY 000 sup  sup   000 c X XY n i i n i i 000 000 000 000 000 1 1 sup  sup      000 000 000 000 000 000 n i i n i i X c XY 1 1  sup    sup    000  000 000 000 000\020 n i i i X c XY 1 0  sup   sup    For a rule X 000 Y a site j sends E\(XY.sup j c *X.sup j  to the polling site and using the secure comparison algorithm, the confidence of the rule is check ed against the threshold value without revealing the rule confidence or the support of any of its itemsets IV  S ECURITY A NALYSIS  A  Proof of security level achieved We use the proof by simulation to prove the security of the proposed protocols. The key idea is to show that a polynomial time simulator can simulate the view of the parties during the execution of the protocol based on their local inputs and the global result. We also use the composition theorem which states that if a function g is securely reduced to another function f and f is computed securely then the computation of f\(g    Theorem The proposed algorithm privately computes L k  in the semi-honest model Proof Step 1 each site locally computes CA k based on the globally known large itemsets L k-1 so no communication occurs in this step, each site can simulate its view by running the algorithm on its own input Step 2 same as step 1; each site locally encrypt the support count for each candidate itemset  Step 3 each polling site receives the encrypted support counts. Assuming the security of the encryption; the encrypted support counts are computationally indistinguishable from a number chosen from a uniform distribution. Hence, the polling site can simulate the encrypted supports received by a uniform random number generato r. The polling site neighbor receives the encrypted sum of the support counts concealed by a random number r generated by the polling site, hence, the value obtained by the neighbor site after the decryption is computationally indistinguishable from a random number Each polling site engages in a secure comparison protocol with its neighbor to decide whether the global support exceeds the threshold value. Hence, no information is disclosed in this step that can not be simulated based on the site input and output Therefore, based on Steps 1, 2, and 3, we can conclude that the proposed protocol securely calculates L k  B  Collusion problem We propose a modification to ensure the secrecy of the support count of an itemset that may be a part of a sensitive P3ARM Finding global heavy k itemsets privately Input  n 3 sites numbered 1..n  the minimum support threshold s  The global heavy k-1 itemsets L k-1   Step 1 Candidate sets generation For each site S i  Generate CA k  Apriori-ge n L k-1   Step 2 Candidate sets support count excess sent to the corresponding polling sites  For each polling S j   For each candidate set X whose polling site is S j  Compute E p j+1 X.esup i     End for   Send encrypted support counts excess to S  j    End for End for  Step 3 Find Global heavy k-itemsets For each polling S  j   For each candidate set X whose polling site is S  j   Generate a random r  Send E P j+1 r 000 000 n j 1 X.esup j  to S j+1    S  j+1 computes D q j+1 E P j+1 r 000 000 n j 1 X.esup j   S  j and S  j+1 securely compare r 000 000 n j 1 X.esup j  against r  If r 000 000 n j 1 X.esup j r  S  j+1 broadcasts X to all site as a globally heavy itemset  End for  End for   Fig. 1.  P3ARM Pseudo-Code Proceedings of the 2006 IEEE Workshop on Information Assurance United States Military Academy, West Point, NY 1-4244-0130-5/06/$20.00 ©2006 IEEE 80 


  0 10000 20000 30000 40000 50000 60000 70000 80000 20 40 60 80 100 Num. of Pollling Sites Messages 0 10000 20000 30000 40000 50000 60000 bits Total Num. Messages Avg. Message Length Fig. 3.  The Effect of Changing Number of Polling Sites on Number of Messages and Average Message Length  0 20000 40000 60000 80000 100000 20 40 50 100 200 Num. of Itemsets per Polling Site Messages 0 5000 10000 15000 20000 25000 bits Total Num. Messages Avg. Message Length Fig. 4. The Effect of Setting a Minimum Number of Itemsets per Site on Number of Messages and Average Message Length association rule \(the complicati ons associated with a certain medical procedure for exampl e polling and co-polling site for this itemset may reveal the support count at all other sites. For this purpose, we assume that the cryptosystem has the threshold decryption property For a sensitive itemset X  D co-polling sites are responsible for decrypting the sum X.sup instead of one co-polling site Each of  the D sites hold a secret share of the private key and if sufficiently many of them cooperate, they decrypt the support count. However, no co alition below the threshold value is able to decrypt the encrypted support count; this represents a higher privacy level for sensitive itemsets V  P ERFORMANCE A NALYSIS  We assume we have n sites, the total database size is DB  it is enough to have an upper bound of this value count is presented in m  000 000 DB 2 log bits. Let t be the number of bits in the output of the encryption of a support count. The number of candidate itemsets at round k is CA k and the number of polling sites is P initially we have P = n later we will suggest a modification in the basic scheme where P will be a subset of n for performance reasons A  Computation Cost At each site, the computation cost increase due to encryption is O\(t 3 CA k  where t 3 represents the bit-wise cost of modular exponentiation, this is the cost of encrypting the support count of each candidate itemsets in CA k Each polling site performs multiplication of encrypted support counts for its subset of candidate itemsets \(with upper bound of CA k putation cost overhead of O\(t 2 CA k  The computation overhead of the secure comparison algorithm is O 002\034 t where 002\034 is a security parame  B  Storage Cost The storage overhead at one site is O\(t  CA k  which is the size of the encrypted supports For a polling site, additional storage of O\(t  n *|CA k P is needed for the encrypted supports received from all other n-1 sites. Finally, the copolling site has an additional storage cost of O\(t *|CA k P  which is the size of the encrypted sum of the support values C  Communication Cost At step 2 of the protocol, each site sends the support count of the candidate itemsets to the corresponding polling site which is O\(n* t * |CA k  bits of communication. The polling site sends the encrypted support count to the decryption site this requires O\(t * |CA k  bits of communication. At step 3, the broadcast of the global heavy itemsets can be done by broadcasting a CA k bit string where a bit is set to 1 if the itemset is globally supported, 0 otherwise. This requires O\(n CA k  bits of communication. The overhead of the secure comparison algorithm is O 002\034 t bits of communication where 002\034  is a security parameter [8 Since all encrypted support counts are sent in one message to the corresponding polling sites, the number of messages exchanged in our scheme is a function of number of polling sites. Fig. 3 shows the effect of changing the number of polling site P on number of exchanged messages and the average message length, in bits. For our evaluation, we used the mining results from where a sy nt het i c dat a base was generated from the IBM Almaden generator. The database was created with parameters DB|=1M transactions with 1K  distinct items. We will further assume that the database is distributed among n 100 sites, and since the results in  does not report the number of candidates at each round, we will assume that the number of candidates is 4 times the number of frequent itemsets Another possible extension is to set a minimum number of support counts per message, we will call it Lmin Fig. 4 shows the effect of changing Lmin on both number of messages and the average message length Hence, setting number of polling sites as a system parameter has two advantageous results By limiting the number of sites acting as polling sites, the communication overhead can be adjusted based on network conditions and consequently enhancing the performance The protocol becomes scalable for large number of sites Proceedings of the 2006 IEEE Workshop on Information Assurance United States Military Academy, West Point, NY 1-4244-0130-5/06/$20.00 ©2006 IEEE 81 


 TABLE  2 C OMPARISON B ETWEEN KCP AND P3ARM  S CHEMES   KCP P3ARM Computation Cost Encrypting candidate itemests generated by each site O\(t 3 CA k n Decrypting candidate itemests generated by each site O\(t 3 CA k n Secure comparison O\(t 002\034   Encrypting the support count of each candidate itemsets in CA k  O\(t 3 CA k   Multiplying encrypted support counts at polling sites O\(t 2 CA k   Secure comparison O\(t 002\034   Communication Cost Finding union of locally large itemsets O\(n 2 t*|CA k  Summing support count of itemsets O\(m*n*|CA k  Secure comparison algorithm  O\(t 002\034   Sending the support count of the candidate itemsets to the corresponding polling site  O\(n*t*|CA k  The polling site sends the encrypted support count to the copolling site O\(t*|CA k  Secure comparison algorithm  O\(t 002\034  Broadcast of globally supported itemsets O\(n*|CA k   Collusion Scenarios The collusion between sites i+1  and site i-1 compromises the input of site i for all itemsets during the whole protocol execution If site i colludes with site i 000 1  it can learn the size of its intersection with site i+1  Collusion between sites 0 and 1 may reveal the actual itemsets  The collusion between polling and co-polling site reveals the support count of an itemset at other sites Scalability Not scalable to large number of sites because of the communication overhead and the fact that the number of exchanged messages is a function of numbe r of sites Scalable to large number of sites because of the lower communication cost and the fact that the number of exchanged messages is a function of the number of polling sites  VI  C OMPARISON OF P3ARM AND KCP In this section we compare and contrast it with P3ARM. KCP runs in two phases to discover candidate itemsets and to determine which of the candidate itemsets meet the global support threshold. The first phase uses commutative encrypti on. Each party encrypts its own itemsets, then the alread y encrypted itemsets of every other party. These are passed around, with each site decrypting, to obtain the complete set. In the second phase, an initiating party passes its support count, plus a random value to its neighbor. The neighbor adds its support count and passes it on. The final party then engages in a secure comparison with the initiating party to determine if the final result is greater than the threshold plus the random value P3ARM achieves many enhancements on this scheme as follows In P3ARM, for each itemset, two sites are assigned as polling sites. This reduces the communication cost and the computation costs by an O\(n Also in KCP, fake items are added to the communicated itemsets which adds to the communication overhead. On the other hand, in our solution all candidate sets are tested against the threshold, this is to avoid revealing the information of which itemsets are supported at a site. This causes no overhead on local computation cost but adds a communication overhead in our scheme especially for the 1itemsets. This overhead is minimized in later rounds as itemsets are filtered out by the Apriori procedure KCP dedicates two sites, namely sites 0 and 1, to collect the encrypted commonly supported itemsets from the even odd\tes, this organization raises a potential collusion problem between these two sites. By randomly assigning the polling sites to the itemsets, the probability of collusion is minimized in P3ARM Also, it should be noted that in KCP the number of messages exchanged is a function of number of sites, as the candidate itemsets must be encrypted \(and later decrypted all the participating sites. So, as number of participating sites increases, the scheme becomes im practical. On the other hand the number of messages in P3ARM depends on number of designated polling sites which is considered as a system parameter and can be adjusted to enhance the performance P3ARM achieves high level of parallelism since comparisons of support counts are done between different pairs of polling and co-polling sites in parallel, which reduces the total execution time of the protocol KCP suffers from three poten tial collusion problems: The collusion between sites i+1 and site i-1 which compromises the secrecy of site i for all itemsets during the whole protocol execution. Also, if site i colludes with site i 000 1 it can learn the number of commonly supported itemsets with site i+1  Finally, the collusion between sites 0 and 1 may reveal the actual itemsets. A solution is proposed to solve the first collusion problem by splitting the input between the sites. On the other hand, our solution suffers from one collusion problem; the collusion between the polling and co-polling sites. If these two sites collude they can reveal the local supports of an itemset. However, the probability of such collusion is minimized in our protocol due to the fact that the itemsets are arbitrary assigned to polling sites, that is, it is not known, at the beginning of the protocol, which site will be assigned to which itemsets, as the supported itemsets are only discovered during the execution of the protocol. Table 2 summarizes the differences be tween the two approaches VII  R ELATED W ORK  There is an increasing number of algorithms for privacypreserving data mining motivated by the increase in the need for applying mining tasks on sensitive data owned by different mutually untrusting autonomous parties. Mainly, privacypreserving data mining solutions can be classified based on the core approach used into data randomization solutions and cryptography-based solutions The former solutions reveal randomized information about each record in the data set in exchange for not having to rev eal the original records. The Proceedings of the 2006 IEEE Workshop on Information Assurance United States Military Academy, West Point, NY 1-4244-0130-5/06/$20.00 ©2006 IEEE 82 


 latter ones rely on a secure protocol using cryptography primitives and tools employed by the participating parties Privacy is achieved if at th e end of the execution of the protocol no party knows anything except its own input and the mining results Methods presented in and [11 are based on randomization approach. In  distortion technique is used. A mining process for generating frequent itemsets from the distorted database was presented along with a set of optimizations to address the fact that mining the distorted database is more expensive than mining the true database. The presented algorithm achieves privacy of over 80% and an error of less th t h e randomized response technique is used to conduct the data mining computation. The proposed method builds decision tree classifiers from the disguised data Recent approaches for privacy-preserving data classification are proposed in and [13 where cryptographic tools are used to minimize the information  Vai d y a and C l i f t on present  a cry p t ography based protocol that addresses the problem of mining association rules across two da tabases where the columns in the table are at different site s, splitting each row. There is a join key present in both databases and the remaining attributes are present in one database or the other, but not both. The goal is to find association rules involving attributes other than the join key. The method is based on a privacy-preserving scalar product protocol, and an efficient protocol for computing scalar product while preserving privacy of the individual values The advantage of the randomization approach is its performance, but it achieves this at the cost of accuracy. On the other hand, the cryptography-based approach ensures that the results are the same as the results obtained from the original algorithms - without the privacy concerns - but is generally more expensive than randomizati Our work follows the second approach with the goal of improving the performance while achieving privacy VIII  C ONCLUSION  We proposed P3ARM, a new efficient protocol for mining association rules over horizontally partitioned data. The protocol works for three or more parties under the semi-honest model. P3ARM introduces minimal overhead to the mining task due to the privacy requirements. The key idea is to arbitrary assign polling sites to collect the itemsets’ supports in encrypted forms. We proved that our solution outperforms the one presented in by an order of n where n is the number of sites participating in the protocol\n both the communication and the computation overhead. It also minimizes the probability and effect of collusion between the participating sites Directions for future work include combining the data randomization and SMC approaches for privately mining association rules. This should enhance the protocol performance, minimize the information leakage and realize high output precision. Another possible extension to reduce the number of communicated messages is to have the sites probabilistically send the local support value for an itemset Obviously, the mining results in this case will be an approximation. The accuracy of the results may be enhanced by providing information on the da ta distribution at each site to recover the missing values A CKNOWLEDGMENT  We would like to thank Ahmed Belal for his valuable discussions related to this paper R EFERENCES    L. F. Cranor, J. Reagle, and M. S. Ackerman, “Beyond Concern Understanding Net Users’ Attitudes about Online Privacy”, technical report, AT&T Labs-Research, April 1999 http://www.research.att.com/resources trs/TRs/99/99.4/99 4.3/report.htm   M. Kantarcioglu, and C. Clifton Privacy-Preserving Distributed Mining of Association Rules on Ho rizontally Partitioned Data IEEE Transactions on Knowledge and Data Engineering July 2003   O. Goldreich Foundations of Cryptography: Volume II: Basic Applications Cambridge University Press, 2004   R. Agrawal, and R. Srikant, “Fas t Algorithms for Mining Association Rules”, in proceedings of the 20th VLDB Conference September 1994   D. W. Cheung, V. T. Ng, Ada W. Fu, and Yongjian Fu, “Efficient Mining of Association Rules in Distributed Databases IEEE Transactions on Knowledge and Data Engineering December 1996   J. Benaloh, “Dense Probabilistic Encryption”, in proceedings of the Workshop on Selected Areas of Cryptography May 1994   J. Katz, S. Myers, and R. Ostr ovsky, “Cryptographic Counters and Applications to Electronic Voting”, in proceedings of Advances in Cryptology EUROCRYPT’01 2001   I. F. Brandt, “Efficient Cryptogr aphic Protocol Design Based on El Gamal Encryption”, in proceedings of the 8th International Conference on Information Security and Cryptology, ICISC’05 December 2005   S. Agrawal, V. Krishnan, and J. R Haritsa, ”On Addr essing Efficiency Concerns in Privacy-Preserving Mining”, in proceedings of the 9th Conference on Database Systems for Advanced Applications DASFAA’04 March 2004   S. J. Rizvi, and J. R. Haritsa, “M aintaining Data Privacy in Association Rule Mining”, in proceedings of the 28th VLDB Conference 2002   W. Du, and Z. Zhan, “Using Randomized Response Techniques for Privacy-Preserving Data Mining”, in proceedings of SIGKDD’03 ACM Press, August 2003   R. N. Wright and Z. Yang, “Pri vacy-Preserving Bayesian Network Structure Computation on Distributed Heterogeneous Data”, in proceedings of SIGKDD’04 ACM Press, August 2004   Z. Yang, S. Zhong, and R N. Wright, “Privacy-Preserving Classification of Customer Data Without Loss of Accuracy”, in proceedings of the 5th SIAM International Conference on Data Mining, SDM’05 April 2005   J. Vaidya, and C. Clifton, ”Privacy Preserving Association Rule Mining in Vertically Partitioned Data”, in proceedings of SIGKDD’02 ACM Press, July 2002   W. Du, Y. S. Han and S. Chen Privacy-Preserving Multivariate Statistical Analysis: Linear Regression and Classification”, in proceedings of the 4th SIAM International Conference on Data Mining  SDM’04 April 2004 Proceedings of the 2006 IEEE Workshop on Information Assurance United States Military Academy, West Point, NY 1-4244-0130-5/06/$20.00 ©2006 IEEE 83 


4. The slope of the tangent line at a cut point gives the likelihood ratio \(LR 5. The area under the curve is a measure of test accuracy Figure 4 demonstrates the area under the curve of the decision tree and artificial neural network. Decision Tree curve follows the left-hand border and then the top border of the ROC space more accurately then the Artificial neural network so decision tree has a better area under the curve and hence a more accurate test 6. Conclusion Two data mining modeling techniques resulted in models with varying degrees of sensitivity and specificity. Based upon examination of these models the Decision Tree approach produced a superior predictive model. This model employed medication history, and safety and environmental hazards to discriminate Fallers from Non-Fallers with a high degree of sensitivity, and specificity. The model predicted safety score and medication score as the two most important factors in predicting falls. Clinicians may evaluate these characteristics of their VNA patients when attempting to identify patients who have a high likelihood of falling in the future U M U V M U 1  SpeclO:iW OOI I!I!Tree I!I!Neurai Figure 4: ROC Chart for Decision Tree and Artificial Neural Network 7. Acknowledgements This project was supported by Visiting Nurse Association, Louisville, KY References 1] Desouza, K.C. \(2001 healthcare management" In Proceedings of the First International Conference on Management of Healthcare and Medical Technology Enschede Netherlands: Institute for Healthcare Technology Management 2] Desouza, K.C. \(2002 with artificial intelligence". Westport, CT: Quorum Books 3] Ramaprasad, A. \(1996 mining". Journal of Database Marketing 4 \(1 75 4] M. Kantardzic, \(2002 Models, Methods, and Algorithms", IEEE Press &amp John Wiley 5] Baxt, W.G. and Skora, J. \(1996 Validation of Artificial Neural Network Trained to Identify Acute Myocardial Infarction." The Lancet 347 12-15 6] Berkowitz, M., O'Leary, P., Kruse, D., &amp; Harvey C. \(1998 medical and social costs". New York: Demos Medical Publishing, Inc 7] Cabena, P., Hadjinian, P., Stadler, R., Verhees, J., &amp Zanasi, A. \(1998 concept to implementation". Upper Saddle River NJ: Prentice Hall, Inc 8] Kraft, M.R., Desouza, K.C., and Androwich, I 2002 injury patients using neural networks and nursing diagnoses data". In Proceedings of the Second International Conference on Management of Healthcare and Medical Technology, Chicago Illinois. July. \(In Press 9] Kraft, M., Desouza, K.C., and Androwich, I 2002 Issues and Process." In M. Khosrow-Pour \(Editor Issues &amp; Trends of Information Technology 


Issues &amp; Trends of Information Technology Management in Contemporary Organizations, Vol. I Hershey, PA: Idea Group Publishing, pp. 168-171 10] Walczak, S. and Scharf, J. E. \(2000 Surgical Patient Costs Through Use of an Artificial Neural Network to Predict Transfusion Requirements." Decision Support Systems 30 \(2 125-138 11] Gillespie, L.D., Gillespie, W.J., Robertson, M.C Lamb, S.E., Cumming, R.G., &amp; Rowe, B.H. \(2004 Interventions for preventing falls in elderly patients". Cochrane Database of Systematic Reviews. \(2 12] Prather, J.C., Lobach, D.F., Goodwin, L.K., Hales J.W., Hage, M.L., &amp; Hammond, W.E. \(1997 Medical data mining: knowledge discovery in a clinical data warehouse". Proceedings/AMIA Annual Fall Symposium 101-5 13] Hobbs, G.R. \(2001 informatics". American Journal of Health Behavior 25 \(3 14] Keith, H., Robyn, S., Kate, M., Jane, S., Jenny, G Peteris, D., &amp; Freda, V. \(2000 Research Institute 15] Getting started with Enterprise Miner Software From SAS Online Tutorial. Retrieved October 8, 2004 from http://www.stat.purdue.eduHosong/stat598m1EMT doc 16] Christos, S. and Dimitrios, S., Neural Networks Retrieved on October 8, 2004 from http://www.csic.comell.edU/20 lIneural_ network 17] Swartzbeck, E. \(1983 elderly". Nursing Management, 14 \(12 18] Witte, N. \(1979 Journal of Nursing, 79 \(10 19] Preter, A.F. \(2004 analysis in machine learning". Retrieved on October 8,2004 from pre></body></html 


efficiency then AOFI. However utilization of fuzzy concept hierarchies provides more flexibility in reflecting expert knowledge and so allows better modeling of real-life dependencies among attribute values, which will lead to more satisfactory overall results for the induction process. The drawback of the computational cost may additionally decline when we notice that, in contrast to many other data mining algorithms, hierarchical induction algorithms need to run only once through the original \(i.e. massive dataset. We are continuing an investigation of computational costs of our approach for large datasets ACKNOWLEDGMENT Rafal Angryk would like to thank the Montana NASA EPSCoR Grant Consortium for sponsoring this research REFERENCES 1] J. Han , M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann, New York, NY 2000 2] J. Han, Y. Cai, and N. Cercone  Knowledge discovery in databases: An attribute-oriented approach  Proc. 18th Int. Conf. Ver y Large Data Bases, Vancouver, Canada, 1992, pp. 547-559 3] J. Han  Towards Efficient Induction Mechanisms in Database Systems  Theoretical Computing Science, 133, 1994, pp. 361-385 4] J. Han, Y. Fu  Discovery of Multiple-Level Association Rules from Large Databases  IEEE Trans.  on KD E, 11\(5 5] C.L. Carter, H.J. Hamilton  Efficient AttributeOriented Generalization for Knowledge Discovery from Large Databases  IEEE Trans. on KDE 10\(2 6] R.J. Hilderman, H.J. Hamilton, and N. Cercone  Data mining in large databases using domain generalization graphs  Journal of Intelligent Information Systems, 13\(3 7] C.-C. Hsu  Extending attribute-oriented induction algorithm for major values and numeric values   Expert Systems with Applications , 27, 2004, pp 187-202 8] D.H. Lee, M.H. Kim  Database summarization using fuzzy ISA hierarchies  IEEE Trans . on SMC - part B, 27\(1 9] K.-M. Lee  Mining generalized fuzzy quantitative association rules with fuzzy generalization hierarchies  20th NAFIPS Int'l Conf., Vancouver Canada, 2001, pp. 2977-2982 10] J. C. Cubero, J.M. Medina, O. Pons &amp; M.A. Vila  Data Summarization in Relational Databases through  Fuzzy Dependencies  Information Sciences, 121\(3-4 11] G. Raschia, N. Mouaddib  SAINTETIQ:a fuzzy set-based approach to database summarization   Fuzzy Sets and Systems, 129\(2 162 12] R. Angryk, F. Petry  Consistent fuzzy concept hierarchies for attribute generalization  Proceeding of the IASTED Int. Conf. on Information and Knowledge Sharing, Scottsdale AZ, USA, November 2003, pp. 158-163 13] Toxics Release Inventory \(TRI available EPA database hosted at http://www.epa.gov/tri/tridata/tri01/index.htm The 2005 IEEE International Conference on Fuzzy Systems790 pre></body></html 


the initial global candidate set would be similar to the set of global MFIs. As a result, during the global mining phase the communication and synchronization overhead is low  0 2 4 6 8 1 0 Number of Nodes Figure 5. Speedup of DMM 4.4.2 Sizeup For the sizeup test, we fixed the system to the 8-node con figuration, and distributed each database listed in Table 2 to the 8 nodes. Then, we increased the local database sire at each node from 45 MB to 215 MB by duplicating the initial database partition allocated to the node. Thus, the data distribution characteristics remained the same as the local database size was increased. This is different from the speedup test, where the database repartitioning was per formed when the number of nodes was increased. The per formance of DMM is affected by the database repartitioning to some extent, although it is usually very small. During the sizeup test, the local mining result of DMM is not changed at all at each node The results shown in Figure 6 indicate that DMM has a very good sizeup property. Since increasing the size of local database did not affect the local mining result of DMM at each node, the total execution time increased just due to more disk U 0  and computation cost which scaled almost linearly with sizeup 5 Conclusions In this paper, we proposed a new parallel maximal fre quent itemset \(MFI Max-Miner \(DMM tems. DMM is a parallel version of Max-Miner, and it re quires low synchronization and communication overhead compared to other parallel algorithms. In DMM, Max Miner is applied on each database partition during the lo 0 45 90 135 180 225 270 Amwnt of Data per Node \(ME Figure 6. Sizeup of DMM cal mining phase. Only one synchronization is needed at thc end of this phase to construct thc initial global candi date set. In the global mining phase, a top-down search is performed on the candidate set, and a prefix tree is used to count the candidates with different length efficiently. Usu ally, just a few passes are needed to find all global maximal frequent itemsets. Thus, DMM largely reduces the number of synchronizations required between processing nodes Compared with Count Distribution, DMM shows a great improvement when some frequent itemscts are large \(i.e long patterns employed by DMM for efficient communication between nodes; and global support estimation, subset-infrequency based pruning, and superset-frequency based pruning are used to reduce the size of global candidate set. DMM has very good speedup and sizeup properties References I ]  R. Agrawal and R. Srikant  FdSt Algorithms for Mining As sociation Rules  Pmc. o f f h e  ZOrh VLDB Conf, 1994, pp 487499 2] R. Agrawal and I. C. Shafer  Parallel Mining of Association Rules  IEEE Trans. on Knowledge and Dura Engineering Vol. 8, No. 6, 1996, pp. 962-969 3] R. I. Bayardo  Efficient Mining Long Patlems from Databases  Proc. ofrhe ACM SIGMOD Inf  l Conf on Man ogemenr ofDara, 1998, pp. 85-91 4] S.  M. Chung and J. Yang  A Parallel Distributive Join Al gorithm for Cube-Connected Multiprocessors  IEEE Trans on Parallel and Disrribured Systems, Vol. 7, No. 2, 1996, pp 127-137 51 M. Snir, S. Otto. S. Huss-Lederman, D. Walker, and J. Don gana, MPI: The Complete Reference, The MIT Press, 1996 


gana, MPI: The Complete Reference, The MIT Press, 1996 6] R. Rymon  Search through Systematic Set Enumeralion   Pmc. of3rd Inr  l Con$ on Principles of Knowledge Repre sentation and Reasoning, 1992, pp. 539-550 507 pre></body></html 


sketch-index in answering aggregate queries. Then Section 5.2 studies the effect of approximating spatiotemporal data, while Section 5.3 presents preliminary results for mining association rules 5.1 Performance of sketch-indexes Due to the lack of real spatio-temporal datasets we generate synthetic data in a way similar to [SJLL00 TPS03] aiming at simulation of air traffic. We first adopt a real spatial dataset [Tiger] that contains 10k 2D points representing locations in the Long Beach county \(the data space is normalized to unit length on each dimension These points serve as the  airbases  At the initial timestamp 0, we generate 100k air planes, such that each plane \(i uniformly generated in [200,300], \(ii, iii destination that are two random different airbases, and iv  the velocity direction is determined by the orientation of the line segment connecting its source and destination airbases move continually according to their velocities. Once a plane reaches its destination, it flies towards another randomly selected also uniform in [0.02, 0.04 reports to its nearest airbase, or specifically, the database consists of tuples in the form &lt;time t, airbase b, plane p passenger # a&gt;, specifying that plane p with a passengers is closest to base b at time t A spatio-temporal count/sum query has two parameters the length qrlen of its query \(square number qtlen of timestamps covered by its interval. The actual extent of the window \(interval uniformly in the data space \(history, i.e., timestamps 0,100 air planes that report to airbases in qr during qt, while a sum query returns the sum of these planes  passengers. A workload consists of 100 queries with the same parameters qrlen and qtlen The disk page size is set to 1k in all cases \(the relatively small page size simulates situations where the database is much more voluminous specialized method for distinct spatio-temporal aggregation, we compare the sketch-index to the following relational approach that can be implemented in a DBMS. Specifically, we index the 4-tuple table lt;t,b,p,a&gt; using a B-tree on the time t column. Given a count query \(with window qr and interval qt SELECT distinct p FROM &lt;t,b,p,a&gt WHERE t?qt &amp; b contained in qr The performance of each method is measured as the average number of page accesses \(per query processing a workload. For the sketch-index, we also report the average \(relative Specifically, let acti and esti be the actual and estimated results of the i-th query in the workload; then the error equals \(1/100 set the number of bits in each sketch to 24, and vary the number of sketches The first experiment evaluates the space consumption Figure 5.1 shows the sketch index size as a function of the number of sketches used \(count- and sum-indexes have the same results more sketches are included, but is usually considerably smaller than the database size \(e.g., for 16 signatures, the size is only 40% the database size 0 20 40 60 80 


80 100 120 140 160 8 16 32 number of sketches size \(mega bytes database size Figure 5.1: Size comparison Next we demonstrate the superiority of the proposed sketch-pruning query algorithm, with respect to the na  ve one that applies only spatio-temporal predicates. Figure 5.2a illustrates the costs of both algorithms for countworkloads with qtlen=10 and various qrlen \(the index used in this case has 16 sketches also illustrate the performance of the relational method which, however, is clearly incomparable \(for qrlen?0.1, it is worse by an order of magnitude we omit this technique Sketch-pruning always outperforms na  ve \(e.g., eventually two times faster for qrlen=0.25 increases with qrlen, since queries returning larger results tend to set bits in the result sketch more quickly, thus enhancing the power of Heuristics 3.1 and 3.2. In Figure 5.2b, we compare the two methods by fixing qrlen to 0.15 and varying qtlen. Similar to the findings of [PTKZ02]4 both algorithms demonstrate  step-wise  growths in their costs, while sketch-pruning is again significantly faster The experiments with sum-workloads lead to the same observations, and therefore we evaluate sketch-indexes using sketch-pruning in the rest of the experiments 4 As explained in [PTKZ02], query processing accesses at most two paths from the root to the leaf level of each B-tree regardless the length of the query interval Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE sketch-pruning naive relational 0 100 200 300 400 500 600 700 800 900 0.05 0.1 0.15 0.2 0.25 number of disk accesses query rectangle length 300 0 100 200 400 500 600 1 5 10 15 20 number of disk accesses query interval length a qtlen=10 b qrlen=0.15 Figure 5.2: Superiority of sketch-pruning \(count As discussed in Section 2, a large number of sketches reduces the variance in the resulting estimate. To verify this, Figure 5.3a plots the count-workload error of indexes 


using 8-, 16-, and 32- sketches, as a function of qrlen qtlen=10 error \(below 10 it increases slowly with qrlen used, however, the error rate is much higher \(up to 30 and has serious fluctuation, indicating the prediction is not robust. The performance of 16-sketch is in between these two extremes, or specifically, its accuracy is reasonably high \(average error around 15 much less fluctuation than 8-sketch 32-sketch 16-sketch 8-sketch relative error 0 5 10 15 20 25 30 35 0.05 0.1 0.15 0.2 0.25 query rectangle length relative error 0 5 10 15 20 25 30 35 1 5 10 15 20 query interval length a qtlen=10, count b qrlen=0.15, count relative error query rectangle length 0 5 10 15 20 25 0.05 0.1 0.15 0.2 0.25 relative error query interval length 0 5 10 15 20 25 30 1 5 10 15 20 c qtlen=10, sum d qrlen=0.15, sum Figure 5.3: Accuracy of the approximate results The same phenomena are confirmed in Figures 5.3b where we fix qrlen to 0.15 and vary qtlen 5.3d \(results for sum-workloads number of sketches improves the estimation accuracy, it also leads to higher space requirements \(as shown in Figure 5.1 Figures 5.4a and 5.4b show the number of disk accesses for the settings of Figures 5.3a and 5.3b. All indexes have almost the same behavior, while the 32-sketch is clearly more expensive than the other two indexes. The interesting observation is that 8- and 16-sketches have 


interesting observation is that 8- and 16-sketches have almost the same overhead due to the similar heights of their B-trees. Since the diagrams for sum-workloads illustrate \(almost avoid redundancy 32-sketch 16-sketch 8-sketch number of disk accesses query rectangle length 0 50 100 150 200 250 300 350 400 0.05 0.1 0.15 0.2 0.25 number of disk accesses query interval length 0 50 100 150 200 250 300 350 1 5 10 15 20 a qtlen=10 b qrlen=0.15 Figure 5.4: Costs of indexes with various signatures Summary: The sketch index constitutes an effective method for approximate spatio-temporal \(distinct aggregate processing. Particularly, the best tradeoff between space, query time, and estimation accuracy obtained by 16 sketches, which leads to size around 40 the database, fast response time \(an order of magnitude faster than the relational method average relative error 5.2 Approximating spatio-temporal data We proceed to study the efficiency of using sketches to approximate spatio-temporal data \(proposed in Section 4.1 as in the last section, except that at each timestamp all airplanes report their locations to a central server \(instead of their respective nearest bases maintains a table in the form &lt;time t, plane p, x, y&gt;, where x,y with parameters qrlen and qtlen distinct planes satisfying the spatial and temporal conditions. For comparison, we index the table using a 3D R*-tree on the columns time, x, and y. Given a query, this tree facilitates the retrieval of all qualifying tuples, after which a post-processing step is performed to obtain the Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE number of distinct planes \(in the sequel, we refer to this method as 3DR method introduces a regular res  res grid of the data space, where the resolution res is a parameter. We adopt 16 sketches because, as mentioned earlier, this number gives the best overall performance Figure 5.5 compares the sizes of the resulting sketch indexes \(obtained with resolutions res=25, 50, 100 the database size. In all cases, we achieve high compression rate \(e.g., the rate is 25% for res=25 evaluate the query efficiency, we first set the resolution to the median value 50, and use the sketch index to answer workloads with various qrlen \(qtlen=10 


workloads with various qrlen \(qtlen=10 size \(mega bytes database size 0 20 40 60 80 100 120 140 160 25 50 100 resolution Figure 5.5: Size reduction Figure 5.6a shows the query costs \(together with the error in each case method. The sketch index is faster than 3DR by an order of magnitude \(note that the vertical axis is in logarithmic scale around 15% error observations using workloads with different qtlen Finally, we examine the effect of resolution res using a workload with qrlen=0.15 and qtlen=10. As shown in Figure 5.6c, larger res incurs higher query overhead, but improves the estimation accuracy Summary: The proposed sketch method can be used to efficiently approximate spatio-temporal data for aggregate processing. It consumes significantly smaller space, and answers a query almost in real-time with low error 3D Rsketch number of disk accesses query rectangle length 1 10 100 1k 10k 0.05 0.1 0.15 0.2 0.25 16 14% 15 15% 13 relative error number of disk accesses query interval length 1 10 100 1k 10k 1 5 10 15 20 16 15% 15% 12% 11 relative error a qtlen=10, res=25 b qrlen=0.15, res=25 0 500 1000 1500 2000 2500 25 50 100 number of disk accesses resolution 20% 15% 14 relative error c qrlen=0.15, qtlen=10 


c qrlen=0.15, qtlen=10 Figure 5.6: Query efficiency \(costs and error 5.3 Mining association rules To evaluate the proposed algorithm for mining spatiotemporal association rules, we first artificially formulate 1000 association rules in the form \(r1,T,90 with 90% confidence i randomly picked from 10k ones, \(ii in at most one rule, and \(iii Then, at each of the following 100 timestamps, we assign 100k objects to the 10k regions following these rules. We execute our algorithms \(using 16 sketches these rules, and measure \(i  correct  rules divided by the total number of discovered rules, and \(ii successfully mined Figures 5.7a and 5.7b illustrate the precision and recall as a function of T respectively. Our algorithm has good precision \(close to 90 majority of the rules discovered are correct. The recall however, is relatively low for short T, but gradually increases \(90% for T=25 evaluated in the previous sections, the estimation error decreases as the query result becomes larger \(i.e., the case for higher T 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 precision HT 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 recall HT a b Figure 5.7: Efficiency of the mining algorithm Summary: The preliminary results justify the usefulness of our mining algorithm, whose efficiency improves as T increases Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE 6. Conclusions While efficient aggregation is the objective of most spatio-temporal applications in practice, the existing solutions either incur prohibitive space consumption and query time, or are not able to return useful aggregate results due to the distinct counting problem. In this paper we propose the sketch index that integrates traditional approximate counting techniques with spatio-temporal indexes. Sketch indexes use a highly optimized query algorithm resulting in both smaller database size and faster query time. Our experiments show that while a sketch index consumes only a fraction of the space required for a conventional database, it can process 


required for a conventional database, it can process queries an order of magnitude faster with average relative error less than 15 While we chose to use FM sketches, our methodology can leverage any sketches allowing union operations Comparing the efficiency of different sketches constitutes a direction for future work, as well as further investigation of more sophisticated algorithms for mining association rules. For example, heuristics similar to those used for searching sketch indexes may be applied to improve the brute-force implementation ACKNOWLEDGEMENTS Yufei Tao and Dimitris Papadias were supported by grant HKUST 6197/02E from Hong Kong RGC. George Kollios, Jeffrey Considine and were Feifei Li supported by NSF CAREER IIS-0133825 and NSF IIS-0308213 grants References BKSS90] Beckmann, N., Kriegel, H., Schneider, R Seeger, B. The R*-tree: An Efficient and Robust Access Method for Points and Rectangles. SIGMOD, 1990 CDD+01] Chaudhuri, S., Das, G., Datar, M., Motwani R., Narasayya, V. Overcoming Limitations of Sampling for Aggregation Queries. ICDE 2001 CLKB04] Jeffrey Considine, Feifei Li, George Kollios John Byers. Approximate aggregation techniques for sensor databases. ICDE, 2004 CR94] Chen, C., Roussopoulos, N. Adaptive Selectivity Estimation Using Query Feedback. SIGMOD, 1994 FM85] Flajolet, P., Martin, G. Probabilistic Counting Algorithms for Data Base Applications JCSS, 32\(2 G84] Guttman, A. R-Trees: A Dynamic Index Structure for Spatial Searching. SIGMOD 1984 GAA03] Govindarajan, S., Agarwal, P., Arge, L. CRBTree: An Efficient Indexing Scheme for Range Aggregate Queries. ICDT, 2003 GGR03] Ganguly, S., Garofalakis, M., Rastogi, R Processing Set Expressions Over Continuous Update Streams. SIGMOD, 2003 HHW97] Hellerstein, J., Haas, P., Wang, H. Online Aggregation. SIGMOD, 1997 JL99] Jurgens, M., Lenz, H. PISA: Performance Models for Index Structures with and without Aggregated Data. SSDBM, 1999 LM01] Lazaridis, I., Mehrotra, S. Progressive Approximate Aggregate Queries with a Multi-Resolution Tree Structure. SIGMOD 2001 PGF02] Palmer, C., Gibbons, P., Faloutsos, C. ANF A Fast and Scalable Tool for Data Mining in Massive Graphs. SIGKDD, 2002 PKZT01] Papadias,  D., Kalnis, P.,  Zhang, J., Tao, Y Efficient OLAP Operations in Spatial Data Warehouses. SSTD, 2001 PTKZ02] Papadias, D., Tao, Y., Kalnis, P., Zhang, J Indexing Spatio-Temporal Data Warehouses ICDE, 2002 SJLL00] Saltenis, S., Jensen, C., Leutenegger, S Lopez, M.A. Indexing the Positions of Continuously Moving Objects. SIGMOD 2000 SRF87] Sellis, T., Roussopoulos, N., Faloutsos, C The R+-tree: A Dynamic Index for MultiDimensional Objects. VLDB, 1987 TGIK02] Thaper, N., Guha, S., Indyk, P., Koudas, N Dynamic Multidimensional Histograms 


SIGMOD, 2002 Tiger] www.census.gov/geo/www/tiger TPS03] Tao, Y., Papadias, D., Sun, J. The TPR*Tree: An Optimized Spatio-Temporal Access Method for Predictive Queries. VLDB, 2003 TPZ02] Tao, Y., Papadias, D., Zhang, J. Aggregate Processing of Planar Points. EDBT, 2002 TSP03] Tao, Y., Sun, J., Papadias, D. Analysis of Predictive Spatio-Temporal Queries. TODS 28\(4 ZMT+01] Zhang, D., Markowetz, A., Tsotras, V Gunopulos, D., Seeger, B. Efficient Computation of Temporal Aggregates with Range Predicates. PODS, 2001 ZTG02] Zhang, D., Tsotras, V., Gunopulos, D Efficient Aggregation over Objects with Extent PODS, 2002 Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE pre></body></html 


