Quantitative Association Rules Based on Half-Spaces An Optimization Approach Ulrich R 250 uckert Lothar Richter and Stefan Kramer Technische Universit 250 at M 250 unchen Institut f 250 ur Informatik/I12 Boltzmannstr 3 D-85748 Garching b M 250 unchen Germany  rueckert richter kramer  in.tum.de Abstract We tackle the problem of 336nding association rules for 
quantitative data Whereas most of the revious approaches operate on hyperrectangles we propose a representation based on half-spaces Consequently the left-hand side and right-hand side of an association rule does not contain a conjunction of items or intervals but a weighted sum of variables tested against a threshold Since the downward closure property does not hold for such rules we propose an optimization setting for 336nding locally optimal rules A simple gradient descent algorithm optimizes a parameterized score function where iterations optimizing the 336rst sep 
arating hyperplane alternate with iterations optimizing the second Experiments with two real-world data sets show that the approach 336nds non-random patterns and scales up well We therefore ropose quantitative association rules based on half-spaces as an interesting new class of patterns with a high potential for applications 1 Introduction Soon after the introduction of association rules for itemsets researchers began to realize that association rules would also be useful for quantitative data Most of the generalizations and extensions of association rules to quan 
titative data either require a discretization of the numerical attributes 5 or a characterization of the numerical attributes in the right-hand side by their means and standard deviations 1 The discretization process r leads to a loss of information in the data set In the following we present a l approach that works directly on the continuous data without the need for any discretization or the calculation of statistical moments It s quantitative association rules of the form 223if the weighted sum of some variables is greater than a threshold then a different weighted sum of variables 
is with high probability greater than a second threshold\224 r instance consider a table with wind strength temperature and the wind chill index Approaches so r applied to this data would approximate the relationship among the variables by a bundle of quantitative association rules In contrast the approach proposed here would 036nd a weighted sum of wind strength and temperature on the left-hand side and the wind-chill index n the right-hand side Thus it allows for the discovery of non-axis-parallel regularities and can account for cumulative effects 
of several variables Since the downward closure property frequently used in conventional association rule mining does not hold for this type of rule we cast the problem of 036nding such rules as an optimization problem The aim is to 036nd rules that are locally optimal with respect to a parameterized score function Consequently the user can adjust the parameters of the presented algorithm to obtain association rules that match her individual interests For instance it is possible to specify target values for certain parameters such that the algorithm 
attempts to 036nd rules near the target penalizing rules that are too r ff while simultaneously optimizing the rules\222 con\036dence The whole framework is very 037exible in several directions and can easily be adapted to incorporate user constraints In summary the paper has two main contributions 036rstly the representation of quantitative association rules based on half spaces and secondly the optimization setting for 036nding such rules 2 Quantitative Association Rules Based on Half-Spaces 
As outlined  the aim of this paper is to extend the association rule framework to quantitative data In general an association rule is an implication of the form 223if the leftProceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


 2 0 2 4 3 2 1 0 1 2 3 4 5 000_ 000 a 2 0 2 4 3 2 1 0 1 2 3 4 5 000_ 000 b Figure 1 Two non-perpendicular hyperplanes 001 and 002 a and two perpendicular hyperplanes 001 and 002 b hand side condition is true for an instance then with high probability a right-hand side condition is also true\224 In the traditional setting the conditions on the right-hand side and left-hand side are based on hyperrectangles of discrete attributes To extend association rules to continuous data we therefore need to decide which kind of 223conditions\224 the quantitative association rules should be based on Of course there are lots of different ways to impose conditions on numerical data At the core we would expect from a useful condition that it separates the instance space in two subspaces the space of instances that meets the condition and the one that does not The border between those two subspaces can then be conveniently expressed by some separation function  For numerical data it makes sense to select a smooth separation function to minimize the error that is caused by random noise or measurement errors in the data In this paper we will focus on hyperplanes  a particularly simple t powerful class of separation functions From a geometrical perspective a hyperplane 001 is n y a vector 257 001 and an intercept 001 0  n instance x is then assigned to one half-space if the dot product 257 001 267 x  001 0 is positive and to the other half-space if it is negative In 036gure 1 b the one-dimensional hyperplane 001 i.e a line separates the two-dimensional space into two half-spaces one left of 001  the other right of 001  In the case of association rules the use of hyperplanes as conditions boils down to testing a weighted sum of variables against a threshold i.e an instance x in an n dimensional space meets the condition 001 001 000 n 1 if 001 1 x 1  001 2 x 2  267\267\267  001 n x n 002\212 001 0 With this one could build an association rule such as x 1 002 31 003 0  9 x 5 1  2 x 6 002 250 In a particular medical application this association rule might be interpreted as 223if the body mass index is greater than or equal to 31 then the weighted sum of the systolic and diastolic blood pressure is greater than or equal to 250\224 Of course it is quite easy to generate a large number of trivial association rules with high con\036dence r example the association rule 1  5 x 1 002 5 003 2 x 1 002 4 has con\036dence 100 t does not give any ew insight More generally situations like the one in 036gure 1 a are problematic we have two hyperplanes 001 and 002 in a two-dimensional space that de\036ne an association rule 001 1 x 1  001 2 x 2 002\212 001 0 003 002 1 x 1  002 2 x 2 002\212 002 0  The problem is that 001 and 002 are highly correlated If an instance is left of the 001 hyperplane it is very likely to be left of the 002 hyperplane as well simply because the space that is right of 002  but left of 001 is much smaller than the space left of 001 and left of 002 1  For our purposes it is therefore essential that 001 and 002 are uncorrelated i.e they have to be perpendicular as in 036gure 1 b 3 Quantitative Association Rule Mining The main problem with 036nding good quantitative association rules is that the space of rules is uncountably in\036nite and therefore not suited to an enumeration strategy n particular the downward closure property does not hold for such rules and thus we have to abandon the idea of generating the complete set of solutions r we can adopt an optimization approach where the user can specify clearly the sort of rules she is looking for and the algorithm returns locally optimal solutions While this may seem unusual for association rule mining it is common practice in other areas for instance clustering e.g K-means clustering and Bayesian learning e.g the EM algorithm In the following we describe one particular algorithm for mining quantitative association rules in this setting First we de\036ne a score function to assess the 223interestingness\224 of an association rule Then we sketch a simple optimization algorithm searching for association rules with a low score Before we go into further detail r we need to introduce the basic setting and some notational conventions r mining quantitative association rules we are n a data set X containing m instances  Each instance is n s a vector of n real values i.e x 001 000 n  so that X 004 000 n  We are now looking for association rules that are de\036ned by two hyperplanes 001   001 0 001 1 001 n  T and 002   002 0 002 1 002 n  T  The 001 hyperplane speci\036es the condition on the left-hand side of the association rule the 002 hyperplane speci\036es the right-hand side Both hyperplanes are n in Hessian normal form the 001 0 value of a hyperplane 001 is the intercept  i.e the hyperplane\222s distance to the origin The direction vector 257 001   001 1 001 n  T speci\036es the slope of the hyperplane The hyperplanes 001 and 002 are perpendicular if 257 001 T 257 002 0  Due to space constraints we are not able to give the exact scoring function and the algorithm here and have to refer to 1 Of course in a strict mathematical sense it does not make sense to compare the 223sizes\224 of subspaces because all subspaces are in\036nite anyy A more formal justi\036cation would demand that the resulting probability distributions are independent for uniform data Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


 2 0 2 4 3 2 1 0 1 2 3 4 5 000_ 000  Figure 2 Con\336dence is optimal if the distribution is uneven left of 001  while contrast is optimal if it is even right of 001  the long version of the paper for details First of all we are mainly interested in association rules with high con\336dence  i.e the fraction of instances in X  that ful\036ll both conditions 001 and 002 divided by the fraction of instances that ful\036ll only the 001 condition should be as high as possible Figure 2 illustrates this idea If an instance x is located left of 001 and below 002  it contributes to a high con\036dence score If it is located left of 001  but above 002 it decreases the con\036dence measure The following function is minimal for high con\036dence l  001 002 X  212 001 x 001 X 003  004  001 x  267 2 003  004  002 x  212 1 where 003  x  1 1 e 025 x is the well-known sigmoid function and 004  005 x  is the signed distance of instance x to the hyperplane 005  Since we use the sigmoid function instead of the sharp step function l is differentiable and puts lower weights to instances in the vicinity of the hyperplane therewith compensating for noise in the data A second criterion for the interestingness of an association rule is its coverage  The coverage is simply the fraction of instances in the data set that satisfy the left-hand side condition Unfortunately the coverage of interesting rules is not clear a priori If the coverage of an association rule is very large the rule is true for almost the whole data set Such rules often express trivial dependencies in the data On the other hand if the coverage of a rule is very small the pattern describes a very local phenomenon that might just be a random 037uctuation instead of a structural property of the underlying data Thus the coverage values for interesting association rules are somewhere in between depending on the data at hand and the knowledge about the data In practice the desired coverage or equivalently the support is often determined empirically Similarly to the con\036dence score we design a function c  001 g t X  that is minimal if the coverage of 001 is near to the user-speci\036ed target value t  The g parameter determines how overage should be weighted relative to con\036dence The con\036dence and coverage scores determine what the optimization algorithm is looking for on the left side of 001 in 036gure 2 Just like in traditional association rule mining there is no constraint regulating the distribution of instances on the right side of 001  For quantitative association rule mining this can be a problem one can simply move the 002 hyperplane upwards until it is located above all instances While this s maximal con\036dence the resulting association rule is not very interesting because the right-hand side condition is true for all instances anyway One y to overcome this problem is to regulate the distribution of instances that are right of 001 with regard to 002  We call this criterion contrast  The rationale is that the 223contrast\224 between the instance above and below 002 should be as low s possible on the right side of 001 in 036gure 2 Again we formalize this criterion using a function r  001 002 X   that is minimal for low contrast settings r humans who have to interpret the resulting association rules there is one more pragmatic criterion the components of the 001 and 002 vectors are usually not zero This means that the resulting association rule contains n addends on both sides of the implication Usually the user prefers 036nding sparse association rules i.e rules where most coef\036cients are zero and only the relevant coef\036cients are given Those rules are shorter and thus easier to interpret and validate To account for these pragmatic considerations one can add a term a  001 h  to penalize non-sparse association rules The user can adjust the importance of sparsity relative to the other scores using the parameter h  The 036nal interestingness scoring function simply calculates the sum of those scores L  001 002 g t h X  l  001 002 X  c  001 g t X   r  001 002 X  a  001 h  a  002 h  A high score indicates that the association rule is uninteresting with regard to the selected parameter settings a low score means we found an interesting rule As the scoring function is continuous there usually is a whole subspace of 223good\224 rules and it is easy to modify a rule with a low score to some small extent and obtain a rule with an even lower score We are therefore aiming at 036nding association rules with optimally low score that is the local optima of the scoring function subject to the constraint that 257 001 T 257 002 0  This constrained optimization problem can be tackled using established methods from optimization theory We take an approach that alternatingly keeps 001 036xed while optimizing 002 and vice versa Empirical results in section 4 indicate that only a few iterations are suf\036cient to 036nd such an optimum As any other optimization procedure this algorithm can get stuck in local optima with comparably high scores r the sake f simplicity we use random restarts to obtain association rules with low score Of course one can utilize simulated annealing or any other global optimization straty s well A high sparseness parameter leads to rules that Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


 000\357 150 000\357 100 000\357 50 0 50 0 5 10 15 20 25 30 Score Figure 3 Distribution of scores on the original black and the permuted grey data sets have a ew large and many small but non-zero coef\036cients A post-processing step is required to set the small coef\036cients to zero while retaining the perpendicularity of 001 and 002  4 Experimental Results To assess the applicability and feasibility of the described algorithm we implemented a version in MATLAB r our 036rst experiment we chose the gene expression data set of Hughes et al  The data set contains the e x pression levels of 6316 genes in the yeast genome measured for 300 diverse mutations and chemical treatments of yeast cells We selected the 50 genes with the largest standard deviation for our experiments The parameters were set as follows g 1  0  t 0  5  and the sparseness parameter h s set to 0.1 0.3 and 0.5 respectively As expected contrast and coverage were centered around the target value of 0.5 in the experiments In the long version of the paper a biological interpretation of 036ve f the best rules is given To assess the robustness and statistical signi\036cance of the rules we performed a randomization test where the values of the columns are permutated randomly to generate a new data set with the same distribution t o structural relations between the columns We then ran the algorithm ten times on the permuted data set and noted the best score found This process s repeated one hundred times to get an estimate of the distribution of scores that can be expected on random t similar data Figure 3 ives the resulting histograms for the original and the permuted data The scores for the permuted data are peaked around 30 while the original data features a large number of association rules in the range between 50 and 150 Thus we can be highly con\036dent that the induced rules describe indeed structural properties of the yeast data set In practical applications we would recommend this randomization approach to focus on signi\036cant 036ndings In order to investigate the scalability of the optimization algorithm with respect to the size of the data set we experData Set Overall Number of Runtime per Size Runtime Line Searches Line Search 100 2.6 s 9 0.14 s 1,000 7.4 s 7 0.28 s 5,000 23 s 2 1.0 s 10,000 46.9 s 7 2.7 s 50,000 264 s 8 14.7 s 100,000 623 s 3 27.1 s 300,000 2004 s 3 87.2 s 500,000 3529 s 0 176.5 s Table 1 Runtimes of the optimization algorithm as a function of data set size imented with the r Type\224 data set containing 581,012 instances from the UCI repository W e remo v e d the discrete attributes leaving ten continuous attributes describing cartographic properties of 30 x 30 meter land cells We normalized the data set so that each column has a mean of zero and a standard deviation of one We then applied the optimization algorithm on subsets of different size with the t parameter set to 0.5 g set to 1 and h set to 0.5 The experiments were performed on a Pentium IV 2.8GHz machine As the runtime of the optimization algorithm depends on the number of line search steps and the runtime per line search the actual runtime varies for different random restarts We therefore give the total runtime the number of line searches that were performed and the runtime per line search for the various data set sizes in table 1 The table shows that the number of line search steps remains below thirty for all data set sizes and that the runtime per search step scales favorably with the data set size References  Y  Aumann and Y  Lindell A statistical theory for quantitati v e association rules Journal of Intelligent Information Systems  20\(3 2003  C Blak e and C Merz UCI repository of machine learning databases 1998  T  Hughes et al Functional disco v ery via a compendium of expression pro\036les Cell  102:109\226126 July 2000  U R 250 uckert L Richter and S Kramer Quantitative association rules based on half-spaces An optimization approach Technical Report TUM-I0412 Institut f 250 ur Informatik TU M 250 unchen 2004  R Srikant and R Agra w al Mining quantitati v e association rules in large relational tables In H V Jagadish and I S Mumick editors Proc of the 1996 ACM SIGMOD International Conference on Management of Data  pages 1\22612 1996  G I W ebb  Disco v e ring associations with numeric v ariables In Proc of the 7th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pages 383\226388 ACM Press 2001  J W ijsen and R Meersman On the comple xity of mining quantitative association rules Data Mining and Knowledge Discovery  2\(3 1998 Proceedings of the Fourth IEEE Internati onal Conference on Data Mining \(ICDM\22204 0-7695-2142-8/04 $ 20.00 IEEE 


degree three. The constants in the polynomial can be calculated by least squares method. The general form of the formula describing the model is shown in Equation 1 4500 4000 3500 U' 3000 2500 .. 2CJOO S 1500 lCJOO 500 o  V-V  1     lOOK 200K 300K 400K SOOK 600K 700K 800K 900K l000K number of transactions IN __ minsup?o.7% ...-min&amp;up:l Figure 2. Execution time of the Apriori algorithm by the number of transactions 10 o  t   0.5% 0.6% 0.7% 0.8% 0.9% 1.0% 1.1% 1.2% 1.3% 1.4% 1.5 minimum wpport \(%I I--Measured ....... Predicted I Figure 3. The execution time It&lt;; a function of the minimwn support using Tl817DIOK 300000 250000 g 200000 .. ?150000 100a00 E 50000 " n c 0 0.5% 0.6% 0.7% o.S% 13.9% 1.11 minimum support {sl 11I1-candidate ils..camlidate II 2-candidato [] 3-:andidste [] 4-candidat&amp; II 5-candidate 01 II 7-cand',dat" olH:andidate II 9-candidatb .10-candidale Figure 4. The number of candidates in each level using the dataset T1817DIOK Table 1. Meaning oftlte synthetic dataset parameters T Average transaction length I Average size of frequent itemsets D Number of transactions VI. PARAMETER IDENTIFICATION The measurements for parameter identification were made by different dataset parameters and support thresholds. The plIliill\\eter T varied from 10 to 25 by 5, the parameter I was in the range of 2 to 8. The performance model does not contain the parameter I, because this parameter cannot be identified before the mining process Earlier experiences showed that there is a liner relationship between the response time and number of transaction hence this pllliill\\eter was fixed to 100.000. The maximal number of the hems that can occur in a transaction was 1000 The parameter of the performance model \(Equation 1 can be calculated by least square method. Table 2 contains the identified parameters ofthe modeL 


Table 2. Identified parameter of the model C1 2.62274E-05 C;! -0.001015643 C3 0.01436588 C4 -0.065519084 269 2500 2000 i - 1500  i 1000 500 o _Measured __ Predicted 9 17 25 33 41 49 57 65 73 81 89 97 105 113 121 129 137 numer Of measurements Figure 5. The calculated and the measured time by different database parameter settings To calculate the error of the model the following function was defmed \(also used in [9 e= ?i: lfi - l i n i=1 2 where n is the number of the measurements, fi is the measured time and!;" is the calculated time Figure 5 shows the calculated and the measured execution times by all the different parameter settings where the minimum support threshold changes from 0.5 tom 1 %. It is clearly visible in Figure 5 that the prediction function follows well the behaviour characteristics of the real execution times when varying the different parameters. The average relative error of the model calculated according to Equation 2 was 1 n If; -ll e=-L I I =0.123445 n i=1 f 3 VII. MODEL V ALIDA nON After the model identification process several measurements were taken to validate the model. Table 3 contains the synthetic database parameters that were used during the model validation Figure 6 shows the measured mining time and the predicted mining time by the different datasets. It is possible to realise that the model follows well the measured execution time In some cases, especially in case of low support values the relative error of the model can be the double \(max 30 Table 3. Synthetic dataset parameters during the model validation T 18,22 7,8 D 50K, lOOK, 250K 270 The average relative error of the model calculated according to Equation 2 was e=? tlh -ll n 1=4 h 0.133185 \(4 The average error ratio is not significantly different from the error ratio of the identified model. Therefore the introduced performance model can be used for predicting the execution time of the Apriori algorithm in the investigated dataset parameter and support value intervals VIII. CONCLUSION In this paper a basic level-wise association rule mining algorithm, the Apriori algorithm was developed and its analytical performance model was introduced. Three input parameters were taken into the consideration in this model Two parameters, namely the number of transactions and average size of transactions describes the input dataset The third parameter, the minimum support threshold is the input parameter of the mining process. The constant 


input parameter of the mining process. The constant parameters of the model can be identified by few measurements using least square method A simple formula was given to anticipate the performance of the algorithm, described welI the behaviour of the data mining algorithm for a wide range of parameters. The main contribution of the paper was predicting the behaviour of a complex probability based data mining algorithm in a relatively simple, closed numerical form allowing a good estimation of execution times The model was validated by comparing the calculated and measured performance. Experimental results showed that the suggested model is reasonably accurate in a wide domain having an average error below 15 percent 1 2 3 4 5- 6 1 S 9 10 11 12 13 14 15 16 17 1$ 19 2\(121 22 23 204 25  16 27 2S 29 30 I ....... M88$urai - p,.:lided I Figure 6_ Model validation: the C11lculated and the measured time The introduced model can be used not only for predicting execution times of the Apriori algorithm, but also for predicting the response time of the other level-wise association rule mining algorithms. The reason behind this is that these algorithms are based on the Apriori algorithm thus they inherit several properties from it. Of course the identification of the constants in the formula must be repeated when moving to different algorithm in the same way as when moving to different hardware environments IX. ACKNOWLEDGEMENTS This work was supported by IBM Hungary and the fund of the Hungarian Academy of Sciences for control research and the Hungarian National Research Fund \(grant number T042741 X.REFERENCES 1] R. Agrawal, T. Imielinski and A.Swami, "Mining association rules between sets of items of large databases" in Proc. of the ACM SIGMOD Int/'/ Conf On Management of Data, Washington, D.C.,USA 1993, pp 207-216 2] R. Agrawal and R. Srikant, "Fast algorithms for mining association rules" in Proc. 20'h Very Large Databases Conference, Santiago, Chile, 1994, pp 487-499 3] J. S. Park, M. Chen, and P. S. Yu, "An effective hash based algorithm for mining association rules" in Proc of the 1995 ACM Int. Con/. on Management of Data San Jose, California, USA, 1995, pp. 175-186 4] S.Brin, R. Motawani, J.D. Ullman and S. Tsur Dynamic Item set counting and implication rules for market basket data" in Froc. of the ACM SIGMOD Inti '1 Con/. On Management of Data, Tucson Arizona, USA, 1997, pp. 255-264 5] M. J. Zaki, "Scalable algorithms for association mining", IEEE Transaction on Knowledge and Data Engineering, Vol 12. No 3. May/June 2000, pp. 372390 6] J.Han, J. Pei and Y. Yin, "Mining frequent patterns without candidate generation" in Proc. of the 2000 ACM-SIGMOD Int'l Conf. On Management of Data Dallas, Texas, USA, 2000, pp. 1-12 71 David R. Helman, David A. Bader, and Joseph Jara A randomized parallel sorting algorithm with an experimental study", Journal of Parallel and Distributed Computing, 52\(1 8] J. Landrum, J. Hardwick, and Q.F. Stout, "Predicting algorithm performance", Computing Science and Statistics, 30, 1998, pp. 309  314 9] P. Cremonesi and C. Gennaro, "Integrated Performance Models for SPMD Applications and MIMD Architectures", IEEE Transactions on Parallel and Distributed Systems, Vol. 13, No.7., 2002, pp 745-757 


745-757 10] U. Meyer et aI., Algorithms for Memory Hierarchies LNCS 2625, Springer-Verlag, Berlin, 2003, pp. 32035 4 11] Mohammed J. Zaki, "Scalable Algorithms for 271 Association Mining", IEEE Transaction on Knowledge and Data Engineering, Vo1.l2, No.3 2000, pp. 372-390 pre></body></html 


support count, the smaller the best cutting level 0 5 10 15 20 25 30 35 40 1 2 3 4 5 6 Cutting level T im e s ec  Figure 8. Execution times of CBW for various  s with minsup count = 12 0 1 2 3 4 5 36912 Minimum support count B es t  Figure 9. Evolution of the best cutting level under different minsups 4.2 Synthetic database We next compared the four algorithms on the synthetic data sets under different minsups. The results were shown in Figures 10, 11 and 12. Our observations were as follows 1. CBW outperformed all other methods in all cases 2. While all algorithms suffered from the combinatorial exploration of itemsets due to low support constraints, our CBW exhibited the best in maintaining its performance 3. The longer the itemsets become, the worse all four algorithms performed. The reason is that the cost for candidate generation, support counting Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C and conditional pattern and FP-tree construction grows as the itemset length increases 0 400 800 1200 1600 2000 0.0 0.5 1.0 1.5 2.0 2.5 minsup T im e se c  CBW FP-growth Partition Apriori Figure 10. Execution times of Apriori, Partition 


Figure 10. Execution times of Apriori, Partition FP-growth and CBW on T15.I4.D200K 0 400 800 1200 1600 2000 2400 0.0 0.5 1.0 1.5 2.0 2.5 minsup T im e se c  CBW FP-growth Partition Apriori Figure 11. Execution times of Apriori, Partition FP-growth and CBW on T15.I6.D200K We also evaluated the execution times of CBW under different cutting levels \(minsup = 1.0 influence of minsup to the cutting levels. We only showed the results for T6.I4.D100K in Figure 13 and 14; similar results were observed for the other datasets The results conformed to those observed in Figures 8 and 9 Finally, we conducted an experiment to evaluate the scalability of the four algorithms. The results were shown in Figure 15, where we omitted Apriori because its performance was significantly inferior to the others As the figure showed, our CBW exhibited the best in scalability while FP-growth exhibited the worst 0 400 800 1200 1600 2000 2400 0.0 0.5 1.0 1.5 2.0 2.5 minsup T im e se c  CBW FP-growth Partition Apriori Figure 12. Execution times of Apriori, Partition FP-growth and CBW on T15.I8.D200K 0 20 40 60 80 100 120 1 2 3 4 5 6 Cutting level T im e s ec  


Figure 13. Execution times of CBW on T6.I4.D100K for various  s 5. Concluding remarks 5.1. Summary In this paper, we have described a new efficient algorithm for frequent itemsets mining. Unlike contemporary algorithms that either adopt a top-down or a bottom-up traversal throughout the itemset lattice to search for frequent itemsets, our algorithm employs a clever guess on the most promising itemset level cutting-level there. Then it performs a downward search, followed by an upward search to discover all other frequent itemsets. Empirical study showed that our algorithm is more than an order of magnitude faster than the Apriori variants Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C 01 2 3 4 5 0.25%0.50%0.75%1.00 minsup B es t cu tt in g l ev el Figure 14. Evolution of the best cutting level for CBW under different minsups, running on T6.I4.D100K Our CBW algorithm has been incorporated into an online multidimensional association rule mining system currently under development [10]. In the future we will incorporate into CBW the taxonomy information and extend it to allow multiple minimum support specification [13 0 100 200 300 400 500 600 700 800 2 4 6 8 10 12 14 16 18 20 Number of transactions \(x 10,000 T im e s ec  CBW FP-growth Partition Figure 15. Scalability evaluation of CBW, FPgrowth and Partition running on T15.I8.D200K with minsup = 1.0 5.2. Comparison with related work To our knowledge, [9] is the only work on combining the top-down and the bottom-up searches for association mining. But their approach and 


for association mining. But their approach and intention are quite different from ours First, rather than starting from the middle of the search space and progressively searching towards both ends, their approach proceeds from both ends of the search lattice and progressively searches towards the middle Second, their approach aims at discovering, instead of all frequent itemsets, the maximal frequent itemsets i.e., itemsets having no supersets, which work is quite simple compared to the work for discovering all frequent itemsets. Furthermore, on applying their approach to the work of frequent itemsets mining, the top-down pruning" technique on which their approach relies will become useless because the subsets of a frequent itemset found in top-down search still have to been counted to know their supports. In this way, the top-down search becomes unnecessary and their method will degenerate to the Apriori algorithm On the contrary, we believe that our method can be adapted to the problem of mining maximal frequent itemsets [1][3][9]. Indeed, we are currently working on applying our CBW to this problem and hope to have result in the near future References 1] R.C. Agarwal, C.C. Aggarwal, and V.V.V. Prasad Depth first generation of long patterns," in Proceedings of 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2000, pp 108?118 2] R. Agrawal and R. Srikant, "Fast Algorithms for Mining Association Rules," in Proceedings of the 20th VLDB Conference, 1994, pp. 487?499 3] R.J. Bayardo Jr., "Efficiently Mining Long Patterns from Databases," in Proceedings of 1998 ACM SIGMOD International Conference on Management of Data Seattle, Washington, USA, 1998, pp. 85?93 4] S. Brin, R. Motwani, J.D. Ullman, and S. Tsur Dynamic Itemset Counting and Implication Rules for Market Baseket Data," SIGMOD Record, Vol. 26, 1997 pp. 255?264 5] Database Research Group in the Department of Computer Science and Engineering at the Chinese University of Hong Kong http://www.cse.cuhk.edu.hk/~kdd/program.html 6] J. Han, J. Pei, and Y. Yin, "Mining Frequent Patterns Without Candidate Generation," in Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, Dallas, TX, USA, 2000, pp. 1?12 7] J. Hipp, U. Guntzer, and G. Nakhaeizadeh, "Algorithms for Association Rule Mining?A General Survey and Comparison," SIGKDD Explorations, Vol. 2, 2000, pp 58?64 8] J. Hipp, U. Guntzer, and G. Nakhaeizadeh, "Mining Association Rules: Deriving a Superior Algorithm by Analyzing Today  s Approaches," in Proceedings of 4th European Symposium on Principles of Data Mining and Knowledge Discovery \(PKDD  00 9] D. Lin and Z.M. Kedem, "Pincer-search: An Efficient Algorithm for Discovering the Maximum Frequent Set Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C IEEE Transactions on Knowledge and Data Engineering, Vol. 14, No. 3, 2002, pp. 553?566 10] W.Y. Lin, J.H. Su and M.C. Tseng, "OMARS: The Framework of an Online Multi-dimensional Association Rules Mining System," in Proceedings of the 2nd International Conference on Electronic Business, Taipei Taiwan, 2002, pp. 216?225 11] J.S. Park, M.S. Chen, and P.S. Yu, "An Effective HashBased Algorithm for Mining Association Rules," in Proceedings of the 1995 ACM SIGMOD International 


Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data, San Jose, CA USA, 1995, pp. 175?186 12] A. Savasere, E. Omiecinski, and S. Navathe, "An Efficient Algorithm for Mining Association Rules in Large Databases," in Proceedings of the 24th VLDB Conference, 1995, pp. 432?444 13] M.C. Tseng and W.Y. Lin, "Mining Generalized Association Rules with Multiple Minimum Supports," in Proceedings of International Conference on Data Warehousing and Knowledge Discovery, Munich Germany, 2001, pp. 11-20 14] M.J. Zaki, "Scalable Algorithms for Association Mining," IEEE Transactions on Knowledge and Data Engineering, Vol. 12, No. 2, 2000, pp. 372?390 Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C pre></body></html 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


