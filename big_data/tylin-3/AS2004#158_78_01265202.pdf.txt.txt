html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">CBW: An  Efficient Algorithm for Frequent Itemset Mining  Ja-Hwung Su Institute of Information Engineering I-Shou University Kaohsiung 840, Taiwan bb0820@ms22.hinet.net Wen-Yang Lin Dept. of Information Management I-Shou University Kaohsiung 840, Taiwan wylin@isu.edu.tw   This work was partially supported by the National Science Council of ROC under grant No.NSC902213-E-214-040 Abstract Frequent itemset generation is the prerequisite and most time-consuming process for association rule mining. Nowadays, most efficient Apriori-like algorithms rely heavily on the minimum support constraint to prune a vast amount of non-candidate itemsets. This pruning technique, however, becomes less useful for some real applications where the supports of interesting itemsets are extremely small such as medical diagnosis, fraud detection, among the others. In this paper, we propose a new algorithm that maintains its performance even at relative low supports. Empirical evaluations show that our algorithm is, on the average, more than an order of magnitude faster than Apriori-like algorithms 1. Introduction Mining association rules from a large database of business data, such as transaction records, has been a hot topic within the area of data mining. This problem is motivated by applications known as market basket analysis to find relationships between items purchased by customers [2], that is, what kinds of products tend to be purchased together An association rule is an expression of the form X Y, where X and Y are sets of items. Such a rule reveals that transactions in the database containing items in X tend to contain items in Y, and the probability, measured as the fraction of transactions containing X also containing Y, is called the confidence of the rule. The support of the rule is the fraction of the transactions that contain all items both in X and Y For an association rule to hold, the support and the confidence of the rule should satisfy a user-specified minimum support, called minsup, and minimum confidence, called minconf, respectively. The problem of mining association rules is to discover all association rules that satisfy minsup and minconf In general, the work of association rules mining can be decomposed into two phases: \(1 generation: find out all itemsets that sufficiently exceed the minsup, and \(2 the frequent itemsets generate all association rules having confidence higher than the minconf. Since the second phase is straightforward and less expensive, we concentrate only on the first phase for finding all frequent itemsets Nowadays, most efficient Apriori-like algorithms rely heavily on the minimum support constraint to prune a vast amount of non-candidate itemsets. This pruning technique, however, becomes less useful for some real applications where the supports of interesting itemsets are extremely small, such as medical diagnosis, fraud detection, among the others This is because the number of candidate itemsets 


This is because the number of candidate itemsets exponentially increases as the minimum support threshold decreases, and ultimately, almost all itemsets will become candidates at very low support threshold In this paper, we propose a new algorithm, called CBW, which employs a bi-directional search strategy and hybridizes various techniques in frequent itemset generation. Empirical evaluations show that our algorithm can maintain its performance even at relative low support thresholds, and can be more than two orders of magnitude faster than Apriori-like algorithms The rest of this paper is organized as follows. A review of previous work is given in Section 2. In Section 3, we describe the proposed algorithm for finding frequent itemsets. Empirical evaluations of our algorithm on Foodmart2000 and IBM  s synthetic data set are described in Section 4. Finally, conclusion and future work are stated in Section 5 Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C 2. Previous work In the literature, there have been a substantial number of methods for mining association rules. The most well-known and influential algorithm is Apriori 2], which uses an a priori knowledge of frequent kitemsets to generate candidate \(k+1 employs an innovative technique to prune nonpromising candidates The most criticized drawback of Apriori is that when the cardinality of the longest frequent itemsets is k, Apriori needs k passes of database scans. In addition the Apirori algorithm is computation-intensive in generating the candidate itemsets and counting the support values, especially for applications with very low support threshold and/or a vast amount of items Many variants thus were proposed to improve the efficiency, including DHP [11], Partition [12], DIC [4 Eclat [14], Top-down [14], FP-growth [6], among the others. Although these variants adopt different techniques and employ in different view of points, they can be categorized from three different algorithmic aspects: \(1 horizontal counting; \(2 search \(BFS DFS 3 Search direction: top-down vs. bottom-up. Although the first two aspects have been addressed in [7][8], no comparison has revealed the influence of the last aspect. Table 1 shows a three-dimensional classification of prevailing Apriori-like algorithms Table 1. A three-dimensional classification view of prevailing Apriori-like algorithms Search direction bottom-up top-down Counting Search strategy: Search strategy strategy: DFS BFS DFS BFS counting  FPgrowth Apriori DHP DIC Top-down intersection Eclat Partition Counting Strategy. This refers to the methods used to count the occurrences of candidate itemsets. Up to date, there are two main approaches: horizontal counting and vertical intersection. The horizontal counting determines the support value of a candidate itemset by scanning transaction one by one, and increasing the counter of the itemset if it is a subset of the transaction. This approach works well for a rarely 


the transaction. This approach works well for a rarely occurred candidate because only those transactions containing that itemset need to be inspected. The candidate look up operation, however, is costly for candidates of large size Vertical intersection, on the other hand, is employed when the database is represented as a vertical format such that each record is associated with an item to store the identifiers of the transactions containing that item called tidlist. Though the vertical intersection scheme eliminates the I/O cost for database scan, it has the following deficiency: When the support count of a candidate itemset is quite less than the number of transactions, there occurs a large amount of unnecessary intersections Search direction. Nowadays, most Apriori-like approaches adopt bottom-up traversal of the search space, starting from all frequent 1-itemsets upward to the longest frequent itemsets. The main advantage of this paradigm is that it can effectively prune the search space by exploiting downward closure property: once an itemset is recognized as infrequent, all of its supersets are infrequent as well. This advantage fades however, when most of the maximal frequent itemsets locating near the largest itemset of the search lattice due to a relatively small support threshold. In this case there are very few itemsets to be pruned Another itemset traversal is employed in the opposite direction, i.e. starting from the longest itemsets downward to the frequent 1-itemsets, or topdown for short. This strategy is traditionally adopted for discovering maximal frequent itemsets [1][3][13 But notice that though all of the frequent itemsets can be derived from their maximal ones, further counting strategies are required to obtain their exact supports for computing the confidences of association rules Meanwhile, if there are vast numbers of items and/or the support threshold is very low, many infrequent itemsets have to be visited before the maximal frequent itemsets are identified. This is why most work on frequent itemsets mining embraces the bottom-up paradigm instead Search strategy. While the search direction guides the way that the search space is exploited, the search strategy refers to the order in which itemsets are visited Most Apriori-like algorithms employ breadth-first search because it can facilitate the pruning of candidates with downward closure. This strategy however, requires more memory to keep the frequent subsets of the pruned candidates An alternative strategy called DFS, on the other hand, recursively visits the descendants of an itemset In the literature [14], this strategy is usually combined with the counting strategy of vertical intersection because it suffices to keep in memory the tidlists corresponding to the itemsets on the path from the root down to the currently inspected one Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C 3. The proposed Cut-Both-Ways \(CBW algorithm 3.1. Algorithm basics As mentioned in the previous section, the performance bottleneck of frequent itemsets generation lies in two aspects: the database scan and support counting. Most contemporary efficient algorithms for frequent itemset generation are devoted to attack these two issues. We notice, however, almost of these algorithms suffer from performance degradation as the minimum support decreases; they behave well under large minimum supports, but as the minimum support decreases, their performances decrease significantly 


decreases, their performances decrease significantly Unfortunately, in some applications, the minimum support must be specified relatively small to mine interesting patterns from database The effect of a lower support threshold has two facets: On the one hand, much more candidate itemsets are generated and inspected, and on the other hand, the cardinalities of maximum frequent itemsets become larger. Therefore, the computation of support counting grows dramatically. To alleviate this problem, we propose an algorithm called CBW \(Cut the space &amp employ Both-Ways search illustrated in Figure 1 Figure 1. Concept illustration of CBW Viewing the solution space as a pyramid that contains frequent itemsets located at different levels equal to their cardinalities, we first pursue an appropriate cutting level ? to divide the space into two different parts. After identifying all frequent itemset at this level, we perform a downward search to enumerate all frequent itemsets below the cutting level ? and determine their support values, followed by an upward search to enumerate all frequent itemsets with cardinalities larger than The insight behind this paradigm is that, as stated in Section 2, no approach based on single algorithmic strategy performs the best in all cases. Bottom-up search will suffer too many database scans as well as wasted set enumeration to count the supports of candidates at higher cardinality levels, especially in the case of low support threshold. Top-down search, on the other hand, can not utilize the anti-monotone property to prune the search space. Furthermore, according to the investigation in [7], vertical intersection performs much better in counting the support values of candidates with larger cardinality, while horizontal counting favors the opposite situation. All of these suggest hybridizing different algorithmic approaches to attack the itemset mining problem Our paradigm has the following features. First, by guessing the appropriate cutting level, we can identify the most promising cardinality to perform downward search. If our guess is correct, i.e., most of the itemsets under this level are frequent, then the effectiveness of Apriori pruning is overwhelmed by its cost. Therefore it is more economic to enumerate all candidate itemsets and count their supports within one database scan Second, for upward searching frequent itemsets with cardinalities larger than the cutting level, the synergy gained from the Apriori pruning and vertical intersection can save lots of unnecessary computations 3.2. Algorithm detail Before carrying out the paradigm shown in Figure 1 we need to determine an appropriate cutting level, a crucial factor to the effectiveness of CBW. As will be clear later, if the cutting level is too low, unnecessary intersections will happen frequently during upward searching. On the contrary, if the cutting level is too high, the downward search will spend much more time in itemsets enumeration and counting their supports Therefore, we have to trade the favors between upward search and downward search to find an appropriate cutting level. An insightful idea is to pursue the average cardinality of frequent itemsets, expecting that most of the frequent itemsets will appear in this level This value, however, is impossible to be obtained without knowing the frequent itemsets. We thus adopt a simple heuristic described in the following Definition 1. Let D be a transaction table and ti the i-th transaction. The cutting level ? is defined below   


         D t minsupi  Cutting level  Itemset Pyramid Frequent 1-itmesets Frequent \(??1 Frequent ?-itmesets Frequent \(?+1 Downward search Longest frequent itemsets Upward search  Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C where INT[r] denote the nearest integer of r, for r ? 1 and ti ?minsup be the set of items in ti with support larger than minsup. More specifically ti ?minsup = {x | x ? ti and sup\(x For an illustration, consider the transaction data in Table 2. Assume that minsup = 40%. Then, the frequent 1-itemsets include {A}, {B}, {C}, and {D The cutting level ? thus is \(3 + 2 + 1 + 4 + 3 + 2 + 3 3 + 3 + 3 Table 2. An example transaction data tid items 1 2 3 4 5 6 7 8 9 10 B, C, D, E A, C B A, B, C, D A, B, D C, D A, B, D, E B, C, D B, C, D, E B, C, D Detail description of the CBW algorithm is given in Figures 2, 3, 4 and 5, where sup\(X of an itemset X and T denotes the set of tidlists Input: The transaction database D and minimum support minsup Output: The set of frequent itemsets F 1. scan D to generate all frequent 1-itemsets F1 2. Trans\(D, T, F1, F2 3. Dwnsearch \(D, DF, F?, ?, minsup 4. Upsearch \(T, UF, F?, ?, minsup 5. return F = DF ? UF Figure 2. Algorithm CBW The algorithm starts with scanning the database to generate the set of all frequent 1-itemsets F1, which is necessary for computing the cutting level defined in Equation 1 


Procedure Trans is responsible for three different tasks: \(1 2 the set of frequent 2-itemsets F2; and \(3 the database into vertical tidlists T. For each scanned transaction t, the number of frequent items is accumulated for later computation of ?. Also, for each frequent item, we add this tid into its tidlist if the cardinality of t is no less than 3. In essence, only those transactions with cardinality larger than ? have to be 1. C2 = the set of candidate 2-itemsets generated from F1 2. numf = 0 3. for i = 1 to |D| do 4.    scan the i-th transaction ti 5.    delete the items in ti that is not in F1 6.    numf += | ti 7. if | ti| ? 3 then 8.        add this tid into the tidlist of each item in ti 9. for each 2-subset X of ti and X ? C2 do 10. X.count 11. end for 12. F2={X | sup\(X 13. ? = numf / |D 14. return F2 and Figure 3. Procedure Trans 1. for i = 1 to |D| do 2.    scan the i-th transaction ti 3.    delete the items in ti that appear less than two itemsets in F2 4. for each subset X of ti and 3 ? |X| ? ? do 5. X.count 6. end for 7. DF={X | sup\(X 8. return DF Figure 4. Procedure Dwnsearch 1. if F? = ? return UF 2. read the set of tidlists T and prune non-candidate tidlists 3. k = ?, Fk = F 4. repeat 5.    k 6.    Ck = the set of new candidate k-itemsets generated from Fk-1 7.    for each X ? Ck do 8.        perform bit-vector intersection on X 9.        compute the support of X, sup\(X 10.    endfor 11.    Fk = {X| sup\(X 12.    UF = UF ? Fk 13. until Fk 14. return UF Figure 5. Procedure Upsearch kept because the upward search starting from level ?+1 But recall that in this stage we still have no idea of Cardinality 3 represents the best we can achieve currently to facilitate tidlist pruning. To reduce the memory requirement, all of the tidlists generated in Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C this phase are stored in disk for later use in the course of upward search For example, consider Table 2 again. We have C2 A, B}, {A, C}, {A, D}, {B, C}, {B, D}, {C, D Since item E does not appear in F1, there is no need to create the tidlist of E. Furthermore, tids of t2, t3 and t6 are not included in the tidlist of any frequent item because their cardinalities are less than 3. The resulting tidlists after this stage is shown in Figure 6 item tidlist A B C 


C D 4, 5, 7 1, 4, 5, 7, 8, 9, 10 1, 4, 8, 9, 10 1, 4, 5, 7, 8, 9, 10 Figure 6. The resulting tidlists generated by procedure Trans Next, procedure Dwnsearch is executed. Each transaction is scanned, and, according to Proposition 1 those items that appear less than two frequent itemsets of F2 are pruned. The trimmed transaction then undergoes set enumeration to generate all candidate itemsets with cardinalities between ? and 3, and count their supports Proposition 1. If an item x appears in less than k?1 frequent itemsets of Fk?1, then x is not contained in any frequent itemset of Fk Rationale. Let I be a k-itemset of Fk, I = {a1, a2   ai = x, aI+1  ak}. Note that I has exactly k \(k?1 itemsets, of which there are k?1 itemsets containing item x, except itemset {a1, a2  ai?1, ai+1  ak According to downward closure, if I is frequent then all of its subsets should be frequent as well. The proposition then follows For example, for the first transaction t1, item E is pruned first because it is not frequent. Transaction t2 and t3 are discarded because their cardinalities are less than 3. In this way, we can get the support counts of all itemsets. Finally, we discard the itemsets with supports less than minsup. The resulting 3-itemsets is {{B, C D After generating all frequent itemsets with cardinalities no more than ?, the CBW algorithm performs upward searching for the other frequent itemsets. Following the Apriori paradigm, the Upsearch procedure generates the frequent itemsets level by level in a bottom-up fashion starting from the frequent itemsets at level ?. It first checks if F? is empty. Otherwise, it reads the tidlists T and prunes the tidlists for items that appear in less than ? frequent itemsets of F?. Furthermore, to accelerate the intersections of tidlists for counting the itemsets, we adopt the techniques of fast intersection [14] and caching intermediate result [8 4. Empirical evaluations To evaluate the performance of CBW, we tested several data sets, including Foodmart2000 provided in Microsoft SQL2000, and several synthetic data T6.I4.D100K, T15.I4.D200K, T15.I6.D200K and T15.I8.D200K, which were generated using the IBM data generator [2]. The experiments were performed on a HP LH6000R workstation with 1GB RAM and 18GB HD running Windows 2000 Server For comparison, we has implemented two leading Apriori variants: Apriori and Partition, the former is based on horizontal counting strategy using hash tree structure, while the latter relies on vertical tidlists intersection. But note that for pair comparison with other methods, our implementation of Partition did not partition" the database into several chunks. That is our Partition is an in-core method. We also included a publicly available implementation of FP-growth provided by Database Research Group at Chinese University of Hong Kong [5] in all evaluations 4.1. Foodmart2000 database We first compared the execution times of the three approaches on Foodmart2000, using different minimum support counts ranging from 3 to 15. Table 3 showed the characteristics of Foodmart2000. The results were shown in Figure 7 


results were shown in Figure 7 Table 3. Data parameters of Foodmart2000 Parameter Value D| Number of transactions 60000 t| Average size of transactions  4 N Number of items 200 We observed that 1. Because the support counts of most itemsets in Foodmart2000 are very small, the minsup must be set relatively small to generate interesting patterns. But a small minsup leads to a huge amount of candidate itemsets, causing too many redundant intersections of tidlists. As such, the computation of support count using tidlist intersection is more than that using hash tree That is why Apriori outperforms Partition 2. At high minimum supports, the speedup of CBW over Apriori is not significant, because the number of candidate itemsets at lower itemset Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C levels generated by CBW is larger than that by Apriori. But at higher itemset levels, the counting cost of CBW is much less than that of Apriori 3. FP-growth performs well for high support thresholds but degrades rapidly as the support threshold decreases. This is because though FPgrowth does not explicitly generate and store candidate itemsets, it needs to construct conditional pattern base and from which to construct FP-tree to generate all frequent itemsets When minsup is very low, the overhead spent on building the conditional pattern base and conditional FP-tree, and recursively traversal of the tree to generate frequent patterns will overwhelm the cost saved in candidate itemset generation 0 100 200 300 400 500 600 700 3691215 Mininmum support count T im e se c  CBW FP-growth Partition Apriori Figure 7. Execution times of Apriori, Partition, FPgrowth and CBW on Foodmart2000 We also conducted experiments to see the influence of different cutting levels. The results were depicted in Figure 8. We observed that the cutting level has great effects on the performance of CBW, and that there exists a best cutting level. The reason is that counting at a higher level would speed the execution of upward searching but increase the computation of downward searching. On the contrary, a lower cutting level would speed up the execution of downward searching but slow down the performance of upward searching Figure 9 shows that the best cutting level is affected by the minimum support count; the larger the minimum support count, the smaller the best cutting level 


support count, the smaller the best cutting level 0 5 10 15 20 25 30 35 40 1 2 3 4 5 6 Cutting level T im e s ec  Figure 8. Execution times of CBW for various  s with minsup count = 12 0 1 2 3 4 5 36912 Minimum support count B es t  Figure 9. Evolution of the best cutting level under different minsups 4.2 Synthetic database We next compared the four algorithms on the synthetic data sets under different minsups. The results were shown in Figures 10, 11 and 12. Our observations were as follows 1. CBW outperformed all other methods in all cases 2. While all algorithms suffered from the combinatorial exploration of itemsets due to low support constraints, our CBW exhibited the best in maintaining its performance 3. The longer the itemsets become, the worse all four algorithms performed. The reason is that the cost for candidate generation, support counting Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C and conditional pattern and FP-tree construction grows as the itemset length increases 0 400 800 1200 1600 2000 0.0 0.5 1.0 1.5 2.0 2.5 minsup T im e se c  CBW FP-growth Partition Apriori Figure 10. Execution times of Apriori, Partition 


Figure 10. Execution times of Apriori, Partition FP-growth and CBW on T15.I4.D200K 0 400 800 1200 1600 2000 2400 0.0 0.5 1.0 1.5 2.0 2.5 minsup T im e se c  CBW FP-growth Partition Apriori Figure 11. Execution times of Apriori, Partition FP-growth and CBW on T15.I6.D200K We also evaluated the execution times of CBW under different cutting levels \(minsup = 1.0 influence of minsup to the cutting levels. We only showed the results for T6.I4.D100K in Figure 13 and 14; similar results were observed for the other datasets The results conformed to those observed in Figures 8 and 9 Finally, we conducted an experiment to evaluate the scalability of the four algorithms. The results were shown in Figure 15, where we omitted Apriori because its performance was significantly inferior to the others As the figure showed, our CBW exhibited the best in scalability while FP-growth exhibited the worst 0 400 800 1200 1600 2000 2400 0.0 0.5 1.0 1.5 2.0 2.5 minsup T im e se c  CBW FP-growth Partition Apriori Figure 12. Execution times of Apriori, Partition FP-growth and CBW on T15.I8.D200K 0 20 40 60 80 100 120 1 2 3 4 5 6 Cutting level T im e s ec  


Figure 13. Execution times of CBW on T6.I4.D100K for various  s 5. Concluding remarks 5.1. Summary In this paper, we have described a new efficient algorithm for frequent itemsets mining. Unlike contemporary algorithms that either adopt a top-down or a bottom-up traversal throughout the itemset lattice to search for frequent itemsets, our algorithm employs a clever guess on the most promising itemset level cutting-level there. Then it performs a downward search, followed by an upward search to discover all other frequent itemsets. Empirical study showed that our algorithm is more than an order of magnitude faster than the Apriori variants Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C 01 2 3 4 5 0.25%0.50%0.75%1.00 minsup B es t cu tt in g l ev el Figure 14. Evolution of the best cutting level for CBW under different minsups, running on T6.I4.D100K Our CBW algorithm has been incorporated into an online multidimensional association rule mining system currently under development [10]. In the future we will incorporate into CBW the taxonomy information and extend it to allow multiple minimum support specification [13 0 100 200 300 400 500 600 700 800 2 4 6 8 10 12 14 16 18 20 Number of transactions \(x 10,000 T im e s ec  CBW FP-growth Partition Figure 15. Scalability evaluation of CBW, FPgrowth and Partition running on T15.I8.D200K with minsup = 1.0 5.2. Comparison with related work To our knowledge, [9] is the only work on combining the top-down and the bottom-up searches for association mining. But their approach and 


for association mining. But their approach and intention are quite different from ours First, rather than starting from the middle of the search space and progressively searching towards both ends, their approach proceeds from both ends of the search lattice and progressively searches towards the middle Second, their approach aims at discovering, instead of all frequent itemsets, the maximal frequent itemsets i.e., itemsets having no supersets, which work is quite simple compared to the work for discovering all frequent itemsets. Furthermore, on applying their approach to the work of frequent itemsets mining, the top-down pruning" technique on which their approach relies will become useless because the subsets of a frequent itemset found in top-down search still have to been counted to know their supports. In this way, the top-down search becomes unnecessary and their method will degenerate to the Apriori algorithm On the contrary, we believe that our method can be adapted to the problem of mining maximal frequent itemsets [1][3][9]. Indeed, we are currently working on applying our CBW to this problem and hope to have result in the near future References 1] R.C. Agarwal, C.C. Aggarwal, and V.V.V. Prasad Depth first generation of long patterns," in Proceedings of 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2000, pp 108?118 2] R. Agrawal and R. Srikant, "Fast Algorithms for Mining Association Rules," in Proceedings of the 20th VLDB Conference, 1994, pp. 487?499 3] R.J. Bayardo Jr., "Efficiently Mining Long Patterns from Databases," in Proceedings of 1998 ACM SIGMOD International Conference on Management of Data Seattle, Washington, USA, 1998, pp. 85?93 4] S. Brin, R. Motwani, J.D. Ullman, and S. Tsur Dynamic Itemset Counting and Implication Rules for Market Baseket Data," SIGMOD Record, Vol. 26, 1997 pp. 255?264 5] Database Research Group in the Department of Computer Science and Engineering at the Chinese University of Hong Kong http://www.cse.cuhk.edu.hk/~kdd/program.html 6] J. Han, J. Pei, and Y. Yin, "Mining Frequent Patterns Without Candidate Generation," in Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, Dallas, TX, USA, 2000, pp. 1?12 7] J. Hipp, U. Guntzer, and G. Nakhaeizadeh, "Algorithms for Association Rule Mining?A General Survey and Comparison," SIGKDD Explorations, Vol. 2, 2000, pp 58?64 8] J. Hipp, U. Guntzer, and G. Nakhaeizadeh, "Mining Association Rules: Deriving a Superior Algorithm by Analyzing Today  s Approaches," in Proceedings of 4th European Symposium on Principles of Data Mining and Knowledge Discovery \(PKDD  00 9] D. Lin and Z.M. Kedem, "Pincer-search: An Efficient Algorithm for Discovering the Maximum Frequent Set Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C IEEE Transactions on Knowledge and Data Engineering, Vol. 14, No. 3, 2002, pp. 553?566 10] W.Y. Lin, J.H. Su and M.C. Tseng, "OMARS: The Framework of an Online Multi-dimensional Association Rules Mining System," in Proceedings of the 2nd International Conference on Electronic Business, Taipei Taiwan, 2002, pp. 216?225 11] J.S. Park, M.S. Chen, and P.S. Yu, "An Effective HashBased Algorithm for Mining Association Rules," in Proceedings of the 1995 ACM SIGMOD International 


Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data, San Jose, CA USA, 1995, pp. 175?186 12] A. Savasere, E. Omiecinski, and S. Navathe, "An Efficient Algorithm for Mining Association Rules in Large Databases," in Proceedings of the 24th VLDB Conference, 1995, pp. 432?444 13] M.C. Tseng and W.Y. Lin, "Mining Generalized Association Rules with Multiple Minimum Supports," in Proceedings of International Conference on Data Warehousing and Knowledge Discovery, Munich Germany, 2001, pp. 11-20 14] M.J. Zaki, "Scalable Algorithms for Association Mining," IEEE Transactions on Knowledge and Data Engineering, Vol. 12, No. 2, 2000, pp. 372?390 Proceedings of the 37th Hawaii International Conference on System Sciences - 2004 0-7695-2056-1/04 $17.00 \(C pre></body></html 


per by Ganti et. al. [9] deals with the measurement of similarity \(or deviation, in the authors  vocabulary between decision trees, frequent itemsets and clusters Although this is already a powerful approach, it is not generic enough for our purpose. The most relevant research e?ort in the literature, concerning pattern management is found in the ?eld of inductive databases Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE meant as databases that, in addition to data, also contain patterns [10], [7]. Our approach di?ers from the inductive database one mainly in two ways. Firstly, while only association rules and string patterns are usually considered there and no attempt is made towards a general pattern model, in our approach no prede?ned pattern types are considered and the main focus lies in devising a general and extensible model for patterns Secondly, di?erently from [10], we claim that the peculiarities of patterns in terms of structure and behavior together with the characteristic of the expected workload on them, call for a logical separation between the database and the pattern-base in order to ensure e?cient handling of both raw data and patterns through dedicated management systems Finally, we remark that even if some languages have been proposed for pattern generation and retrieval 14, 11], they mainly deal with speci?c types of patterns \(in general, association rules sider the more general problem of de?ning safe and su?ciently expressive language for querying heterogeneous patterns 7. Conclusions and Future Work In this paper we have dealt with the issue of modelling and managing patterns in a database-like setting Our approach is enabled through a Pattern-Base Management System, enabling the storage, querying and management of interesting abstractions of data which we call patterns. In this paper, we have \(a de?ned the logical foundations for the global setting of PBMS management through a model that covers data patterns and intermediate mappings and \(b language issues for PBMS management. To this end we presented a pattern speci?cation language for pattern management along with safety constraints for its usage and introduced queries and query operators and identi?ed interesting query classes Several research issues remain open. First, it is an interesting topic to incorporate the notion of type and class hierarchies in the model [15]. Second, we have intentionally avoided a deep discussion of statistical measures in this paper: it is more than a trivial task to de?ne a generic ontology of statistical measures for any kind of patterns out of the various methodologies that exist \(general probabilities Dempster-Schafer, Bayesian Networks, etc. [16 nally, pattern-base management is not a mature technology: as a recent survey shows [6], it is quite cumbersome to leverage their functionality through objectrelational technology and therefore, their design and engineering is an interesting topic of research References 1] Common Warehouse Metamodel \(CWM http://www.omg.org/cwm, 2001 2] ISO SQL/MM Part 6. http://www.sql99.org/SC32/WG4/Progression Documents/FCD/fcddatamining-2001-05.pdf, 2001 3] Java Data Mining API http://www.jcp.org/jsr/detail/73.prt, 2003 4] Predictive Model Markup Language \(PMML http://www.dmg.org 


http://www.dmg.org pmmlspecs v2/pmml v2 0.html, 2003 5] S. Abiteboul and C. Beeri. The power of languages for the manipulation of complex values. VLDB Journal 4\(4  794, 1995 6] B. Catania, A. Maddalena, E. Bertino, I. Duci, and Y.Theodoridis. Towards abenchmark for patternbases http://dke.cti.gr/panda/index.htm, 2003 7] L. De Raedt. A perspective on inductive databases SIGKDD Explorations, 4\(2  77, 2002 8] M. Escobar-Molano, R. Hull, and D. Jacobs. Safety and translation of calculus queries with scalar functions. In Proceedings of PODS, pages 253  264. ACMPress, 1993 9] V. Ganti, R. Ramakrishnan, J. Gehrke, andW.-Y. Loh A framework for measuring distances in data characteristics. PODS, 1999 10] T. Imielinski and H. Mannila. A database perspective on knowledge discovery. Communications of the ACM 39\(11  64, 1996 11] T. Imielinski and A. Virmani. MSQL: A Query Language for Database Mining. Data Mining and Knowledge Discovery, 2\(4  408, 1999 12] P. Kanellakis, G. Kuper, and P. Revesz. Constraint QueryLanguages. Journal of Computer and SystemSciences, 51\(1  52, 1995 13] P. Lyman and H. R. Varian. How much information http://www.sims.berkeley.edu/how-much-info, 2000 14] R.Meo, G. Psaila, and S. Ceri. An Extension to SQL for Mining Association Rules. Data Mining and Knowledge DiscoveryM, 2\(2  224, 1999 15] S. Rizzi, E. Bertino, B. Catania, M. Golfarelli M. Halkidi, M. Terrovitis, P. Vassiliadis, M. Vazirgiannis, and E. Vrachnos. Towards a logical model for patterns. In Proceedings of ER 2003, 2003 16] A. Siblerschatz and A. Tuzhillin. What makes patterns interesting in knowledge discovery systems. IEEE TKDE, 8\(6  974, 1996 17] D. Suciu. Domain-independent queries on databases with external functions. In Proceedings ICDT, volume 893, pages 177  190, 1995 18] M.Terrovitis, P.Vassiliadis, S. Skadopoulos, E. Bertino B. Catania, and A. Maddalena. Modeling and language support for the management of patternbases. Technical Report TR-2004-2, National Technical University of Athens, 2004. Available at http://www.dblab.ece.ntua.gr/pubs Proceedings of the 16th International Conference on Scientific and Statistical Database  Management \(SSDBM  04 1099-3371/04 $ 20.00  2004 IEEE pre></body></html 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


