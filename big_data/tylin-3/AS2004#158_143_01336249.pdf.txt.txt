html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Interpreting  Association Rules in Granular Data Model via Decision Logic Tsau. Young.\("T. Y Department of Computer Science San Jose State University San Jose, CA 95192-0462 tylin@cs.sjsu.edu Abstruct - Based on machine oriented modeling, a formal theory of association rules bas been developed; the theory allow us to mining association rule mining by solving a set of linear inequality. Unfommately, these rules are un-interpreted in the sense, we cannot generate a proper formula using the originally given symbols \(attribute values discovered rules. In this paper, we develop a theory to generate such formula of given symbols to interpret them I. INTRODUCTION In the Foundation on Data Mining and Discovery Workshop 6], we view that data mining as a procedure that transforms extracts or discovers from  the most conservative view, namely, mathematical deductions are the only acceptable reasoning in the procedure and call it deductive data mining. In other words, we treat data as the axiom" and patterns as "interesting theorems" to be derived kom axioms mathematically. In classical statistics, a similar view has been taken, and is called descriptive statistics Taking such a view, does not imply that we are restricting ourselves in utilizing possible means and tools, but imply that data miners need to identify clearly what are the bidden assumptions. With this view, data mining \(we will focus on association rule mining 1 universe, a set of entities. It is the input data for data mining From data mining point of view, it is merely a table of symbols; these symbols play the roles of semantics primitive 1.1 bytes 1.2 represent some pieces of real world phenomena, concepts or objects 1.3 system 1161 2 tuples are patterns 3 bitmaps, the pattern may not be comprehensible to human. As symbols are meaningful to human, we need to represent the patterns in \(algebraic or logic symbols and the operators in the expressions are meaningful to human, formally, we say the \(algebraic or logic expressions of symbols are interpretations or explanations of the pattems 11. GRANULAR DATA MODEL A. Relation, Bilmapsfrom Database Theory In setting up a database, a relational table is often viewed as a representation of a set V of real world entities in terms of a family of attributes A ={A', A2, ..., A"}. Let us set it up more formally: Let Dom consists of current distinct values of an attribute. A knowledge representation is a map E V + K ;  v + \(ki,kz that maps each entity to a tuple of attribute values. The set of K c Doin\(A AZ A called a bag relation. In the traditional relational theory, the independent variables play no roles. So we are forced to consider bag relations, because distinct entities may have the same representation. So, we use the notion of graph \(v, f\(v the pair of independent variable and function. It will be referred to as an information table or simply table. Io this format, we can avoid the bag theory; see Table 2.1 For 


format, we can avoid the bag theory; see Table 2.1 For convenience, we may also treat an attribute as a map A': V --f Dom\(A Then, we can consider an equivalence relation defined as follows vl = v2 iff A'\(v VJ The equivalence relation will be denoted by Q'; note that the inverse image of an attribute value defmes a granule \(an equivalence class this paper B. Bitmap lndexes and Granular Data Model \(GDM Bitmap index is a common notion in database theory; the following example is taken from a popular database text \([3 PP 702 51 0-7803-8376-1/04/620.00 Cnnvriuht 7nn4 TFFF Example: Suppose we are given a set V of six entities, called the universe of entities. We represent them in six tuples; see Table 2.1. This relation consists of two attributes, F and G, of type integer and string respectively v4 v6 Table 2.1 baz foo bar baz A bitmap index for the fmt attribute, F, would have three bit vectors. The fust, for value 30, is 110001, because the first second, and sixth tuple have F = 30. The other two, for 40 and 50, respectively, are 001010 and 000100, A bmap index for second attribute, G, would also have three bit-vectors 100100, 010010, and 001001; see middle column of Figure 2.1 Fi e2.1 30 = 110001 40 =001010 50 =000100 Foo =100100 Bar =010010 Baz =001001 Next, we will interpret the bit in terms of set theory. A bit vector is a representation of a subset of V. We will refer this subset as an elementary granule [5], and the attribute value the name of the elementary granule; Previously, we have called it elementary neighborhood [8], [9]. The bit vector 110001, of F = 30 says that the fust, second and sixth entities are selected, that is, the bit vector represents the elementary granule, {el, e2, e6 vector, 001010, of F = 40 represents the elementary granule e3, e5 Figure 2.1 gives partition F and G Using Figure 2.1, we replace attribute values of Table 2.1 by their respective bit vectors or elementary granules, we get Table 2.2 \(Bit Table GDM Both Bit Table and GDM are information tables of granules. In other words, their attribute values are granules equivalence class standard subset notations. For computation, we will use Bit Table, while conceptual discussions, we will use subset notation, because subset notations give better mental pictures for granules Table 2.3 Granular Data Model \(GDM el ,  e4 e2, e5 el ,  e4 e2, e5 e3, e6 We will denote the GDM by \(V, \(F, G language, we may use GDM for both bit and set notations As the granule in GDM has no symbols \(Foo, Bar, Baz, 30.40 


As the granule in GDM has no symbols \(Foo, Bar, Baz, 30.40 50 bear no human interpretations, so it is an information table with un-interpreted attribute values; we will refer to it as un interpreted information table 111. SYNTACTIC NAKIRE OF DATA AND PATIFRNS A Syntactic Properties of Relational Tables From DDM point of view, an information table is a table of symbols stored in computer system. No human perceived semantics of these symbols are implemented \(stored symbols, foo, bar, baz, 30, 40, 50 in Table 2.1, similarly, the symbols \(110001 OOlOlO 001001 el, e2, e6}, {e3, e5}, . . . ,{e3, e6 symbols, no human perceived semantics of these symbols are stored. So it is easy to see the followings Proposition. Three information tables are isomorphic Table 2 . k  Table 2.2 ETable 2.3 in the sense one table may convert to another by replacing one set of symbols, foo, . . .,50 by their respective bitmaps 1 lOOOl 001001 e3, e6 From DDM view, three tables are exactly the same, they will produce the same set of pattems, namely, we have the following [5 Theorem. Isomorphic information tables \(bag relations isomorphic pattems 58 This theorem implies that we can do data mining on either original table or GDM B. \(Un The symbols, foo, bar, baz, 30,40, 50 in Table 2.1 are the typical data in DDM; in AI, they are called semantic primitive which means We should stress that those names are given by human, so cannot be automated. This explains why we have been only able to fmd un-interpreted pattems. We will formalize the interpretations B. A Theory of Un-interpreted atfributes Recall that A ={A  A*, . . ., A   Boolean algebra. Previously we note that each attribute induces an equivalence relation, the join and meet operations of this lattice comespond to the intersection and mion of equivalence relations \(partitions 1 undefined considerations, ne power set 2  forms a lattice, in fact a notion human, One implemented in the system 2  semantics  refer to the interpretations by such interpretation are not The symbols, foo, bar, . . . ,50 of Table 2.1. and their formulas have  apparent semantics  to human. We call them interpreted symbols, or expressions. The association rules found in TABLE 2.1 are interpreted pattems On the other hands, the expressions \(110001 OOlOOl and {el, e2, e6  apparent semantics  as foo and 50 do. We call the bitmaps, granules and their respective formulas un-intetpreted symbols and formula The pattems found in GDMare un-interpretedpattems. It needs a special translation table, such as Figure 2.1, to support the interpretations. We would like to stress here that the capability of Figure 2.1. is very limited; it only do token interpretation. In this paper, we will extend the the translation beyond the Figure 2.1 IV. AlTIUBUTES, PARTITIONS AND GENERALIZED PAlTERNS Most of the results are recalling from [ 5 ] ,  but from different prospects. An attribute is also called a feature; we will use them interchangeably A.  Understanding the limits OfAtfributes in DDM The  theoretical  intent of relational schema is to  define   


The  theoretical  intent of relational schema is to  define   the semantics \(of a table However, the system has never been able to implement it even a good approximation Suppose there is an attribute, COLOR, in a relational schema. The intent of creating such an attribute is to represent what human think about the concept of COLOR. So the values are, of course  yellow    blue    orange  and etc. However DDM is looking in reverse way Could the set of such values in the systems  define  the concept of COLOR the way human haveperceived The answer is an obvious  No  The question is: What is an attribute in DDM? In [5 ] ,  we concluded an attribute and its values are a named partition and namedgranules Gist from the intuition Let n\(v intersection of equivalence relations and meet is the  union   where the  union  is the smallest coarsening of its components. As we have observed earlier that an attribute induces a partition \(equivalence relation Lee  s notation, he, the set of induced partitions of 2A is called a relation lattice and observed that 1. The join operator in he \(induced from 2A from that of n\(V 2. So he is a subset, but not a sublattice, of n o Such an embedding is an unnatural one, but Lee focused his efforts on it. We have, instead, taken the natural embedding The smallest lattice generated by hI3 is called the \(Lin  s relation lattice and denoted by L\(Q  Q2 Q    A2 An a partition interpretation \(derived from A ={A  A2, ..., A   Unfortunately, not every element in L\(Q The difference between L\(Q contains all the join of distinct attributes. Many such joins have not been interpreted by human, so they are un-interpreted attributes. We need a formal language that can generate logic formula based on the terms given in the original table \(e,g Table 2.1 The pair \(V, L\(Q Lin  s Definition The smallest lattice containing all the coarsening of L\(Q by L*\(e C. DerivedAtfributes - Feature Constructions Theorem 5.1. \(V, L*\(Q attributes of \(V, Q constructions 59 Next few theorems have reported in ICDMOZ foundation of data mining workshop [6], [7]. lo terms of current language they are un-interpreted patterns Theorem 5.2. Any granule G in a partition P E L\(Q cardinality exceeds the support is an un-interpreted generalized pattems \(generalized undirected association rules Theorem 5.3. Let Lo be the smallest element in L*\(Q possible unions of the Lo-granules that meet the threshold is an un-interpreted generalized pattem \(generalized association rules Let us set up some notations. Let the variables x,. x2, x3, xa take either zero or one. Let us defme an operation of binary number x and a set S. We write S*x to be defmed by The two equations S*x=S, ifx=l S * x = 0 ,  ifx=0 Every element of L*\(Q every granule in C is a union of some G-granules \(granules in G cardinality of the set . Then the following expression represents the cardinality of all possible unions of G-granules 


represents the cardinality of all possible unions of G-granules thatare 2 threshold Theorem 5.4. Then the following union Cl*Xl U . . . U c,* x is a granule that represents a un-interpreted \(undirected association rule, if its cardinality  where s is the threshold Remark The cardinal number of L*\(Q number [l] of \(GI, where G is the smallest partition and IC/ is the cardinality of the partition. The total number of derived attributes is very large. However the complexity of too high. Let s be the threshold and \(GI=g, then the possible minimal solutions" is bounded by the combination gCs . We will report the calculation on real world data in future report soon V DECISION LOGIC @L In association rule mining, the only pattems that have been mined are sub-tuples. If we want to mine complex patterns there are needs for a language to express them. We decide to borrow decision logic for this purpose [15 A. The Syntax I a these names are meaningful to human, but they are primitive in the formal system;, they are not defmed by other notions in the formal system b A A; each attribute value is very meaningful to human hut formally, it is a primitive d negation, and, or, implication, equivalence connective represents usual logical operators 2 The smallest set satisfies the following a called atomic formulas any A' E A and v E dom\(A b cp A q cp v q cp + q rp cp C\(cp q cp q cpW\( q A are lattice operations B. The Semantic We will denote v I= cp or v I= cp when V is understood, if an object v E V satisfies a formula cp in V. So we will say v I= cp, iff 1 V v \(= &lt;A, v&gt; iff p\(v, A v v I= \(9 A q v I= \(cp v q We have many usual formulas, such as v \(= \(9 + q We associate the formula cp, the following set lcpl = \( v : v E V a n d v I =  c p } .  V V It is called the meaning of cp. A formula is said to be hue if IcpI = V; cp is logically equivalenf to q iff their m e a h g s  are V the same, i.e., Icp I v  = 1qIv Using GDM's terminology, we have  V&gt 60 I9 +9Iv =-191v "I'II I -@ I v  = - I9 Iv In [9], we approach the DL-language informally we "quote here the general form Theorem 1. There is a one-to-one correspondence between each column of a relational table and a partition of the universe V 2. Each attribute value is defined by one and only one elementary granule \(equivalence class 3. A logical formulas of attribute values is defined by and only 


by a set theoretical relationship among elementary granules of all columns C. The Deductive System ofDecision Logic Modus ponens is the only rule 1 2 1 2 a E V a n d v S u b Aj A' E A c lt; Ai, U&gt; : for every U E dom\(A1 and every A' E A, v # U We need few auxiliary notations and results: Let 0 and 1 denote falsity and truth Formula of the form lt;A1, vl&gt; A &lt;A2, v2&gt; A,. . ..&lt; A", v.&gt is called P-basic formula or P-formula, where vI E dom \(A and P E A. The specific Axiom \(a assumption that each entity can have exact one value in each attribute. The Axiom \(h must be taken once. This is saying that dom\(A domain of attribute A. The Axiom \(c the negation in such a way that instead of saying that an object does not possesses a given property we can say that it has one of the remaining properties. It implies the closed word assumption. Let Zv \(P P disjunction of all P-basic formulas satisfied in V. The closed word assumption can he express in the following Proposition I= Z \(P Note that commercial DBMS usually have this assumption For example, the output of not red colour consists of all non red colours. A formula is a theorem, denoted by I- 9 , if it is v v derivable from the axioms. The set of theorems of decision logic is identical with the set of theorems of propositional calculus with specific axioms \(a c VI. GENERALIZED PATTERNS AND INTERPRETATIONS In last section, we have several theorems on un-interpreted pattems. We need some formal way that can interpret them in terms of the symbols \(semantic primitives table \(e,g, Table 2.1 Let us first recall a theorem and comments from [9]; The formal theorem was referred to single column representation hut it also had commented that "One should note that this theorem is valid, even when we consider a collection of partitions." So the theorem is valid for general cases; so we quote" the general form here Theorem 6.1 1. There is a one-to-one correspondence between each column of a relational table and a partition of the universe V 2. Each attribute value is defined by one and only one elementary granule \(equivalence class 3. A logical formulas of attribute values is defmed by and only by a set theoretical relationship among elementary granules of all columns In [9],  we refer these granules as machine semantics of attribute values and logical formulas respectively. Let us interpret the un-interpreted \(undirected developed in Theorem 5.4. Each C,, Cz . . . ,C, is a granule of G=Q'nQ2n. . .nQ" . So, in terms of elementary granules of \(Q', Q',. . .,Q"}, we have Name\(Ci gi g g So we have the interpretations of Theorem 5.4 as Theorem 6.2 Then the un-interpreted patterns C]*Xl U . . . U I&amp;* x has the following interpretation \(a formula of DL Name\(CI C2 C,J* x where Name\(Cj gjl g2 go the Name The last formula is a formula of decision logic [15] and Cj is 


The last formula is a formula of decision logic [15] and Cj is referred to as the meaning of its name J J 61 VU. CONCLUSIONS Data, pattems and interpretations are three important ingredients in \(undirected paper, we formalize the current state of database mining: Data are the bare data; it is a table of symbols. The pattems are the repeated data. The results are somewhat surprising 1. Patterns are properties of the isomorphic class, not an individual relation - This implies that the notion of pattems have not matured yet, also explains why there are so many extracted association rules. In fact, many of them have no real world meanings We have the following two striking results 2. All possible attributes \(features 3. Generalized associations can be found by solving integral linear inequalities Unfortunately, the computation cost is enormous. This may signifies the current notion of data and pattems \(implied by the algorithms 4. Decision logic can be viewed as a systematic buman interpretation of data In the current state of arts in association rule mining, a pattem is simply a repeated data that may have no real world meaning So some semantic oriented data mining may have to be explored [9], [IO], [ I l l REFERENCES I] Richard A. Brualdi, Introductory Combinatorics, Prentice Hall, 1992 2] John E. Freund, Modem Elemen- Statistics, Prentice Hall, 1952 3] Hector Garcia-Molina, JeErey D. Ullman. Jennifer Widom, Database Systems-The Complete Book, Prentice Hall, 2002 4] T. T. Lee  Algebraic Theory of Relational Databases   The Bell System Technical Joumal Vol 62, No 10 December, 1983, pp.3159-3204 5 ]  T. Y. Lin  Attribute \(Feature Attributes from Data Mining Prospect  in: the Proceedings of Intemational Conference on Data Mining, Maebashi Japan, Dec 9-12,2002, pp.282-289 6] T. Y. Lin  Mathematical Foundation of Association Rules--Mining Associations by Solving Integral Linear Inequalities  In: the Proceedings of the Workshop on the Foundation of Data Mining and Discovery, which is a part of Intemational Conference on Data Mining, Maebashi Japan, Dec 9-12,2002, pp 81-88 Foundation of Database Mining  in: the Proceedings of 9   Intemational Conference, RSFDGrC 2003, Chongqing China, May 2003, Lecture Notes on Artificial Intelligence LNAI 2639, Springer-Verlag, 403-405 Approach  In: Methodologies for Knowledge Discovery and Data Mining, Lecture Notes in Artificial Intelligence 1574, Third Pacific-Asia Conference \(PAKDD1999 Beijing, April 26-28, 1999,24-33 9] Tsau Young Lin  Data Mining and Machine Oriented Modeling: A Granular Computing Approach  Joumal of Applied Intelligence, Kluwer, Vol. 13, No 2, September October,2000, pp.113-124 Approach  In: Methodologies for Knowledge Discovery and Data Mining, Lecture Notes in Artificial Intelligence 1574, Third Pacific-Asia Conference, Beijing, April 26 28,1999,24-33 I I] T. Y. Lin  Granular Computing on Binary Relations I Data Mining and Neighborhood Systems  In: Rough Sets In Knowledge Discovery, A. Skowom and L. Polkowski eds 121 T. Y. Lin  The Power and Limit of Neural Networks   


121 T. Y. Lin  The Power and Limit of Neural Networks   Proceedings of the 1996 Engineering Systems Design and Analysis Conference, Montpellier, France, July 1-4, 1996 Vol. 7, 49-53 13] T. Y. Lin and N. Cercone, Rough Sets and Data Mining Analysis of Imprecise Data, T. Y. Lm and N. Cercone eds 2nd print Foundation of Database Mining  in: the Proceedings of 9   International Conference, RSFDGrC 2003, Chongqing China, May 2003, Lecture Notes on Artificial Intelligence LNAI 2639, Springer-Verlag, 403-405 15]. Z. Pawlak, Rough sets. Theoretical Aspects of Reasoning about Data, Kluwer Academic Publishers, 1991 I61 A. Barr and E.A. Feigenbaum, The handbook of Artificial Intelligence, Willam Kauhann 1981 7] T. Y. Lin  Deductive Data Mining: Mathematical 8] Tsau Young Lin  Data Mining: Granular Computing IO] T. Y. Lin  Data Mining: Granular Computing 14] T. Y. Lin  Deductive Data Mining: Mathematical 62 pre></body></html 


100 34 33 34 DSD u u u                          u             Fig.6: Over Hiding problem of setting  1 in S No matter the left-hand or right-hand equation, the support of {1, 2} in D' is 0. That is, item 1 and item 2 never appear toge ther, and they are mutual exclusive! This situation almost never happens in the normal database. The attackers may interest in this situation and infer that {1, 2} is hidden deliberately. To hide the sensitive patterns, only need to make their supports smaller than minimum support and need not to decrease their support to 0. To solve the problem, we inject a probability ? which is called Distortion probability into this approach. Distortion probability is used only when the column j of the sanitization matrix S contains only one  1  i.e. Sjj = 1 0 1 d   m k k j i k  S D  m j n i j i  d d d d   1  1     D  i j  h a s   j probability to be set to 1 and 1  j probability to be set to 0 Lemma 1: Given a minimum support ?, and a level of confidence c. Let {i, j} be a pattern in Marked-Set, nij be the support count of {i, j}. ? is the Distortion probability of column j Without loss of generality, we assume that Sij  1. If ? satisfies    D n i j  u  u  V U   a n d    


   c x n xijnx D x ij t          u    1 0 UU V where D| is the number of transactions in D, we can say that we are c confident that {i, j} isn  t frequent in D Proof: If probability policies are not considered and Sij  1, that is, while a row \(transaction according to the matrix multiplication discussed in the previous section, D'tj will be set to 0. We can view the nij original jtiD jtjtjt ijn DDD 21     f o r  e a c h  r o w  t i  c o n t a i n s   i   j   i n  D  a s  realizations of nij independent identically distributed \(i.i.d om variables ijn X X X      2 1      e a c h  w i t h  t h e  s a m e  d i s t r i b u  tion as Bernoulli distribution, B \(1 X i    1  a n d  t h e  f a i l u r e  i s  d e f i n e d  a s  X i    0     i    1  t o  n i j   T h e  p r o b  ability ? is attached to the success outcome and 1?? is attached to the failure outcome. Let X be a random variable and could be viewed as the number of transactions which include {i, j} in D such that jin     2 1    T h u s   t h e  d i s t r i b u t i o n  o f  X is the same as Binomial distribution, X?B \(nij              otherwise nx x n XP ij xnxij x ij 0 1,0     U the mean of X = nij?, and the variance of X = nij?\(1 If we want to hide {i, j} in D', we must let its support in D' be smaller than ?. Therefore, the expected value of X must be s m a l l e r  t h a n     D u V    T h a t  i s     m u s t  s a t i s f y     D n i j  u  u  V U   Moreover, the probability of the number of success which is 


Moreover, the probability of the number of success which is equal to or smaller than     1     u  D V   s h o u l d  b e  h i g h e r  t h a n  c  Therefore, U must satisfy    c x n xijnx D x ij t           u    1 0 UU V  If ? satisfies the two equations, we can say that we are c confident that {i, j} isn  t frequent in D When nij &gt; 30, the Central Limit Theorem \(C.L.T used to reduce the complexity of the equation and speed up the execution time of the sanitization process Moreover, if several entries in column j of S are equal to  1 such as Sij  1, Skj  1, Smj  1  etc, several candidate Dist ortion probabilities such as iU , kU , mU , etc are gotten. The Dis tortion probability of column j, ? j, is set to the minimal candid ate Distortion probability to guarantee that all corresponding pair-subpatterns have at least c confident to be hidden in D 3.3. Sanitization Algorithm Fig.7: Sanitization Algorithm According to the probability policies discussed above, the sanitization algorithm D'n  m = Dn  m  Sm  m is showed in Fig.7 Notice that, if the sanitization process causes all the entries in row r of D' equal to 0, randomly choose an entry from some j Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE where Drj = 1, and set D'rj = 1. It is to guarantee that the size of the sanitized database equals the size of the original database 4. Performance Evaluation 4.1. Performance Quantifying There are three potential errors in the problem. First of all some sensitive patterns are hidden unsuccessfully. Secondly some non-sensitive patterns cannot be mined from D'. And the third, new artificial patterns may be produced. However, the third condition never happens in both of our approach and SWA Besides, we also introduce the other two criteria, dissimilarity and weakness. All of the criteria are discussed as follows Criterion 1: some sensitive patterns are frequent in D'. This condition is denoted as Hiding Failure [10], and measured by     DP DP HF H H , where XPH represents the number of patterns contained in PHwhich is mined from database X Criterion 2: some non-sensitive patterns are hidden in D'. It is also denoted as Misses Cost [10], and measured by      DP DPDP MC 


MC H H H      w h e r e        X P H  r e p r e s s ents the number of patterns contained in ~PH which is mined from database X Criterion 3: the dissimilarity between the original and the sanitized database is also concerned, and it is measured by      n i m j ij n i m j ijij D DD Dis 1 1 1 1    Criterion 4: according to the previous section, Forward Inference Attack is avoided while at least one pair-subpattern of a sensitive pattern is hidden. The attack is quantified by        DPDP DPairSDPDP Weakness HH HH    where DPairS is the set of sensitive patterns whose pair subsets can be completely mined from D 4.2. Experimental Results Table 1: Experiment factor The experiment is to compare the performance between our approach and SWA which has been compare with Algo2a [4 and IGA [10] and is so far the algorithm with best performances published and presented in [12] as we know. The IBM synthetic data generator is used to generate experimental data. The dataset contains 1000 different items, with 100K transactions where the average length of each transaction is 15 items. The Apriori algorithm with minimum support = 1% is used to mine the dataset and 52964 frequent patterns are gotten Several sensitive patterns are randomly selected from the frequent patterns with length two to three items to be seeds With the several seeds, all of their supersets are included into the sensitive patterns set since any pattern which contains sensitive patterns should also be sensitive 0 0.05 0.1 0.15 0.2 0.25 0.3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern H id 


id in g F ai lu re SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern W e ak n es s SP SWA Fig.8: Rel. bet. RS and HF. Fig.9: Rel. bet. RS and weakness Fig.8, Fig.9, Fig.10, Fig.11 and Fig.12 show the effect of RS by comparing our work to SWA. Refer to Fig.8, because the level of confidence in our sanitization process takes the minim um support into account, no matter how the distribution of the sensitive patterns, we still are C confident to avoid hiding failure problems. However, there is no correlation between the disclos ure threshold in SWA and the minimum support. Under the sa me disclosure threshold, if the frequencies of the sensitive patte rns are high, the hiding failure will get high too. In Fig.9, beca use our work is to hide the sensitive patterns by decreasing the supports of the pair-subpatterns of the sensitive patterns, the val ue of weakness is related to the level of confidence. However according to the disclosure threshold of SWA, when all the pair subpatterns of the sensitive patterns have large frequencies, it may cause serious F-I Attack problems. Hiding failure and wea kness of SWA change with the distributions of sensitive patterns 0 0.1 0.2 0.3 0.4 0.5 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern M is se s C o st SP SWA 0 0.005 0.01 0.015 0.02 0.025 0.03 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern D is si m 


m il ar it y SP SWA Fig.10: Rel. bet. RS and MC.  Fig.11: Rel. bet. RS and Dis In Fig.10 and Fig.11, ideally, the misses cost and dissimilar ity increase as the RS increases in our work and SWA. However if the sensitive pattern set is composed of too many seeds, a lot of victim pair-subpatterns will be contained in Marked-Set. And as a result, cause a higher misses cost, such as x = 0.04 in Fig.10 Moreover, refer to the turning points of SWA under x = 0.0855 to 0.1006 in Fig.10 and Fig.11. The reason of violation is that the result of the experiment has strong correlation with the distri bution of the sensitive patterns. Because the sensitive patterns Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE are chosen randomly, several variant factors of the sensitive patt erns are not under control such as the frequencies of the sensiti ve patterns and the overlap between the sensitive patterns if the overlap between the sensitive patterns is high, some sensitive patterns can be hidden by removing a common item in SWA Therefore, decrease the misses cost and the dissimilarity 0 0.5 1 1.5 2 2.5 3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern E x ec u ti o n T im e h  SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set H id in g F a il u re SP SWA 


SWA Fig.12: Rel. bet. RS and time. Fig.13: Rel. bet. RL2 and HF Fig.12 shows the execution time of SWA and our approach As shown in the result, the execution time of SWA increases as RS increases. On the other hand, our approach can be separated roughly into two parts, one is to get Marked-Set which is strong dependent on the data, the other is to set sanitization matrix and execute multiplication whose execution time is dependent on the numbers of transactions and items in the database. In the experi ment, because the items and transactions are fixed, therefore, the execution time of our approach is decided by the setting of Marked-Set which makes the execution time changing slightly 0 0.05 0.1 0.15 0.2 0.25 0.3 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set W ea k n es s SP SWA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set M is se s C o st SP SWA Fig.14: Rel. bet. RL2 and weakness. Fig.15: Rel. bet.RL2 and MC 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set D is si m il a ri ty SP SWA Fig.16: Rel. bet. RL2 and Dis Fig.13, Fig.14, Fig.15and Fig.16 show the effect of RL2 Refer to Fig.13 and Fig.14, our work outperforms SWA no matter what RL2 is. And our process is almost 0% hiding failure 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


