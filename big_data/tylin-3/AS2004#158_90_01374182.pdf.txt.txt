html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">A Scalable  Algorithm for Mining Maximal Frequent Sequences Using Sampling Congnan Luo and Soon M. Chung Dept. of Computer Science and Engineering Wright State University Dayton, Ohio 45435, USA Abstract In this paper, we propose an ef?cient scalable algorithm for mining Maximal Sequential Patterns using Sampling MSPS space than other algorithms because both the subsequence infrequency based pruning and the supersequence frequency based pruning are applied. InMSPS, sampling technique is used to identify long frequent sequences earlier, instead of enumerating all their subsequences. We propose how to adjust the user-speci?ed minimum support level for mining a sample of the database to achieve better performance. This method makes sampling more ef?cient when the minimum support is small. A signature technique is utilized for the subsequence infrequency based pruning when the seed set of frequent sequences for the candidate generation is too big to be loaded into memory. A pre?x tree structure is developed to count the candidate sequences of different sizes during the database scanning, and it also facilitates the customer sequence trimming. Our experiments showed MSPS has very good performance and better scalability than other algorithms 1 Introduction Mining sequential patterns from large databases is an important problem in data mining. With numerous practical applications such as consumer market-basket data analysis and web-log analysis, it has become an active research topic. Since it was introduced in [2], many algorithms have been proposed, but most of them are to discover the full set of frequent sequences In pure bottom-up, breadth-?rst search algorithms such as GSP [8] and PSP [6], only subsequence infrequency based pruning is used to reduce the number of candidate sequences. So, if a sequence with length l is frequent, all of its 2l subsequences must be enumerated ?rst. Thus, if This research was supported in part by Ohio Board of Regents, LexisNexis, NCR, and AFRL/Wright Brothers Institute \(WBI some frequent sequences are long, the overhead of enumerating all of their subsequences is so much that mining the full set of frequent sequences is impractical. An alternative approach is mining only the maximal frequent sequences. A frequent sequence is maximal if none of its supersequence is frequent. Mining only the the set of maximal frequent sequences \(MFS reduced a lot by using the supersequence frequency based pruning. In interactive data mining, after mining the set of maximal frequent sequences quickly, we can selectively count the interesting patterns subsumed by this set by scanning the database just once For the association rule mining, many ef?cient algorithms were proposed to mine maximal frequent itemsets 5]. However, differences between the two kinds of mining make those algorithms very dif?cult or impossible to be applied for the maximal frequent sequence mining. A critical question for the maximal frequent sequence mining is how to look ahead for longer or maximal frequent sequences at a reasonable cost. If the look-ahead is not performed effectively, its cost can offset the gain from the supersequence frequency based pruning, like the cases of AprioriSome and DynamicSome algorithms [2 In this research, we combined the Apriori candidate generation method [1, 2, 8] and the supersequence frequency pruning for mining maximal frequent sequences. This is achieved by using a sampling technique. The main search strategy of MSPS is bottom-up and breadth-?rst. But after the pass 2 over the database, we mine a small random sam 


the pass 2 over the database, we mine a small random sample database ?rst starting with the candidate 3-sequences i.e., sequences of 3 items quent 2-sequences. The local maximal frequent sequences that are found from the the sample database starting with the global candidate 3-sequences, are veri?ed in a top-down fashion against the original database, so that we can ef?ciently collect the longest frequent sequences covered by them. Then, the bottom-up search is resumed from the pass 3, and supersequence frequency based pruning is applied at each pass The main contributions of this research are: 1 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE MSPS algorithm and optimization methods were developed for mining maximal frequent sequences. MSPS outperforms GSP considerably, and it also shows better scalability than SPAM [3] and SPADE [11]. 2 to apply the sampling technique for sequence mining was studied thoroughly. In association rule mining using sampling, lowering the user-speci?ed minimum support \(denoted by minsup ple to guarantee no misses \(i.e., false negatives case of sequence mining, where the search space is much larger, if the user-speci?ed minsup is small, simply using this minsup or a lowered one often misleads the mining on a sample, such that the cost of mining the sample itself and verifying the sample results could be too high. For MSPS we proposed a theoretic method of adjusting the small userspeci?ed minsup to avoid this problem The rest of the paper is organized as follows. Section 2 introduces the basic concepts of sequence mining. Section 3 reviews some related works on sequence mining and sampling. Section 4 describes the MSPS algorithm. The experimental results and performance analyses are presented in Section 5, and Section 6 contains some conclusions 2 Sequence Mining Let I = {i1, i2, . . . , in} be a set of items. An k-itemset i is a set of k items denoted by {im1 , im2 , . . . , imk}, where 1 ? m1 &lt; m2 &lt; . . . &lt; mk ? n. A sequence s is an ordered list of itemsets denoted by &lt; s1, s2, . . . , sk &gt where each si, 1 ? i ? k, is an itemset. A sequence sa =&lt; a1, a2, . . . , ap &gt; is contained in another sequence sb =&lt; b1, b2, . . . , bq &gt; if there exist integers 1 ? j1 &lt j2 &lt; . . . &lt; jp ? q such that a1 ? bj1 , a2 ? bj2 , . . . , ap bjp . If sa is contained in sb, sa is a subsequence of sb and sb is a supersequence of sa. An item may appear at most once in an itemset, but it may appear multiple times in different itemsets of a sequence. If there are k items in a sequence, the length of the sequence is k, and we call it a ksequence. For example, a 3-sequence &lt; {A}, {B,C} &gt; is a subsequence of a 5-sequence &lt; {C}, {A,D}, {B,C} &gt For simplicity, these two sequences can be represented as A?BC and C ?AD ?BC Given a databaseD of customer transactions, each transaction consists of a customer-id, transaction-time and an itemset which includes all the items purchased by the customer in that single transaction. All the transactions of a customer can be viewed as a customer sequence, where these transactions are ordered by their transaction times We denote a customer sequence t as &lt; T1, T2, . . . , Tm &gt which means the customer has m transactions in the database and each transaction Ti, 1 ? i ? m, contains all the items purchased in that transaction. A customer supports a sequence if the sequence is contained by the customer sequence. The support for a sequence in database D is de?ned as the fraction of total customers who support this sequence. Given a user-speci?ed minimum support, denoted by minsup, a sequence is frequent if its support is greater than or equal to minsup. The problem of sequence 


greater than or equal to minsup. The problem of sequence mining is to ?nd all the frequent sequences in the database with respect to a user-speci?ed minsup. If a sequence is frequent and none of its supersequences is frequent, then it is a maximal frequent sequence Based on the above de?nitions, two properties are often utilized to speed up the sequence mining: 1 quence of an infrequent sequence is not frequent, so it can be pruned from the set of candidates. This is called subsequence infrequency based pruning. 2 of a frequent sequence is also frequent, so it can be pruned from the set of candidates. This is called supersequence frequency based pruning In [8], the above de?nition of sequence mining was generalized by incorporating time constraints, sliding time windows, and taxonomy. This generalization makes the sequence mining more complex. For example, a sequence A?BC ?D ?GH is frequent does not necessarily mean that its subsequence A ? BC ? GH is also frequent, because the subsequence may not satisfy the time constraints In this research, we consider the nongeneralized sequential pattern discovery 3 Related Work Mining sequential patterns was introduced in [2] with AprioriAll, AprioriSome and DynamicSome algorithms Although AprioriSome and DynamicSome try to generate and count long candidate sequences before enumerating all their subsequences, their performance is usually worse than that of AprioriAll. The reason is too many false candidates are generated without being pruned by the subsequence infrequency based pruning. The performance gain from the supersequence frequency based pruning is not enough to offset the cost of counting so many false candidates GSP [8] was proposed for generalized sequence mining GSP requires multiple passes on the database. At pass k the set of candidate k-sequences are counted against the database and frequent k-sequences are determined. Then the candidate \(k + 1 frequent k-sequences for the next pass. This process will continue until no candidate is generated. Even though GSP is much faster than AprioriAll, it has a very high overhead of enumerating every single frequent subsequence when there are some long patterns. This is also the main weakness of other Apriori-like algorithms, such as PSP [6]. For PSP, a pre?x tree was developed as the internal data structure to organize and count candidates more ef?ciently SPADE [11] works on the databases with a vertical idProceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE list format, where a list of \(customer-id, transaction-time pairs are associated with each item, and the candidates are counted by intersecting the id-lists. A lattice-theoretic approach is used to decompose the search space into small pieces so that all working id-lists can be loaded into memory. Pre?xSpan [7] projects a large sequence database recursively into a set of small post?x subsequence databases based on the currently mined frequent pre?x subsequences Then, the subsequent mining is con?ned to each small projected database. A memory-based pseudo-projection technique is developed to save the computation cost of projection and the memory space for projected databases. SPAM 3] uses a vertical bitmap representation of the database for candidate generation and counting. A bitmap is created for each item in the database, where each bit corresponds to a transaction. If transaction j contains item i, then bit j in the bitmap for item i is set to 1; otherwise, it is set to 0 SPAM also uses a depth-?rst traversal of the Lexicographic sequence tree and an Apriori-based pruning of candidates These three algorithms \(SPADE, Pre?xSpan, and SPAM 


SPAM their performance may not be scalable in certain cases. For SPADE, if the database is in the horizontal format, where the transactions form the tuples in the database, transforming it to a vertical one requires extra disk space with roughly the same size. This may be a problem in practice if the database is large. Even if the database is in the vertical format, to ef?ciently count 2-sequences, SPADE proposes transforming it back to the horizontal one on the ?y This usually requires much time and memory for very large databases and results in performance degradation Pre?xSpan may be challenged when the database has a large number of customer sequences and items. A large number of items often produce many combinations at the early stage of mining, and it requires Pre?xSpan to construct more projected databases. If the database is very large, the cost of projection will be high and much more memory is necessary. SPAM is claimed to be a memorybased algorithm. According to our tests, its scalability is much more sensitive to the number of items and the database size than other algorithms. The comparison between MSPS, GSP, SPADE and SPAM is presented in detail in the performance analysis section In [10], sampling was evaluated as an ef?cient way to mine an approximate set of frequent itemsets. In [4], the FAST algorithm mines a large sample ?rst to accurately estimate the support of 1-itemsets, and then progressively re?ne the initial sample to obtain a small ?nal sample. FAST reports the set of frequent itemsets in the ?nal sample as the results. Our research is more related to [9] because both try to speed up the mining of the exact ?nal result using sampling. In [9], a lowered minsup is used to mine the sample, so that the probability a frequent itemset is missed from the sample result would be small. Then, one more database scan is needed to ?nd the misses and the overestimates. In 9], the focus was mainly on how to make the sample result include all frequent itemsets without missing. On the other hand, our main goal is not the sample result itself, but obtaining some knowledge that can speed up the mining of exact ?nal result. Thus, the misses are allowed to some extent. In practice, the cost for avoiding the misses can be a concern. Therefore, a critical question is how much sampling can speed up the mining of the exact result, and we explored this question from the point of balancing the cost and the gain of sampling 4 MSPS Algorithm Like GSP, MSPS also uses the candidate generation then counting approach to perform the mining, but the performance is improved very much by combining the supersequence frequency based pruning into its bottom-up breadth-?rst search. It has the following original components: 1 tial subsequence infrequency based pruning when the set of frequent k-sequences is too big to be loaded into memory totally for the generation of candidate \(k + 1 2 tree structure is developed, and it also facilitates the customer sequence trimming. 3 frequency based pruning, sampling is used to ?nd long frequent patterns early. 4 and robust in sequence mining, a theoretic method of adjusting the small user-speci?ed minsup for mining the sample database was proposed In this section, we ?rst present an overview of MSPS and then describe the candidate generation and pruning, candidate counting and sampling in detail. The following notations will be used in our description: DB is the original database and db is a small random sample of DB. If a sequence is frequent in DB, it is called a global frequent sequence. LDBk is the set of all global frequent k-sequences and CDBk is the set of all candidate k-sequences generated from LDB\(k?1 


from LDB\(k?1 DB is the set of all global maximal frequent sequences. If a sequence is frequent in the sample db we call it a local frequent sequence. Ldbk , Cdbk , and MFSdb are the sets of all local frequent k-sequences, candidate ksequences and maximal frequent sequences in the sample respectively 4.1 Description of MSPS The basic idea of MSPS is simple: if some long frequent patterns are found early, they can be used to prune the search space so that the mining can speed up. To ?nd long frequent patterns, a small sample db is mined ?rst. We must balance Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE the gain from the supersequence frequency based pruning and the cost for mining the sample and then verifying the sample result. MSPS consists of three phases Phase 1: LDB1 and LDB2 are determined. Candidate 3sequences are generated from LDB2 . To count candidate 2-sequences, a two-dimensional array is used The entry at position \(i, j array contains the counts of three candidates i ? j, ij and j ? i Phase 2: A random sample is drawn from DB, then how much the user-speci?ed minsup should be adjusted for mining the sample is determined. We mine the sample starting with CDB3 in a bottom-up, breadth-?rst manner. The local maximal frequent sequences are extracted to construct MFSdb. Then we perform a topdown search for long global frequent sequences from MFSdb. All those sequences in MFSdb are considered as global candidates and counted against DB. If a k-sequence, \(k &gt; 3 k? 1 subsequences are considered as candidates for the next pass. For a frequent k-sequence, we stop splitting it and put it into the set LongFSDB if none of its supersequences is already in this set. For a newly generated candidate \(k?1 in LongFSDB, we remove it from further consideration. We also check if the newly generated candidate k?1 identi?ed as infrequent. If yes, this candidate \(k ? 1 sequence must be split again Phase 3: The bottom-up search suspended at the end of Phase 1 is resumed from pass 3. With LongFSDB we can apply the supersequence frequency based pruning on the candidates generated at each pass. The candidates which appear in LongFSDB or have any supersequence in LongFSDB don  t need to be counted They are simply considered as frequent and used for the candidate generation for the next pass. Finally MFSDB is extracted from all global frequent sequences found 4.2 Candidate Generation and Pruning Both in Phases 2 and 3, we have performed the bottomup, breadth-?rst search on the sample and the original database, respectively. At pass k, the candidates are generated in two steps Join Step: we generate local \(global k + 1 sequences by joining Ldbk with Ldbk \(LDBk with LDBk as in the GSP algorithm. For any two local \(global frequent k-sequences s1 and s2 in Ldbk \(LDBk subsequence obtained by dropping the ?rst item of s1 is the same as the subsequence obtained by dropping the last item of s2, a new candidate is generated by extending s1 with the last item in s2. The added item starts a new itemset for s1 if it was a separate itemset in s2. Otherwise, it becomes part of the last itemset in s1 Prune Step: In both phases 2 and 3, the subsequence in 


Prune Step: In both phases 2 and 3, the subsequence infrequency based pruning is applied. The local \(global candidate \(k + 1 length k which is not in Ldbk \(LDBk cially in Phase 3, since we have LongFSDB, the supersequence frequency based pruning also can be performed. Thus, we remove global candidate \(k + 1 sequences which are in LongFSDB or have any supersequence in it A weakness of GSP is the way that a large LDBk is processed. When the user-speci?ed minsup is very small, LDBk could be too large to be loaded into memory totally. For this case, GSP proposed using a relational merge-join technique to generate candidates. But in this manner, subsequence infrequency based pruning cannot be applied because the whole LDBk is not available in memory and retrieving the relevant portions of LDBk from a disk requires too frequent swaps. Without subsequence infrequency based pruning usually the performance of GSP degrades a lot. In MSPS we adopted a new method to solve this problem. If some Ldbk in Phase 2 \(LDBk in phase 3 ory, we give each local \(global teger signature which is highly correlated to the content of the sequence. Following is a simple example of the signature, where t is the number of itemsets in the sequence; mi is the number of items in ith itemset; Iij is the jth item in the ith itemset; Ci, 1 ? i ? t, is the weight imposed on the ith itemset; and C0 is the weight imposed on the total number of itemsets C0 ? t t i=1 Ci ?mi mi j=1 Iij All the signatures are sorted and put into an array Compared with the case of loading the whole Ldbk \(LDBk into memory, the signature array requires much less space Thus, we can load working portions of Ldbk \(LDBk the signatures into memory at the same time. When a new candidate \(k+1 k-subsequences are computed and searched in the signature array. If any one of them is not in the array, the candidate should be removed. Since all the signatures are in memory subsequence infrequency pruning can still be applied. It is possible that two or more k-subsequences have the same signature. However, that probability is very low. Our experiments showed that signatures are much more ef?cient than Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE hashing. MSPS performs much better than GSP when the seed set of frequent sequences for generating the candidates cannot be loaded into memory totally at some passes. If the memory cannot hold all the candidates generated, they can be processed part by part 4.3 Counting Candidate Sequences During the top-down search for long patterns covered by MFSdb, to reduce the number of passes, we need to count candidates of different sizes at each pass over the database For that purpose, we developed a new pre?x tree structure Since it is much more ef?cient than the hash tree, we also use it to count the candidates of the same size during the bottom-up search in Phases 2 and 3. We ?rst describe our pre?x tree and the customer sequence trimming technique and then compare it with the pre?x tree used for PSP [6 4.3.1 Overview of the Pre?x Tree and the Customer Sequence Trimming The following example shows how the pre?x tree works Suppose we have 10 candidates of length 2 or 3. The pre?x tree is constructed as shown in Figure 1. Each node is as 


tree is constructed as shown in Figure 1. Each node is associated with a pointer. If the path from the root to a node represents a candidate, the pointer points to the candidate otherwise, it is NULL. A node may have two types of children. The  I-extension  child means the item represented by the child node is in the same itemset with the item represented by its parent node. The  S-extension  child means the item represented by the child node starts a new itemset. All the S-extension \(I-extension linked together, and only the ?rst child is linked to their parent node by a dashed \(solid and 5 are the S-extension children of node 1, and the corresponding pathes represent the candidates A?A and A?E respectively. Nodes 6 and 7 are I-extension children, and their pathes represent AC and AD, respectively To speed up the counting, a bit vector is associated with the pre?x tree to facilitate the customer sequence trimming. In this example, we have 8 items in the database A,B,C,D,E, F, and H . Since B, F , and G do not appear in any candidate, they should be ignored during counting Thus, the bit vector is set as \(10111001 bit position means item i appears in the pre?x tree. All the bits are initialized to 0, and the corresponding bits are set to 1 as we insert candidates into the pre?x tree Given a customer sequence s = ABCD ? ADEFG B?DH , we trim it to s? = ACD?ADE?DH using the bit vector ?rst. Then a recursive method is used to count all the candidates contained in s?. At the root node, we check each item in ACD ?ADE ?DH to see if it is in the root node  s S-extension children. The ?rst item of s? is A, and Candidates 10 9 8 7 6 5 3 2 1 4 ABCD ? ADEFG ? B ?DH Bit Vector \(10111001 1 2 CA 3 D 4 6 7 8 9 10 11 12 13 EA C H A E E A D H Root 5 DH ADE ? DH E ? DH DE ? DH CD ? ADE ?DH ADE ? DH DH D ? ADE ? DH ADE ? DH 


ADE ? DH DH DH DE ? DH Figure 1. Pre?x Tree of MSPS it appears as the ?rst S-extension child of the root node. So we recursively call the count function at the root node with two sequence segments. The segment CD?ADE?DH is used in the call for node 1  s I-extension link, while ADE DH is for its S-extension link. Then, we can locate the second item of s?, C, at node 2. Since node 2 has no Sextension child, only one recursive call with the segment D?ADE?DH is made for its I-extension link. The third item of s?, D, is the last item of the ?rst itemset in s?. Only one call with segment ADE ? DH is made for node 3  s S-extension link. The fourth item of s?, A, can be located it at node 1 again, and we make two recursive calls. One is for the node 1  s I-extension link with DE ? DH , and the other one is for its S-extension link with DH . Then we process the remaining items in s?, one by one, in the same way. Whenever we locate an item at some node, if the pointer associated with the node is not NULL and the count of the corresponding candidate is not increased yet \(for the current customer sequence The root node is processed differently from other nodes At the root node, there is no constraint on which items in the customer sequence should be checked against the root  s S-extension link because the ?rst item of a candidate can appear anywhere in the customer sequence. At other nodes there are always some constraints. Let  s see how to make recursive calls at node 1 along its I-extension link. Recall that we have made two recursive calls at the root node with segments, CD ? ADE ? DH and DE ? DH , for node 1  s I-extension. Now we process them at node 1. Since the two segments are speci?ed for node 1  s I-extension link we should check the items in their ?rst itemsets, CD and DE, against node 1  s I-extension link. For CD ?ADE DH , since C appears at node 6 which has no child, we stop there by just increasing the count of AC. Another item Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE D, appears at node 7. We increase the count of AD and make recursive calls for node 7  s links. Since D is the last item of the ?rst itemset in CD ? ADE ? DH , only one recursive call with the segment ADE ? DH is made for node 7  s S-extension link. For another sequence segment DE ?DH at node 1, two items of the ?rst itemset, D and E, are checked. D is located at node 7. Since the count of AD is already increased before, we should not increase it again. Two recursive calls are made at node 1 for node 7  s links. One is with E ? DH for node 7  s I-extension link and the other is with DH for the S-extension link. We can ignore E because it is not an I-extension child of node 1 This process will continue until a leaf node is reached or the sequence segment is empty 4.3.2 Features of the Pre?x Tree and the Customer Sequence Trimming There are some major differences between our pre?x tree and the PSP  s pre?x tree: 1 candidates of different sizes, whereas PSP  s pre?x tree is only used to count the candidates of the same size. 2 prove the candidate counting, a bit vector is associated with our pre?x tree to facilitate the customer sequence trimming 3 size of our pre?x tree when we count the candidates against the whole database Due to both 2 more ef?cient. In our pre?x tree structure, the I-extension children \(and S-extension children 


children \(and S-extension children gether. During the candidate counting, we frequently need to locate the items in the customer sequences along these links Obviously, making the tree smaller or reducing the number of search operations can enhance the counting process In MSPS, by performing supersequence frequency based pruning in Phase 3, only a part of the candidate set needs to be processed. Thus, our pre?x tree is usually much smaller than PSP  s pre?x tree at each pass. Moreover we also reduce the search operations by trimming the customer sequences. In PSP, the items not in the pre?x tree are not trimmed from the customer sequence. Thus, when these items are processed, they are searched along the corresponding links exhaustively, even though they are not in those links. This unnecessary search cost is not trivial when the number of customer sequences is large. MSPS can avoid this problem. As the mining process makes progress, fewer and fewer items would remain in the longer candidate patterns, and the customer sequence trimming can save a lot of time 4.4 Analysis of the Sampling Here, we discuss some important issues in sampling. For both frequent itemset mining and sequence mining, if a pattern is found frequent in db but turns out to be infrequent in DB, it is an overestimate. On the other hand, if a pattern is infrequent in db but actually frequent in DB, it is a miss Both our research and [9] try to mine the exact result with the help of sampling. While we focused on how to maximize the performance improvement, more attention was given in [9] on how to reduce the probability of misses To achieve that goal, two methods were suggested in [9]: 1 mine a large sample, and 2 sup for mining the sample. These two methods can reduce the misses but also potentially degrade the overall performance. Mining a large sample cuts the merit of sampling while lowering the user-speci?ed minsup may generate a large number of overestimates. Obviously, a complete sample result without misses does not necessarily mean the best overall performance. In MSPS, the cost related to sampling includes all the overhead of mining the sample and verifying the sample results, whereas the performance gain is from the supersequence frequency based pruning. The effectiveness of this pruning is determined by how many long frequent patterns can be found from the sample. As different settings of sample size and the adjusted minsup for mining the sample are used, the overall performance varies accordingly. Thus, we pay our attention to the sample size and the adjusted minsup in the following discussion 4.4.1 Sample Size In [9, 10], the minimum sample size that guarantees a small chance of misses with certain con?dence is given by the Chernoff boundary. Unfortunately, this theoretic guideline is not quite practical because it is too conservative. In MSPS, a large sample can improve the quality of sample results with fewer misses and overestimates. Consequently verifying the sample result can be done quickly and the supersequence frequency based pruning can be very effective But the overhead of mining a large sample is high. On the other hand, with a small sample, the overhead of mining sample is low, but MFSdb may be in bad quality. Then, the cost of verifying the sample result containing many overestimates would be high. If the small sample size makes the minimum support count for mining the sample \(i.e minsup ? |db| or lowered minsup ? |db ing the sample itself may take a long time. Thus, a small sample does not necessarily mean a lower cost. Furthermore, if only few long frequent sequences are found under the border formed by MFSdb, then the supersequence frequency based pruning will not be effective, either. That  s why the sample should not be too large or too small Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  


Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE In general, we do not know the distribution characteristic of the database to be mined, so it is hard to determine the best sample size. MSPS allows users to choose a plausible sample size empirically. In our experiments, we set the sample size as 10% of the original database size. By using a default sample size, how to balance the cost related to the sampling and the quality of sample result mainly depends on the adjusted minsup for mining the sample. Even though the default sample size may not be the best one all the time with the method of adjusting the minsup, it works very well in practice according to our extensive experiments 4.4.2 Adjusting the User-speci?ed Minimum Support for Mining the Sample In the sample mining result, a certain rate of misses is tolerable. Our tests show that, for a missing k-sequence, if most of its long subsequences, such as subsequences with length k ? 1 or k ? 2, are found, then the supersequence frequency pruning is not affected much. In practice, as long as the sample size is not too small, the probability that most of these subsequences are also missed is quite low. Compared to misses, overestimates could be a bigger problem Once an infrequent k-sequence is identi?ed frequent in db at pass k, then it may be joined with many other k-sequences to generate a large number of false candidates in mining the sample. Most importantly, the situation may become even worse when the minimum support count for mining the sample is very small. We found this is more serious for sequence mining than for frequent itemset mining, because the search space is much bigger. For MSPS, it not only degrades the ef?ciency of mining the sample, but also causes a high cost to identify the overestimates In [9], they proposed using the lowered minsup for the sample, however they did not consider the case that the userspeci?ed minsup is very small. In that case, it is dangerous to lower the minsup further. In this research, we investigated how to avoid the overestimates in the case of small user-speci?ed minsup, because such mining task is more time-consuming There are three different cases that can happen when MSPS is used: 1 ply using it or even a lowered one to mine the sample works ne. Only a small number of misses and overestimates occur in our tests. This is usually safe because our default sample size is not very small. 2 sup is small, the sampling technique is challenged. Using a lowered minsup or even the original user-speci?ed minsup for mining the sample often causes many overestimates because lowered minsup?|db| or minsup?|db| is too small Even though increasing the sample size could be a solution for this case, it limits the merit of sampling. Thus, we consider increasing the minsup a little to mine the sample, hoping it will limit the overestimates to a reasonable level. In that case, more misses may occur. However, even though there is a missing pattern, as long as most of its long subsequences are still contained in the sample result, the supersequence frequency based pruning is not affected much. 3 In some rare cases, the user-speci?ed minsup is extremely small. Then, just increasing the minsup for mining the sample cannot solve the problem. We must consider increasing the sample size too. Actually, both 2 the same technical question: when user-speci?ed minsup is small, how to increase the minsup for mining the sample of a certain size? We must keep in mind that if the increase in the minsup for mining the sample is not enough, the problem of overestimates cannot be solved. On the other hand, if it is increased too much, we may not ?nd any long patterns from the sample Consider the original database DB and an arbitrary se 


quence X . If the support of X in DB is PX , then the probability that a customer sequence randomly selected from DB contains X is also PX . Let  s consider a random sample S with m customer sequences that are independently drawn from DB with replacement. The random variable TX which represents the total number of customer sequences containing X in S, has a binomial distribution of m trials with the probability of success PX . In general, if m is greater than 30, TX can be approximated by a normal distribution whose mean is m ? PX and the standard deviation is  m ? PX ? \(1? PX In MSPS, suppose that we draw a sample S with m customer sequences from DB, and then try to use the point estimator P ?X = TX/m to estimate the support of X in the population of DB. Then, P ?X is an unbiased estimator with mean m ? PX/m = PX and standard deviation m ? PX ? \(1? PX  PX ? \(1 ? PX If we assume that the support of X in DB, PX , is the user-speci?ed minsup that we want to estimate, then P ?X which is observed from a sample S, should be around PX with a normal distribution as described above. If the adjusted minsup is denoted as P ??X , we can assume P ??X &gt; PX because our goal is to ?nd out how much we should increase the minsup for mining the sample. If we use P ??X to mine the sample, the probability that the sequence X can be found as a local frequent sequence in db is 1?PZ , where Z P ??X ? PX  PX ? \(1? PX the z-score Let  s consider the standard deviation of P ?X PX ? \(1? PX 1? PX PX ? 1/2 0, 1/2]. Since minsup is usually smaller than 50%, we can assume the value of PX ? \(1? PX interval of [0,minsup]; that means, the standard deviation of P ?X is increasing in this PX interval. If the support of another sequence Y in DB is lower than the minsup PX Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE then both the mean and the standard deviation of observed P ?Y should be smaller than those of P ?X , respectively Therefore, compared with P ?X , the distribution curve of P ?Y is shifted left and is shaper. Thus, if we set the adjusted minsup for mining the sample as P ??X , the probability that an infrequent sequence Y is identi?ed as frequent in S should be lower than 1 ? PZ . This guarantees the probability that any infrequent sequence is identi?ed as frequent in the sample is lower than 1 ? PZ . In our experiments the critical value of Z is set to 1.28, where PZ = 0.90 such that the probability of the overestimate is at most 10%. From Z = \(P ??X ? PX  PX ? \(1? PX can drive P ??X = PX + Z  PX\(1? PX PX = minsup and m = |db This formula provides a theoretic guideline for adjusting the user-speci?ed minsup to mine the sample. Even though this adjusted minsup value may not be the best one all the time, it worked well in most of our experiments 5 Performance Analysis To compare MSPS with other algorithms, we implemented GSP and obtained the source codes of SPAM and 


SPADE from their authors  web sites. All the experiments were performed on a SuSE Linux PC with a 2.6 GHz Pentium processor and 1 Gbytes main memory MSPS was compared with others on various databases and we evaluated the scalability of these algorithms in terms of the number of items and the number of customer sequences. We also investigated how the sample size and the adjusted minsup for mining the sample affect the performance of MSPS. Since the sampling technique is probabilistic, we ran MSPS 100 times for each test. The average execution time of the 100 runs was reported as the performance result. The default sample size was ?xed as 10% of the test database for all experiments. The databases used in our experiments are synthetically generated as in [2]. The database generation parameters are described in Table 1 For all databases, NS = 5000 and NI = 25, 000; and the names of these databases re?ect other parameter values used to generate them 5.1 Performance Comparison We ran MSPS, GSP, SPADE and SPAM on three databases with medium sizes of about 100 Mbytes. The number of items in these databases is 10,000. In our tests SPAM could not mine these databases, and its run was terminated by the operating system. Our machine is a 32-bit system, but the user address space is limited to 2 Gbytes. In all these tests, SPAM always required more than 2 Gbytes memory, hence caused the termination Table 1. Parameters Used in Database Generation D Number of customers in the database C Average number of transactions per customer T Average number of items per transaction S Average length of maximal potentially frequent sequences I Average length of maximal potentially frequent itemsets N Number of distinct items in the database NS Number of maximal potentially frequent sequences NI Number of maximal potentially frequent itemsets As discussed before, when the user-speci?ed minsup is small, simply using it or a lowered one to mine the sample may cost too much due to so many overestimates. In practice, we may not know the data distribution characteristics of the database to be mined. Thus, we conservatively assumed that all user-speci?ed minsups in our tests are small and simply increased them a little bit for mining the sample. The adjusted minsup for each test is computed using the formula given before. The probability that an overestimate occurs is set to 10% at most, i.e., Z = 1.28 The test results are shown in Figure 2. With the optimization components integrated, MSPS performs much better than GSP because it processes fewer candidates in a much more ef?cient way. The advantage of SPADE is the ef?cient counting of the candidates by intersecting the idlists. However, when mining a medium size database with 400,000 customers, the counting for LDB2 in SPADE is inef?cient and degrades the overall performance very much Considering both factors, we can say that if there are not enough number of candidates to be counted, SPADE cannot show its ef?ciency. That is why SPADE is even worse than GSP when the minsup is big, as shown in some of the gures When the minsup is decreased, more and more candidates appear during the mining. In that case, the overhead of GSP in candidate generation, pruning, and especially the counting using a huge hash tree increases drastically For MSPS, this situation is considerably improved by using the supersequence frequency based pruning, the pre?x tree structure, and the customer sequence trimming. When many passes are required for the mining, most candidates usually appear after pass 2, hence MSPS can outperform GSP further when the minsup is decreased. This improvement also makes MSPS better than SPADE in most tests on 


ment also makes MSPS better than SPADE in most tests on the medium size databases. Only when the minsup is very small, SPADE can beat MSPS 5.2 Scalability Evaluation Both SPADE and SPAM need to store a huge amount of intermediate data to save their computation cost. When Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 0200 400 600 800 1000 1200 1400 0.3 0.2 0.1 0.08 0.06 Minimum Support Ex ecu tion Ti me se c GSP SPADE MSPS a 0 500 1000 1500 2000 2500 3000 0.3 0.25 0.2 0.15 0.12 Minimum Support Ex ecu tion Ti me se c GSP SPADE MSPS b 0 1000 2000 3000 4000 5000 0.3 0.25 0.2 0.15 0.12 Minimum Support Ex ecu tion Ti me se c GSP SPADE MSPS c Figure 2. Performance Comparison on Medium Size Databases the memory space requirement is over the memory size available, CPU utilization drops quickly due to the frequent 


available, CPU utilization drops quickly due to the frequent swapping. Compared with them, MSPS and GSP process the customer sequences one by one, hence only a small memory space is needed to buffer the customer sequences being processed. MSPS can also handle the situation that LDBk or C DB k can not be totally loaded into memory by using the signatures as explained in Section 4. Therefore MSPS does not require the memory space as much as GSP SPADE and SPAM Many real-life customer market-basket databases have tens of thousands of items and millions of customers, so we evaluated the scalability of the mining algorithms in these two aspects. First, we started with a very small database D1K-C10-T5-S10-I2.5 and changed the number of items from 500 to 10,000. The user-speci?ed minsup was 0.5 To run MSPS on such a small database with only 1000 customers, we selected the whole database as the sample and keep the user-speci?ed minsup unchanged to mine it Since MSPS does not apply the sampling on such a small database, supersequence frequency based pruning is not performed in mining. Thus, in this case, SPADE and SPAM performed better than MSPS and GSP as long as their memory requirement is satis?ed As the number of items is increased, SPAM shows its scalability problem. Theoretically, the memory space required to store the whole database into bitmaps in SPAM is D ? C ? N/8 bytes. For the id-lists in SPADE, it is about D ?C ? T ? 4 bytes. But we found these values are usually far less from their peak memory space requirement during the mining, because the amount of intermediate data in both algorithms is quite huge Compared with SPAM, SPADE divides search space into small pieces so that only the id-lists being processed need to be loaded into memory. Another advantage of SPADE is that the id-lists become shorter and shorter with the progress in mining, whereas the length of the bitmaps does not change in SPAM. These two differences make SPADE much more space-ef?cient than SPAM Second, we investigated how they perform on C10-T5S10-I2.5-N10K when the user-speci?ed minsup is 0.18 We ?xed the number of items as 10,000 and increased the number of customers from 400,000 to 2,000,000. SPAM cannot perform the mining due to the memory problem For SPADE, we partitioned the test database into multiple chunks for better performance when its size was increased Otherwise, the counting of CDB2 for a large database could be extremely time-consuming. We made each chunk contain 400,000 customers so that it is only about 100 Mbytes which is one tenth of our main memory size. Figure 3 shows that the scalability of MSPS and GSP are quite linear. As the database size is increased, MSPS performs much better than the others When database is relatively small with only 400,000 customers, SPADE performed the best, about 20% faster than MSPS. But SPADE cannot maintain a reasonable scalability as the database becomes larger, and MSPS starts outperforming SPADE. When the database size is increased from 1600K customers to 2000K customers, there is a sharp performance drop in SPADE, such that it is even slower than GSP. In that case, MSPS is faster than SPADE by a factor of about 8. As discussed before, counting CDB2 is a performance bottleneck for SPADE, because the transformation of a large database from the vertical format to the horizontal format takes too much time. When the database is very large, the transformation also requires a large amount of memory and frequent swapping, hence the performance drops drastically. Partitioning the database can relieve this problem to some extent but does not solve it completely. In addition, for the database with a large number of items and customers, SPADE needs more time to intersect more and 


customers, SPADE needs more time to intersect more and longer id-lists Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 400 800 1200 1600 2000 Number of Customers \('000s Ex ecu tion Ti me se c GSP SPADE MSPS Figure 3. Scalability: Number of Customers on C10-T5-S10-I2.5-N10K, minsup=0.18 Finally, we mined a large database D2000K-C10-T5S10-I2.5-N10K, which takes about 500 Mbytes, for various minsups. This database is partitioned into 5 chunks for SPADE, and the results are shown in Figure 4 Based on our tests, we found SPADE performs best for small size databases. For medium size databases, MSPS performs better for relatively big minsups while SPADE is faster for small minsups. When database is large SPADE  s performance drops drastically and MSPS outperforms SPADE very much. If the user-speci?ed minsup is big and there are very few long patterns, GSP may perform as well as, or even better than, others due to its simplicity and effective subsequence infrequency based pruning 100 1000 10000 100000 0.33 0.3 0.25 0.2 0.18 Minimum Support Ex ec uti on Ti me se c GSP SPADE MSPS Figure 4. Performance on a Large Database D2000K-C10-T5-S10-I2.5-N10K 6 Conclusions In this paper, we proposed a new algorithm MSPS for mining maximal frequent sequences using sampling. MSPS combined the subsequence infrequency based pruning and the supersequence frequency based pruning together to reduce the search space. In MSPS, a sampling technique is used to identify potential long frequent patterns early. When the user-speci?ed minsup is small, we proposed how to adjust it to a little bigger value for mining the sample to avoid many overestimates. This method makes the sampling technique more ef?cient in practice for sequence mining. Both the supersequence frequency based pruning and the customer sequence trimming used in MSPS improve the candidate counting process on the new pre?x tree structure developed. Our extensive experiments proved that MSPS is a practical and ef?cient algorithm. Its excellent scalability 


makes it a very good candidate for mining customer marketbasket databases which usually have tens of thousands of items and millions of customer sequences References 1] R. Agrawal and R. Srikant  Fast Algorithms for Mining Association Rules  Proc. of the 20th VLDB Conf., 1994, pp 487  499 2] R. Agrawal and R. Srikant  Mining Sequential Patterns  Proc. of Int  l Conf. on Data Engineering, 1995, pp. 3  14 3] J. Ayres, J. Gehrke, T. Yiu, and J. Flannick  Sequential Pattern Mining Using a Bitmap Representation  Proc. of ACM SIGKDD Conf. on Knowledge Discovery and Data Mining 2002, pp. 429  435 4] B. Chen, P. Haas, and P. Scheuermann  A New TwoPhase Sampling Based Algorithm for Discovering Association Rules  Proc. of ACM SIGKDD Conf. on Knowledge Discovery and Data Mining, 2002, pp. 462  468 5] S. M. Chung and C. Luo  Ef?cient Mining of Maximal Frequent Itemsets from Databases on a Cluster of Workstations  to appear in IEEE Transactions on Parallel and Distributed Systems 6] F. Masseglia, F. Cathala, and P. Poncelet  The PSP Approach for Mining Sequential Patterns  Proc. of European Symp. on Principle of Data Mining and Knowledge Discovery, 1998, pp. 176  184 7] J. Pei, J. Han, B. Mortazavi-Asl, H. Pinto, Q. Chen, U Dayal, and M. C. Hsu  Pre?xSpan: Mining Sequential Patterns Ef?ciently by Pre?x-Projected Pattern Growth  Proc of Int  l. Conf. on Data Engineering, 2001, pp. 215  224 8] R. Srikant and R. Agrawal  Mining Sequential Patterns Generalizations and Performance Improvements  Proc. of the 5th Int  l Conf. on Extending Database Technology, 1996 pp. 3  17 9] H. Toivonen  Sampling Large Databases for Association Rules  Proc. of the 22nd VLDB Conf., 1996, pp. 134  145 10] M. J. Zaki, S. Parthasarathy, W. Li, and M. Ogihara  Evaluation of Sampling for Data Mining of Association Rules  Proc. of the 7th Int  l Workshop on Research Issues in Data Engineering, 1997 11] M. J. Zaki  SPADE: An Ef?cient Algorithm for Mining Frequent Sequences  Machine Learning, 42\(1  60 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE pre></body></html 


NIMBUS NOAA 8 NOAA 15 NPOESS Preparatory Project Orbital Maneuvering Vehicle Orbcomm Orbiting Carbon Observatory Orbiting Solar Observatory-8 Orbview 2 Orion 1 P78 Pas 4 Phoenix Pioneer P-30 Pioneer Venus Bus/Orbiter Small Probe and Large Probe Pioneer-I 0 Polar QuikSCAT Radarsat Reuven High Energy Solar Spectroscopic Imager REX-II Rosetta Instruments Sage III SAMPEX Satcom C3 Satcom C4 SBS 5 SCATHA Seastar SMART-1 SNOE Solar Dynamics Observatory Solar Maximum Mission Solar Mesosphere Explorer Solar Radiation and Climate Experiment Solar Terrestrial Relations Observatory Space Interferometry Mission Space Test Program Small Secondary Satellite 3 Spitzer Space Telescope SPOT 5A Stardust  Sample Return Capsule STEPO STEPI STEP3 STEP4 STRV ID Submillimeter Wave Astronomy Satellite Surfsat Surveyor Swift Gamma Ray Burst Exporer Synchronous Meterological Satellite-I TACSAT TACSAT 2 TDRS F7 Tellura Terra Terriers Tethered Satellite System Thermosphere Ionosphere Mesosphere Energetics and Dynamics TIMED TIROS-M TIROS-N TOMS-EP Total Ozone Mapping Spectrometer TOPEX TRACE Triana Tropical Rain Measuring Mission TSX-5 UFO I UFO 4 UFO 9 Ulysses Upper Atmosphere Research Satellite Vegetation Canopy Lidar VELA-IV Viking Viking Lander Viking Orbiter Voyager Wilkinson Microwave Anisotropy Probe WIND WIRE XMM X-Ray Timing Explorer XTE XSS-iO XSS-ii APPENDIX B FIELDS COLLECTED Mission Mission Scenario Mission Type Launch Year Launch Vehicle Bus Model Bus Config Bus Diameter Location Expected Life Flight Profile Flight Focus Technical Orbit Currency Total Cost Including Launch Unit Sat Costs Average Sat Cost Production Costs Launch Costs Operations Cost Orbit Weight Wet Weight Dry Weight Max Power EOL Power BOL Power  of Batts 17 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


