html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">1Mining  Complex Relationships in the SDSS SkyServer Spatial Database Bavani Arunasalam, Sanjay Chawla, Pei Sun and Robert Munro School of Information Technologies, University of Sydney bavani,chawla,psun2712,rmunro}@it.usyd.edu.au Abstract In this paper we describe the process of mining complex relationships in spatial databases using the Maximal Participation Index \(maxPI property of discovering low support and high confidence rules. Complex relationships are defined as those involving two or more of: multi-feature colocation, self-co-location, one-to many relationships self-exclusion and multi-feature exclusion. We report our results of mining complex relationships in data extracted from the Sloan Digital Sky Survey \(SDSS database 1. Introduction Spatial data mining is a process of extracting interesting patterns from large spatial databases. In particular, mining co-location patterns is an important spatial data mining task. Co-location patterns represent subsets of spatial features whose instances are often located in close spatial proximity. We demonstrate the application of our spatial data mining technique to the Sloan Digital Sky Survey \(SDSS database in order to extract interesting complex spatial relationships \(positive, negative and mixed among the various celestial objects The Sloan Digital Sky Survey \(SDSS established with the objective to collect and store accurate data about large number of astronomical objects in order to address a broad range of astronomical questions. The goal of the SDSS is to image all  bright  objects in 1/4 of the Northern sky in five different wavelengths of light. The SDSS project is now underway having begun its standard operations in April 2000, and is planned to last for five years [3 A spatial relationship is one between features in an Euclidean space, defined in terms of the colocational trends over that space. An example is to determine the confidence of the `where there's smoke there's fire  with respect to a set of coordinates, each representing the feature, smoke or fire. The task here would be to determine whether fire occurs in the neighbourhood of smoke more than is randomly likely Neighbourhoods are defined in terms of cliques also known as neighbour-sets any set of items such that all items in that set colocate. For example, in Figure 1 and Table 1, the colocational pattern {A, C}occurs 3 times, v, vii and x In spatial data, two features are typically said to colocate if they are positioned within a distance d of one another. As has been assumed in Figure 1, d is usually a constant, but it may also be defined as varying locally within the space or with respect to a given feature The mining of association rules is a welldeveloped field. However most of the methods developed for mining confident cliques as transactions, fail to capture many spatial phenomena of interest, as most mining techniques have been optimized for `marketbasket  data. Spatial data is fundamentally different from market-basket data both in its basic nature and distributional tendencies One factor unique to spatial data is that the number of transactions a single item may participate in is potentially unbounded, while in a market-basket 


in is potentially unbounded, while in a market-basket this is limited to one \(obviously, two people may purchase the same toothpaste product, but not the same exact tube in spatial data. The upper limit in a marketbasket is multiple purchases of only one product, which is less likely than an equivalent spatial situation of an area of Proceedings of the 28th Annual International Computer Software and Applications Conference  COMPSAC  04 0730-3157/04 $20.00  2004 IEEE 2monoculture forest. Similarly, there may be direct relationships between features that don't co-locate, as between animals displaying territorial behaviour making such relationships intrinsically more interesting in spatial data Figure 1: An example of spatial co-locational patterns Table 1: Cliques in Figure 1 No Clique i C1,D1 ii C5, D1 iii C5, C6 iv A4,D2, D5 v A5, C3,D3 vi A5,D3,D6 vii A1, B1, C2 viii B2, D4 ix A2, D4 x A3, B3, C4 A complex relationship is simply any combination of these different relationships. It is important to note that while the relationships are defined as complex the phenomena they represent are often very simple for example 1. Crime is more likely to o-occur on streets with no lighting near a subway station 2. Many feral \(non-native of forest implies the absence of native ones 3. Dogs only act as pack animals in non-urban environments More details of complex relationships could be found in [1]. An algorithm to mine positive and negative association rules was described in [2] though it does not involve spatial components Perhaps the most fundamental difference between spatial and other data in a transactional representation is the notion of significance. A co-location is considered significant if it occurs more than is randomly likely. In transactions representing marketbaskets, the transactions, by definition, represent the complete space of the data \(there are no empty baskets may be represented by frequency of the features in relation to the number of transactions. In spatial data however, the random likelihood of a co-location depends on the volume of the space from which it was taken 2. Maximal Participation Ratio In this section we will briefly describe the notion of Maximal Participation Index \(maxPI in [4] where more details can be found 2.1. Participation ratio Given a co-location pattern L and a feature f? L the participation ratio of f, pr \(L, f the support of L divided by the support of f. For example, in Figure 1, the support of {A, B, C} is 2 and the support of C is 6, so pr\({A,B,C},C 2.2. Maximal participation index Given a co-location pattern L, the maximal participation index of L, maxPI\(L the maximal participation ratio of all the features in L 


the maximal participation ratio of all the features in L that is maxPI\(L L,f For example, in figure 1, maxPI\({A,B,C max\(pr\({A,B,C},A A,B,C},B pr\({A,B,C},C 2/5,2/3,2/6 A high maximal participation index indicates that at least one spatial feature strongly implies the pattern. By using maxPI, rules with low frequency but high confidence can be found, which would be pruned by a support threshold 2.3. Mining rules with low support and high confidence using maxPI As discussed above the maxPI could be used to generate rules with low support and high confidence For example if A is an infrequent feature the support of {A, B} will be very low and hence will be pruned by the support threshold D5 D2 A4 A1 B1 C2 C1 D1 C5 C6 C3 D3 D6 A5 C4 B3 A3 B2 D4 A2 Proceedings of the 28th Annual International Computer Software and Applications Conference  COMPSAC  04 0730-3157/04 $20.00  2004 IEEE 3However the confidence of the rule A?B, which is given as conf \(A?B could be high The maxPI directly addresses the problem. maxPI is given as maxPI of {A,B} = max\(pr\({A,B},A A,B},B This shows that a high confidence for the rule A?B will lead to high maxPI, which will prevent rules with low support and high confidence being pruned 2.4. The weak monotonic property of maxPI Maximal participation index is not monotonic with respect to the pattern containment relations. For example, in figure 1, \(maxPI\({A,C 3/5&lt;\(maxPI\({A,B,C out in [4] the maximal participation index does have the following weak monotonic property If R is a k-colocation pattern, then there exists at most one \(k-1  of R such that maxPI\(R   R Relying on this weak monotonic property, the Apriori-like algorithm can be modified to mine confident patterns by using a maxPI threshold 3. Notations used Absence: The absence of an item is represented by negation, for example  A. \(Note: this is not the equivalent of the set-theory  A, meaning the presence of any item other than A Self-co-location: Multiple instances of a feature multiple items    following the feature, for example A 


the feature, for example A 4. Algorithm for Mining complex relationships using maximal participation index Following are the steps involved in mining complex relationships in the SDSS database 1. For every instance \(record its neighbours within a specified distance1. The neighbours of each instance form a clique and 1 This is not strictly a clique. However, while being easier to program, it does not seem to have much effect on the results hence a transaction. Therefore the number of transactions in the data set is equal to the number of instances 2. To every clique we add features representing the absence of items and the presence of multiple items 3. Apply the maxPI algorithm to the transactions automatically pruning trivial/nonsensical colocations such as A  A and A+?A. Return a set frequent of co-locations with a specified minimum threshold 4. Generate rules from the form the frequent cliques with a specified minimum confidence 5. Data Preparation and Experiments The data for this experiment was obtained from the SDSS Data Release 1, online catalogue service This data is now available in a Microsoft SQL Server format [3]. The online data contained information about over 150000 celestial objects. The objects are classified into 25 categories. However the current online data has objects in 17 of these categories only Among these objects, we extracted only the galaxies because 90% of the objects were classified as  Galaxy  We further classified the galaxies into  main  galaxies and Luminous Red Galaxies \(LRG The main galaxies are closer \(to the Earth brighter than the LRG. In the SDSS database the LRG were flagged with a particular value. We classified the  main  and LRG galaxies further into Early and Late galaxies using the UV &amp; red light magnitudes From Hubbles Tuning Fork model [5] it follows that early galaxies are elliptical in shape and late are spiral irregular. Table2 lists the different types of galaxies and the corresponding symbol used in this paper 6. Results We applied the maxPI algorithm to the data set extracted from the SDSS database and some of the interesting rules generated are shown in Table 3. The entire result set could be obtained from http://www.cs.usyd.edu.au/~chawla/SDSS.html. From Table 2 we see that Feature A and C are early galaxies and hence they are elliptical in shape Features B and D are late galaxies and hence they are spiral in shape. The results of our experiment conform to the well know fact that when elliptical galaxies colocate the spiral galaxies are excluded Proceedings of the 28th Annual International Computer Software and Applications Conference  COMPSAC  04 0730-3157/04 $20.00  2004 IEEE 4Table 2: List of object types and their symbols Table 3: The results Neighbourhood distance d: 1 megaparsec min confidence: 70 A    B D F  c o n f    7 1  9 2 5 4    B  D F  c o n f    9 0  1 3 7. Conclusion We have reported preliminary results about the application of spatial data mining techniques to 


application of spatial data mining techniques to extract potentially useful patterns from SDSS database. Our results show that our methods are capable of extracting well-known relationships from the database 8. Acknowledgements Thanks to Chris Bowman for helping us create the data set 9. References 1] R.Munro, S. Chawla, and P.Sun  Complex Spatial Relationships  In Proc. of the IEEE ICDM, 2003, pp 227234 2] X.Wu, C.Zhang, and S.Zhang  Mining Both Positive and Negative association Rules  InProc. 19th International Conference on Machine Learning \(ICML-2002 3] J. Gray, A.S. Szalay, A. Thakar, P. Kunszt, C Stoughton, D. Slutz, and J. vandenBerg  Data Mining the SDSS SkyServer Database  Microsoft Tech Report, MSRTR-2002-01,2002 4] Y.Huang, H.Xiong, and S.Shekhar  Mining confident co-location rules without a support threshold  In Proc. 18th ACM Symposium on Applied Computing \(ACM SAC 5] Martin, V.J. and E. Saar, Statistics of the Galaxy Distribution. Chapman &amp; Hall/CRC, 2002 Objects Symbol GALAXY_LRG_EARLY A GALAXY_LRG_LATE B GALAXY_MAIN_EARLY C GALAXY_MAIN_LATE D HOT_STD E QSO F Proceedings of the 28th Annual International Computer Software and Applications Conference  COMPSAC  04 0730-3157/04 $20.00  2004 IEEE pre></body></html 


border points to improve classicluster separation. This cleaning can facilitate most subsequent supervised and unsupervised learning tasks. It is also possible, after the cleaning, to go back to the original feature space. Thus, benefiting both from lower dimensionality and lower data cardinality The cost of the high dimensional MembershipMap can also be offset by the following additional benefits: \( i values are mapped to the interval [O,l]; \(ii tributes can be mapped to numerical labels, thus, data with mixed attribute types can be mapped to numerical labels within O,l]. As a results, a larger set of DM algorithms can be investigated and \(iii interpreted and thresholded in the new space Even though the attributes are treated independently, they do get combined at a later stage. Hence, information such as correlations is not lost in the membenhip space. In fact, treat ing the attributes independently in the initial \(1-D clustering is analogous to traditional feature scaling methods such as 25-29 Ju/y, 2004 * Budapest, Hungary min-max or normalization, except that the MembershipMap approach does not destroy intricate properties of an attribute's distribution using a single global transformation. In a sense our approach applies a specialized local transformation to each from the PossibilisticMap without my knowledge about the true class labels, is higher for those samples located near the m e  class centroid subcluster along an attribute, and is thus non-linear. :~ 1.0  _ 5 .. _ Oe 0.1 VI. EXPERIMENTAL RESULTS L _ -  _ gt We illustrate the performance of the proposed transforma tion using several benchmark data sets'. These are the Iris Wisconsin Breast Cancer, Heart Disease, and the satellite data the 3 classes. The Wisconsin Breast Cancer data has 9 features O P sets. The Iris data has 4 features and 50 samples in each of 0 0  0 5  1 .o 1.5 29 D/slan- fO f r Y B  desa --an  a   and 2 classes, namely, benign and malignant. These 2 classes have 444 and 239 samples respectively. The Heart Disease data has 13 features and 2 classes. The first class, absence of heart disease, has 150 samples. and the second class, presence of heart disease, has 120 samples. The satellite data has 35 partitioned into a training set \(4,435 2,000 information extracted from the Membership maps The first step in the MembershipMap transformation con sists of a quick and rough segmentation of each feature. This is a relatively easy task that can rely on histogram thresholding _ -  _  g o a  -_ features, 6 classes, and a total of 6,435 samples. This data is L = _  I_ _ The ground truth of these sets will he used to validate the 6.4 O B  0 8  Memberohip I" erfYsl51800 b Fig. 3. Validating identified regions of interest or clustering I-D data. The results reported in this paper were obtained using the Self-Organizing Oscillators Network SOON  of finding the optimal number of clusters in an unsupervised way. We have also hied other simple clustering algorithms such as the K-Means, where the number of clusters i s  specified after inspecting the I-D projected data. The results were comparable. Next, fuzzy and possibilistic labels were assigned 


comparable. Next, fuzzy and possibilistic labels were assigned to each sample using equations \(3 4 q=1. Finally, the fuzzy \(possibilistic to create the FuzzyMap \(PossibilisticMap 15 dimensions, the Wisconsin Breast Cancer maps have 50 dimensions, the Heart Disease maps have 39 dimensions, and the Satellite maps have 107 dimensions To validate the degree of purity, we use the ground truth and compute the membership of each sample using eq.\(3 wherc the distances are computed with respect to the classes centroids. Fig. 3\(b versus their membership in the true class. The distribution shows a positive correlation indicating that the purity com puted using the FuzzyMap without any knowledge about the true class labels can estimate the degree of sharing among the multiple classes, i.e, boundary points. Moreover, we notice that all samples from class I have high purity, and the few samples with low purity belong to either class 2 or 3. This information is consistent with the known distribution of the Iris data where class 1 is well-separated while classes 2 and 3 overlap A. Identifiiing Regions of Interest B. Clustering the Membership Maps To identify seed points, noise and outliers for the Iris data This experiment illustrates the advantage of using the Mem- we compute the typicality of each  sample. Points with high bership Maps to improve clustering. We apply the Fuzzy C- typicality would correspond to seed  points, while points with Means to cluster the different data sets in the original space, low typicality would correspond  to noise points. Since the FuzzyMap, and PossibilisticMap. For all cases, we fix the Iris data has 4 dimensions, the  identified points could not be visualized as in Fig. 2. Instead, we rely on the ground truth to Of clustcrj to the actual number  Of classes, use the same random points as initial centers. validate the results. We use the class labels and compute  the centroid of each Then, we compute the distance from To assess the validity of each partition we  use the true class each point to the centroid of its class, Theoretically, points laheis to the purity Of the The  are with small distances would correspond to seeds, and points shown in Table I .  For each confusion  matrix, the column refers Fig, 3\(a  distances to a plot of the sample typicality their distance As can be seen, both maps improve the clustering  results for to the true class centroid. The distribution clearly suggests data we note here that a partition a negative Typicality, which was extracted only of the Iris data could be obtained using other  clustering algorithms and/or mnltinle clusters to renresent each class Available at "hnp: / /~~ . ics .uc i .edui  mlearn/MLRepositoryhhnP However, our goal here is to  illustrate that the Membership 1151 TABLE I CLUSTERINO RESULTS FUZZ-IEEE 2004 that is uncovered, and can even be avoided by going hack to the original feature space after data reduction and cleaning Data Set /I Originalspace 111 FutzyMap 1 1  PossMap Thus, benefiting both from lower  dimensionality and lower I 50 0 0 11 The projection and clustering steps in the MembershipMap are not restricted to the original attributes. In fact, our approach can be combined with other feature reduction techniques For instance, Principal Component Analysis can he used to Breast 422 22 Cancer 19 220 3 236 3 236 117 33 ~ ~~ ~~~ ~~~ I Disease 11 23 97 111 21 99  Eigenvectors, and then the membershir, transformation can he auulied on the Droiected TABLE II CLASSIFICATION RESULTS _ 


_ data in the same way. In fact, one can even project the data on selected 2-D or 3-D subsets of the attributes, and then apply membership transformations. I Original Space [ FuzyMap I PossMap Training I 89.29% 1 88.86% I 93.57 Testing I 85.50% I 86.30% I 87.30% ACKNOWLEDGMENTS This material  is based upon work supported by the National Science Foundation under Grant No. IIS-0133415 Maps can improve the distribution of the data so  that simple clustering algorithms are reliable. REFERENCES C. Classification using the Membership Maps This experiment uses the 36-D satellite data to illustrate the use of the Membership Maps to improve data classification Only the training data is used to learn the mapping. The test data is mapped using the same cluster parameters obtained for the training data. We train a Back Propagation Neural Networks with 36 input units, 20 hidden units, and 6 output units on the original data and two other similar nets with 107 input units, 20 hidden units, and 6 output units on the Fuzzy and possibilistic maps. Table I1 compares the classification results in the 3 spaces. As can he seen, the results on the testing data are slightly better in both transformed spaces. The above results could be improved by increasing the number of hidden nodes to compensate for the increased dimensionality in the mapped spaces. Moreover, the classifications rates could be improved if one exploits additional information that could be easily extracted from the possibilistic and fuzzy maps For instance, by using boundary points for bootstrapping and filtering noise points VII. CONCLUSIONS We have presented a new mapping that facilitates many data mining tasks. Our approach strives to extract the underlying sub-concepts of each attribute, and uses their orthogonal union to define a new space. The sub-concepts of each attribute can be easily identified using simple I-D clustering or histogram thresholding. Moreover, since fuzzy and possibilistic labeling can tolerate uncertainties and vagueness, there is no need for accurate sub-concept extraction. In addition to improving the performance of clustering and classification algorithms by taking advantage of the richer information content, the MembershipMaps could he used to formulate simple queries to extract special regions of interest, such as noise, outliers boundary, and seed points There is a natural trade-off between dimensionality and information gain. Increased dimensionality of the Member shipMap can be offset by the quality of hidden knowledge 1152 I ]  U. Fayyad, F. PiatetskqGhapiro, P. Smyth, and R. Uthurusamy. Ad wnces in Knowledge Discowr,v and Data Mining, MIT Press. 1996 Z] D. Pyle. Doto Preporrnion for Data Mining, Morgan Kaufmann 1999 131 R. N. Shepard  The analysis of proximilies: multidimensional scaling with an unknown distance function I and 11  PsychomePiko. vol. 27 pp. 125-139 219-246. 1962 4] T. Kohonen. Sel/lOqpnizotion and Associative Memoy, Springer Verlag, 1989 5] J. W. Sammon  A nonlinear mapping for data analysis  IEEE Transactions on Computms. vol. 18. pp. 401-409. 1969 6] H. V. Jagadish  A retrieval technique for similar shape  in ACM SICMOD. 1991, pp. 208-217 71 Chnstos Faloutsos and King-Ip Lin  FastMap: A fast algorithm for indexing, dala-mining and visualization of traditional and multimedia datasets  in SICMOD. 1995. 00. 163-174 SI  H. Almkllim and T. G. Dienerich  Learning with many irrelevant feamres  in Ninth National Conf AI, 1991, pp. 547-552 91 K. Kim and L. A.  Rendell  The feahrre selection problem: Traditional methds and a new algonthm  in Tenth Notional Con/ AI, 1992. pp 129-1 34 I O ]  Jyrki Kivinen and Heikki Mannilq  The power of sampling in knowledge discovety  1994. pp. 77-85 I11 R.O.Duda and P. E. Ha&amp; Patfen, Closslficnfion and Scene Analysis John Wiley and Sons. 1973 I21 L. Kaufman and P. R O U S S ~ ~ U W ,  Finding Groups in Data: An lnmducrion 


I21 L. Kaufman and P. R O U S S ~ ~ U W ,  Finding Groups in Data: An lnmducrion to Cluster Ano!vsis, John Wiley and Sons. 1990 I31 J. C. Bezdek, Pattem Recognition with Fury Objective Function Algorithms. Plenum Press, New York, 1981 1141 H.  Frigui and R. Krishapuram  A robust competitive clustering algorithm with applications in computer vision  IEEE Pons. Pan Annlwis Mach. Intell., vol. 21, no. 5. pp. 45M65, 1999 I51 R. Knshnapuram and J. Keller  A possibilistic approach lo clustering   IEEE Trans. Fuzzy Sysfems, vol. 1,  no. 2. pp. 98-110, May 1993 I61 F. R. Hampel, E. M. Roncheni, P. J. Rousseeuw. and W. A. Stahel Robust Stnti.vtics the Appmoch Based on Influence Funoions, John Wiley &amp; Sans, New York 1986 1171 2. Wang and G.  Klir. F u q  measure theo?, Plenum Press. New York 1992 I81 M.B. Rhouma and H. Frigui  Self-organization of a population of coupled oscillaton with application to clustering  IEEE Trans. Pntl Anntvsis Mach. Intell., vol. 23, no. 2. pp. 180-195, 2001 pre></body></html 


   c x n xijnx D x ij t          u    1 0 UU V where D| is the number of transactions in D, we can say that we are c confident that {i, j} isn  t frequent in D Proof: If probability policies are not considered and Sij  1, that is, while a row \(transaction according to the matrix multiplication discussed in the previous section, D'tj will be set to 0. We can view the nij original jtiD jtjtjt ijn DDD 21     f o r  e a c h  r o w  t i  c o n t a i n s   i   j   i n  D  a s  realizations of nij independent identically distributed \(i.i.d om variables ijn X X X      2 1      e a c h  w i t h  t h e  s a m e  d i s t r i b u  tion as Bernoulli distribution, B \(1 X i    1  a n d  t h e  f a i l u r e  i s  d e f i n e d  a s  X i    0     i    1  t o  n i j   T h e  p r o b  ability ? is attached to the success outcome and 1?? is attached to the failure outcome. Let X be a random variable and could be viewed as the number of transactions which include {i, j} in D such that jin     2 1    T h u s   t h e  d i s t r i b u t i o n  o f  X is the same as Binomial distribution, X?B \(nij              otherwise nx x n XP ij xnxij x ij 0 1,0     U the mean of X = nij?, and the variance of X = nij?\(1 If we want to hide {i, j} in D', we must let its support in D' be smaller than ?. Therefore, the expected value of X must be s m a l l e r  t h a n     D u V    T h a t  i s     m u s t  s a t i s f y     D n i j  u  u  V U   Moreover, the probability of the number of success which is 


Moreover, the probability of the number of success which is equal to or smaller than     1     u  D V   s h o u l d  b e  h i g h e r  t h a n  c  Therefore, U must satisfy    c x n xijnx D x ij t           u    1 0 UU V  If ? satisfies the two equations, we can say that we are c confident that {i, j} isn  t frequent in D When nij &gt; 30, the Central Limit Theorem \(C.L.T used to reduce the complexity of the equation and speed up the execution time of the sanitization process Moreover, if several entries in column j of S are equal to  1 such as Sij  1, Skj  1, Smj  1  etc, several candidate Dist ortion probabilities such as iU , kU , mU , etc are gotten. The Dis tortion probability of column j, ? j, is set to the minimal candid ate Distortion probability to guarantee that all corresponding pair-subpatterns have at least c confident to be hidden in D 3.3. Sanitization Algorithm Fig.7: Sanitization Algorithm According to the probability policies discussed above, the sanitization algorithm D'n  m = Dn  m  Sm  m is showed in Fig.7 Notice that, if the sanitization process causes all the entries in row r of D' equal to 0, randomly choose an entry from some j Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE where Drj = 1, and set D'rj = 1. It is to guarantee that the size of the sanitized database equals the size of the original database 4. Performance Evaluation 4.1. Performance Quantifying There are three potential errors in the problem. First of all some sensitive patterns are hidden unsuccessfully. Secondly some non-sensitive patterns cannot be mined from D'. And the third, new artificial patterns may be produced. However, the third condition never happens in both of our approach and SWA Besides, we also introduce the other two criteria, dissimilarity and weakness. All of the criteria are discussed as follows Criterion 1: some sensitive patterns are frequent in D'. This condition is denoted as Hiding Failure [10], and measured by     DP DP HF H H , where XPH represents the number of patterns contained in PHwhich is mined from database X Criterion 2: some non-sensitive patterns are hidden in D'. It is also denoted as Misses Cost [10], and measured by      DP DPDP MC 


MC H H H      w h e r e        X P H  r e p r e s s ents the number of patterns contained in ~PH which is mined from database X Criterion 3: the dissimilarity between the original and the sanitized database is also concerned, and it is measured by      n i m j ij n i m j ijij D DD Dis 1 1 1 1    Criterion 4: according to the previous section, Forward Inference Attack is avoided while at least one pair-subpattern of a sensitive pattern is hidden. The attack is quantified by        DPDP DPairSDPDP Weakness HH HH    where DPairS is the set of sensitive patterns whose pair subsets can be completely mined from D 4.2. Experimental Results Table 1: Experiment factor The experiment is to compare the performance between our approach and SWA which has been compare with Algo2a [4 and IGA [10] and is so far the algorithm with best performances published and presented in [12] as we know. The IBM synthetic data generator is used to generate experimental data. The dataset contains 1000 different items, with 100K transactions where the average length of each transaction is 15 items. The Apriori algorithm with minimum support = 1% is used to mine the dataset and 52964 frequent patterns are gotten Several sensitive patterns are randomly selected from the frequent patterns with length two to three items to be seeds With the several seeds, all of their supersets are included into the sensitive patterns set since any pattern which contains sensitive patterns should also be sensitive 0 0.05 0.1 0.15 0.2 0.25 0.3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern H id 


id in g F ai lu re SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern W e ak n es s SP SWA Fig.8: Rel. bet. RS and HF. Fig.9: Rel. bet. RS and weakness Fig.8, Fig.9, Fig.10, Fig.11 and Fig.12 show the effect of RS by comparing our work to SWA. Refer to Fig.8, because the level of confidence in our sanitization process takes the minim um support into account, no matter how the distribution of the sensitive patterns, we still are C confident to avoid hiding failure problems. However, there is no correlation between the disclos ure threshold in SWA and the minimum support. Under the sa me disclosure threshold, if the frequencies of the sensitive patte rns are high, the hiding failure will get high too. In Fig.9, beca use our work is to hide the sensitive patterns by decreasing the supports of the pair-subpatterns of the sensitive patterns, the val ue of weakness is related to the level of confidence. However according to the disclosure threshold of SWA, when all the pair subpatterns of the sensitive patterns have large frequencies, it may cause serious F-I Attack problems. Hiding failure and wea kness of SWA change with the distributions of sensitive patterns 0 0.1 0.2 0.3 0.4 0.5 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern M is se s C o st SP SWA 0 0.005 0.01 0.015 0.02 0.025 0.03 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern D is si m 


m il ar it y SP SWA Fig.10: Rel. bet. RS and MC.  Fig.11: Rel. bet. RS and Dis In Fig.10 and Fig.11, ideally, the misses cost and dissimilar ity increase as the RS increases in our work and SWA. However if the sensitive pattern set is composed of too many seeds, a lot of victim pair-subpatterns will be contained in Marked-Set. And as a result, cause a higher misses cost, such as x = 0.04 in Fig.10 Moreover, refer to the turning points of SWA under x = 0.0855 to 0.1006 in Fig.10 and Fig.11. The reason of violation is that the result of the experiment has strong correlation with the distri bution of the sensitive patterns. Because the sensitive patterns Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE are chosen randomly, several variant factors of the sensitive patt erns are not under control such as the frequencies of the sensiti ve patterns and the overlap between the sensitive patterns if the overlap between the sensitive patterns is high, some sensitive patterns can be hidden by removing a common item in SWA Therefore, decrease the misses cost and the dissimilarity 0 0.5 1 1.5 2 2.5 3 0.0205 0.04 0.0618 0.0855 0.1006 ratio of sensitive pattern E x ec u ti o n T im e h  SP SWA 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set H id in g F a il u re SP SWA 


SWA Fig.12: Rel. bet. RS and time. Fig.13: Rel. bet. RL2 and HF Fig.12 shows the execution time of SWA and our approach As shown in the result, the execution time of SWA increases as RS increases. On the other hand, our approach can be separated roughly into two parts, one is to get Marked-Set which is strong dependent on the data, the other is to set sanitization matrix and execute multiplication whose execution time is dependent on the numbers of transactions and items in the database. In the experi ment, because the items and transactions are fixed, therefore, the execution time of our approach is decided by the setting of Marked-Set which makes the execution time changing slightly 0 0.05 0.1 0.15 0.2 0.25 0.3 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set W ea k n es s SP SWA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set M is se s C o st SP SWA Fig.14: Rel. bet. RL2 and weakness. Fig.15: Rel. bet.RL2 and MC 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.3784 0.6392 0.7646 0.9081 1 ratio of large2 in sensitive pattern set D is si m il a ri ty SP SWA Fig.16: Rel. bet. RL2 and Dis Fig.13, Fig.14, Fig.15and Fig.16 show the effect of RL2 Refer to Fig.13 and Fig.14, our work outperforms SWA no matter what RL2 is. And our process is almost 0% hiding failure 


The reason of the hiding failure of SWA is the same in Fig.8 Notice the result at x = 0.7646 in Fig.14, because the hiding failure is occurred at the seeds of the sensitive patterns, a high weakness is produced As shown in Fig.15 and Fig.16, the misses cost and dissimil arity of our work decreases as RL2 increases. This is because the larger RL2 is, the less effect on non-sensitive patterns. Also weakness and dissimilarity of SWA are independent of RL2 5. Conclusion In the paper, a novel method improving the balance between sensitive knowledge protecting and discovery on frequent patte rns has been proposed. By setting entries of a sanitization matrix to appropriate values and multiplying the original database by the matrix with some probability policies, a sanitized database is gotten. Moreover, it can avoid F-I Attack absolutely when the confidence level given by users approximates to 1. The experimental results revealed that although misses cost and dissimilarity between the original and sanitized database of our process are little more than SWA, ours provide more safely protection than SWA. Unlike SWA, our sanitization process could not suffer from F-I Attack and the probability policies in our approach also take the minimum support into account, the users only need to decide the confidence level which affects the degree of patterns hiding 6. Reference 1] M. Atallah, E. Bertino, A. Elmagarmid, M. Ibrahim and V. Verykios Disclosure Limitation of Sensitive Rules", Proc. of IEEE Knowledge and Data Engineering Exchange Workshop 1999 2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules. VLDB, Santiago, Chile, 1994 3] R. Agrawal and R. Srikant. Privacy preserving data mining. In ACM SIGMOD, Dallas, Texas, May 2000 4] E. Dasseni, V. Verykios, A. Elmagarmid and E. Bertino, Hiding Association Rules by Using Confidence and Support", Proc. of 4th Intl Information Hiding Workshop \(IHW 5] A. Evfimievski, J. Gehrke, and R. Srikant. Limiting Privacy Breac hed in privacy preserving data mining. SIGMOD/PODS, 2003 6] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD 2002 7] M. Kantarcioglu and C. Clifton. Privacy-preserving distributed mining of association rules on horizontally partitioned data. In ACM SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, June 2002 8] Guanling Lee, Chien-Yu Chang and Arbee L.P Chen. Hiding sensitive patterns in association rules mining. The 28th Annual International Computer Software and Applications Conference 9] Y. Lindell and B. Pinkas. Privacy Preserving Data mining. In CRYPTO, pages 36-54, 2000 10] S. R. M. Oliveira and O. R. Za  ane. Privacy Preserving Frequent Itemset Mining. In Proc. of IEEE ICDM  02 Workshop on Privacy Security, and Data Mining 11] S. R. M. Oliveira and O. R. Za  ane. Algorithms for Balancing Priv acy and Knowledge Discovery in Association Rule Mining. IDEAS  03 12] S. R. M. Oliveira and O. R. Za  ane. Protecting Sensitive Knowledge By Data Sanitization, ICDM  03 13] S. R. M. Oliveira, O. R. Za  ane and Y  cel Saygin. Secure Association Rule Sharing, PAKDD-04 14] Benny Pinks. Cryptographic Techniques For Privacy-Preserving D ata Mining. ACM SIGKDD Explorations Newsletter Vol. 4, Is. 2, 2002 15] S. J. Rizvi and J. R. Haritsa. Maintaining data privacy in association rule mining. VLDB, 2002 16] J. Vaidya and C. W. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD2002 17] Verykios, V.S.; Elmagarmid, A.K.; Bertino, E.; Saygin, Y.; Dasseni E. Association rule hiding. IEEE Transactions On Knowledge And Data Engineering, Vol. 16, No. 4, April 2004 Proceedings of the 29th Annual International Computer Software and Applications Conference  COMPSAC  05 0730-3157/05 $20.00  2005 IEEE pre></body></html 


pre></body></html 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


