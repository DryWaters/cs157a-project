EDP-ORD: Efficient Distributed/Parallel Optimal Rule Discovery Sahar M. Ghanem Mona A. Mohamed Magdy H. Nagi Computer and Systems Engineering Department Faculty of Engineering, Alexandria University Alexandria, Egypt sghanem@alex.edu.eg Abstract  Association rule discovery algorithms generate all rules satisfying minimum support and confidence thresholds These techniques yield too many rules and are infeasible when the minimum support is low. Recently, Li [1 p r o p o s ed t h e  Optimal Rule Discovery ORD\ algorithm that discovers a family of rule sets that maximizes a range of interestingness metrics other than the commonly used confidence metric In addition, the discovered optimal class association rule set is the minimum subset of rules with the same predictive power as the complete class association rule set. Moreover ORD is significantly more efficient than association rule discovery independent of the data structure and the implementation Due to the existence of huge amounts of data, it is important to investigate efficient methods for distributed/parallel mining of rules. In this paper, we propose EDP-ORD an efficient distributed/parallel extension of the ORD algorithm. We theoretically disclose a relationship between locally large and globally large rules and use it in reducing the number of generated rules and the exchanged messages at each site/partition. Moreover, we empirically compare EDP-ORD with a naïve distributed/parallel ORD version on five benchmark datasets. The experimental results shows that the reduction in number of generated rules at each site can reach 44% while the reduction in total size of exchanged messages can reach 58 Keywords- data mining; association rule discovery; rule-based classifiers; distributed rule discovery; parallel rule discovery I I NTRODUCTION The term data mining is often used as a synonym for the process of extracting useful information from databases. Data mining is the application of specific algorithms for extracting structure from data. Rules are among the most expressive and human-understandable representation of knowledge, so rule discovery has been a major issue in data mining [1  Association rules discovery is the task of inferring rules which states that certain values occur with other values above a certain frequency in data with some pre-specified certainty Association rule discovery generates all rules satisfying some constraints, but yields too many rules and is infeasible when the minimum support is small. Optimized rule discovery is an efficient alternative that generates a smaller set of rules by pruning redundant rules. Classification rule mining aims to discover a small set of rules in the database to form an accurate classifier. For association rule mining, the target of mining is not predetermined, while for classification rule mining there is one ORD [1 i s  a n  o p t i m a l c l a ss a sso c i a t i o n r u l e m i n i n g  algorithm that prunes the candidate rules according to an interestingness metric and is independent of the data presentation.  In addition, the discovered optimal class association rule set is the minimum subset of rules with the same predictive power as the complete class association rule set [2    The development of distributed/parallel algorithms for efficient mining of associative classification rules has its importance because many large databases are distributed in nature. In addition, mining association rules in huge databases or data warehouses\ may require substantial processing power and a distributed/parallel system is a possible solution. This motivates us to extend the centralized ORD to a distributed optimal rule discovery for mining associative classification rules in a horizontally partitioned database. We adapt a local pruning strategy at every site that efficiently finds the optimal rules when compared to a naïve approach It has been well know that the major cost of mining rules is the computation of the frequently occurring patterns. In a distributed scenario, locally large patterns can be easily computed but they are not necessary be globally large. One possible solution is to broadcast all counts of all patterns no matter they are locally large or small. This is a naïve solution that might lead to passing huge number of messages The main contribution of our work is that we disclose a relationship between ORD locally large and globally large rules and explore it to generate a smaller set of candidates at each site and reduce the number/size of messages to be passed. In addition, experiments are conducted to study the reduction achieved The rest of the paper is organized as follows: in section II we survey the related research; in section III we outline the definitions, notations, and the original/centralized ORD algorithm; in section IV we present a naïve ORD extension NDP-ORD\, the ORD local pruning theorem and its proof, and the developed EDP-ORD; in section V we detail the experimental setup and results; in section VI we conclude the paper and discuss future research II R ELATED W ORK The benefits of mining association rules and its wide applications have led to several proposals for fast mining of association rules [3   978-1-4577-0681-3/11/$26.00 ©2011 IEEE 956 


It is widely recognized that the set of association rules can rapidly grow to be unwieldy and that it contains redundancy 4 pr e s e n t a  f r a m e w or k for a s s o c i a t i o n r u l e m i n i n g ba s e d on  the concept of closed frequent itemsets. The set of all closed frequent itemsets can be orders of magnitude smaller than the set of all frequent itemsets One type of optimal rule set is k-largest rule sets, which contain the top k rules measured by an interestingness metric   H ow e v e r  t h e  t o p k ru l e s  m a y c o m e f r o m  t h e s a m e s e c t i o n  of data and leave some records in a data set uncovered by rules The problem of not reasonably covering the data set exists in other optimal rule sets, such as the SC optimality rule set [6 and rule sets defined by all confidence and bond [7 The definition of optimal rule sets in [1  i s  a s p ecial constraint rule set with a zero confidence improvement, which consists of rules whose confidences are greater than confidences of all their simpler form rules. A PC optimality rule set [6 po s t  p r u n e s  t h e c o n s t r a i nt  a s s o c i a t i o n ru l e s e t w i t h  a  zero confidence improvement. The definition of optimal rules in ORD [1 v e r y cl o s e t o th i s  d e f i n i ti o n  an d co n s i d e r s co v e r i n g  the data set With the rapid development of the Internet/Intranet distributed databases have become a broadly used environment in various applications [8 T h e  a l g o r i t h m s for di s t r i but e d  mining of association rules can be divided into two classes One that focuses on data partition optimization for a centralized database so as to enhance the search efficiency, and the other considers a setting where the data is arbitrarily partitioned horizontally among the parties and focuses on parallelizing the communication [8   T h e s e c o n d a p p r o a c h  is  a mo r e a p p e al i n g  solution for systems which are naturally distributed over large expenses, such as stock exchange and credit card systems. Fast distributed mining of association rules \(FDM  a l go ri t h m  belongs to the second class of distributed association rule mining III ORD O PTIMAL R ULE D ISCOVERY Considering a relational data set D with m attributes, the following definitions hold  A record  T is a set of attribute-value pairs. All the records in D are categorized by a set of classes C  c 1 1 1  c 2 2 2   c k k k  C  k   A pattern  P is a subset of a record. An l pattern contains l attribute-value pairs. For simplicity, we use uppercase letters \(e.g P and Q to stand for patterns and lowercase letters \(e.g a  b to stand for an attribute-value pair. We abbreviate P U Q as PQ and P U a as Pa   An association rule is an implication P  Q where P  Q     A classification rule is an implication P  c where c  C  The support of pattern P  sprt  P is the ratio of the number of records containing P to the number of records in D  The support of implication P  Q  sprt  P  Q  sprt  PQ   The support count of a pattern cnt  P is the number of records in D containing P i.e cnt  P  sprt  P  x  D   The cover set of pattern P  cvr  P is the set of IDs of records containing P  The cover set of implication P  Q  cvr  P  Q cvr  PQ    c stands for a special class occurring in a record where c does not occur, where sprt  c 1  sprt  c  Similarly sprt  P  1  sprt  P   Many interestingness metrics have been proposed to measure interestingness of rules [10  A r u le  w i t h a  higher value in a metric is more interesting than a rule with a lower one. ORD theorem and corollaries apply when Interest is defined by confidence, odds ratio, lift interest or strength\ gain, added-value, Klosgen conviction, p-s \(or leverage\, Laplace, cosine, certainty factor, or Jaccard. Each metric defines an optimal rule set. The following Interest metrics are used in our classification study [1  o        P sprt Pc sprt c P confidence   o        1       C D P sprt D Pc sprt c P Laplace     o          c P sprt c sprt P sprt c P conviction      A rule is a strong implication whose both support and interestingness are not less than given thresholds  Given two rules P  c and Q  c where P   Q we say that the latter is more specific than the former and the former is more genera l than the latter. That is cvr  Q    cvr  P and cvr  Q  c    cvr  P  c  Therefore, the removal of a specific rule from a rule set does not reduce the coverage of that rule set  A rule set is optimal with respect to an interestingness metric if it contains all rules except those with no greater interestingness than one of its more general rules Let X be a pattern where X  and let c be a class PX is a proper super pattern of P  PQ is a super pattern of P When Q    PQ  P and PQX  PX  PQX is a proper super pattern of P  P  X is a pattern with the following support sprt  P  X  sprt  P   sprt  PX  Further, we have sprt  PX  sprt  PX   sprt  P  X  sprt  P  X  ORD Theorem \(Antimonotonic property If sprt  PX  c  sprt  P  c then rule PX  c and all its more-specific rules will not occur in an optimal rule set defined by an Interest metric ORD Corollary-1 \(Closure property If sprt  P  sprt  PX  then rule PX  c for any c and all its more specific rules do not occur in an optimal rule set defined by an Interest metric 978-1-4577-0681-3/11/$26.00 ©2011 IEEE 957 


ORD Corollary-2 \(Termination property If sprt  P  c 0 then all more-specific rules of the rule P  c do not occur in an optimal rule set defined by an Interest metric See o r p r o o f s o f O R D t h e o r e m  a nd c o r o l l a r i e s   T h e  practical implication of the above theorem is that once sprt  PX  c  sprt  P  c is observed, it is not necessary to search for more specific rules of PX  c for example PQX  c Those more specific rules will not be in an optimal rule set Rule PX  c is removed since it is not in an optimal rule set either The practical implication of Corollary-1 is that, once sprt  PX  sprt  P is observed, it is not necessary to search for rules with PQX as their antecedent for any Q Those rules will not be in the optimal rule set The practical implication of Corollary-2 is that once sprt  P  c 0 is observed, it is not necessary to search for more specific rules of P  c Those more-specific rules will not be in an optimal rule set. Rule P  c is kept since it may be in an optimal rule set The ORD-Pruning function detailed next is the direct application of ORD theorem and its corollaries A Itemset Tree and Support Counting Tree Recently, the Itemset Tree \(IST\ data structure has been introduced as a representation of a transaction file [11  1 2   a n d  is used for efficient ad-hoc mining queries. In addition, its use is extended for association rule mining [3   Th e main  ad v a n t ag e of the IST representation is that the database is read into a data structure that can fit in memory which speeds up its access instead of accessing the file on an external storage device In the proposed work, we adapt the IST as a data structure to be used for support counting in the ORD algorithm Originally, the IST is used for association rule mining that has no class information available. In the proposed work, we use the IST for mining association classification rules. The node structure and the tree construction algorithm are modified to reflect the existence of class information In the process of finding all the frequent patterns, we are not interested in the non-supported items. The non-supported items are the items that have a support less than the userspecified minimum support. The IST is cleaned after its construction \(traversed for removing the non-supported items 3  Th e  r e s u lt ed tr ee  i s c a ll ed  th e  Support Counting Tree SCT\. SCT has a reduced size, where the number of nodes in the tree and the number of items within a node are reduced B Centralized Optimal Rule Discovery \(ORD A pattern is frequent if its support is not less than the minimum support. A pattern is potentially frequent \(i.e candidate\ only if all its sub-patterns are frequent Let CA l be the candidate rules at the l th iteration \(i.e l pattern rules\, and L l be the frequent rules kept after ORD pruning of CA l  L l CA l are represented by a linked list of items; where an item is l pattern P a set of the pattern classes a n d a p a t t e r n  c las s is   c las s  c  cnt  Pc CLASS is a linked list of \(class c  cnt c There is pruning in the candidate generation function CandidateGen\ detailed below. In addition, after counting the support of candidates ORD-Pruning is applied. Because of the possible skewed distribution of classes, a single global support is not suitable for a variant rule targeting different classes. In ORD pruning, it is suggested to use a recall threshold instead of a global support. The recall of rule P  c is defined as sprt  Pc  sprt  c The recall is the support in the data subset containing c  In the ORD algorithm detailed in Fig. 2, any interestingness metric discussed above can be used as a rule-selection criterion and the output rule set is an optimal rule set defined by that metric. We have to consider maximum length for the generated rules to stop the candidate generation process for large number of items An association discovery algorithm searches for all frequent patterns, however ORD pruning significantly reduces candidates for searching. The set of ORD candidates is a very small subset of frequent patterns. This trend is more evident when the minimum support is low. Therefore, ORD has significantly less computational complexity than association rule discovery Consider the dataset in Fig. 1 where a, b, and c are items and y and z are classes. Assume that the minimum recall is 0 i.e. it is required to generate all rules Generating all patterns without ORD pruning we have the z,1   b  y  3  a n d   c   z,1 a t  t h e f i rs t l e v e l    a b  y  2    a c  y  2  z  1     b c   y,1 at th e s eco n d l ev el  a b c   y  1    at th e th ir d l e v e l   I n  th i s  case, the total number of rules is ten Applying ORD, the first-level candidates are used to prune the second-level candidates as illustrated in Fig. 1, where a crossed class is removed and a boxed class is terminated. y in candidates \(a b, y\ and \(b c, y\ is terminated because of sprt a b¬y\=0 and sprt b c¬ y\=0 \(ORD Corollary-2\. Both y and z are removed in candidate \(a c, y z\ since sprt a c sprt c holds \(ORD Corollary-1\. After rules have been formed candidates \(ab, \, \(ac, \ and \(bc, \ are removed As a result, their super candidate will not be generated, and the candidate generation process stops at the second-level. In this case, the total number of rules is only seven \(3 rules less Function CandidateGen L l  returns CA l 1 for each pair of candidates P l 1 s  C s nd P l 1 t  C t L l  insert candidate P l 1  C n CA l 1 where  P l 1  P l 1 st and C  C s  C t  for all P l  P l 1  if candidate P l  C l does not exist in L l  then remove P l 1  C  and return  else C  C  C l endfor  if the target set C of P l 1  C s empty  then remove the candidate  endfor 978-1-4577-0681-3/11/$26.00 ©2011 IEEE 958 


Function ORD-Pruning CA l  is the minimum recall for each candidate P  C n CA l  for each c C test the frequency individually  if spr t Pc  sprt  c  then remove c from C test the satisfaction of ORD Corollary-2  elseif sprt  P  c 0 then mark c terminated endfor  if C is empty then remove candidate P  C and return   for each l 1\-level subset P  P  test the satisfaction of ORD Corollary-1  if sprt  P  sprt  P  then empty C  test the satisfaction of ORD Theorem  elseif sprt  P  c  sprt  P  c  then remove c from C  endfor  if C is empty then remove candidate P  C  endfor IV D ISTRIBUTED P ARALLEL O PTIMAL R ULE D ISCOVERY In this paper, we propose a novel extension for the ORD algorithm in a distributed/parallel scenario. It is assumed that the database is horizontally partitioned into n sites/partitions and it is required to mine the same optimal rules as if the whole database is located in a centralized location We first introduce a naïve extension to the ORD algorithm in the distributed case. Then, we adapt the local pruning suggested in [9 f o r e f f i c i e n t  d i s t ri b u t e d O R D   T h e  l o c a l  pruning is originally suggested for association rule mining and a global support threshold is used. However, as mentioned in section III, the ORD algorithm mines associative classification rules and uses a recall threshold. The recall of rule P  c is defined as sprt  Pc  sprt  c  A database D has D records that are distributed into n sites {S 1 S 2 S n i.e D is horizontally partitioned into n partitions D 1  D 2  D n  of size D i each, such that     1 012   n i i D D The global support count of a class c equals the summation of its local support counts at the n sites 012   n i i c cnt c cnt 1      Apriori [13 a n d i t s  di s t r i bu t e d e x t e n s i o n  9 m i n e a l l  globally large association rules \(where there is no target class that satisfy certain minimum support threshold s That is, the algorithm finds all rules P  Q such that cnt  PQ  s x  D  where 012   n i i PQ cnt PQ cnt 1     and cnt i  PQ is the local support count of pattern PQ at site S i  On the other hand, ORD and its distributed extension \(we propose\ mine all globally large classification rules that satisfy minimum recall threshold  That is, the distributed ORD algorithm finds all classification rules P  c such that cnt  Pc  x cnt  c where 012   n i i Pc cnt Pc cnt 1      Let L be defined as the globally large classification rules where  l l L L  and L l is defined as the globally large l patterns in L i.e. classification rules having l items in the antecedent\. In addition, let CA l be defined as the candidate set at the l th ORD iteration; i.e CA l CandidateGen L l1  A NDP-ORD: Naïve Distributed/Parallel ORD For comparison, we outline a naïve distributed extension of ORD as detailed in Fig. 3. At each iteration, the algorithm generates the candidate sets CA l at every site by applying the CandidateGen function on the set of large patterns L l 1 found at the previous iteration. Every site then computes the local support counts of all these candidate patterns and broadcast them to all the other sites. Subsequently, all the sites can find the globally large patterns L l for that iteration, and then proceed to the next iteration Consider the same centralized dataset is horizontally partitioned between two sites as follows   S 1   S 2  The 1-candidates generated at S 1 are  [a, \(y,1\\(z,1   b  y,1 c  y  1  z  1  a n d t h e  1 c a ndi d a t e s a t  S 2 are   [a, \(y,2    b, \(y,2  c  y  1  A f t e r b r o a dc a s t i n g t h e  l o c a l  s upp o r t counts S 1 and S 2 have the same 1-candidates support counts [a z,1   b  y  3  c  y  2 z  1  T h e  2-c a ndi da t e s  generated at S 1 are [a b,\(y,1    a  c   y  1  z  1    b  c  y  1  a n d  the 2-candidates at S 2 are [a b,\(y,1    a  c   y  1    b  c  y  0   After broadcasting the local support counts S 1 and S 2 have the same 2-candidates support counts a b, \(y, 2 a c    y  2  z  1     b c, \(y,1  S i n ce th e s a m e s u p p o r t co u n t s  ar e a v ai la b l e a t b o t h  sites, the ORD Prunes the same rules B EDP-ORD: Proposed Efficient Distributed/Parallel ORD As proofed in [9   th e r e i s an i m p o r t an t r e l a ti o n s h ip  between large/frequent patterns and the sites in a distributed database every globally large pattern must be locally large at some site\(s If a pattern P is both globally large and locally large at a site S i  P is called gl-large at site S i The set of gllarge patterns at a site will form a basis for the site to generate its own candidate sets. Two monotonic properties can be observed from the locally large and gl-large patterns. First, if a pattern P is locally large at a site S i then all of its subsets are also locally large at site S i Secondly, if a pattern P is gl-large at a site S i then all of its subsets are also gl-large at site S i  Notice that a similar relationship exists among the large patterns in the centralized case Following is an important theorem based on which an efficient technique for candidate sets generation in the distributed case is developed. We assume that initially every site S i  i 1 to n  locally count the local support counts for all classes cnt i  c j for j 1 to k and broadcast those counts to the other sites. Therefore, the global support count for each class cnt  c j  j 1 to k is available a   b   c   y a        c   z a   b        y a        c   y  b       y  978-1-4577-0681-3/11/$26.00 ©2011 IEEE 959 


 Algorithm: ORD Input data set D min-recall min interestingness \012 Output an optimal rule set R 1:  Set R l 1 CA 1 initialize to empty; IST = initialize to empty   Count support of 1pattern candidate classification rules 2 for each record in D 3 for each item P  c in the record 4:              search for the item P in CA 1 5 if found then increment cnt  Pc y 1 6 else insert item P  c  CA 1 and set cnt  Pc o 1 7:      increment cnt  c  by 1 in the CLASS list 8:      insert the record in the IST  end for 9 L 1 1 1 ORD-Pruning CA 1    Build the support counting tree \(SCT 10: SCT = using L 1 1 1 remove the nonsupported items form IST and set the guiding information bits 11:  Form and add rules P  c from L 1 1 1  to R if the rule satisfies min interestingness \012 and remove terminated c s 12 l 2 CA 2  CandidateGen L 1  13 while CA l is not empty  14: Count support of CA l using SCT  15 L l ORD-Pruning  CA l    Form and add rules to R that satisfy mininterestingness \012 16 for each P  C  L l 17 for each c in C  18  add rule P  c to R if it satisfies mininterestingness \012 19 if c is terminated then remove c from C 20 l  CA l CandidateGen L l 1   end while 21:  Return the rule set R a   b   c   y a        c z a   b        y a        c   y b       y     a, y z b, y\                    \(c, y z  a b, y\                \(a c,y z\     \(bc,y  a b c y     a, y z\                   \(b, y\                    \(c, y z  a b, y\              \(a c,y z\    \(b c,y Figure 1 ORD Pruning Example ORD Local Pruning Theorem If a classification rule P  c  is globally large, there exists a site S i 1  i  n such that P and all its subsets are gl-large at site S i   P  c is globally large if     c cnt Pc cnt   015  P  c is locally large at site S i if n c cnt Pc cnt i       015 proof If     c cnt Pc cnt   015 is to hold where 012   n i i Pc cnt Pc cnt 1     and 012   n i i c cnt c cnt 1     Let the local support count of rule P  c at site S i cnt i  Pc  a i for i  1 to n and let the global support count for the class c  cnt  c   B j  Then the rule P  c  is globally large, if and only if 015    012  n i j i B a c P recall 1   Assume a 1 a 2 a n A  and the above summation is equal exactly to in this case n B A j   015 If the summation recall  P  c is to be greater than  and/or an arbitrary a x is to be less than A there should be at least one a i at site S i that is greater than A  Figure 2 Centralized ORD Algorithm In this case, the rule is considered locally large at that site Therefore, rule P  c  is locally large at site S i if and only if n c cnt Pc cnt a i i        015  end of proof Let GL i denotes the set of gl-large patterns at site S i and GL i  l  denotes the set of gl-large lpatterns at site S i  It follows from the above theorem if rule P  c   L l then   site S i  such that all l 1\subsets of the rule  GL i  l 1 The straightforward set of the candidates generated at the l th ORD iteration CA l CandidateGen L l1 However, the l pattern candidates generated from l 1 gl-large itemsets at site S i CG i  l  CandidateGen GL i l 1 Since GL i l 1  L  l 1 then CG i l  CA l Let  n i l i l CG CG 1      represent the candidate sets generated from the gl-large items at all sites. Then we have CG l  CA l and the number of candidates in CG l could be much smaller than the number of candidates in CA l The difference 978-1-4577-0681-3/11/$26.00 ©2011 IEEE 960 


between the two sets depends on the distribution of the classes in the n sites If a class distribution is uniform, rules associated with this class will belong to almost every GL i  l  and the difference between CG l and CA l will be negligible. However, if a class distribution is normal, rules associated with this class will belong to few sites and the difference between CG l and CA l will be significant Let X  L l the set of all large l patterns\. It follows from the above theorem that, there exists a site S i such that all l 1 subsets of X are gl-large at S i Hence X  CG i  l  the candidates generated at the l th iteration at S i and therefore X  CG l We can conclude that L l  CG l The above theorem forms a basis for an efficient generation of the candidate sets as follows 1 Every site S i at the l th ORD iteration locally generates the candidates CG i  l  from its gl-large patterns of previous iteration GL i l 1 and at the same time counts the candidates local support 2 Every site collects the support counts of its candidates from other sites and sums the global support of those candidates to find its gl-large patterns GL i l   3 Based on GL i l  the next iteration candidates can be generated Based on ORD local pruning theorem, some candidates in CG i  l  can be pruned before count exchange starts. If a candidate is not locally large at site S i there is no need to count its global support because that candidate is either small or will be locally large at some other site. Every site is responsible to find the global support only for its locally large candidates However, a candidate could be locally large at more than one site. The next step is added to the outlined steps above 1.1 Prune CG i  l  to find the locally large candidates LL i  l   We can notice that, in order to supply the support count exchanges of step 2, each site must have two sets of support counts. For local pruning, S i has to find the local support counts of its CG i  l  In addition, S i has to find the local support counts of other sites candidates LL j  l  j  i A simple solution is to separate the counting steps but that would degrade the performance. Another solution is to let every site S i broadcasts its gl-large GL i l  set at the end of each iteration. Then at the beginning of the next iteration, every site could generate all candidates for all sites and perform all the counting in one step as detailed in the algorithm Fig. 3. Note that broadcasting a message delivers the same copy of the message to every site While, in exchanging messages, a request message is first sent that contains the required patterns support and the response message includes only the requested counts to the asking site Crafting the exchanged messages reduces the required communication bandwidth as illustrated in the experiments Consider the same horizontally partitioned dataset above The 1-candidates generated at S 1 are [a, \(y,1\\(z,1   b  y  1   c, \(y,1\\(z,1  a nd a t  S 2 are   [a, \(y,2  b  y  2   c  y  1   S 1 and S 2 request the counts of its generated candidates only after this step S 1 z,1  b   y  3  c   y  2   z,1 a n d S 2 has  the candidates [a, \(y,3   b  y  3  c   y  2      Note that the candidates \(a, z\ and \(c, z\ are not maintained by S 2 and therefore the exchanged message sizes and the computational complexity of the following iterations are reduced In the naïve case, every local support count of every pattern is broadcast from every site to every other site, i.e the size of the exchanged message for that pattern is O n 2 where n is the number of sites. However, if the efficient ORD is applied, the best case scenario is that every pattern is found to be locally large at one site only and its local support counts are sent to that site reducing the size of the exchanged messages for that pattern to O n An upper-bound pruning could be performed after local pruning \(step 14 at Fig. 3\ and before exchanging the counts. For this pruning, every site has to send the individual local support counts \(at the different sites\ of its gl-large patterns in the broadcast message \(step 17 at Fig.3\. At site S i  at the l th iteration, an upper bound for the global s upport count of a locally large pattern P  c   LL i  l  is 012     n i j j j i Pc cnt Pc cnt Pc cnt  1   max     and  1      min   max     l Q Pand Q Qc cnt Pc cnt j j If this upper bound is less than   c cnt  015 the pattern is pruned from LL i  l   V E XPERIMENTAL R ESULTS In this section, we empirically evaluate the performance of the proposed EDP-ORD in comparison with the naïve extension of ORD in five data sets from UCML repository [14  described in Table I. We do not specify the type of optimal rule set since the same candidate set generates an optimal rule set defined by any interestingness metric discussed in Section III In addition, we employ the minimum recall threshold, which is a ratio for an individual class. A pattern is frequent if it is frequent in at least one class. An empirical estimation of the complexity for a distributed rule-discovery algorithm is the number of candidates it searches and the exchanged message sizes The sizes of the datasets in our study range from 150 records to 4177 records and have number of attributes in the range from 4 to 38. We compare the searched candidates for the two algorithms and the total exchanged messages size for different values of minimum recall and when the number of sites n is 2, 3, and 4. Table II shows the searched candidates by the two versions of distributed ORD algorithms \(N for naïve E for Efficient\ at each site, while Table III shows the total exchanged messages size. The results show that the local pruning has significantly less computational complexity than the naïve. The number of candidate sets found in ED-ORD at each site is reduced to 20-44% of that in ND-ORD. The total messages size in ED-ORD is reduced to 16-58% of that in NDORD. The reduction in the number of candidate sets and message size in ED-ORD is significant. The performance gain of ED-ORD is very important in distributed/parallel systems in which the communication is an important performance factor This shows that ED-ORD becomes more effective when the system is scaled up 978-1-4577-0681-3/11/$26.00 ©2011 IEEE 961 


 Algorithm: NDP-ORD @ S i Input a data set D i min-recall min interestingness \012 Output an optimal rule set R 1: Set R l 1; Construct IST for D i 2: Generate CA i1 the candid ate 1 pattern rule set at S i and count their local support 3: Count the local support for all classes in CLASS cnt i  c j  j 1 k 4 Broadcast the 1-pattern rule set and th e classes local support counts 5  n i i CA CA 1 1 1    L 1 ORDPruning  CA 1  6: SCT = Clean IST and set the guiding information bits 7: Form and add rules from L 1 o R that satisfy mininterestingness \012 8 l 2 CA 2 CandidateGen L 1  9 while CA l is not empty  10:    Count local support \(using SCT\ of CA l patterns  11 Broadcast local support counts of CA l candidates  12:    Sum the local supports of CA l patterns to find their global support  13 L l  ORD-Pruning CA l   14:     Form and add rules \(from L l to R that satisfy mininterestingness \012  15 l CA l CandidateGen L l 1   end while 16: return the rule set R Algorithm: EDP-ORD @ S i Input a data set D i min-recall min interestingness \012 Output an optimal rule set R 1: Set R l 1; Construct IST for D i 2: Generate CG i1 the candidate 1-pattern rule set at S i and count their local support 3: Count the local support for all classes in CLASS cnt i  c j  j 1 k 4 Broadcast the classes local support counts to compute its global counts 5 LL i1 locally prune CG i1 and keep only the locally large 1-pattern rules   P  c is locally large at S i if n c cnt Pc cnt i       015 6  Exchange LL i1 to collect the candidates global support 7  GL i 1 ORD-Pruning  LL i 1   GL i 1 is the gl-large 1pattern set 8  Broadcast GL i 1 Find  n x x GL GL 1 1 1   9 SCT = Clean IST and set the guiding information bits 10 Form and add rules \(from GL 1  to R that satisfy mininterestingness \012 11  l 2 for x 1 to nCG x 2 CandidateGen GL x 1   n x x CG CG 1 2 2   12  while CG l is not empty  13:  Count the local support \(using SCT\ of CG l patterns  14 LL il locally prune CG il and keep only the locally large l patterns at S i  15 Exchange LL il to collect the candidates global support  16 GL il ORD-Pruning LL il   GL il is the gl-large l pattern set  17 Broadcast GL il Find  n x xl l GL GL 1    18 Form and add rules from GL l  to R that satisfy mininterestingness \012  19 l  for x 1 to nCG xl CandidateGen GL xl    n x xl l CG CG 1    end while 20 return the rule set R Figure 3 Naïve Distributed/Parallel ORD Algorithm Figure 4 Efficient Distributed/Parallel ORD Algorithm 978-1-4577-0681-3/11/$26.00 ©2011 IEEE 962 


TABLE I D ATASETS Dataset Iris Breast Cancer Anneal Car Evaluation Abalone records 150 699 798 1728 4177 attributes 4 10 38 6 8 classes 3 2 6 4 29 TABLE II A VERAGE N UMBER OF R ULES a n 2 min recall Iris N Iris E Abal N Abal E Total N Total E reduction  0.1 46.5 38.8 2056 1349 2470.3 1953.4 20.92458 0.2 35.2 25.5 749 439 1408.2 1091.9 22.4613 0.3 31.7 20.9 425 267 1077.6 691.4 35.8389 0.4 27.2 15.5 374 146 892.4 496.1 44.40834 0.5 26 14.3 344 108 451.6 267.9 40.67759 b n 4 min recall Anne N Anne E Car N Car E Abal N Abal. E reduction  0.1 879.2 832.1 365.5 209 1 2001 1607 20.54372 0.2 578.8 534.8 114.8 74.3 590 389 30.04637 0.3 450 412.1 72.8 53.7 381 125 28.74936 0.4 358.8 310 48.7 38.8 338 147 33.88095 0.5 293.7 254.8 45.8 37 328 140 20.91444 TABLE III T OTAL MESSAGES S IZE a n 2 min recall Iris N Iris E Abal N Abal E Total N Total E reduction  0.1 245 120 20945 16445 28703 23995 16.40102 0.2 158 70 11693 8289 18148 13653 24.76844 0.3 155 59 9415 5891 15310 8170 46.63571 0.4 129 47 9641 3491 13078 5430 58.47887 0.5 129 47 8971 2281 3253 2248 30.90445 VI C ONCLUSION AND F UTURE W ORK In this paper, we proposed and studied an efficient and effective distributed/parallel extension of the optimal rule discovery, ORD, algorithm. We assumed a horizontally partitioned dataset and a message passing communication model. We considered a single class as a consequence but the algorithm can be easily applied for rules with a pattern as the consequence We theoretically proofed a relation between locally large and globally large patterns that is used for local pruning at each site to reduce the searched candidates.  We derived a locally large threshold using a globally set minimum recall threshold Local pruning achieves a reduction in the number of searched candidates and this reduction has a proportional impact on the reduction of exchanged messages. The performance study has demonstrated that EDP-ORD generates a much smaller set of candidate sets and requires significantly smaller amount of messages when compared to a naïve extension NDP-ORD This shows that EDP-ORD is a scalable distributed efficient and effective technique for ORD in large databases We implemented only local pruning, however we discussed an upper-bound pruning that can help in achieving further reduction. Future extensions include studying upper-bound and polling-site pruning [9  an d  ex t e n d in g th e a l g o r ith m in  ca s e o f  vertically partitioned data set. In addition, we are interested in studying robust rule-based classifiers [15  a n d p r i v a c y i ssu e s  i n  data mining R EFERENCES 1 J. Li, On Optimal Rule Discovery, IEEE Transactions on Knowledge and Data Engineering, vol. 18,  no. 4,  April  2006, pp. 460-471 2 J. Li, H. Shen, and R. Topor, Mining the Optimal Class Association Rule Set, Knowledge Based Systems, vol. 15, no. 7, 2002, pp. 399-405 3 M  Ya k o u t   A  M  Ha fe z  a n d H H  A l y   Mining Frequent Itemsets Using ReUsable Data Structure, Proc. of the 2007 International Conference on Data Mining \(DMIN 2007\, June 2007, pp. 148-155 4 M.J. Zaki, Mining Non Redundant Association Ru les, Data Mining and Knowledge Discovery, vol. 9, no. 3, 2004, pp. 223-248 5 G.I. Webb and S.  Zhang, K Optimal Rule Discovery, Data Mining and Knowledge Discovery, vol. 10, no. 1, 2005, pp. 39-79 6 R. J. Bayardo and R. Agrawal, Mining the Most Interesting Rules IBM Research Report, 1999, pp. 145-154 7 E. Omiecinski, Alternative Interest Measures for Mining Associations in Databases, IEEE Trans. Knowledge and Data Eng., vol. 15 no. 1 Jan. 2003, pp. 57-69 8 F. Xu An Algorithm on Distributed Mining Association Rules  ICMIT 2005: Control Systems and Robotics, vol. 6042, May  2006 9 D  W  C h e u n g J  H a n   V   T  N g  A   W  Fu a n d  Y  Fu    AFast Distributed Algorithm for Mining Association Rules  Proc. of the 1996 International Conference on Parallel and Distributed Information Systems \(PDIS'96\, Miami Beach, Florida, USA, Dec. 1996, pp.31-42  P  T a n  V   K u m a r  and J   S r i v a s t a v a    Selecting the Right Objective Measure for Association Analysis Information Systems, vol. 29, no. 4 2004, pp. 293-313  A  H a f e z   V   V  R a ghv an  a n d J  D e og un    The Item-Set Tree: A Data Structure for Data Mining  Proc. of Ist Int'l Conf. on Data Warehousing and Knowledge Discovery\(DaWak99\, Aug. 1999, pp. 183-192  M  K u b a t  A   H a f e z  V  V   R a gh av a n  J  R   L e kkal a   and  W  K   C h en    Itemset Trees for Targeted Association Querying  IEEE Transactions Knowledge and Data Engineering, vol. 15, no. 6, Nov. 2003, pp. 15221534  R. Agrawal and R. Srikant, Fast Algorithms for Mining Association Rules, Proc. of the 20th Intl Conference on Very Large Databases Sept. 1994  E  K  C   B l ake and C  J   M e r z   U C I R e p o s i t o r y  of M a ch i n e L e ar n i n g  Databases, http://www.ics.uci.edu mlearn/MLRepository.html, 1998  H. Hu and J. Li, Using Association Rules to Make Rule Based Classifiers Robust, Proc. 16 th Australasian Database Conf ADC\,2005, pp. 47-52 978-1-4577-0681-3/11/$26.00 ©2011 IEEE 963 


                    NRMSE Moving Average Exp Smoothing Kalman Filter Fourier Transform Wavelet MK-Wavelet Association Rules C ompound Measure 0.0 0.2 0.4 0.6 0.8 1.0 normalised error a Similarity error against Mean Waiting Time                    NRMSE Moving Average Exp Smoothing Kalman Filter Fourier Transform Wavelet MK-Wavelet Association Rules Compound Measure 0.0 0.2 0.4 0.6 0.8 1.0 normalised error b Similarity error against Mean Execution Time  Figure 5 Normalised errors of all methods against mean waiting time and mean execution time Lower is better Method  p value  p   0  50  25 0  125 NRMSE 0.38 510 224 3 Mov Avg 0.22 1120 504 22 Exp Smooth 0.36 488 252 1 Kalman 0.30 693 360 12 Fourier 0.36 589 252 7 Wavelet 0.21 1412 522 65 MK-Wavelet 0.22 1344 504 68 Ass Rules 0.25 640 450 5 Compound 0.20 1510 558 182 Table 2 Two-sample Anderson-Darling test per method for all 1874 pairwise permutations of segments Results are the number of times the null hypothesis could not be rejected where scheduling already took place and the descriptive attributes are less useful However the timing of the event is more important which is properly modelled through the wavelet methods Consequently the wavelet methods have less error and less variance than all other methods in this case A wavelet-based method biased by a transfer function is therefore a good choice in both the waiting and execution time cases In the case of our scientic workloads the association rule measure only has a signicant impact when the descriptive attributes inuence the system We can conrm this for DQ2 workload from our own operational experience 5.4 Hypothesis test We complement the accuracy evaluation with a statistical hypothesis test The two-sample Anderson-Darling test can validate if the empirical distribution functions of two independent samples follow from the same distribution It is a non-parametric k-sample test that does not make assumptions about the distribution function and is therefore an ideal candidate for cases without ground truth Our null hypothesis is that both samples come from the same distribution i.e the similarity error is minimal We prepare all 1874 workload segments by normalising their time-dependent attribute to a relative start of zero Additionally we replace all values in the descriptive attributes into unique numerical representations for easy calculation of an attributes empirical distribution Then we evaluate all segments with each available method and present the results in table 2 For each method we give the mean P-value of the test and the number of times we could not reject the null hypothesis for a given statistical signicance   We want this number to be as close as possible to the maximum of 1874 The higher the number the more often the particular method identied two segments as similar which corresponds to a lower error for the similarity measure The results conrm our previous performance metrics evaluation At a signicance of  0  5 the results show four good candidates our compound measure the two wavelet methods and the moving average These results are also true for error  W and error  E in the metrics evaluation The same observation still holds at  0  25 but only at  0  125 we can see the clear advantage of the compound measure Whereas all other methods mostly reject the null hypothesis the compound measure still provides an acceptable result at about 11 with 182 non-rejects Again as in the metrics evaluation the association rule method by itself is seemingly useless but in combination with the wavelet method provides a signicant boost to error reduction 6 CONCLUSIONS The performance evaluation of a system strongly depends on the input workload If fundamentally dierent workloads are used for evaluations then conclusions drawn from the results are likely to be non-representative Therefore we propose a compound measure to quantify the similarity between two workloads This compound measure comprises two independent methods the rst one to analyse the timedependent attribute in the workload and the second one to analyse the descriptive attributes in the workload The rst method uses the discrete wavelet transform to derive and compare components that describe the periodic time and frequency behaviour of the time-dependent attribute The novel idea of this approach is that we take advantage of the property of the inverse discrete wavelet transform that guarantees that the original signal can be reconstructed from the scaled coecients This idea improves upon exist 


ing work because the approach is free from any assumptions on the structure of the attribute and does not have to rely on statistical approximations We evaluate the approach using two synthetic data sets to establish the upper and lower bounds of the covered similarity space We nd that we can cover the whole similarity space and that we are only constrained by the random number generation To validate the method against this constraint we investigate the specic inuence of the random number generation by using the method as a test of randomness Additionally operational systems usually exhibit large amounts of noise in their workload therefore we validate the method against noise as well Our ndings show that it is highly resistant to noise whereas other commonly employed methods yield to the law of large numbers or fail completely Additionally the method can consistently identify dissimilar behaviour as well even though it is not as good at that task which is only partially true for other commonly employed methods The second method uses association rule analysis to identify and quantify the relationship between descriptive attributes of the workload The novel idea of this approach is to eliminate the use of values and instead focus on the attributes themselves and therefore rank the relative usage of the attribute That way the most important building blocks of the workload can be compared directly The algorithm is described and then evaluated using synthetic data sets to establish the upper and lower bounds of the covered similarity space This time we nd that we are only constrained by the amount of learnt rules which follows from the used rule learning algorithm At least 16 rules need to be available to cover the whole similarity space when the apriori algorithm is used The method itself however does not require a specic rule learning algorithm We then present our compound measure that can address problematic workload characteristics We use the transfer function concept to weight specic events in the waveletmethod based on relative attribute dependencies Then we conduct an empirical study to evaluate all methods on operational workload from seven large-scale distributed systems Two important characteristics stand out First the time and frequency behaviour is surprisingly well-modelled with a wavelet method Second the association rule by itself is seemingly useless However the inclusion of descriptive attributes improves accuracy when determining similarity of workloads We show that the compound measure improves upon existing work by evaluating it against two important scheduling metrics mean waiting time and mean execution time The analysis of descriptive attributes for the similarity in addition to the analysis of the time-dependent attribute yields a compound similarity measure that can improve accuracy by 24.5 At the same time the standard deviation can be reduced by 10.4 We conrm these results with an independent statistical hypothesis test a twosample Anderson-Darling statistic and show the advantage of the compound measure even at higher signicance levels of  0  125 with only 11 dierence Our results strongly reassure our initial expectations that the compound measure is a good choice for large-scale and data-intensive systems that have to deal with enormous amounts of events and previously unknown dependencies in their workload Furthermore even though we only used scientic workloads in our evaluations we did not observe any constraint that would limit the use to scientic workloads 7 FUTURE WORK As a result of this work we can continue to evolve our simulation eort for DQ2 We will use the similarity measure to evaluate the results of our future workload models to specically address problems like data transfer cycles distributed le caching or popularity-based deletion However the measure can still be improved If it is suspected that dependencies change over time then the rate of change needs to be investigated Extending the association rule measure with time-evolving graphs may prove to be appropriate for such cases Additionally we only used the Haar wavelet as suggested by previous work We are interested in investigating the inuence of dierent types of wavelets on the analysis of non-periodic burst behaviour An implementation of the similarity measure including already pre-processed versions of the used workloads from the evaluation can be downloaded from our website  8 ACKNOWLEDGEMENTS We are grateful to the following teams for providing traces via the Grid Workloads Archive The AuverGrid team the DAS-2 team the Grid5000 and OAR teams the HEP eScience Group at Imperial College London for the LCG traces the NorduGrid team and J Morton and C Chrush for the SHARCNET traces 9 REFERENCES 1 R A g r a w a l T I m i e l i  nski and A Swami Mining association rules between sets of items in large databases ACM SIGMOD Record  22\(2 June 1993  L  B ergroth H Hak o nen and T  R aita A surv ey of longest common subsequence algorithms In 7th International Symposium on String Processing Information Retrieval  page 39 A Coru na ES Sept 2000 IEEE Computer Society  C  Borgelt and R  K ruse Induction of asso ciation rules apriori implementation In 14th Conference on Computational Statistics  pages 395400 Berlin DE 2002 Physica  M  B ranco Distributed data management for large scale applications  PhD thesis School of Electronics and Computer Science University of Southampton UK Nov 2009  M  B ranco E Zalusk a D de Roure M  L assnig a nd V Garonne Managing very large distributed data sets on a data grid Concurrency and Computation Practice and Experience  22\(11 Aug 2010  R  G  B ro wn D  E ddelbuettel a nd D Bauer Dieharder A random number test suite http://www.phy.duke.edu rgb/General/dieharder.php Oct 2009  L  C  C arrington M Laurenzano A Sna v e ly  R  L  Campbell Jr and L P Davis How well can simple metrics represent the performance of hpc applications In ACM/IEEE International Conference for High Performance Computing Networking Storage and Analysis  Seattle WA USA 2005 ACM  G  C asale N Mi a nd E Smirni C WS a mo del-driv e n scheduling policy for correlated workloads In ACM SIGMETRICS International conference on 


measurement and modeling of computer systems  pages 251262 New York NY USA June 2010 ACM  K  P  C han a nd A W.-C F u Ecien t t ime s eries matching by wavelets In 15th International Conference on Data Engineering  pages 126133 IEEE 1999  C Chateld The analysis of time series an introduction  CRC Press 1997  I Daub ec hies Ten lectures on wavelets  Society for Industrial and Applied Mathematics 1992  P  A Dinda a nd D R OHallaron H ost l oad prediction using linear models Cluster Computing  3\(4 2000  D F r eedman and P  D iaconis On the h istogram as a density estimator L2 theory Probability Theory and Related Fields  57\(4 1981  A Graps An in tro duction to w a v e lets IEEE Computational Science  Engineering  2\(2 1995  J Han J P e i Y Yin and R  M ao Mining frequen t patterns without candidate generation A frequent-pattern tree approach Data Mining and Knowledge Discovery  8\(1 2004  T Ho eer T  S c hneider a nd A Lumsdaine Characterizing the inuence of system noise on large-scale applications by simulation In ACM/IEEE International Conference for High Performance Computing Networking Storage and Analysis New Orleans LA USA 2010 IEEE Computer Society  A Iosup H Li M  J an S  A no ep C  Dumitrescu L Wolters and D H J Epema The grid workloads archive Future Generation Computer Systems  24\(7 July 2008  A Kramp e  J  L epping a nd W Sieb en A h y brid markov chain model for workload on parallel computers In ACM International Symposium on High Performance Distributed Computing  pages 589596 Chicago IL USA June 2010 ACM  M Lassnig C ERN P H-ADP DDMLAB public website http://cern.ch/ddmlab-public April 2011  M Lassnig T  F ahringer V  G aronne A  M olfetas and M Branco Stream monitoring in large-scale distributed concealed environments In 5th IEEE International Conference on e-Science  pages 156163 Oxford UK Dec 2009 IEEE Computer Society  M Lassnig T  F ahringer V  G aronne A  M olfetas and M Branco Identication modelling and prediction of non-periodic bursts in workloads In 10th IEEE/ACM International Symposium on Cluster Cloud and Grid Computing  pages 485494 Melbourne AU May 2010 IEEE Computer Society  H Li W orkload dynamics on clusters and g rids The Journal of Supercomputing  47\(1 2009  H Li and M  Muskulus Analysis and m o d eling o f j ob arrivals in a production grid ACM SIGMETRICS Performance Evaluation Review  34\(4 Mar 2007  U Lublin and D  G  F eitelson T he w o rkload on parallel supercomputers modeling the characteristics of rigid jobs Journal of Parallel and Distributed Computing  63\(11 Nov 2003  D P  Mandic a nd J A Cham b e rs Recurrent Neural Networks for Prediction Learning Algorithms Architectures and Stability  Wiley 2001  M Matsumoto a nd T Nishim ura M ersenne Twister a 623-dimensionally equidistributed uniform pseudo-random number generator ACM Transactions on Modeling and Computer Simulation  8\(1 Jan 1998  T N Minh L W o lters and D  E p e ma A realistic integrated model of parallel system workloads In 10th IEEE/ACM International Conference on Cluster Cloud and Grid Computing  pages 464473 Melbourne AU May 2010 IEEE Computer Society  J C Mogul Emergen t mis eha v ior v s complex software systems ACM SIGOPS Operating Systems Review  40\(4 Oct 2006  K Mohror a nd K L Kara v a nic Ev aluating similarity-based trace reduction techniques for scalable performance analysis In 22nd Annual International Conference on Supercomputing  page 55 ACM 2009  Q P a n L Zhang G Dai and H  Z hang T w o denoising methods by wavelet transform IEEE Transactions on Signal Processing  47\(12 Dec 1999  P  Ratn F  Mueller B  R  d e Supinski a nd M Sc h u lz Preserving time in large-scale communication traces In 22nd Annual International Conference on Supercomputing  pages 4655 ACM 2008  F W Sc holz and M  A  S tephens K sample anderson-darling tests Journal of the American Statistical Association  82\(399 1978  Z R Struzik and A  S ieb e s Principles of Data Mining and Knowledge Discovery  volume 1704 of Lecture Notes in Computer Science  chapter The Haar Wavelet Transform in the Time Series Similarity Paradigm pages 1222 Springer 1999  E Theresk a and G  R  G anger IR ONMo del Robust performance models in the wild In ACM SIGMETRICS International conference on measurement and modeling of computer systems  pages 253264 Annapolis MD USA June 2008 ACM  F W a silewski P yW a v elets Discrete W a v e let Transform in Python http://www.pybytes.com/pywavelets May 2010  G I W e bb OPUS A n e cien t admissible algorithm for unordered search Journal of Articial Intelligence Research  3:431465 1995  M J Zaki S calable a lgorithms f or asso ciation m ining IEEE Transactions on Knowledge and Data Engineering  12\(3 May 2000 


association rules and decision trees on analysis of diabetes data from the DiabCare program in France stud health technol inform 2002;90:557-61 6] J.Mondelle Simeon and Rober, Hilderman Exploratory Quantitative Contrast Set Mining:A Discretization Approach, 19th IEEE International Conference on Tools with Artificial Intelligence - Vol.2 ICTAI 2007 7] D.Newman, J. S.Hettich, C.L.S. Blake, and C.J. Merz, UCI Repository of machine learning databases,Irvine, CA: University of California, Department of Information and Computer Science.1998 last accessed: 1/10/2009 8] J.Han, and M.Kamber, Data mining: Concepts and techniques, San Francisco: Morgan Kaufmann Publisher, pp.47- 94, 2006 9] Glenn J. Myatt  Making sense of data: A Practical Guide to Exploratory Data Analysisand   Data Mining:Wiley\(2007 10] G. Chen, AND T.Astebro,  How to deal with missing categorical data: Test of a simple Bayesian method, Organ. Res. Methods 6, 3 2003 11] R.Agrawal, T. Imielinski, & A. Swami, Database mining aperformance perspective, IEEE Transactions on Knowledge and Data Engineering, 5\(6 1993 Special issue on Learning and Discovery in Knowledge-Based Databases 12] R.Agrawal, T.Imielinski, & A.Swami,Mining association rules between sets of items in large databases, In Proc. ACM-SIGMOD int. conf. management of data \(SIGMOD93 USA \(pp. 207216 13] Ian H.Witten and Elbe Frank, Datamining Practical Machine Learning Tools and Techniques, Second Edition, Morgan Kaufmann, .San Fransisco, 2005 14] S.Brin, R. Motwani, J.D. Ullman,  & S.Tsur, Dynamic itemset counting andimplication rules for market basket data, Proceedings of the ACM SIGMODInternational Conference on Management of Data pp. 255-264, Tucson, AZ, May 1997,ACM Press 15] M.J.Zaki, S. Parthasarathy, M. Ogihara, & W.Li, W. New algorithms for fast discovery of association rules, Proceedings of the 3rd International Conference on KnowledgeDiscovery and Data Mining \(KDD 1997,AAAI Press 16] B.Liu, W. Hsu, & Y.Ma, Pruning and summarizing the discovered association, Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 125-134 San Diego, CA, August 1999 


17] J.Han, J.Pei, & Y.Yin, Y,  Mining frequent patterns without candidate generation, Proceedings of the ACM SIGMOD International Conference on Management of Data,  Dallas, TX, May 2000 18] Y.Li, & L.Sweeney, Adding semantics and rigor to association rule learning: the GenTree approach, Technical Report CMU ISRI 05-101 2005 19] M.Rangsipan, Structure-Based Rule Selection Framework for Association Rule Mining of Traffic Accident Data, CIS 2006: 231239                                 


           334 


21] S. Baker and S.K. Nayar, A Theory of Catadioptric Image Formation, IEEE International Conference on Computer Vision \(ICCV pp.35-42, Jan, 1998 22] S.K. Nayar, Catadioptric Omnidirectional Cameras, IEEE Conference on Computer Vision and Pattern Recognition \(CVPR 488, Jun, 1997 23] A.Victorino, La commande referencee capteur: une approche robuste au proble`me de navigation, localisation et cartographie simultanees pour un robot dinterieur. PhD thesis, LUniversite de Nice-Sophia Antipolis, Inria Sophia Antipolis, 2002 3524 


ec  d Fig. 5: Computation Performance Comparison Tab. 4: Computation Savings by TOP-MATA K Connect K Retail K Wap La12 50 58.35% 100 0.01% 200 0.83% 23.04 150 55.91% 400 2.65% 400 30.12% 45.38 250 53.61% 700 1.84% 800 20.03% 25.95 350 48.28% 1100 3.95% 1600 13.06% 27.89 450 43.12% 1400 1.48% 3200 6.14% 12.70 550 39.36% 1700 4.00% 6400 5.63% 7.11 Second, Fig. 5 shows the results of four data sets computed by TOP-MATA and TOP-DATA, respectively. As can be seen, in general, TOP-MATA shows a better performance than TOP-DATA. And as the increase of the ? value, the advantage tends to be even more impressive for these four data sets 4.3. The Computation Saving of TOP-MATA As can be seen in the Tab. 4, four data sets, enjoy signi?cant computation savings brought by TOP-MATA. We can conclude that the computation saving is a major factor for the performance of TOP-MATA. That is, compared with TOP-DATA, a higher computation saving implies a much better performance of TOP-MATA. Since this saving is more signi?cant as the increase of the items, TOP-MATA works better for large scale data sets with a large number of items 5. Conclusion In this paper, we studied the problem of searching for top? item pairs with the highest cosine values among all item pairs. Speci?cally, we provided a novel algorithm TOPMATA which employ a Max-First traversal strategy for ef?ciently performing top-? cosine similarity search. Extensive experimental results veri?ed the effectiveness of the algorithms, And TOP-MATA algorithm is superior to TOPDATA for large-scale data sets with multiple items Acknowledgment This research was partially supported by the National Natural Science Foundation of China \(NSFC No. 70901002 and the Ph.D. Programs Foundation of Ministry of Education of China \(No. 20091102120014 


REFERENCES 1] R. Agrawal, T. Imielinski, and A. Swami, Mining association rules between sets of items in large databases, in SIGMOD 1993 2] C. Alexander, Market Models: A Guide to Financial Data Analysis. John Wiley & Sons, 2001 3] W. Kuo, T.-K. Jensen, A. Butte, L. Ohno-Machado and I. Kohane, Analysis of matched mrna measurements from two different microarray technologies Bioinformatics, vol. 18, p. 405C412, 2002 4] H. Xiong, X. He, C. Ding, Y. Zhang, V. Kumar, and S. Holbrook, Identi?cation of functional modules in protein complexes via hyperclique pattern discovery in PSB, 2005 5] J. Han, H. Cheng, D. Xin, and X. Yan, Frequent pattern mining: Current status and future directions DMKD, vol. 15, no. 1, pp. 5586, 2007 6] P.-N. Tan, M. Steinbach, and V. Kumar, Introduction to Data Mining. Addison-Wesley, 2005 7] S. Brin, R. Motwani, and C. Silverstein, Beyond market basket: generalizing association rules to correlations, in SIGMOD 1997, Tucson, AZ, 1997, pp 265276 8] E. Omiecinski, Alternative interestmeasures formining associations, TKDE, vol. 15, pp. 5769, 2003 9] H. Xiong, S. Shekhar, P.-N. Tan, and V. Kumar Exploiting a support-based upper bound of pearsons correlation coef?cient for ef?ciently identifying strongly correlated pairs, in KDD 2004, 2004, pp 334343 10] I. Ilyas, V. Markl, P. Haas, P. Brown, and A. Aboulnaga, Cords: Automatic discovery of correlations and soft functional dependencies, in SIGMOD 2004 2004, pp. 647658 11] J. Zhang and J. Feigenbaum, Finding highly correlated pairs ef?ciently with powerful pruning, in CIKM 2006, 2006, pp. 152161 12] H. Xiong, W. Zhou, M. Brodie, and S. Ma, Top-k correlation computation, JOC, vol. 20, no. 4, pp 539552, 2008 13] S. Zhu, J. Wu, and G. Xia, Top-k cosine similarity interesting pairs search, in 


http://datamining.buaa.edu.cn/TopKCos.pdf 14] M. Zaki, Scalable algorithms for association mining, TKDE, vol. 12, pp. 372390, 2000 


enhance item-based collaborative filtering, in 2nd IASTED International Conference on Information and Knowledge Sharing, Scottsdale, Arizona, 2003 476 2010 10th International Conference on Intelligent Systems Design and Applications 


Basi Association Rles Basi c  Association R u les Association is basically connecting or tying up occurrences of Association is basically connecting or tying up occurrences of events Ol dib t f ilt t O n l y d escr ib e se t s o f s i mu lt aneous even t s Cannot describe patterns that iterate over time e g  itemset a  0  b  0  g    Eg If you sense higher data rates on the downlink than normal AND New Route generated Implies high chances of Intrusion AND New Route generated Implies high chances of Intrusion Associative IDS for NextGen Frameworks Dr S Dua LA Tech 20 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


