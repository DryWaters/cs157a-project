Classifying using Speciﬁc Rules with High Conﬁdence R Hern  andez-Le  on   J A Carrasco-Ochoa   J Fco Mart  nez-Trinidad  and J Hern  andez-Palancar   Computer Science Department National Institute of Astrophysics Optics and Electronics Puebla Mexico Email  raudel,ariel,fmartine  inaoep.mx  Data Mining Department Advanced Technologies Application Center La Habana Cuba Email  rhernandez,jpalancar  cenatav.co.cu 
Abstract In this paper we introduce a new strategy for mining the set of Class Association Rules CARs that allows building speciﬁc rules with high conﬁdence Moreover we introduce two propositions that support the use of a conﬁdence threshold value equal to 0  5  We also propose a new way for ordering the set of CARs based on rule size and conﬁdence values Our results show a better average classiﬁcation accuracy than those obtained by the best classiﬁers based on CARs reported in the literature Keywords Data Mining Supervised Classiﬁcation Class Association Rules Association Rule Mining I I NTRODUCTION Classication Association Rule Mining CARM or as 
sociative classication introduced in is a well-kno wn Data Mining technique for the extraction of classication rules CARM integrates Classication Rule Mining CRM  and Association Rule Mining ARM 3 for mining a special subset of association rules called Class Association Rules CARs A classier based on this approach usually consists of an ordered CAR list l  and a mechanism for classifying unseen transactions using l  7 11 Associative classication has been used in several tasks including text segmentation determination of DN A splice junction types mammalian mesenchymal stem cell differentiation and prediction of protein-protein interaction types among others In CARM similar to ARM we have a set of items I 
  i 1 i 2 i n   a set of classes C and a set of labeled transactions D  where each transaction in D comprises a set of items X  I and one class c  C  The support of an itemset X  I denoted as Sup  X   is the fraction of transactions in D containing X see Eq 1 The idea of CARM aims at extracting a set of CARs where a CAR is 
an implication of the form X  c  being X  I and c  C  In CARM as well as in ARM two threshold values are commonly used to determine the interest of a CAR 1 Support The support of a CAR X  c see Eq 2 is the fraction of transactions in D that contains X  c   2 Conﬁdence The condence of a CAR X  c see Eq 3 is the probability of nding c in transactions that also contain X  which represents how strongly the rule antecedent 
X implies the rule consequent c  Sup  X   D X    D  1 where D X is the set of transactions in D containing X  Sup  X  c  Sup  X  c   2 Conf  X  c  Sup  X 
 c  S up  X  3 Many studies 12 ha v e pointed out the combinatorial number of association rules that could be obtained when a small support threshold is used To address this problem some works 9 10 propose to prune the search space each time a CAR satises predened support and condence thresholds this strategy allows us to obtain general small rules However this strategy has two main drawbacks 1 It does not allow to generate specic large rules some of which could be more interesting i.e with higher condence 2 It extends a candidate CAR satisfying the support threshold until this CAR also satises the condence threshold which implies that many branches of the CARs search space could be explored in vain 
In this paper we introduce a new pruning strategy to generate the set of CARs The rules computed by this strategy are specic large rules with high condence For mining the set of CARs an appropriate condence threshold value supported by two propositions is used Besides we propose a new way for ordering the set of CARs based on the size of the CARs and their condence value The ordered CARs together with the Best K rules mechanism are integrated in a CAR based classier called CAR-IC which is more accurate than CBA CMAR CPAR TFPC and HARMONY classiers This paper is organized as follows The next section describes the related work The third section introduces our classier In the fourth section the experimental results are 
Ninth Mexican International Conference on Artificial Intelligence 978-0-7695-4284-3/10 $26.00 © 2010 IEEE DOI 10.1109/MICAI.2010.24 75 


shown Finally the conclusions as well as the future work are given in section 036ve II R ELATED WORK Several classi\036ers based on CARs have been developed  4 5 7 11 19 In general these classi\036ers can be divided in two groups according to the strategy used for computing the set of CARs 1 Two Stage classi\036ers In a 036rst stage all the CARs satisfying the support and con\036dence thresholds are mined and later in a second stage a classi\036er is built by selecting a small subset of CARs that fully covers the training set The classi\036ers CBA CMAR 7 and MCAR follo w this strate gy  2 Integrated classi\036ers In these classi\036ers the subset of CARs is generated directly The classi\036ers CPAR  TFPC 4 and HARMONY 19 follo w this strategy In the literature regardless of the strategy used for computing the set of CARs there are 036ve main schemes for ordering CARs a CSA Con\036dence Support Antecedent size The CSA ordering scheme sorts the rules in a descending order according to their con\036dence Those CARs that share a common con\036dence value are sorted in a descending order according to their support and in case of tie CSA sorts the rules in ascending order according to the size of their rule antecedent This scheme has been used by the CBA classi\036er b ACS Antecedent size Con\036dence Support The ACS ordering scheme is a variation of CSA But it takes into account the size of the rule antecedent as 036rst ordering criterion followed by con\036dence and support The classi\036er TFPC follo ws this scheme c WRA Weighted Relative Accuracy The WRA ordering scheme proposed in assigns to each CAR a weight and then sorts the set of CARs in a descending order according the assigned weights The WRA has been used to order CARs in two versions of the TFPC classi\036er 10 Gi v en a rule A 004 B the WRA is computed as follows WRA  A 004 B  Sup  A  Conf  A 004 B   Sup  B  d LAP Laplace Expected Error Estimate The LAP ordering scheme was introduced by Clark and Boswell  and it has been used to order CARs in CP AR classi\036er Gi v en a rule A 004 B  in the LAP is de\036ned as follows LAP  A 004 B  Sup  A 004 B  S up  A   C  where C is the set of prede\036ned classes e 002 2 Chi-Square The 002 2 ordering scheme is a well known technique in statistics which can be used to determine whether two variables are independent or related After computing an additive 002 2 value for each CAR this value is used to sort the set of CARs in a descending order in the CMAR classi\036er There are three main satisfaction mechanisms reported in the literature 7 10 1 Best rule  This mechanism selects the 036rst best rule in the order that satis\036es the transaction to be classi\036ed unseen data and then the class associated to the selected rule is assigned to this transaction 2 Best K rules  This mechanism selects the best K rules for each class that satisfy the transaction to be classi\036ed and then the class is determined using these K rules according to different criteria 3 All rules  This mechanism selects all rules that satisfy the transaction to be classi\036ed and then these rules are used to determine the class of the new transaction Algorithms following the Best rule mechanism could suffer biased classi\036cation or over\036tting since the classi\036cation is based on only one rule On the other hand the All rules mechanism includes rules with low ranking for classi\036cation and this could affect the accuracy of the classi\036er Since the Best K rules mechanism has been the most used satisfaction mechanism for CAR based classi\036ers and it reports the best results we will use it in our work III P ROPOSED CLASSIFIER First of all in subsection III-A we propose two propositions that support the use of a con\036dence threshold value equal to 0.5 for mining the CARs An important part of our classi\036er is the pruning strategy used during the mining of the set of CARs thus in subsection III-B we introduce this strategy In section III-C we propose a new way for ordering the set of CARs and describe how the pruning strategy and the new ordering are used to de\036ne a new CAR based classi\036er A Determining the conﬁdence threshold All the CAR based classi\036ers use different support and con\036dence thresholds for mining the set of CARs The threshold values used in those works must be carefully de\036ned by the user there is not a guideline that helps to the user to choose these values In this paper we propose to use a con\036dence threshold that allows to obtain CARs with different antecedent avoiding ambiguity at the classi\036cation stage In order to determine an appropriate con\036dence threshold we introduce two propositions The proposition 1 guarantees that the sum of the con\036dence values of all CARs having identical antecedent is 1  Later in proposition 2 we show that only one CAR can have a con\036dence value greater than 0  5  
76 


Proposition 1 Let X be an itemset and C   c 1 c 2 c m  be the set of prede\036ned classes the following equation is ful\036lled m 002 i 1 Conf  X 004 c i  4 The demonstration is immediate from the de\036nitions of itemset support Eq 1 CAR support Eq 2 and CAR con\036dence Eq 3 Proposition 2 Let X be an itemset and C   c 1 c 2 c m  be the set of prede\036ned classes only one CAR X 004 c k  c k 003 C  can have a con\036dence value greater than 0  5  Since Conf  X 004 c  takes values between 0 and 1  the demonstration is immediate from the proposition 1 Based on proposition 2 if we set the con\036dence threshold to 0  5  for each itemset X we can obtain at most one CAR having X as antecedent and in this way ambiguity at the classi\036cation stage is avoided For our experiments we will use this value as con\036dence threshold B Mining the set of CARs In order to generate the set of CARs we follow the ideas of CA a frequent itemset mining algorithm which according to the experiments shown in outperforms other ef\036cient algorithms for mining frequent itemsets like Apriori used in CBA Fp-growth used in CMAR and TFP used in TFPC In for mining ARs the authors propose partitioning the itemset space into equivalence classes grouping itemsets of the same size k which have a common  k  1 length pre\036x An equivalence class grouping k itemsets will be denoted as EC k  In our proposal unlike the algorithm proposed in we consider each prede\036ned class c 003 C as another item and we propose to divide the CAR space into equivalence classes de\036ned by the following equivalence relation The CARs of size k that share the consequent the same class and the 036rst k  2 items of the antecedent which has k  1 items belong to the same equivalence class Similar to CA in order to compute support v alues we take advantage of bit-to-bit operations by representing the dataset as an m x n binary matrix being m the number of transactions and n the number of items including the class items In the literature regardless of the used strategy for mining the CARs each CAR satisfying the support and con\036dence thresholds is extended by adding a new item The extended CAR called candidate CAR is searched in the transaction dataset to obtain their support and con\036dence values Recent algorithms for mining the set of CARs 9 10 prune the CAR search space each time a CAR satisfying the support and con\036dence thresholds is found it means that CARs satisfying both thresholds are not extended anymore This strategy produces more general rules Besides these algorithms do not prune the CAR search space when a CAR only satis\036es the support threshold instead of it they continue extending the CAR until it satis\036es the con\036dence threshold With this pruning strategy at most one CAR is obtained for each branch of the CARs search space but many branches could be explored in vain In order to allow generating more speci\036c rules with high con\036dence we introduce the following pruning strategy Let X be an itemset c be a class and i be an item if the candidate CAR X 004 c does not satisfy either support threshold or con\036dence threshold we do not extended the CAR anymore i.e we prune the CARs space avoiding to generate candidate CARs from CARs that not satisfy the support and con\036dence thresholds Otherwise if the candidate CAR X 004 c satis\036es the support and con\036dence thresholds we follow extending the CAR while Conf  X 005 i 004 c    Conf  X 004 c   thus allowing to obtain for each branch many CARs with high con\036dence C Ordering and Classifying Once the set of CARs has been generated the CAR list is sorted As it was mentioned earlier for classifying we use more speci\036c rules with high con\036dence therefore we propose a new way for sorting the set of CARs First we sort the set of CARs in a descending order according to the size of the CARs and in case of tie we sort the tied CARs in a descending order according to their con\036dence Algorithm 1 CAR-IC training phase Input training dataset db Output the classi\036er 1 Answer 006\007 2 CARs 006 Generating CARs  db  3 Answ er 006 Ordering CARs  C ARs  4 return Answer Algorithms 1 and 2 show respectively the pseudo code of the training phase and classi\036cation phase of CAR-IC In the training phase Alg 1 the Generating C ARs function computes the set of CARs from the training dataset and after that the Ordering C ARs function sorts the obtained set of CARs in the way described above Algorithm 2 CAR-IC classi\036cation phase Input set of sorted CARs  unseen transaction t Output the assigned class 1 Answer 006\007 2 BestK 006 Select BestK  t  3 Answ er 006 Classify  BestK  4 return Answer F or classifying unseen transactions we decided to use the 
77 


Best K rules satisfaction mechanism since it has reported the best results 7 10 In the classi\036cation phase Alg 2 to classify an unseen transaction t  for each class the Best K rules we used K=5 as in the other evaluated classi\036ers covering t are selected using the Select BestK function Later the class having the greatest average of the con\036dence values of their selected K rules is assigned to t  Classif y function If there is a tie one of the tied classes is randomly assigned If no rule covers t  the default class is assigned in our algorithm we use the majority class as default class IV E XPERIMENTAL RESULTS In this section we report some experimental results comparing our classi\036er CAR-IC against the best classi\036ers based on CARs reported in the literature CBA CMAR  CP AR 11 TFPC 4 and HARMONY 19 Other good classi\036ers like RCBT and DDPMine 23 were not included in the experiments because the authors of these works did not provide their programs and from the description of the algorithms given in their papers it is impossible to implement them Moreover the 036rst one was evaluated using only four gene expression datasets which were not provided by the authors either and the second one was evaluated using only 8 unusual datasets from the UCI repository Table I D ATASET CHARACTERISTICS  Dataset  instances  items  classes adult 48842 97 2 anneal 898 73 6 ecoli 336 34 8 037are 1389 39 9 glass 214 48 7 heart 303 52 5 hepatitis 155 56 2 horseColic 368 85 2 iris 150 19 3 led7 3200 24 10 letRecog 20000 106 26 mushroom 8124 90 2 pageBlocks 5473 46 5 pima 768 38 2 w aveform 5000 101 3 The 036rst four classi\036ers were downloaded from the Frans Coenenís homepage http://www.csc.liv.ac.uk 010 frans and for HARMONY we used the accuracy values reported in  since the program w as not pro vided by the authors and it is impossible to implement the classi\036er from the paper Experiments were done using ten-fold cross-validation reporting the average accuracy of the ten folds Our tests were performed on a PC with an Intel Core 2 Duo at 1.86 GHz CPU with 1 GB DDR2 RAM running Windows XP SP2 As in other works 4 7 11 e xperiments were conducted using several datasets 15 in our case see characteristics in Table I The chosen datasets were originally Table II C LASSIFICATION ACCURACY  Dataset CBA CMAR CPAR TFPC HARMONY CAR-IC adult 84.21 79.72 77.24 80.79 81.90 82.11 anneal 94.65 89.09 94.99 88.28 91.51 91.80 ecoli 83.17 77.01 80.59 58.53 63.60 82.06 037are 84.23 83.30 64.75 84.30 75.02 85.98 glass 68.30 74.37 64.10 64.09 49.80 68.12 heart 57.33 55.36 55.03 51.42 56.46 53.21 hepatitis 57.83 81.16 74.34 81.16 83.16 84.56 horseColic 79.24 80.06 81.57 79.06 82.53 82.47 iris 94.00 92.33 94.70 95.33 93.32 96.06 led7 66.56 72.31 71.38 68.71 74.56 72.71 letRecog 28.64 26.25 28.13 27.57 76.81 73.14 mushroom 46.73 100.00 98.52 99.03 99.94 98.54 pageBlocks 90.94 87.98 92.54 89.98 91.60 91.82 pima 75.03 72.85 74.82 74.36 72.34 75.23 w aveform 77.58 72.22 70.66 66.74 80.46 73.06 A verage 72.56 76.27 74.89 73.96 78.20 80.72 T able III R ANKING POSITION BASED ON ACCURACY  Dataset CBA CMAR CPAR TFPC HARMONY CAR-IC adult 15 64 3 2 anneal 25 16 4 3 ecoli 14 36 5 2 037are 34 62 5 1 glass 21 45 6 3 heart 13 46 2 5 hepatitis 53 43 2 1 horseColic 54 36 2 1 iris 46 32 5 1 led7 63 45 1 2 letRecog 36 45 1 2 mushroom 61 53 2 4 pageBlocks 46 15 3 2 pima 25 34 6 1 w aveform 2 4 5 6 1 3 A verage 3.13 4.00 3.73 4.53 3.13 2.27 tak en from the UCI Machine Learning Repository and their numerical attributes were discretized by the author of  using the LUCS-KDD discretized/normalized ARM and CARM Data Library The discretization technique used in LUCS-KDD is different from those used in 7 11 thus the performance reported in tables II and III may be different from those reported in previous studies even for the same classi\036er and the same dataset For CBA CMAR CPAR and TFPC classi\036ers we used the support threshold set to 0  01 and the con\036dence threshold set to 0  5  as their authors suggest In the authors of HARMONY obtained the best results using a support threshold of 0  5  In our classi\036er CAR-IC we used the con\036dence threshold also set to 0  5  based on our previous analysis see section III-A Table II shows the accuracy of all tested classi\036ers on the 15 datasets and Table III shows the ranking position obtained by each classi\036er according to its accuracy value Analyzing these tables we can see that CBA has the worst performance in average accuracy while it has a good performance in 
78 


average ranking this is because although CBA has low accuracy values for some datasets it reaches the 036rst place in 3 of the tested datasets and the second place in other 4 datasets On the contrary CMAR has a good average accuracy but a poor average ranking w.r.t the other evaluated classi\036er Our proposed classi\036er CAR-IC has the best average classi\036cation accuracy outperforming all other classi\036ers and having in average a difference in accuracy of more than 2  5 w.r.t the classi\036er in the second place Additionally CAR-IC has the best average ranking position w.r.t the other evaluated classi\036er The classi\036er with the second best performance was HARMONY which was the second best in average accuracy as well as in average ranking Although the original implementations of CBA CMAR and CPAR use different discretization/normalization techniques we consider interesting to show in Table IV a comparison of the accuracies obtained by our classi\036er CAR-IC against the best reported accuracies of all the evaluated classi\036ers In the case of HARMONY the authors did not report which technique was used for discretization/normalization Despite the discretization/normalization technique is not the same CAR-IC obtains the best average accuracy being 1.09 better than the second best V C ONCLUSION In this paper we have proposed an accurate classi\036er based on CARs This classi\036er called CAR-IC introduces a new pruning strategy for obtaining speci\036c rules with high con\036dence moreover we propose two propositions that support the use of a con\036dence threshold equal to 0.5 for mining the rules Besides we propose a new way for ordering the set of CARs using the size of the CARs and their con\036dence value The experimental results show that CAR-IC has better performance than CBA CMAR CPAR TFPC and HARMONY classi\036ers In general CAR-IC has the best average classi\036cation accuracy as well as the best average ranking As future work we are going to study the problem of producing rules with multiple labels it means rules with multiple classes in the consequent For this we will study the use of con\036dence thresholds smaller than 0  5 allowing to obtain more than one rule with the same antecedent A CKNOWLEDGMENT This work was partly supported by the National Council of Science and Technology of Mexico under the project CB2008-01-106443 and grant 228056 R EFERENCES  B Liu W  Hsu and Y  Ma Inte grating classi\036cation and association rule mining In Proceedings of the 4th International Conference on Knowledge Discovery and Data Mining pp 8086 New York NY USA 1998  J R Quinlan C4.5 programs for machine learning In Mor gan Kaufmann Publishers Inc San Francisco CA USA 1993  R Agra w al and R Shrikant F ast Algorithms for Mining Association Rules In Proceedings of the 20 th International Conference on Very Large Data Bases pp 487-499 Santiago Chile 1994  F  Coenen P  Leng and L Zhang Threshold T uning for Improved Classi\036cation Association Rule Mining In Proceedings of Paci\036c-Asia Conference on Advances in Knowledge Discovery and Data Mining PAKDD 05 Vol 3518 pp 216225 Hanoi Vietnam 2005  F  Berzal J C Cubero D S  anchez and J M Serrano ART A Hybrid Classi\036cation Model In Machine Learning Vol 54 Number 1 pp 67-92 2004  R Hern  andez J Hern  andez J A Carrasco and J Fco Mart  031nez Algorithms for Mining Frequent Itemsets in Static and Dynamic Datasets In Intelligent Data Analysis Vol 14 Number 3 pp 419-435 2010  W  Li J Han and J Pei CMAR accurate and ef 036cient classi\036cation based on multiple class-association rules In Proceedings IEEE International Conference on Data Mining ICDM 01 pp 369-376 2001  F  Thabtah P  Co wling and Y  Peng MCAR multi-class classi\036cation based on association rule In Proceedings of the 3rd ACS/IEEE International Conference on Computer Systems and Applications pp 33 2005  Y  J W ang Q Xin and F  Coenen A No v el Rule Order ing Approach in Classi\036cation Association Rule Mining In Proceedings of the 5 th international conference on Machine Learning and Data Mining in Pattern Recognition pp 339348 Leipzig Germany 2007  Y  J W ang Q Xin and F  Coenen Hybrid Rule Ordering in Classi\036cation Association Rule Mining In Transaction on Machine Learning and Data Mining MLDM 08 Vol 1 Number 1 pp 1-15 2008  X Y in and J Han CP AR Classi\036cation based on Predicti v e Association Rules In Proceedings of the Third SIAM International Conference on Data Mining San Francisco CA 2003  M Zaki S P arthasarathy  M Ogihara and W  Li Ne w Algorithms for Fast Discovery of Association Rules In 3rd International Conference on Knowledge Discovery and Data Mining pp 283-286 USA 1997  N La vra  c P Flach and B Zupan Rule Evaluation Measures A Unifying View In Proceedings of the 9th International Workshop on Inductive Logic Programming ILP 99 pp 174185 1999  P  Clark and R Boswell Rule Induction with CN2 Some Recent Improvments In Proceedings of European Working Session on Learning ESWL 91 pp 151-163 Porto Portugal 1991  A Asuncion and D J Ne wman UCI Machine Learning Repository In http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 
79 


Table IV C OMPARISON OF THE ACCURACIES OBTAINED BY CAR-CI AGAINST THOSE ACCURACIES REPORTED BY THE METHODS CBA CMAR CPAR TFPC AND HARMONY Dataset CBA-R CMAR-R CPAR-R TFPC-R HARMONY-R CAR-IC adult 84.20 80.10 76.70 80.80 81.90 82.11 anneal 97.90 97.30 98.40 88.30 91.51 91.80 ecoli 83.17 77.01 80.59 58.53 63.60 82.06 037are 84.20 84.30 64.75 84.30 75.02 85.98 glass 73.90 70.10 74.40 64.50 49.80 68.12 heart 81.90 82.20 82.60 51.40 56.46 53.21 hepatitis 81.80 80.50 79.40 81.20 83.16 84.56 horseColic 82.10 82.60 84.20 79.10 82.53 82.47 iris 94.70 94.00 94.70 95.30 93.32 96.06 led7 71.90 72.50 73.60 57.30 74.56 72.71 letRecog 28.64 25.50 28.13 26.40 76.81 73.14 mushroom 46.70 100.00 98.52 99.00 99.94 98.54 pageBlocks 90.90 90.00 92.54 90.00 91.60 91.82 pima 72.90 75.10 73.80 74.40 72.34 75.23 w aveform 80.00 83.20 80.90 74.40 80.46 73.06 A verage 76.99 79.63 78.88 73.66 78.20 80.72  W Wang Y J Wang R Ba nares Z Cui and F Coenen Application of Classi\036cation Association Rule Mining for Mammalian Mesenchymal Stem Cell Differentiation In Proceedings of the 9th Industrial Conference on Advances in Data Mining Applications and Theoretical Aspects ICDM 09 pp 51-61 Leipzig Germany 2009  S H P ark J A Re yes D R Gilbert J W  Kim and S Kim Prediction of protein-protein interaction types using association rule based classi\036cation In BMC Bioinformatics Vol 10 Number 1 2009  E Cesario F  F olino A Locane G Manco and R Ortale Boosting text segmentation via progressive classi\036cation In Knowledge and Information Systems Vol 15 Number 3 pp 285-320 2008  J W ang and G Karypis On Mining Instance-Centric Classi\036cation Rules In IEEE Transactions on Knowledge and Data Engineering Vol 8 Number 11 pp 1497-1511 2006  F  Coenen The LUCS-KDD discretised/normalised ARM and CARM Data Library Department of Computer Science The University of Liverpool UK In http://www.csc.liv.ac.uk 002 frans/KDD/Software/LUCS-KDDDN 2003  J W ang and G Karypis HARMONY  Ef 036ciently mining the best rules for classi\036cation In Proceedings of SDM pp 205216 2005  G Cong K L T an Anthon y K H T ung and X Xu Mining top-K covering rule groups for gene expression data In Proceedings of the 2005 ACM SIGMOD international conference on Management of data pp 670-681 2005  H Cheng X Y an J Han and S Y  Philip Direct Discriminative Pattern Mining for Effective Classi\036cation In Proceedings of the 2008 IEEE 24th International Conference on Data Engineering pp 169-178 2008 
80 


world databases presents a future direction for further research  Figure 1: Execution time v/s Minimum Support  References 1 R. A g r a w a l a nd R. Srik a n t F a s t A l g o rithm s  f o r Mining  Association Rules,î In Proc. of the 20th VLDB Conference  Santiago, pp. 487-499, Chile, 1994 2 R. A g ra w a l T  Im i e linsk i, a nd A. Swami. ìMining Association Rules between Sets of Items in Large Databases.î In Proceedings of ACM SIGMOD pages 207-216, May 1993 3 R. A g ra wa l, R. Srik a n t M in i n g se que ntia l pa t t e r ns In proc. of the 11th International Conference on Data Engineering \(ICDE'95  pages 3-14, March 1995 4 M Blu m R.W  Flo y d   V   P r att, R.L  Riv e st an d R E. T a rjan  Time bounds for selection,î J. Comput. Syst. Sci. 7\(1973\, pp 448-461  Total number of data items Number of elements in MEDIUM Number of elements in LOW 10 139 0 20 3238 127 30 21899 1923 Figure 2: Size of MEDIUM and LOW v/s number of data items 5 Fe r e nc B odo n  A f a s t  A P R I O R I i m plem e n ta tion I n pr oc o f  IEEE ICDM Workshop on Frequent Itemset Mining Implementations \(FIMI'03 Melbourne, Florida, USA, 2003 6 Feren c Bo d o n   A T r i e b as ed A P RIORI Im p l e m en tatio n f o r  Mining Frequent Item Sequences.î In ACM SIGKDD Workshop on Open Source Data Mining Workshop \(OSDMí05 pages 56-65 Chicago, IL,USA, 2005  M  C h en  J Han an d  P  S   Yu  Dat a M i n i n g  A n o v ervi e w  from a Database Perspective IEEE Transactions on Knowledge and Data Engineering vol. 8, no. 6, pp. 866-883, Dec. 1996 8 A r on Culotta A ndre w Mc Ca llum Jona tha n Be tz  I nte g ra tin g probabilistic extraction models a nd data mining to discover relations and patterns in text,î In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics p.296-303, June 04-09, 2006, New York  M M S u fy an Be g P aral l e l an d Di st ri b u t ed Di sco v er y o f  Association Rules In Artifical Intelligence Application Book  Fadzilah Siraj, Eds\ersity Utara Malaysia 10 X ita o Fa n k os Fe ls v  l y i Ste phe n A  Siv o  M onte C a r l o   SASÆ for Monte Carlo studies: a guide for quantitative researchers 11 U  Fa y y e d  G  P i a t e t s k y Sha p iro P. Sm y t h a nd R. U t h u ra s a my  eds.\. ìAdvances in Knowledge Discovery and Data Mining AAAI Press / The MIT Press, 1996 1 W  J F r aw l e y  G  P i at et sk y S h ap i r o an d C M a t h eu s   Knowledge ìDiscovery In Databases: An Overview. In Knowledge Discovery In Databases eds. G. Piatetsky-Shapiro, and W. J Frawley, AAAI Press/MIT Press, Cambridge, MA., 1991, pp. 1-30 13 V  K u m a r  a nd M. J o s h i T utor ia l o n H i g h P e r f or m a nc e D a t a  Mining,î In proc. of International Conference on High Performance Computing \(HiPC-98 Dec. 1998  a v i d L  O l s o n a n d D e s h e n g Wu   D eci s i on  m a k i ng  with uncertainity and data mining.î In X. Li, S. Wang and Z.Y Dong\(Eds Lecture notes in Artificial Intelligence pp. 1-9 Berlin: Springer\(2005 15  P  W r ig ht. K now le dg e D i s c ov e r y I n D a ta ba s e s  T ools a n d  Techniques ACM  Crossroads Winter 1998   
408 


 7. Reference    Fetzer C,Hagstedt, K,Felb er P  Autom atic Deteciton  and Masking of Non-Atomic Exception Handling International Conference On Dependable Systems and Networks, \(DSN2003\10-116    Y e n S J, Lee Y S. ìMining Interesting  Associatio n  R u l es  and Sequential Patternsî. International Journal of Fuzzy Systems, 2004-6 \(4   Alasf f ar A  H, Deogun J S. ìConcept-b a sed Retr iev a l with Minimal Term Setsî. Foundations of Intelligent Systems: 11th Intíl Symposium, Springer, Poland, 2004 114- 122   Qiu Y ong gang,Frei H P  Concept B a sed Quer y   SIGIRí03,2003:16 0-169   Saltom G  W ong A, Y a ng C  S. ìA V ector Sp ace Model for Automation Indexingî. Communications of the ACM 2005, 18\(5\-620   Agrawal R, Srikant R. ìFast Algorithm f or Mining Association Rules in Large Databases.î Proceedings of the 20th International Conference on Very Large DataBases Santiago , Chile , 2004   Park J S. ìUsing A Hash-Based Method with Transaction Trimming forMining Association Rules.î IEEE Transactions on Knowledge and Data Engineering, 2007   Savasere A, Omiecinski E Navathe S  An Ef ficient Algorithm for Mining Association Rules in Large Databases.î Proceedings of the 21st International Conference on Very large Database, Switzerland, 2002  


              


   


                        





