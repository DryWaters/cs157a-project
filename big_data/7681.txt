KNOWLEDGE-BASED APPROACH OF BUILDING PLAN CHECKER SYSTEM USING COMPUTER-AIDED DESIGN \(CAD  Nurazlina Md Sanusi 1  Fac.of Information and Communication Tech Universiti Teknikal Malaysia Melaka Ayer Keroh, Melaka, Malaysia nurazlina utem.edu.my  Emaliana Kasmuri 3  Fac.of Information and Communication Tech Universiti Teknikal Malaysia Melaka Ayer Keroh, Melaka, Malaysia emaliana utem.edu.my  Nuridawati Mustafa 2  Fac.of Information and Communication Tech Universiti Teknikal Malaysia Melaka Ayer Keroh, Melaka, Malaysia nuridawati utem.edu.my  Mohd Sanusi Azmi 4  Fac.of Information and Communication Tech Universiti Teknikal Malaysia Melaka Ayer Keroh, Melaka, Malaysia sanusi utem.edu.my    Abstract Current plan checking process has been a tedious and time consuming process for developers and local authorities with many 
errors and problems. This may result to a late of building development progress and can cost a big number amount of money. The aim of this research is to build a software information system that integrates development plan using computer-aided design \(CAD\d to evaluate the development plan accordin g to the standards. The compliance plan will be stored into knowledge-based repository for use by other local authorities for checking purposes. This build ing plan checking system BPCS\also acts as a repository for the development plan configura tion standards in the data processing system. BPCS also serves as an assessor for the building components in the development plan to determine compliance of the development plan configuration with the deve lopment plan configuration standards using in the system   Keywords: Knowledge-Based, CAD, Building Plan   I. INTRODUCTION  Before any building development starts, an architect needs to submit the building plan to get an approval. The building plans need to be sent to several local authorities for checking purposes. The checking process should be made to make sure that the building plan is comply with the building configuration standards. Once the development plan gets the approv al, then the building development can be started However, the current working flow 
consumes more time and it may leads to wrong evaluation because the checking processes were done manually. All local authorities need to do manual checking for each part of the building whether it comply with the standards or not. It may lead to missed checking some important parts of the building and all the local authorities need to check for their own specific parts. Refer Figure 1   
 Figure1: Manual Checking Work Flow   Therefore, the motivation to develop BPCS is to overcome the manual process problems and to computerize the plan checking process. The objective is also to spee d up the checking process Knowledge- based repository will be used to store all the compliance building structure plan for the use of other local authorities to do checking As overall, the paper contents are organized as follows. Firstly, the related works will be discussed. Then, the frameworks design is delivered followed by the future works including the hardware and software that will be use in this research. Finally, the conclusion are presented  II. RELATED WORKS  According to [1  kn o w l e d g ebase d sy st em  KBS\nswers the specific questions that analyst 
2009 International Conference on Future Computer and Communication 978-0-7695-3591-3/09 $25.00 © 2009 IEEE DOI 10.1109/ICFCC.2009.65 8 
2009 International Conference on Future Computer and Communication 978-0-7695-3591-3/09 $25.00 © 2009 IEEE DOI 10.1109/ICFCC.2009.65 8 


pose and gives advice to enhance their problem solving abilities. This is important because if an organization wants to keep the cost of solving the problem as low as possi ble. KBS is graded according to its coverage, accuracy and speed in resolving the problem While according to [2 nown fact that for individuals “l earning from failures” is a painful but successful process to create knowledge Experiences are stored in knowledge bases which are accessible by means of diagnostic tools Repetitive errors in production were reducing significantly But, in [3 wled g e b ased repo sitory is being used for storing and retrieving business components. In their component-based development \(CDB\, finding the right components quickly is vital to developing business applications As the number and variety of components increases the component knowledge-based repository provides a viable approach to classifying components However, [4 devel ope d a soft wa re  management system to maintain a balance between tracking needs of management, the creative needs of software developers and reliability needs of software users. The knowledge base is used as the configuration database, code repository and the file system itself  Refer to [5 e k now led g e b ased is applied in order to integrate multi-type data in a hospital environment. Through the introduction of knowledge-based modules, the architecture will be able to provide an active support in the execution of the various task \(e.g selection and retrieval of data also by improving the basic functionalities provided by the underlying \(traditional\ols As refer to all the current and past researches, knowledge-based repository can act as an informative database to store information. It is useful for any systems that apply reusable technique. The selective data will be stored in knowledge repository and will reduce time for the same situation for futu re usage. Therefore knowledge-based repository is being chosen in BPCS to save all the valid parts of the building plan during checking. Later, all the local authorities could share the information in knowledge-base repository while doing the checking process  III. FRAMEWORK DESIGN   To build BPCS, we proposed a framework as shown in Figure 2. The framework consists of components such as data from project building plan configuration \(CAD building plan\ata processing from the CAD building plan, development plan configuration standards and development plan configuration standards compliance results  BPCS receives an input for providing a development plan to be checked to the system in a form intelligible to the system. The data processing system contains standards against which the development plan is to be checked, also in a form intelligible to the system. An evaluation program is to be used for evaluating the development plan according to the standards An output – knowledge based repository receives results of the evaluation                 Figure 2:  BPCS Framework  The BPCS application starts with the CAD building plan being submitted and the building plan configuration will be extracted out from the CAD design. This process will be done by a CAD converter software program. The data will then be stored in a repository for building plan data as an input for data processing system  In the data processing system, a checking process which contains a plan check program will do the checking for building plan standards against the building plan data. An intelligence analysis is to be used where it compares to the specification used that analyzes factors such as rules used, object used material used and etc. Results return by the system is a list of compliance items and not limited to lists of non-complying  The results of evaluation will be stored in a repository – knowledge base contains a comprehensive of compliance design information This information is vital for local authorities to refer to when they carried out the checking process  IV. FUTURE WORKS  In future, BPCS simulates the project building plan compliance checking as follows    CAD Building Checking Process  Building Plan St d d  Building Plan Data  KBS Complianc 
9 
9 


 It identifies physical characteristics of the building plan components such as exterior doors interior windows, rooms and etc  It determines the components of building material used and also the composition  It determine compliance of the development plan configuration with the development plan configuration standards using the control program in the data processing system by evaluating the development plan configuration in the form of multiple dimensional equations   It determine site or structure boundaries in the development plan configuration with the data processing system by finding maximum and minimum values of the site or structure in three dimensions   It may also return material lists with the inspection requirements in cluding a check-off list The codes and symbols may be looked up in a regularly updated and published in electronic boards for reference  VI. IMPLEMENTATION HARDWARE AND SOFTWARE BPCS will be implemented under PC Windows using the .NET platform and object oriented programming language. The BPCS architecture design is generic and the scope is of BPCS knowledge base is limited to structural buildings  VII. CONCLUSION BPCS approach increases the flexibility of computer –supported work for the local authorities when they do the checking process by applying knowledge based techniques. The actual work process for plan checking may consume more time Hence, BPCS has advantages by providing alternative solutions and executions steps to the actual process  REFERENCES  Delic K.A  Hoellm er B  2000\, Knowledge-Based Support in Help-Desk Environments, IT Pr ofessional, Volume: 2,  Issue 1, pages: 44-48   Klam m a R Jar k e M   1998  Dr iving the Or ganizational Learning Cycle: The Case of Computer-Aided Failure Management, Proceedings of th e 6th European Conference on Information Systems \(ECIS’98\, Aix-En-Provence, France Volume: 1, pages: 378-392   V ithar ana P Zahedi F M   Jain H. \(2003\, Knowledge-Based Repository Scheme for Stor ing and Retrieving Business Components: A Theoretical Design and an Empirical Analysis IEEE Transactions On Softwa re Engineering, pages: 649-664  4 a f f n e r S., B i ck ley M Clan cy  L Wh ite K Bev i n s B 2003\, Knowledge-Based Softwa re Management, Proceedings of ICALEPCS2003, Gyeongju, Korea, pages: 439-441   Or en T  L  King D G Bir t a L G  1992  Requir e m e nts for a Repository-Based Simulation Environment, Proceedings of the 24th conference on Winter simulation,pages:747-750            
10 
10 


 


   All the text data types html  txt  xml have very similar distributions, which is a rather intuitive result. Nonetheless, we will see that there are non-trivial differences with respect to false positive rates   All of the compressed data types gz  jpg and pdf exhibit a very compact bell at the high end of the spectrum Jpg objects show a blip of lowentropy data due to header information and relatively small average size \(~90KB contrast gz objects do not recall that each contains a single compressed file and, thus, has a minimal header. The pdf set is the only one the three that contains a notable amount of text meta data\d that manifests itself as a smallish, broad peak around 700   The doc/xls files have the most varied distributions as they have complex internal structure \(essentially, a small file system\d serve as containers for other object types, such as images. This results in peaks at both ends of the spectrum, along with multiple other local peaks reflecting large amounts of table and textual information In the following section we explore the relationship between the entropy measure of a feature and its false positive \(FP\ probability  4. Entropy-based FP Prediction  It is worth noting that the relationship between the entropy of a feature and its uniqueness is rather complex a fact that appears to be underappreciated in the digital forensics literature Occasionally, the entropy number tells pretty much the whole story: for the very low end of the spectrum, it basically means that the features are mostly the same repeat character with some small variations. Thus, we can be quite certain that this kind of data produces weak features that cannot be reliably attributed. Most of the time, however, the entropy measure does not tell us the whole story and should be treated only as a hint, especially when applied to short sequences. For example, assuming a B 64, any sequence containing 64 different characters would have maximum entropy. Yet, that is statistically unlikely so those are usually predictable features, such as tables, that are poor candidates for characteristic features. As the following data shows things are not straightforward through the rest of the spectrum either We begin our discussion with a focus on pure text txt ata as it has the fewest artifacts related to formatting and can serve as a baseline for several other types. After some experiments on network data we decided to work with finer-grain features of 64 bytes so, unless otherwise noted, the data is for B 64 First, let us examine consider the relationship between H norm and the FP rate. In other words, we estimate empirically the conditional probability that a feature is non-unique, given a certain entropy number. The result is shown as a bar chart on Figure 2 \(data has been aggregated by a factor of ten to allow for proper chart display\. We can see that, for extremely low entropy scores, we get close to 100 false positives. That number drops sharply and, for scores of 200+, the probability stays below 10%, with numbers staying below 5% for most of that interval To put these probabilities into perspective consider the overall fraction of features have a particular entropy number. The line graph on Figure 2 gives the cumulative distribution of features as a function of the entropy measure H norm By and large it is good news if we decide to drop from consideration all features with a measure below 200 for example, this would result in the elimination of only 2.21% of the features from consideration. If we moved the cut off point to 250 the first point where FP rate drops below 0.05, we would still keep 97% of the features Yet, the figure shows that, to the right, the FP rate forms another little peak, which we may want to also exclude from consideration. Obviously, the lower we drop the FP threshold for inclusion, the lower the overall unconditional\P rate, and the larger the fraction of excluded features. To better understand this relationship, on Figure 2 we have shown the fraction of included features \(coverage a function of the selected threshold for values from 0.1 to 0.01 The coverage stays almost constant for threshold FP rates down to 0.07, with the corresponding expected overall FP rate of about 0.035. As the threshold drop from 0.06 to 0.03, coverage drops from 0.80 to 0.47, with overall FP rate going from 0.025 to 0.011. Note that lower FP coverage is not all bad it serves as a natural compression by reducing the number of features independently of other compression techniques that may be used, such as Bloom f  th e s earch targ et s are larg e  reducing FP rate may well outweigh some imperfections in coverage. Below a threshold of 0.02 we see a catastrophic collapse as the selection scheme completely loses coverage   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 5 


            0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00 0 100 200 300 400 500 600 700 800 txt: H norm F P  R a t e 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00 D a t a  C D F FP Rat e Data CDF  Figure 2 FP rate and data CDF for plain text 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040 0.045 0.050 0.10 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 FP Threshold Overall FP Rate 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00 Feature Coverage FP Rat e Coverage  Figure 3 Overall FP rate and feature coverage based on FP threshold  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 


 We also examined the distribution of the coverage for the threshold of 0.03 to understand how frequently big gaps in the coverage occur. Since the average distance between two selected features is only 1.59 we used the histogram on Figure 3 to better understand the behavior. It shows the cumulative probability that the gap in coverage falls within a particular interval. Inversely, we can infer the probability that it is greater than any particular number. For example, the probability that the gap exceeds 1024 bytes is 1 on 100,000. It is evident that even when overall coverage drops in half, the local coverage stays largely even, although notable but infrequent gaps do occur Having established a baseline with plain text now consider the behavior of more complex data types, such as html Generally, we would expect for html to be very similar given that it is designed to be human-readable; however, it is of interest to see the effects of common formatting on feature properties As Figure 4 clearly shows, the effect on FP rates is quite dramatic. In particular, the FP rate never drops below 0.027, and, with the exception of very low entropy features, remains consistently higher than the corresponding ones for plain text. While this makes intuitive sense due to the fact that html formatting brings more predictability into the features, we have not seen such issues discussed in the evaluation of similarity schemes. We should emphasize that these results are very generic and any  similarity scheme that picks features at random as all current ones effectively do will see non-trivial differences in its FP rates depending on the underlying data  5. Statistically Improbable Features  We could, of course, reproduce more charts for the different file types; however, that would not answer the question of how to utilize the presented data for similarity discovery One possibility is to use the per-type FP charts and pick features with the lowest FP rates. That would, in principle, produce the lowest average FP rates. However such features tend to be clustered if we estimate \(based on the entropy measure\at a feature will have a low FP, then the neighboring features will have a very similar, or identical entropy as they differ by one byte. Further, we would like a scheme that produces nice coverage without the need to calculate the hash of every single feature Our basic idea is to use the observed distribution of the computed entropy estimates as the basis for selecting statistically improbable features. For that purpose, we associate a precedence rank with each entropy measure value that is proportional to the probability that it will be encountered. In other words, the least likely feature as measured by its entropy score gets the lowest rank, whereas the most common one is assigned the highest. Assuming a fixed feature size of length B we can associate precedence scores for all of the L-B features of an object, where L is the size of the object. Features eliminated due to too low/high score are assigned a special null score to prevent their selection altogether Next, we pick a window size of W and, for every W consecutive rank scores, we select the leftmost minimum score \(this is a sliding window type of calculation\. All the while we keep count of how many times every feature has been picked as the minimum score. Thus, if a feature is picked k times k   1 W  e a n s th a t it is th e lo cal m i n i m u m  within a window of size W  k 1 and can be viewed as a measure of the relative local popularity of a particular feature Finally, we pick a threshold parameter t and select all features with k    t as the fingerprinting features of the object, and hash them using MD5  Figure 5 illustrates the process for a 320-byte snippet from a gz file L 320\or B 64, we can generate 256 consecutive entropy estimates and 256 corresponding rank scores \(top chart\at the entropy estimate changes minimally between neighboring estimates, and generally stays within a narrow range. However, every once in a while, it drifts out of that range, and the rank score drops sharply \(around offsets 40 and 180 on the chart\his usually results in a popular feature, although a drop is not a necessary condition for high popularity \(e.g offset 120\ Figure 5 also shows that for a threshold value of t 16, four features would be selected from the snippet, whereas t 48 results in two features being selected There are two obvious questions that arise in a real implementation: Which distribution should we choose to base our rankings on? Is there enough local variability in entropy scores to support the heuristic that local choices will tend to pick the same features In general, the first question is unlikely to have a single correct answer. A simple generic solution is to have a benchmark set and use it to set the ranks. This could be customized by profiling actual traffic for the specific deployment scenario. Yet, under any circumstances, we can expect that traffic will nontrivially deviate from our definition of typical so if our method is too sensitive to that choice, it would be less useful as it would result in fewer local agreements on the choice of features. To compensate we would have to lower t to select enough features Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


  0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0 100 200 300 400 500 600 700 800 900 H norm F P  R a t e  Figure 4 FP rate by entropy measure for HTML                  0 100 200 300 400 500 600 700 800 900 1000 0 16 32 48 64 80 96 112 128 144 160 176 192 208 224 240                    10 20 30 40 50 60 70  t 48 t 16 Popul ar i t y Scor e Rank Score  Offset                             0 100 200 300 400 500 600 700 800 900 1000 0 16 32 48 64 80 96 112 128 144 160 176 192 208 224 240                    10 20 30 40 50 60 70  t 48 t 16 Popul ar i t y Scor e Rank Score  Offset  Figure 5 Rank and popularity scores for a 320-byte snippet L 320 B 64 W 64  0 100 200 300 400 500 600 700 800 5 152535455565758595 Data Overlap C o m m o n  F e a t u r e s 64-16 64-24 64-32 64-40 64-48 64-56 64-64  Figure 6 Feature selection performance on random data Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


 Therefore, we could choose the doc distribution as our reference point and use it on all the sets. Going back to Figure 1, we can see that it almost serves as a composite of the rest with several local peaks. Actual data will effectively mask out parts of the ranking e.g. compressed data will have little use for entropy ranks to the left of its bell, whereas text data will not need anything to the right of its bell The answer to the second question is a confident yes even random data exhibits a bell-shaped distribution of entropy scores so even within a relatively small of window of 64-128 bytes, a few of the features are likely to stand out as statistically improbable, relative to the rest  6. Measuring File Similarity  We are now ready to go back to our original problem of finding similar objects. To establish a baseline measurement of the effectiveness of the described methods, we ran a controlled experiment using random data and known amounts of overlapping content. Specifically, we used two randomly generated files and produced mixed versions of them in the following fashion: take x  percent of file #1 and mix with 100x percent from file #2. The mixing was done in blocks of 512 bytes and we used 21 values for x 0, 5, 10 100. Note that we selected the blocks at random so even the 100% case is not an identical file but a file containing the same data block in a random permutation Further, we fixed W 64, and varied t from 16 to 64 with a step of 8. Figure 6 summarizes the results As we can see from the chart, the number of common features increases linearly with the increase of the amount of data in common and the slope of the increase is determined by the threshold parameter t  Generally, a lower the value for t means that more features are retained, whereas a higher value selects fewer features and improves compression. In this case t 16 retains an average \(over the different runs\ 847 features, whereas t 64 only 110. Using an MD5 hash to compress the feature representation would mean that our storage requirements would be 13,552 and 7,040 bytes, respectively. This yields corresponding compression ratios of 3.8:1 and 7.2:1 that can be further improved 10 times by using a Bloom filter with 10 bits per element and 0.8% FP rate. Further reductions are possible by selecting a bigger value for W such as 128, 256, 512 The above results can readily be replicated for compressed data types such as jpg, pdf and gz For text, the results are quite similar for unrelated text however, things like style and common topic do tend to yield higher results as the syntactic similarity increases. Generally, the results for non-random data need to be evaluated with respect a baseline FP rate which is relatively straightforward to obtain from a representative set of files. Alternatively, one could use the set to generate a set of duplicate features and ignore them in the actual data A large-scale experimental validation of an early prototype implementation on 4.6GB of real data Table 1\ is the subject of a separate paper. In summary, we were able to detect files within simulated network traffic based on a single packet with 0.9987 to 0.9844 accuracy \(depending on file type\or feature size of 64 and threshold of 16. Our current implementation is capable of 100MB/s sustained throughput on a quad-core processor this includes feature selection, feature hashing, and comparison with a reference set. We expect that an improved implementation would need no more than two cores to sustain that rate  7. Conclusions  In this paper, we presented a new approach to selecting syntactic features for similarity measurements. Our work makes the following contributions   Through empirical data, we have shown that a basic entropy measure can provide a valuable guideline with respect to the uniqueness of a particular feature   We have shown that different data types exhibit different behavior and that similarity measures can be tuned to keep a lid on overall false positive rate   We proposed a new method for selecting characteristic features that does not rely on a Rabin scheme to be content-sensitive. Instead we use an entropy measure and empirical distribution data to sel ect statistically improbable features   The new method is very stable and predictable with the respect to the coverage it produces, and unlike previous work, can easily be tuned to the underlying data  In the immediate future we plan to full develop a practical, high-performance tool that can be used to correlate evidence on a large scale      Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


 8. References  1  B. Bloom Space/Time Tradeoffs in Hash Coding with Allowable Errors  Communications of the ACM  vol 13 no 7, pp. 422-426, 1970 2  S. Brin, J. Davis, H. Garcia-Molina Copy detection mechanisms for digital documents In Proceedings of the ACM SIGMOD Annual Conference San Francisco, CA, May 1995 3  A. Broder, M. Mitzenmacher Network Applications of Bloom Filters: A Survey  Internet Mathematics  vol 1 no 4, pp. 485-509, 2005 4  A. Broder, S. Glassman, M. Manasse, and G. Zweig Syntactic Clustering of the Web In Proceedings of the 6 th International World Wide Web Conference pp 393-404, 1997 5  M. S. Charikar Similarity Estimation Techniques from Rounding Algorithms  In Proceedings of the 34 th Annual ACM Symposium on Theory of Computing 2002 6  C. Y. Cho, S. Y. Lee, C. P. Tan, and Y. T. Tan Network forensics on packet fingerprints. 21st IFIP Information Security Conference \(SEC 2006 Karlstad, Sweden, 2006 7  Henziger, M Finding Near-Duplicate Web Pages: A Large-Scale Evaluation of Algorithms In Proceedings of the 29 th Annual International ACM SIGIR Conference on Research & Development on Information Retrieval Seattle 2006 8  R. Karp, M. Rabin. "Efficient randomized patternmatching algorithms". IBM Journal of Research and Development 31 \(2\249-260, 1987 9  A. Kirsch, M. Mitzenmacher Distance-Sensitive Bloom Filters  Proceedings of the Algorithms Engineering, and Experiments Conference ALENEX 2006 10  J. Kornblum Identifying almost identical files using context triggered piecewise hashing  Proceedings of the 6 th Annual DFRWS Aug 2006, Lafayette, IN 11  U. Manber. Finding similar files in a large file system In Proceedings of the USENIX Winter 1994 Technical Conference, pages 1-10, San Fransisco, CA, USA 1994 12  M. Mitzenmacher Compressed Bloom Filters  IEEE/ACM Transactions on Networks 10:5, pp. 613620, October 2002 13  K. Monostori, R. Finkel, A. Zaslavsky, G. Hodasz, M Pataki Comparison of Overlap Detection Techniques In Proceedings of the 2002 International Conference on Computational Science Amsterdam The Netherlands, \(I\pp 51-60, 2002 14  M. Ponec, P, Giura, H. Brnnimann, J. Wein Highly Efficient Techniques for Network Forensics, In Proceedings of the 14th ACM Conference on Computer and Communications Security, 2007 Alexandria, Virginia 15  H. Pucha, D. Andersen, M. Kaminsky Exploiting Similarity for Multi-Source Downloads using File Handprints In Proceedings of the Forth USENIX NSDI, Cambridge, MA. Apr, 2007 16  M. O. Rabin Fingerprinting by random polynomials Technical report 15-81, Harvard University, 1981 17  S. Rhea, K. Liang, and E. Brewer. Value-based web caching. In Proceedings of the Twelfth International World Wide Web Conference, May 2003 18  V. Roussev, Y. Chen, T. Bourg, G. G. Richard III md5bloom Forensic filesystem hashing revisited  Proceedings of the 6 th Annual DFRWS Aug 2006 Lafayette, IN 19  V. Roussev, G. Richard III and L. Marziale, "Multiresolution similarity hashing", Proceedings of the Seventh Digital Forensic Research Workshop, 2007 20  K. Shanmugasundaram, H. Bronnimann, N. Memon Payload Attribution via Hierarchical Bloom Filters  Proceedings of the ACM Symposium on Communication and Computer Security CCS'04 2004 21  N. Shivakumar and H. Garcia-Molina SCAM: a copy detection mechanism for digital documents In Proceedings of the International Conference on Theory and Practice of Digital Libraries June 1995 22  N. Shivakumar, H. Garcia-Molina Building a scalable and accurate copy detection mechanism In Proceedings of the ACM Conference on Digital Libraries March 1996\, 160-168 23  N. Shivakumar, H. Garcia-Molina Finding nearreplicas of documents on the web In Proceedings of the Workshop on Web Databases March 1998\ 204212 24  S. Schleimer, D. S. Wilkerson, and A. Aiken Winnowing: local algorithms for document fingerprinting. In SIGMOD '03: Proceedings of the 2003 ACM SIGMOD international conference on management of data, pages 76-85, New York, NY USA, 2003. ACM Press Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


This figure presents the data flow through main blocks that can be used a few times and special blocks that must execute only particular functions Here also Configurator of chain and Sequences library for WiMAX/UMTS modules are equipped First one is responsible for right construction of next processing path and second one stores necessary number of elements After signal verification system is reconfigured according to input data and with using the chosen protocol type The OUTPUT connects to radio link and then signal must be transmitted over one of six channel types And at the reception side we configure the receiver in relation to transmitted mode 5 HARDWARE PLATFORM SELECTION Current technologies in a hardware environment allow to test our system in real-time implementation There are a couple of DSP based platforms that can be selected for validation from Lyrtech Inc the Small Form Factor SFF Software Communication Architecture SCA Development the Small Form Factor SFF Softwaredefined Radio SDR Development Platform the Small Form Factor SFF Software-defined Radio SDR Evaluation Module 20 All these platforms are based on TMS320DM6446 DMP SoC from Texas Instruments 21 For the proposed SDR based system we chose the SFFSDR Evaluation board see Figure 13 as far as this platform supports WiMAX technology model based design tools accelerating prototyping implementation of all protocols layers for complete radio stacking extra boards operates with 297MHz ARM926EJ-s RISC CPU and 594MHz C64x DSP in sense of power management this module has MSP430 MCU Due to an availability of Virtex-4 SX35 FPGA from Xilinx 22 this module can perform implementation of full modem processing functions that is very important feature in meaning of multi-protocol architecture of our system We are able to vary our requirements to each protocol inside the same hardware structure Figure 13 SFF SDR development platform by Lyrtech 6 PROTOTYPING THE WiMAX/UMTS SYSTEM The WiMAX/UMTS system is implemented in high-level language as C with the class library The main accent was done on the correct form of the signal processing sequence The path for WiMAX or UMTS signal is determined in the beginning and system should verify its entity leads the system in a relevant direction The main goal of this software implementation is to check how our system can handle the input signal sequence The simulation was carried out for following parameters for each subsystem For UMTS we consider transport block with 1280 bits size frame size is 2400 bits channelization code with 16 chips sequence For WiMAX we generate the bit block is equal to 1280 bits however during channel coding operation this block is divided on turbo coding block that include 384 bits The size of turbo coding block forms from the block determination corresponding modulation type and number of subchannels During software verification we obtained that our proposed model can separate different paths subject to a type of an inter sequence in the software environment The framework of our WiMAX/UMTS system went through one scenario step and now we are directed to an extension of this model For the more detailed system visualization we integrate our C code modules into MATLAB library by means of proper dynamic linking libraries dll compiled by using the MATLAB C compiler Each block can be formed with extended parameters Modules with C code configurate the system work in a host This host implementation will present a prototype This prototype will help to analyze future real-time hardware implementation MATLAB prototype can also provide debugging of real system the result of real system must be equal to our MATLAB prototype Next step of the work development and verification is to implement it into the hardware platform based on DSP DSP based platform allows to organize signal processing in a digital presentation that serves SDR based part of our general system 7 CONCLUSIONS AND FUTURE WORK In this paper we considered the framework of WiMAX/UMTS baseband level system for mobile device in UL transmission direction We presented the different signal processing structures based on OFDM and WCDMA physical layer procedures Our research work is mainly devoted to developing the approach of seamless switching between different subsystems that can be realized by SDR technology implementation To this end we proposed a possible solution to allow coexistence of different data transmission technologies 11 


The position of SDR blocks in common UMTS/WiMAX architecture for mobile terminal was shown in this paper We presented the different blocks of each subsystem and have identified three blocks which can be implemented as common SDR blocks These blocks include channel coding module interleaver module and data mapping module We also demonstrated the work of our system in the software environment Next steps of the UMTS/WiMAX system development are preparation of the specification and implementation of all possible scenarios Each scenario will include particular blocks parameters and common description of main blocks But we have to be carefully in case of main blocks description because there are a plenty of features 8 ACKNOWLEDGMENTS We gratefully acknowledge the company Arslogica that kindly provided us the hardware support for our experimental studies 9 REFERENCES 1 A Samukic UMTS Universal Mobile Telecommunications System development of standards for the third generation Proc of 1998 IEEE GLOBECOM Conf Sydney AUS Nov 8-12 1998 vol.4 pp.19761983 2 N Fourty T Val P Fraisse and J.-J Mercier Comparative analysis of new high data rate wireless communication technologies From Wi-Fi to WiMAX Proc of the IEEE Autonomic and Autonomous Systems and International Conf on Networking and Services ICAS-ICNS 05 Oct 23-28 2005 pp.66-66 3 M Komara SDR Architecture Ideally Suited for Evolving 802.16 WiMAX AirNet Communications SDR Forum Exhibition 2004 4 I Held 0 Klein A Chen C.-Y Huang and V Ma Receiver Architecture and Performance of WLAN Cellular Multi-Mode and Multi-Standard Mobile Terminals Proc of 2004 IEEE VTC Fall Conf Los Angeles CA Sept 26-29 2004 vol 3 pp 2248 2253 5 IEEE Standard for Local and Metropolitan Area Networks Part 16 Air Interface for Fixed Broadband Wireless Access Systems 2004 6 R Weigel and L Maurer  D Pimingsdorfer A Springer RF Transceiver Architectures for W-CDMA Systems Like UMTS State of the Art and Future Trends Proc of the Intern Symp on Acoustic Wave Devices for Future Mobile Communication Systems Chiba JP March 5-7 2001 pp 25-34 7 P-W Fu and K.C Chen A Programmable Transceiver Structure of Multi-rate OFDM-CDMA for Wireless Multimedia Communications Proc of 2001 IEEE Vehicular Technology Conf VTC-Fall 2001 Atlantic City NJ Oct.7-11 2001 vol 3 pp 1942-1946 8 L Zhigang L Wei Z Yan G Wei A Multi-standard SDR Base Band Platform Proc of 2003 International Conference on Computer Networks and Mobile Computing Shanghai PRC Oct 20-23 2003 pp 461 464 9 C Moy A A Kountouris L Rambaud and P Le Corre  Full Digital IF UMTS Transceiver for Future Software Radio Systems  Proc of ERSA 01 Conf Las Vegas NV June 25-28 2001 10 3GPP TS 25.201 Physical layer general description 11 K.R Santhi and G.S Kumaran Migration to 4 G Mobile IP based Solutions Proc of International Conference on Internet and Web Applications and Services/Advanced International Conference Feb 2006 pp 76 76 12 S Zhu M Song Y Li J Song and F Ren Simulation platform of WCDMA based on software defined radio Proc of 2nd ACM International Conference on Mobile Technology Applications and Systems Nov 2005 pp 1-5 13 L Ma and D Jia The Competition and Cooperation of WiMAX WLAN and 3G Proc of 2nd International Conference on Mobile Technology Applications and Systems Nov 2005 pp 15 14 J Mitola III Software Radio Architecture ObjectOriented Approaches to Wireless Systems new ed Wiley New York 2004 15 R Seungwan 0 Donsung S Gyungchul and K Han Perspective of the next generation mobile communications and services Proc of IEEE 2004 Int Symp on Personal Indoor and Mobile Radio Communications PIMRC 2004 Barcelona SP 5-8 Sept 2004 vol.1 pp 643-647 16 E Biglieri Coding for Wireless Channels  Springer New York 2005 12 


17 3GPP TS 25.212 Multiplexing and channel coding FDD 18 IEEE Standard for Local and metropolitan area networks Part 16 Air Interface for Fixed and Mobile Broadband Wireless Access Systems Amendment 2 Physical and Medium Access Control Layers for Combined Fixed and Mobile Operation in Licensed Bands and Corrigendum 1 2006 pp 0_1 822 19 3GPP TS 25.211 Physical channels and mapping of transport channels onto physical channels FDD 20 Data sheet from Lyrtech Inc http available at htp c hwwkneopusff _.l/p.s/lrtehs _sr d21]D ateforomTdf 21 Data sheet from Texas Instruments http available at 22 Data sheet from Xilinx http available at httll/www.xilinx.com 23 L Hanzo W Webb and T Keller Singleand Multicarrier Quadrature Amplitude Modulation  Wiley New York 2000 titled Wireless and Satellite Communications  The research interests of Dr Sacchi are mainly focused on wideband mobile and satellite transmission systems based on space time andfrequency diversity multi-user receivers based on non conventional techniques neural networks genetic algorithms higher-order statistics-based receivers etc cross-layer PHY-MAC design and high-frequency broadband satellite communications He is currently local coordinator for University of Trento of research projects dealing with reconfigurable communication platforms based on MIMO techniques and space-time signal processing ICONA project funded by MIUR and with exploitation of W-band for broadband satellite communications WA VE programs funded by ASI Claudio Sacchi is author and co-author of more than 50 papers published in international journals and conferences and reviewer for international journals and magazines IEEE Transactions on Communications IEEE Transactions on Wireless Communications IEEE Communications Letters IEEE Transactions on Aerospace and Electronic Systems Electronics Letters Wireless Networks IEEE Communications Magazine etc Dr Sacchi is member of the Organizing Committees and Technical Program Committees of international conferences like ICIP ICC GLOBECOM ACM-MOBIMEDIA etc Claudio Sacchi is member of IEEE M'01 SM'07 BIOGRAPHIES Olga Zlydareva is a PhD student of the University of Trento Italy She obtained her Master degree in Design Electronics Systems with specialization in High Radio Frequency Devices from MATI Moscow State Aviation Technological University named after KE Tsiolkovsky Moscow Russia Her research interests have oriented on the Software Defined Radio Technology Wireless Technologies Cellular Technologies Tunable devices Multi-standard systems Multi-protocol systems Physical layer of mobile devices Reconfigurability and Reprogramming of mobile devices The recent research focuses on the development of the baseband level of multistandard mobile devices based on SDR technology Claudio Sacchi was born in Genoa Italy in 1965 He obtained the Laurea degree in Electronic Engineering and the Ph.D in Space Science and Engineering at the University of Genoa Italy Since August 2002 Dr Sacchi has been holding aposition as assistant professor at the Faculty of Engineering of the University of Trento Italy In 2004 he was appointed by the Department of Information and Communication Technology of the University of Trento as leader of the Research Program 13 


  14  Figure 5:  Site B1 Terrain horizon ma sk with 1 degree azimuth spacing  Figure 6:  Site B1 Terrain horizon mask with 1 de gree azimuth spacing, in e quatorial coordinates 


  15  Figure 7: Lunar South Pole Solar Illumination Yearly Average  Figure 8:  Lunar South Pole DTE Visibility Yearly Average 


  16  Figure 9: Lunar North Pole Sola r Illumination Yearly Average  Figure 10:  Lunar North Pole D TE Visibility Yearly Average 


  17  Figure 11: Site A1 Elevation Topography  Figure 12: Site A1 Yearly Average Solar Illumination and DTE visibility, Medium Resolution 


  18   Figure 13:  Site LB Te rrain Horizon Mask  Figure 14:  Theory and Computed values of Average Yearly Solar Illumination 


  19  Figure 15:  Theory and Computed values of Average Yearly DTE Communication  Figure 16:  Heliostat Mirror Design to Eliminate Cable Wrap 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


