The Representative Basis for Association Rules Viet Phan-Luong L.I.M CNRS FRE-2246, C.M.I de l\222Universit6 de Provence 39, rue F Joliot-Curie, 13453 Marseille Cedex 13 France phan@gyptis.univ-mrs.fr Abstract We dejne the concept of the representative basis for in teresting association rules, and an inference system which is purely qualitative The representative basis is unique and minimal with respect to wrt the inference system On the representative basis the inference system is correct and complete Experimental results show that the number of rules in the representative basis is sign$cantly reduced wrt the number of rules generated by other existing approaches 1 Introduction Faced 
with the exponential number of association rules extracted from a dataset there is a trend searching for their concise representations, called generating sets or bases l 2,4 61 A generating set is a small set of informative as sociation rules from which all other interesting association rules can be derived The concept of frequent closed item sets 4 61 is proposed as a theoretical basis for generating sets These approaches have the main common points i They distinguish the basis for exact rules from the basis for approximate rules ii Rules that can be derived by tran sitivity or augmentation are discarded from the basis As 
consequences the inference on these bases has to use tran sitivity and augmentation However, a rule derived by tran sitivity is not always interesting Computing the confidence of the derived rule basing on the confidences of the rules in the basis is necessary to know if it is interesting A rule derived by augmentation on the left or right-hand sides is not always interesting Only the augmentation using items obtained by Galois closure operators 3 can ensure the in terestingness of the derived rule We argue that these com putations are not evident to the end users We present an approach to generating set called the representative basis which is different from the above ap proaches in these two points In 
particular we shall see the inference on the representative basis is very simple and no computation of the confidences or closures is needed 2 Preliminaries Association rules are extracted from a dataset 2  0,Z,R where 0 is a finite set of objects Z is a finite set of items, and R is a binary relation on 0 and Z A sub set I Z is called an itemset of V For each 0 5 0 and each I C 1 let g\(I  o E 0 I 
Vi E I 0 i E R and f\(0  i E Z I Vo E O,\(o,i E R Thecouple f,g is called a Galois connection 3 between 2O and 2\222 The Galois closure operators are the compositions h  f o g and h\222  g o f Let I be an itemset of D h\(I  f\(g\(I is called the closure of I Indeed I 5 h\(I  h\(h\(1 An itemset 
I is closed if h\(I  I The support of I is sup  card\(g\(l O where card\(X is the cardinality of X Given a support threshold, denoted by minsup 0 5 minsup 5 1 an itemset I is frequent if sup\(1 2 minsup I is frequent closed if it is frequent and closed  1 61 An itemset I is called a frequent key itemset 2 if there is no I\222 c I with sup\(1\222 
 sup\(1 An association rule r is an expression of the form I1  12 where 11 I2 5 Z The support and confidence of r denoted by sup\(r and conf r respectively are sup\(r  sup\(11 U I2 and conf\(r  sup\(11 U I2 I1 If con f r  1 then r is called an exact rule. Otherwise T is called an approximate rule Let minsup and rrzincon f be 
the thresholds for supports and confidences of associations rules respectively An association rule r is interesting if sup\(r 2 minsup and con f r 2 mincon f Example 1 The left part of Table 1 represents a dataset in the right part are the itemsets with their supports Table 1 Oid Items A,B,C,E 315 A,AC,BC,CE,BCE B E 415 B,C,E,BE 5 A,B,C,E We have sup\(.4B  CE  sup\(ABCE  215 0-7695-1 119-8/01 17.00 0 2001 IEEE 639 


conf\(AB  CE  1 And sup\(B  CE  315 conf\(B  CE  314 Wrt minsup  215 and minconf  1/2 AB  CE is exact and interesting and B  CE is approximate and interesting But wrt minsup  315 and minconf  415 these rules are not interesting minconf Rep Basis 30 63 70 440 3 Representative Basis Struct. Basis Red Basis 424 1578 384 1221 Definition 1 The representative basis for interesting asso ciation rules extracted front a dataset 2 called the repre sentative basis of V in short wrt the thresholds minsup and mincon f is the set of interesting association rules I  J such that I c J and there is no other interesting association rule I  J such that I C I and J C J The representative basis of the dataset in Table 1 wrt minsup  215 and mincon f  112 consists of the fol lowing rules A  ABCE B  ABCE C  ABCE E  ABCE 0  BCE 0  AC Proposition 1 The itemset on the right-hand side of every association rule in the representative basis is a frequent closed itemset Proposition 2 The itemset on the left-hand side of every as sociation rule in the representative basis is a frequent key itemset Proposition 3 Uniqueness The representative basis of D wrt minsup and mincon f is unique Definition 2 LD System Let I J K be itemsets The LD System consists of the inference rules 1 Weak Left-Augmentation wh IfI  JK is inter esting, then infer that IJ  JK is interesting 2 Decomposition If I  I1 I2 is interesting then infer that I  I1 and I  I2 are interesting Let I denotes the inference using the LD system We have 0  AC k C  AC,A  AC wLa C  AC,A  AC k C  A,A  C Decomposition Sim ilarly we can infer all other interesting rules approximate or exact using the LD system on the representative basis Proposition 4 Soundness Given a set of interesting asso ciation rules A all association rules derived from A using the LD system are interesting Proposition 5 Completeness All interesting association rules extracted from a dataset V wrt minsup and mincon f can be derived from the representative basis of V wrt rninsup and mincon f using the LD system Proposition 6 Minimality Let M be the representative basis of a dataset V wrt minsup and mincon f Then for any association rule T E M T cannot be derived from M  r using the LD system Database Mushrooms Chess Connect-4 4 Experimental Results and Comparisons minsup Rep. Basis Generat Set 20 158 574 1 70 89 1 152074 90 222 18848 References Y Bastide R Taouil N Pasquier, G Stumme and L Lakhal Mining minimal non-redundant association rules using fre quent closed itemsets the 6th Int'l Con on Ded and Obj Databases DoOD 00 Stream of the 1st Int 1 Con on Compuf Logics CL'OO LNCS 1861 972-986, Springer Verlag London UK 2000 Y Bastide R Taouil N Pasquier G Stumme and L Lakhal Pascal un algorithme d'extraction des motifs frequents Technique et Science Informatique  TSI 2001 to appear B Canter and R Wille Formal Concept Andysis Mafhe maticcal Foundations Springer 1999 N Pasquier Y Bastide R Taouil and L Lakhal, "Discov ering frequent closed itemsets for association rules the 7th Int'l cot on Database Theory lCDT'99 1999 N Pasquier Y Bastide R Taouil and L Lakhal Closed Set Based Discovery of Small Covers for Associ ation Rules ISeme journPes Bases de Donnies Avanckes BDA'99\Bordeaux, France 1999 M.J Zaki, "Generating non-redundant association rules the 6th ACM SIGMOD Int'l Con on Knowledge Discovep and Data Mining KDD 2000\Boston MA August 2000 640 


tend to lie close to each other in the reduced-dimension 2D space Detailed examination of the data point id's confirm this understanding of the results In addition there is a sense of the coarse grained inter-cluster spacings The 2D map shows that the designation of the distribution in terms of clusters is somewhat arbitrary the distribution actually being quite loose Nevertheless the relative spacing of the clusters and of the patterns are made more evident in this manner That type of knowledge can be very useful in understanding how multivariate data items might be managed 123 Figure 4 Clusters and reduced-dimension plot For a body of semiconductor compounds data originally described in 5D Another reduced-dimension plot is shown in Figure 5 The data displayed are for another problem entirely but the plot can be used to illustrate the use of the 2D plots for providing a quick grasp of the sensitivity of the position of the data points to changes in the feature values in the original high-dimensional space 3 Other Dimension Reduction Procedures The conventional supervised learning paradigm of neural-networks using multilayer feedforward network architectures and gradient descent learning can be viewed as performing a mapping from a set of vectors in N dimensional space to a corresponding set of points in K dimensional space with K=2 for obtaining 2D maps From that viewpoint it would seem there is nothing new or implausible in seeking to carry out dimension-reduction and to some extent this is true However in conventional supervised learning we know both the inputs and the outputs for a set of data points the training set and the criterion for learning the mapping parameters is that the estimated outputs be as close as possible to the expected values the targets The objective function is the mean of the square of the difference between the two sets of values the known expected values of the outputs and the estimated values given by the mapping The learned mapping minimizes that objective function and should be valid for all new samples as well as the points of the training set What is new in dimension-reduction is that there are no target values and the methods differ depending on the objective function selected  I I t 0  0 Original Patterns II O<Xl 0.15 Q o<X2co.30  O<x3<0.35 x O<x4<0.60 A O<x5<0.60 Y 0 0  0 A 0 A II 0 A 00 D A A Figure 5 Display of Sensitivity of Data Point Position to Changes in Value of Original Features In the following we list some dimension-reduction procedures and describe them in a uniform neural networks framework  a The Karhunen-Loeve Transformation 2 3 Consists of learning a full-dimensioned N to N linear mapping the linear mapping coefficients are learned iteratively by minimizing the mean of the square of the off-diagonal components of the data covariance matrix the result being that the covariance matrix is diagonalised Finally only those few components of the new representation for which the diagonal components are large are retained Note in passing Of course this is not the traditional approach for carrying out the K-L transform and is inefficient computationally but it is a neural network procedure capable of finding the eigenvectors and eigenvalues of the covariance matrix b The Auto-Associative Mapping 4 SI Actually this is a conventional supervised learning system 130 


Given a set of N input values one attempts to map that into a set of identical N output values with use of a multilayer feedforward neural net The challenge come from the requirement that a single set of network parameters suffice for all sets of input values that is for all pattern vectors in a body of data In addition there is a constraint that one of the internal layers only have K nodes The outputs of those nodes are taken to provide the reduced-dimension representation For reduction to 2D K=2 c The Variance Constraint Mapping A neural network is trained to map a set of points in N dimensional data space into a corresponding set of points in K dimensional space subject to the constraint that the trace of the data covariance matrix be conserved d The Distance-Ratio or Metric Ratio Conservation Mapping 7 A general nonlinear mapping is learned on the condition that for any and all pairs of points, the ratio of the original distance to the transformed distance be the same for all the pairs This being impossible in general the constraint used is that the variance of those values be as small as possible This is the method of preference at this juncture of our explorations of the various methods e The Equalized-Orthogonal Projection with Relaxation Mapping 8  A general nonlinear mapping based on the condition that the data covariance matrix in the transformed space be as close as possible to the identity matrix multiplied by a constant f The SAMANN Mapping 9 lo  A general nonlinear mapping patterned after the Sammon algorithm based on the constraint that all inter pattern distances be conserved in the reduced-dimension space This is a more demanding condition than that required of the Metric-Ratio Conserving Mapping g The Self-Organizing-Map SOM or feature map approach ll 12  In this method a set of pattern vectors are mapped onto a 2D grid of nodes on the basis of similarity to reference vectors belonging to the grid points The reference vectors are selected randomly at first from the same domain as the data vectors For each data vector after an association between that data vector and a node reference vector is made the receiving node and neighboring nodes are made to be more like the incoming vector In this way the entire grid becomes self-organised in a clustering manner In other words the patterns rely on the grid nodes to locate and associate with other data vectors similar to it In this way the data vectors gather themselves into clusters organised spatially in the space of the grid nodes These clusters transform the reference vectors and in so doing partition the grid space in a graduated manner The grid is usually a 2D grid and that is how this relates to dimension reduction The relative placement of the clusters may be governed by the topology of the original data but may also be influenced greatly by the initial random assignment of the reference vectors by the volume of data available and by the size and density of the nodes available in the feature map h GTM-The Generative Topographic Mapping The mapping task is cast in the form of determining the data distribution in reduced-dimension space which is consistent with the data distribution in the original data space In the model and narrative developed in 13 one starts with a grid of points in the latent-dimension space and assumes that the set of points x\(i can be mapped in a one-to-one manner into a set of corresponding points y\(i in original data space the mapping being in the form of a linear weighted sum of nonlinear basis functions defined in latent space The values of the weights are learned adaptively The points y\(i do not cover all of data space To remedy this situation one erects Gaussian distributions about all the y\(i In this manner one obtains an estimate of the distributions at any and all points t in data space The variance of the Gaussian may be learned adaptively Now one addresses the fact that the distribution values at the actual data points t\(n are known and proposes that the Bayes relationship can be used to estimate what the corresponding distribution should be in latent space The reestimated distribution should be the same as the original estimated value This condition is used for learning of the linear weights and of the spread of the Gaussians centered about the points y\(i The underlying theory seems to depend on three sets of interacting assumptions all worthy of further study 4 Use of 2D Maps in Machine Learning In Machine Learning research literature there are many articles proposing how knowledge might be represented managed and used For example there are suggestions that it would be useful to differentiate between deep knowledge and shallow knowledge or that it is important to have some knowledge in the form of causal rules and others in the form of heuristic rules and yet other forms of knowledge in the form of frames In other directions of differentiation one distinguishes between various forms of rule inference All of these concerns may or may not be important but the theme of this discussion is that perhaps the main issue is none of those topics We suggest that one of the most important issues is how to handle the multidimensional aspects of data items and of rules It is interesting to note that several diverse research disciplines tend to feel that they can handle quite well the task of describing the design and implementation of an Intelligent Machine System capable of learning and they are almost correct But they all tend to falter 131 


in the matter of handling the complexity brought about by the multidimensional nature of the knowledge items involved This difficulty surfaces in different ways In Fuzzy Set technology the difficulty is manifest in the issue of whether it is ever possible to have a universally valid General Extension Theorem for combining single variable membership function values in inference and search over a body of rules the different antecedents of the various rules are dealt with sequentially rather than in parallel resulting in great computational complexity Dempster-Shafer theory deals with the same issue under the guise of procedures for combining evidence in probabilistic causal reasoning that same issue manifests itself in the fact that very large numbers of different inference paths need to be scanned with each path associated with products of conditional probabilities and priors of dubious accuracy In pattern recognition and neural networks the approach is quite different The many dimensions and the many feature values are treated in parallel This does not mean that the basic mathematical complexities have been banished by magic but rather that the issue is now cast in a form such that the combinatorial complexities can be readily curtailed by appeal to experience In other words in a multidimensional feature space each point in that space might be thought of as corresponding to a specific combination of feature values In the pattern recognition or neural network approach, the data points are allowed to self-organize and one finds that in nearly all cases, not all eventualities happen with equal frequency. In fact data points tend to appear in groups and even these can be described in abstracted form 2D maps being one of these abstracted forms In conclusion we claim that 2D maps can represent effectively all the knowledge entities known to be of importance in intelligent systems technology including models of processes heuristic rules analytically expressed causal relationships and so on In addition 2D maps are well suited to dealing with the effects of experience in accepting new data and in updating knowledge structures 2D maps also can support the processes of chaining and scanning but in efficient parallel manner rather than in sequential manner Because of all this the matter of 2D map formation is worthy of careful study References l Langley P 1996 Elements of Machine Learning Morgan Kaufman Publishers Inc San Francisco CA 2 Fukunaga K and W L G Koontz, 1970 Application of the Karhunen-Loeve expansion to feature selection and ordering IEEE Transactions on Computers Vol 19 pp.3 1 1  318 131 Fukunaga K 1990 Introduction to Statistical Pattem Recognition, Second Ed Academic Press NY 4 Kramer M 1991 Nonlinear principal component analysis using auto-associative neural networks AIChe vol 5 Pao Y.H 1996 Dimension reduction, feature extraction and interpretation of data with network computing Intemational Journal of Pattern Recognition and Artificial Intelligence World Scientific Publishing Co Vol.10 6 Pao Y.H and C.Y Shen 1997. Visualization of pattem data through learning of nonlinear variance constrained dimension-reduction mapping Pattem Recognition Vo1.30 in press 7 Pao Y.H Z Meng S.R. LeClair and B Igelnik 1997 Validation of distance ratio constrained dimension-reduction displays of multidimensional data submitted to Engineering Applications of Artipcial Intelligence also technical report AI WARE Inc Beachwood OH 8 Meng Z and Y.H Pao 1997. Visualization and self organization of multidimensional data through equalized orthogonal projection with relaxation Internal Technical Report Electrical Engineering and Applied Physics Case Westem Reserve University, Cleveland OH 9 Sammon J.W Jr 1989 A nonlinear mapping for data structure analysis IEEE Transactions on Computers vol C 18 pp.401-409 IO Mao J and A.K Jain 1995. Artificial neural networks for feature extraction and multivariate data projection IEEE Transactions on Neural Networks Vol 6 pp.296-316 l I Kohonen T 1982 Self-organized formation of topologically correct feature maps Biological Cybemetics  121 Kohonen T 1995 Self-organization Maps Springer NY 13 Bishop C.M M Svenen and C.K.I Williams 1997 G.T.M the generative topographic mapping accepted for publication in Neural Computation I41 Meng Z Y.H Pao and C.Y Shen, 1996. Optimization with dimension reduction through learning of nonlinear variance constrained mapping Proceedings of the Adaptive Distributed Parallel Computing Symposium Dayton OH 37 pp 233-243 pp.521-535 V01.43 pp.59-69 pp.208-217 132 


C the single-query SQ needs 33 joins and 37 sort opera tions The corresponding query plan has more than 200 nodes This complexity is not easy to handle for the optimizer of the DWDBS Its output might be a rather bad query plan As one example the number of join orders increases exponentially with the number of joins With more than 10 joins it is likely that the optimizer is not able to consider all relevant orders. Additionally it might not detect all common sub-expressions that are part of the query and that could be combined The appropriate and manageable complexity of queries depends on the characteristics of the optimizer used in the DWDBS So far our discussion has shown that in some cases a sequence of queries might run faster than the combined single-query and that in other situations this might not be true Our experiments show examples for both cases Hence, the DSS optimizer has to decide for each query sequence whether it should be combined or not Strategy IZ Partial Combination One alternative is not to combine the whole sequence into a single-query but to merge it into a couple of que ries As we have argued, one single-query could be too complex for the optimizer of a DWDBS to find a good execution plan If the DSS optimizer generates a small number of semi-complex queries this new sequence could be more efficient The optimization task is then to find the proper points where to cut off the original se quence. This could be done based on dependency graphs as given in Figure 8 For some query sequences it is not possible to gener ate an equivalent single-query  because there are depend encies within the sequence This appears in the case of ranking In the query sequence shown in Figure 4 the last query Q4 selects data according to a filter criterion For the sake of simplicity we assumed that the filter value is known in advance. Depending on the way the user de fined the information request and the query generation process of the application this filter predicate might be determined based on data in A3 In this case the last query of the sequence could include a variable instead of the filter predicate. Hence, the partial combination strat egy is applicable The DSS optimizer could combine QI Q2 and 43 as described for Strategy I and leave Q4 as is This strategy is not appropriate for the query se quences we present in this paper Nevertheless it is a suitable strategy for the DSS optimizer because it is only based on the list of queries that constitute the given query sequence Strategy ZIZ Semantic Rewrite Another deciding factor for the DSS optimizer could be based on supplementary analysis of the original query sequence. One method to minimize the runtime of query execution is to minimize the number of temporary tables in the whole sequence, thus removing the overhead for writing to and reading from temporary tables It is also important to avoid repeated references to temporary ta bles when composing the single query Most temporary tables imply joins between underlying tables Repeated referencing of temporary tables in different queries of a sequence implies a redundant computation of these tem porary tables via nested SQL statements when composing the single query Queries which restrict one single previous temporary table by additional projections and/or selections cat1 be propagated to the query creating this preceding tempo rary table i.e the projections and/or selections of the consuming query are moved into the foregoing query by concatenating both projections ConcutPruject  and/or selections ConcarSelect Queries with identical FROM, WHERE, GROUP BY HAVING and ORDER BY clause can be merged into one query by concatenating the SELECT clauses. We refer to this transformation as MergeSelect Queries with identical SELECT FROM GROUP BY HAVING and ORDER BY clauses can sometimes be merged into one query by concatenating the WHEW clauses by AND MergeWhere If the queries differ only in a selection on the same column they can be combined by replacing parts of the WHERE clause by an additional grouping on this column Where Tocroup Queries with identical SELECT FROM WHERE GROUP BY and ORDER BY clause can be merged into one query by concatenating the HAVING clauses by AND MergeHaving Simple transformations are the following QS QS  MergeSelect INSERT INK C5 Icustkey. stddeviation INSERT INlW C5 SELECT al.cuietkey sTwBVlal.urrtpric Icustkey stddevl stdde'r2 FROM lineitem-orders al orderday a2 CL 8TDDBV\(al..ndpric a1.custkey STDDBY\(Il..ndprice I AVC\(a1 ndpris IN 11992,1993,19941 FROM lineitemorders al AND al.cucfkey  C4.custkey orderday a2 GROUP BY al.custkey WHERE a2.orderdate i 11992,1993.1994 GROUP BY al.curtkey SELECT a1.custkey AND a2.ordaryearkey IN l1992,1993;1994 AND a1.custkey  C4.custkey GROUP BY al.aurtkey INSERT 1\260K Cl Icustkey cuscn stddevl, stddev2. turnoverl992 turnoverl993, turnoverl994 CS.stddevlation Cfi.stddeviation Cl.turnoverl992, C2.turnover1993 c3.turnovar1994 SELECT al.custkey. a4.custname FROM cl. c2, c3. c5 cs WHERE c5.cusckey  Cl.custkey AND C5,custkey  C2.custkey AND C5.custkey  C3.custkey AND C5.custkey  C6.custkey AND C5.custkey  a4.cu~tkey customer a4 INSERT 1\260K C7 Icustkey custnme stddevl stddev2, turnoverl992 turnove~1993 rurnoverl994l CS.stddev1. C5.stddev2 C4.turnoverl992 c4.turnover1993 C4.turnoverl994 SELECT C4.custkey 13.~~1tname FROM El C5 Customer a3 WHERE C4.custkey  C5.custkey AND C4.custkey  a3.custkey i Figure 7 Modified Sequences for Request C 182 


We generated two modified query sequences for in formation request C QS+MergeSelect is built by using the Mergeselect transformation for the tables C.5 and C6 of QS and by using a more complex transformation which additionally reduces the number of joins in the new tables C.5 and C7 The transformation process is shown in Figure 7 The other modification QS WhereToGroup uses the WhereToGroup transfor mation to further reduce the number of queries in the sequence 25 Obviously, these rewrite strategies can also be com bined with the single-query strategy and the partial com bination strategy We refer to the queries resulting from these hybrid strategies as SQ+MergeSelect and SQ WhereToGroup where each respective query se quence is combined into one query As a consequence the number of join and sort operations in the rewritten queries drop drastically more than 50 as can be seen in Table 1 4.2 Parallelism In this section we discuss how parallel execution can support the query sequences generated by decision sup port applications. Our focus is not'on intra-query parallel ism We assume that the DWDBS is able to generate par allel query plans for all queries in a sequence, where suit able Apart from this perspective on parallelism it is also interesting to see whether the DSS optimizer can figure out a group of queries in the sequence that could be run in parallel Therefore, our focal point is inter-query paral lelism For each sequence of queries we can determine how the steps of the sequence depend on each other and describe these dependencies for example by means of a graph The dependency graphs for the query sequences A and C of our application scenario are shown in Figure 8 For sequence A the graph shows that AI and A2 could be generated in parallel For sequence C the tables CI C2 and C3 could first be generated in parallel and later in the sequence the same holds for tables C.5 and C6 In general a group of queries can be executed in parallel if none of the queries within the group depends on each other Figure 8 Dependency Graphs The dependency graph for a given sequence of que ries can easily be determined by observing the FROM clauses of all queries The graph consists of the tempo rary tables Temp as nodes An edge from node Temp to Temp exists if the FROM clause of query Q contains Temp In particular no knowledge about the application is required. Hence this optimization strategy turns out to be applicable for the DSS optimizer The implementation of this strategy can be based on methods that generate parallel execution plans for SQL queries as they are used in standard database systems These techniques produce an internal representation of the queries which is extended by data and pipeline paral lelism The same methods could be used to exploit de pendency graphs for query sequences like those in Figure 8 That is why available technology is very well suited for the DSS optimizer 7 21 But how about the interface of the DSS optimizer to the database system The standard interface of a DWDBS allows the application to send one query after the other within one database transaction For each of the SQL statements to be run in parallel a separate connec tion and a separate transaction is necessary. Since those queries that are independent of each other should be started in parallel only their access to the data warehouse base tables could inhibit the use of parallel transactions If we assume that decision support applications only read base tables in the data warehouse the standard SQL in terface of database systems turns out to be sufficient for the optimization strategy we have discussed here 4.3 Statistics Query sequences generated by decision support appli cations usually produce the result data set that is relevant for the end user as the last step of the sequence As soon as this last query terminates the final result is available and all other temporary tables could be dropped For query sequence A in our example table A4 contains all relevant data for the end user whereas tables AI A2 and A3 may be deleted as soon as A4 is complete Each query in a sequence is processed by the opti mizer of the DWDBS One important factor that deter mines the efficiency of query execution is whether all necessary statistical information is available for the opti mizer or not Relevant statistical information is the num ber of rows the number and type of all columns and the distribution of values in all columns of each table that is read by a query. Most database systems do not automati cally update statistical information For example a sepa rate statement is available for Oracle8i that starts the up date of statistical information for a single table 22 The following statement would be necessary to generate sta TISTICS Additional parameters may be used in order tistics for AI ANALYZE TABLE A1 COMPUTE STA 183 


to make available more detailed information on the table and its indexes  We assume that statistical information is up to date for all base tables of the data warehouse. However this is not true for temporary tables generated by a sequence of queries If the execution of a query sequence should be based on current statistics the statements that provide this information have to be added to the sequence. The sequence for query C as given in Figure 8 could be augmented by calling ANALYZE TABLE for the tempo rary tables Cl through C7 The statistical informatioo has to be available for each of these tables before the tables are read by another query in the sequence for the first time The statistical information for table C1 does not have to be present until the query for C4 starts execution This leads to an additional strategy for the DSS opti mizer that is applicable without any knowledge about the application that generated a sequence of queries A se quence could be extended by statements that provide ad ditional statistical information for the DWDBS optimizer For this task the DSS optimizer has to know, how this is done for different database systems and which parameters influence the available statistics It also has to take into account the trade-off between the time that is needed for the update of statistical information and the query execu tion time that is saved with execution plans based on this information. Further enhancements could be achieved by running statistic updates in parallel to other statements in the sequence. This is always possible if the queries do not depend on the temporary table for which the statistic up date is in progress 4.4 Partitioning and Indexing All optimization strategies we have discussed so far are based on rewriting a sequence of SQL statements or enhancing the way it is executed In this section we will briefly discuss some approaches to support OLAP appli cations by changing the schema of the data warehouse Additional indexes on base tables in the data ware house enable the optimizer of the DWDBS to find query plans that are more efficient No rewriting of queries is required It is necessary to find the proper combination of indexes that offer maximum support for all applications on the data warehouse and take into account given con straints on disk space and time for index maintenance If typical query sequences on the data warehouse are known, administration tools of current database systems can be used to establish the suitable set of indexes 5 Partition tables support typical OLAP queries because they reduce the amount of base data that has to be scanned for one sequence of queries. One large fact table could include data for several years If the typical query retrieves data for exactly one year it might be more effi cient to store the facts in partition tables for each year 31 184 The scan of one partition table instead of scanning the complete large fact table might then be sufficient for many queries A smaller volume of data is likely to result in a more efficient retrieval of results This kind of parti tioning is supported by some OLAP tools 16 Applications on a data warehouse can also be sup ported by aggregate tables Especially OLAP applicai ions require a lot of grouping and aggregation If the agge gated data is already present in the data warehouse, que ries run more efficient because less data has to be proc essed and less aggregations have to be computed. Some algorithms to find the proper set of aggregate tables are described in 12  describes a couple of approaches how a given query can be processed based on existing aggregate tables Our experimental results indicate that indexes, aggre gate and partition tables are appropriate means to en hance query sequences. Nevertheless, the DSS optimizer is not the suitable component to create indexes, aggregate tables or partition tables The successful exploitation of these three strategies is based on knowledge about the schema of the data warehouse and the characteristics of all applications that access data in the warehouse. There is another important difference to the optimization ap proaches described in Section 4.1 through 4.3 All strate gies we described there offer a local optimization for ex actly one sequence of queries, whereas additional indexes and additional tables also influence the performance of other queries Therefore they should be part of the DWDBS administration 5 Experimental Results For our experiments we have created the modified schema as described in Section 2.1 in a database system We populated the tables with data based on the originai TPC-H data generated with three different scale factors according to the benchmark specifications Our largest data set for which the results are presented here was pro duced with scale factor 10 The raw data summarizes to 10 GB the main fact table lineitem-orders has about 60 million rows The environment used for the experiments consists of a Sun Enterprise E4500 with 12 processors 12 GB main memory, the object-relational database sys tem IBM DB2AJDB V7.1 and a benchmark tool that is provided with DB2 From a technological point of view we could have used any other market-strength DBMS since we only used techniques that are commonly available e.g paral lelism indexes partitioning as well as statistics The decision for DB2 was twofold First we wanted to collect detailed information on the generated query plans This information is provided by the EXPLAIN utility of DB2 13 Second DB2 is known for its good optimization technology and thus can play a good reference point 


The results given in Table 2 are a subset of more than 400 experiments where one experiment is the execution of one query sequence for one information request on one test data set Each value presented here is the average of at least three experiments Some cells of Table 2 are empty because not all optimization strategies were appli cable to all of the three information requests Most of them are applicable to information request A and request C Business questions that are similar to these two are very likely to build the majority of questions in a real environment Table 2 Experimental Results in seconds Looking at the results in more details one can see that in almost all cases the optimization strategies were suc cessful in reducing the runtime of the information re quests. Sometimes there was only a slight enhancement whereas other strategies led to an execution time that was an order of a magnitude shorter than for the original query The execution time for the original sequence of que ries QS is given in line 1 The modified versions of the query sequences and their combination into a single query SQ as described in Section 4.1 are shown in lines 2 through 6 These results show that combining the statements of a sequence is likely to improve the per formance. However this is not true for information re quest C The sequence QS+WhereToGroup runs much faster than the corresponding single-query Hence the DSS optimizer has to decide in which situation it is ap propriate to combine the queries of a sequence and in which the eventually modified version of the sequence is the best choice Line 7 of Table 2 shows the results for the query se quence that was enhanced by generating additional statis tical information for temporary tables In our experi ments this version of the queries runs always faster than the original sequence. For information request C it was also faster than the very complex single-query SQ whereas for request A it was slower than the single query Therefore, leaving the query sequence as is and generating additional statistical information for tempo 185 rary tables is a supplementary alternative to the other strategies which can be used by the DSS optimizer The third basic alternative for the DSS optimizer is to run the sequence of queries as it was generated but with some queries of the sequence in parallel as described in Section 4.2 The experimental results for this strategy are given in line 8 They show only a slight improvement compared to the original sequence. Especially for infor mation request C the enhancement is less than 10 percent This is because the runtime of the query sequence for this question is dominated by only one query that joins sev eral temporary tables Some results for the strategies that turned out not to be appropriate for the DSS optimizer are given in lines 9 and 10 With additional indexes and partitions we can see a remarkable performance enhancement for informa tion request B Additional partition tables reduced the runtime for information requests A and C where the re duction is between 8 and 25 percent For the information requests A and C we achieved further improvements by other strategies that are useable by the DSS optimizer Hence we do not loose too much optimization potential if the DSS optimizer has to ignore additional indexes and partitions as optimization strategies Nevertheless in a real environment these strategies are mandatory for the administrator of the data warehouse 6 Conclusion In this work we investigated that an optimizer in be tween decision support applications and the DWDBS is essential for the efficient processing of some classes of typical information requests generated by DSS tools Hence we suggested an optimized system architecture for decision support applications In this architecture generated query sequences are rewritten by an additional system component, called DSS optimizer The transfor mation is based on optimization strategies that are inde pendent of the application and use little knowledge about the underlying database system. This DSS optimizer has two main advantages First it is able to support several applications without any need to change their query gen eration algorithms. Second it is capable of applying op timization strategies that are not supported by state-of the-art database systems We set up a realistic application scenario, selected a set of relevant information requests and ran a large series of experiments based on TPC-H data Our experiments have shown that all three strategies we proposed for the DSS optimizer were successful. They reduced the run time of the given query sequences significantly None of the three strategies turned out to be the best in all situa tions Hence it is the task of the DSS optimizer to decide upon the strategy to use This decision should be based on information gained from the query sequence itself and on meta data gained from the underlying database sys 


tem Therefore further work will focus on heuristics that could be used by the DSS optimizer in order to decide which optimization strategy or which combination of strategies should be used for a given sequence of queries Another important issue of future work is the design and implementation of the DSS optimizer The basic technology for a DSS optimizer is in place because all three strategies are based on matured technology Gener ating single-queries is similar to view expansion Deter mining queries within a sequence that could run in paral lel is based on parallel database technology 7 21 Us ing statistical information for query optimization is a ba sic optimization technology Furthermore we want to apply extensible optimization technology as is available with systems like CASCADES 8 In doing so one has to observe that the DSS optimizer is not a general purpose optimizer but only a specific component that supports only a predefined set of optimization strategies Consequently we want to reconsider the main design decisions of an optimizer e.g search space, rule set and cost-based decision making As a result we want to come up with a tailored and efficient DSS optimization compo nent Acknowledgement We would like to thank Leonard Shapiro and Ralf Rantzau for helpful discussions and their comments on an early version of the paper References  11 AberdeenGroup: Bringing Analytical Reporting to Enter prise Business Intelligence Aberdeen Group Boston 1999 R Agrawal J C Shafer Parallel Mining of Association Rules. In TKDE 8\(6 1996 S Chaudhuri R Krishnamurthy S Potamianos K Shim Optimizing Queries with Materialized Views In ICDE March 1995 S Chaudhuri U Dayal An Overview of Data Warehous ing and OLAP Technology In SIGMOD Record Vol 26 No 1 1997 S Chaudhuri V Narasayya AutoAdmin 223What-if\222 Index Analysis Utility In SIGMOD Record Vol 27 No 2 1998 C J Date H Darwen A guide to the SQL standard 4th ed Addison-Wesley, Reading 1997 D DeWitt J Gray Parallel Database Systems The Fu ture of High Performance Database Systems In CACM G Graefe The Cascades Framework for Query Optimiza tion In: DE Bulletin Vol 18 No 3 1995 J Gray S Chaudhuri A Bosworth A Layman D Reichart M Venkatrao F Pellow H Pirahesh Data Cube A Relational Aggregation Operator Generalizing Group-By Cross-Tab and Sub-Totals In Data Mining and Knowledge Discovery, Vol 1 No 1 1997  101 P Gulutzan Trudy Pelzer SQL-99 Complete Really R&D Books, Lawrence, 1999 2 3 4 5 6 7 Vol 35 NO 6 pp 85-92 1992 8 9 ll J Han M Kamber Data Mining Concepts and Tech niques Morgan Kaufmann 2000 1121 V Harinarayan A Rajaraman, J D. Ullman Impleinent ing Data Cubes Efficiently. In SIGMOD Record Vol. 25 No 2 1996 13 IBM: DB2 Command Reference, 1999  141 Informix Data Warehousing for the Retail Industry White Paper http://www.informix.com 1998 15 R Kimball The Data Warehouse Toolkit John Wiley  Sons New York 1996 16 MicroStrategy The Case for Relational OLAP White Paper MicroStrategy, 1995 17 B Mitschang Query Processing in Database Systems in German\Vieweg-Verlag, 1995 IS P O\222Neil G Graefe Multi-Table Joins Through Bit mapped Join Indices In SIGMOD Record Vol 24 No 3 1995 19 P ONeil D Quantas: Improved Query Performance with Variant Indexes In SIGMOD Record Vol 26 No 2 1997 20 C Nippl Providing Efficient Extensible and Adaptive Intra-query Parallelism for Advanced Applications Tech nische Universitat Munchen Dissertation 2000 21 C Nippl B Mitschang TOPAZ A Cost-Based Rule Driven, Multi-Phase Parallelizer. Proc VLDB Conf New York, 1998 22 Oracle Corporation Oracle8i SQL Reference Release 3 8.1.7 Oracle September 2000 23 D Peppers M Rogers Data Warehousing and Retailing DM Reviews October 1998 24 SAS Institute Finding the Solution to Data Mining. White Paper SAS Institute 1998 25 H Schwarz R Wagner B Mitschang Improving the Processing of Decision Support Queries Strategies for a DSS Optimizer University of Stuttgart Faculty of Com puter Science Technical Report TR-2001-02,2001 126 A Sen V S Jacob Industrial-Strength Data Warehous ing In CACM Vol. 41 No 9 1998 27 D Srivastava S Dar S Jagadish A Levy In VLDB September 1996 28 S N Subramanian S Venkataraman: Cost-Based Opti mization of Decision Support Queries using Transient Views. In SIGMOD Record Vol. 27 No 2 29 Thinking Maschines Corporation Darwin Reference Release 3.0.1 Thinking Maschine Corporation 1998 30 D S Tkach: Information Mining with the IBM Intelligent Miner Family. White Paper IBM, 1998 3 11 Transaction Processing Performance Council TPC Benchmark H Decision Support Standard Specification Revision 1.1 O June 1999 32 R Wagner Optimization of an OLAP Application for Retailers in German University of Stuttgart Faculty of\221 Computer Science Project Paper Nr 1770,2000 33 M Zaharioudakis, R Cochrane G Lapis H Pirahesh M Urata Answering Complex SQL queries Using Auto matic Summary Tables In SIGMOD Record Vol 29 No 2 2000 34 Y Zhao P M Deshpande J F Naughton A Shukla Simultanous Optimization and Evaluation of Multiple Dimensional Queries In SIGMOD Record, Vol 27 No 2 1998 186 


International Conference on Very Large Databases 1994  R  J  B ayardo J r  R  A gra w al and D  G unopul os  Constraint-Based Rule Mining in Large Dense Databases In Proceedings of the 15th International Conference on Data Engineering 1999 pp 188\226197 4 S  B rin  R Mo tw an i J.D Ullman  a n d S Tsu r  Dynamic itemset counting and implication rules for market basket data In Proceedings of the ACM SIGMOD Conference on Management of Data 1997 pp 255\226264  A  B roder  On the r esemblance and c ontainment of documents In Compression and Complexity of Sequences SEQUENCES'97  1998 pp 21\22629  D  W  C heung J  H a n V  T  N g  e t a l  A Fast Distributed Algorithm for Mining Association Rules In Proceedings of Conference on Parallel and Distributed Information Systems 1996 pp 31\22642 7 E  C o h e n  Size-Estimatio n F rame w o rk with Applications to Transitive Closure and Reachability Journal of Computer and System Sciences 55 1997 441\226453 8 E  C o h e n  M Datar  S Fu jiw ara A Gio n i s et al Finding Interesting Associations without Support Pruning In Proceedings of the 16th International Conference on Data Engineering  2000  R O D uda and P E H art Pattern Classi\002cation and Scene Analysis  A Wiley-Interscience Publication New York 1973  A  G i oni s  P  Indyk and R  M ot w a ni  S i m i l a ri t y Search in High Dimensions via Hashing In Proceedings of the 25th International Conference on Very Large Databases 1999 11 D Go ld b e r g  D  Nich o l s B.M Ok i an d D  T erry  Using collaborative 002ltering to weave an information tapestry Communications of the ACM 55 1991 1\226 19  S  G uha R  R as t ogi  a nd K  S h i m  C U R E A n Ef\002cient Clustering Algorithm for Large Databases In Proceedings of the ACM-SIGMOD International Conference on Management of Data 1998 pp 73\22684  R  Mot w ani a nd P  R a gha v a n Randomized Algorithms Cambridge University Press 1995  J  S  P a rk M S  C hen and P S  Y u A n ef fect i v e hash-based algorithm for mining association rules In Proceedings of the ACM SIGMOD Conference on Management of Data 1995 pp 175\226186  P r oj ect G u t e nber g  http:..www.gutenberg.net  1999  N Shi v akumar and H  G arcia-Molina B u ilding a Scalable and Accurate Copy Detection Mechanism In Proceedings of the 3rd International Conference on the Theory and Practice of Digital Libraries  1996  H.R  V a rian and P  R esnick E ds C A C M S pecial Issue on Recommender Systems Communications of the ACM 40 1997 


User Anomaly Description programmer2 logs in from beta secretary logs in at night sysadm logs in from jupiter programmer1 becomes a secretary secretary becomes a manager programmer1 logs in at night sysadm becomes a programmer manager1 becomes a sysadm manager2 logs in from pluto Table 12 User Anomaly Description User Normal Anomaly programmer2 0.58 0.79 0.00 secretary  1  1  0.00 sysadm 0.84 0.95 0.00 programmer1 0.31 1.00 0.04 secretary 0.41 0.98 0.17 programmer1  1  1  0.00 sysadm 0.64 0.95 0.00 manager1 0.57 1.00 0.00 manager2 1.00 1.00 0.00 Table 13 Similarity with User's Own Pro\002le tivities of each time segment am pm and nt We treat the 5th week as the training period during which we compare the patterns from each session to the pro\002le of the time segment We record the normal range of the similarity scores during this week The data in the 6th week has some user anomalies as described in Table 12 For each of the anomalous sessions we compare its patterns against the original user's pro\002le and then compare the resulting similarity score against the recorded normal range of the same time segment In Table 13 the column labeled 223Normal\224 is the range of similarity of each user against his or her own pro\002le as recorded during the 5th week A 1 here means that the user did not login during the time segment in the 5th week The column 223Anomaly\224 is the similarity measure of the anomalous session described Table 12 We see that all anomalous sessions can be clearly detected since their similarity scores are much smaller than the normal range For example when the sysadm becomes programmer see Table 12 his/her patterns have zero matches with the sysadm's pro\002le while for the whole 5th week the pm similarity scores are in the range of 0.64 to 0.95 Unfortunately formal evaluation statistics are not available to determine the error rates of this approach However this initial test indicates a path worthy of future study 6 Related Work Network intrusion detection has been an on-going research area 17  M ore r ecent s ystems e g B ro 18   NFR 6  a n d EMERALD  1 9  a ll mad e e x ten s ib ility th eir primary design goals Our research focuses on automatic methods for constructing intrusion detection models The meta-learning mechanism is designed to automate the extention process of IDSs We share the same views discussed in 20 t h at an ID S s houl d b e b ui l t us i n g s t a ndard components We believe that the operating system and networking community should be responsible for building a robust 223Event\224 box In 10  a l gori t h ms for a nal y zi ng us er s h el l c ommands and detecting anomalies were discussed The basic idea is to 002rst collapse the multi-column shell commands into a single stream of strings and then string matching techniques and consideration of 223concept drift\224 are used to build and update user pro\002les We believe that our extended frequent episodes algorithm is a superior approach because it considers both the association among commands and arguments and the frequent sequential patterns of such associations 7 Conclusions and Future Directions In this paper we outline a data mining framework for constructing intrusion detection models The key idea is to apply data mining programs to audit data to compute misuse and anomaly detection models according to the observed behavior in the data To facilitate adaptability and extensibility we propose the use of meta-learning as a means to construct a combined model that incorporate evidence from multiple lightweight base models This mechanism makes it feasible to introduce new ID components in an existing IDS possibly without signi\002cant re-engineering We extend the basic association rules and frequent episodes algorithms to accommodate the special requirements in analyzing audit data Our experiments show that the frequent patterns mined from audit data can be used as reliable user anomaly detection models and as guidelines for selecting temporal statistical features to build effective classi\002cation models Results from the 1998 DARPA Intrusion Detection Evaluation Program showed our detection models performed as well as the best systems built using the manual knowledge engineering approaches Our future work includes developing network anomaly detection strategies and devising a mechanical procedure to translate our automatically learned detection rules into modules for real-time IDSs A preliminary project in collaboration with NFR has just started 12 


8 Acknowledgments We wish to thank our colleagues at Columbia University Chris Park Wei Fan and Andreas Prodromidis for their help and encouragement References 1 R  A g r a w a l  T  I m i e lin sk i a n d A  S w a m i  M in in g a sso c i a tion rules between sets of items in large databases In Proceedings of the ACM SIGMOD Conference on Management of Data  pages 207\226216 1993 2 P  K  C han a nd S  J S t ol f o  T o w ar d p ar al l e l a nd di st r i b u t e d learning by meta-learning In AAAI Workshop in Knowledge Discovery in Databases  pages 227\226240 1993 3 W  W  C ohen Fast ef f ect i v e r ul e i nduct i on I n Machine Learning the 12th International Conference  Lake Taho CA 1995 Morgan Kaufmann 4 U  F ayyad G P i at et sk yS h api r o and P  S myt h  T he KDD process of extracting useful knowledge from volumes of data Communications of the ACM  39\(11\:27\22634 November 1996 5 K  I l gun R  A K e mmer e r  and P  A  P or r a s S t at e t r a nsition analysis A rule-based intrusion detection approach IEEE Transactions on Software Engineering  21\(3\:181\226 199 March 1995 6 N  F  R  I n c  N etw o rk 003ig h t reco rd er  h ttp www n fr co m  1997 7 V  J acobson C  L e r e s and S  M cC anne t cpdump a v ai l a bl e via anonymous ftp to ftp.ee.lbl.gov June 1989 8 C  K o  G Fin k  a n d K  L e v itt Au to m a te d d e t e c tio n o f v u l nerabilities in privileged programs by execution monitoring In Proceedings of the 10th Annual Computer Security Applications Conference  pages 134\226144 December 1994 9 S  K umar and E  H  S paf f or d A s of t w ar e a r c hi t ect ur e t o support misuse intrusion detection In Proceedings of the 18th National Information Security Conference  pages 194\226 204 1995  T  L a ne and C  E  B r odl e y  S equence m at chi n g a nd l ear ni ng in anomaly detection for computer security In AAAI Workshop AI Approaches to Fraud Detection and Risk Management  pages 43\22649 AAAI Press July 1997  W  L e e a nd S  J S t ol f o  D at a m i n i n g a ppr oaches f o r i nt r u sion detection In Proceedings of the 7th USENIX Security Symposium  San Antonio TX January 1998  W  L ee S  J S t ol f o  a nd K W  Mok Mi ni ng i n a d at a\003 o w environment Experience in intrusion detection submitted for publication March 1999  T  L unt  D et ect i n g i nt r uder s i n comput er syst ems I n Proceedings of the 1993 Conference on Auditing and Computer Technology  1993  T  L unt  A  T amar u F  Gi l h am R  J agannat h an P  N eumann H Javitz A Valdes and T Garvey A real-time intrusion detection expert system IDES 002nal technical report Technical report Computer Science Laboratory SRI International Menlo Park California February 1992  H Manni l a and H  T oi v onen Di sco v e r i ng gener a l i zed episodes using minimal occurrences In Proceedings of the 2nd International Conference on Knowledge Discovery in Databases and Data Mining  Portland Oregon August 1996  H Manni l a  H  T oi v onen and A  I  V er kamo D i s co vering frequent episodes in sequences In Proceedings of the 1st International Conference on Knowledge Discovery in Databases and Data Mining  Montreal Canada August 1995  B M ukherjee L T  Heberlein and K  N  L e v itt Netw ork intrusion detection IEEE Network  May/June 1994  V  Paxon B r o  A syst em f o r d et ect i n g n et w o r k i n t r uder s in real-time In Proceedings of the 7th USENIX Security Symposium  San Antonio TX 1998  P  A P o r r a s a nd P  G Neumann E m er al d E v ent m oni t o r i ng enabling responses to anomalous live disturbances In National Information Systems Security Conference  Baltimore MD October 1997  S  S t ai nf or dC h en C ommon i nt r u si on det ect i o n f r a me w o r k  http://seclab.cs.ucdavis.edu/cidf  S  J S t ol f o  A  L  P r odr omi d i s  S  T sel e pi s W  L ee D W  Fan and P K Chan JAM Java agents for meta-learning over distributed databases In Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining  pages 74\22681 Newport Beach CA August 1997 AAAI Press  S unS of t  Mount ai n V i e w  C A  SunSHIELD Basic Security Module Guide  13 


 30 40 50 60 70 80 90 100 110 120 30 40 50 60 70 80 90 10 0 Execution Time [s Number of Nodes Figure 13 Execution time of HPA program pass 2 on PC cluster 2 Scan the transaction database and count the support count Each processing node reads the transaction database from its local disk 000 itemsets are generated from that transaction and the same hash function used in phase 1 s applied to each of them Each of the 000 itemsets is sent to certain processing node according the hash value For the itemsets received from the other nodes and those locally generated whose ID equals the node\220s ID the hash table is searched If hit its support count value is incremented 3 Determine the large itemset After reading all the transaction data each processing node can individually determine whether each candidate 000 itemset satisfy user-specified minimum support or not Each processing node sends large 000 itemsets to the coordinator where all the large 000 itemsets are gathered 4 Check the terminal condition If the large 000 itemsets are empty the algorithm terminates Otherwise the coordinator broadcasts large 000 itemsets to all the processing nodes and the algorithm enters the next iteration 4.2 Performance evaluation of HPA algorithm The HPA program explained above is implemented on our PC cluster Each node of the cluster has a transaction data file on its own hard disk Transaction data is produced using data generation program developed by Agrawal designating some parameters such as the number of transaction the number of different items and so on The produced data is divided by the number of nodes and copied to each node\220s hard disk The parameters used in the evaluation is as follows The number of transaction is 5,000,000 the number of different items is 5000 and minimum support is 0.7 The size of the data is about 400MBytes in total The message block size is set to be 16KBytes according to the results of communication characteristics of PC clusters discussed in previous section The disk I/O block size is 64KBytes which seems to be most suitable value for the system Note that the number of candidate itemset in pass 2 s substantially larger than for the other passes which relatively frequently occurs in association rules mining Therefore we have been careful to parallelize the program effectively especially in pass 2 so that unnecessary itemsets to count should not be generated 14 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


The execution time of the HPA program pass 2 is shown in figure 13 as the number of PCs is changed The maximum number of PCs used in this evaluation is 100 Reasonably good speedup is achieved in this application as the number of PCs is increased 5 Conclusion In this paper we presented performance evaluation of parallel database processing on an ATM connected 100 node PC cluster system The latest PCs enabled us to obtain over 110Mbps throughput in point-to-point communication on a 155Mbps ATM network even with the so-called 217\217heavy\220\220 TCP/IP This greatly helped in developing the system in a short period since we were absorbed in fixing many other problems Massively parallel computers now tend to be used in business applications as well as the conventional scientific computation Two major business applications decision support query processing and data mining were picked up and executed on the PC cluster The query processing environment was built using the results of our previous research the super database computer SDC project Performace evaluation results with a query of the standard TPC-D benchmark showed that our system achieved superior performance especially when transposed file organization was employed As for data mining we developed a parallel algorithm for mining association rules and implemented it on the PC cluster By utilizing aggregate memory of the system efficiently the system showed good speedup characteristics as the number of nodes increased The good price/performance ratio makes PC clusters very attractive and promising for parallel database processing applications All these facts support the effectiveness of the commodity PC based massively parallel database servers Acknowledgment This project is supported by NEDO New Energy and Industrial Technology Development Organization in Japan Hitachi Ltd technically helped us extensively for ATM related issues References  R Agrawal T Imielinski and A Swami Mining association rules between sets of items in large databases In Proceedings of ACM SIGMOD International Conference on Management of Data  pages 207--216 1993  R Agrawal and R Srikant Fast algorithms for mining association rules In Proceedings of International Conference on Very Large Data Bases  1994  A C Arpaci-Dusseau R H Arpaci-Dusseau D E Culler J M Hellerstein and D A Patterson High-performance sorting on Networks of Workstations In Proceedings of International Conference on Management of Data  pages 243--254 1997  D.S Batory On searching transposed files ACM TODS  4\(4 1979  P.A Boncz W Quak and M.L Kersten Monet and its geographical extensions A novel approach to high performance GIS processing In Proceedings of International Conference on Extending Database Technology  pages 147--166 1996 15 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


 R Carter and J Laroco Commodity clusters Performance comparison between PC\220s and workstations In Proceedings of IEEE International Symposium on High Performance Distributed Computing  pages 292--304 1995  D.J DeWitt and J Gray Parallel database systems  The future of high performance database systems Communications of the ACM  35\(6 1992  J Gray editor The Benchmark Handbook for Database and Transaction Processing Systems  Morgan Kaufmann Publishers 2nd edition 1993  J Heinanen Multiprotocol encapsulation over ATM adaptation layer 5 Technical Report RFC1483 1993  M Kitsuregawa M Nakano and M Takagi Query execution for large relations on Functional Disk System In Proceedings of International Conference on Data Engineering  5th pages 159--167 IEEE 1989  M Kitsuregawa and Y Ogawa Bucket Spreading Parallel Hash:A new parallel hash join method with robustness for data skew in Super Database Computer SDC In Proceedings of International Conference on Very Large Data Bases  16th pages 210--221 1990  M Laubach Classical IP and ARP over ATM Technical Report RFC1577 1994  D.A Schneider and D.J DeWitt Tradeoffs in processing complex join queries via hashing in multiprocessor database machines In Proceedings of International Conference on Very Large Data Bases  16th pages 469--480 1990  T Shintani and M Kitsuregawa Hash based parallel algorithms for mining association rules In Proceedings of IEEE International Conference on Parallel and Distributed Information Systems  pages 19--30 1996  T Sterling D Saverese D.J Becker B Fryxell and K Olson Communication overhead for space science applications on the Beowulf parallel workstaion In Proceedings of International Symposium on High Performance Distributed Computing  pages 23--30 1995  T Tamura M Nakamura M Kitsuregawa and Y Ogawa Implementation and performance evaluation of the parallel relational database server SDC-II In Proceedings of International Conference on Parallel Processing  25th pages I--212--I--221 1996  TPC TPC Benchmark 000\001 D Decision Support Standard Specification Revision 1.1 Transaction Processing Performance Council 1995 16 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


In accordance with 1910.97 and 1910.209 warning signs are required in microwave areas For work involving power line carrier systems this work is to be conducted according to requirements for work on energized lines Comments s APPA objects to the absolute requirement implied by the word ensure regarding exposure to microwave radiation and recommends revision of s l iii to read when an employee works in an area where electromagnetic radiation levels could exceed the levels specified in the radiation protection guide the employer shall institute measures designed to protect employees from accidental exposure to radiation levels greater than those permitted by that guide  I1 an employee must be stationed at the remote end of the rodding operation Before moving an energized cable it must be inspected for defects which might lead to a fault To prevent accidents from working on the wrong cable would require identification of the correct cable when multiple cables are present Would prohibit an employee from working in a manhole with an energized cable with a defect that could lead to a fault However if the cable cannot be deenergized while another cable is out employees may enter the manhole but must protect against failure by some means for example using a ballistics blanket wrapped around cable Requires bonding around opening in metal sheath while working on cable Underaround EIectrical Installations t Comments t This paragraph addresses safety for underground vaults and manholes The following requirements are contained in this section Ladders must be used in manholes and vaults greater than four feet deep and climbing on cables and hangers in these vaults is prohibited Equipment used to lower materials and tools in manholes must be capable of supporting the weight and should be checked for defects before use An employee in a manhole must have an attendant in the immediate vicinity with facilities greater than 250 volts energized An employee working alone is permitted to enter briefly for inspection housekeeping taking readings or similar assuming work could be done safely Duct rods must be inserted in the direction presenting the least hazard to employees and APPA recommends that OSHA rewrite section 7\regarding working with defective cables This rewrite would include the words shall be given a thorough inspection and a determination made as to whether they represent a hazard to personnel or representative of an impending fault As in Subsection \(e EEI proposes the addition of wording to cover training of employees in emergency rescue procedures and for providing and maintaining rescue equipment Substations U This paragraph covers work performed in substations and contains the following requirements Requires that enough space be provided around electrical equipment to allow ready and safe access for operation and maintenance of equipment OSHA's position A2-16 


is that this requirement is sufficiently performance oriented to meet the requirements for old installations according to the 1987 NEW Requires draw-out circuit breakers to be inserted and removed while in the open position and that if the design permits the control circuits be rendered inoperative while breakers are being inserted and removed stated in the Rules and requests that existing installations not be required to be modified to meet NESC APPA recommends that Section u 4 i which includes requirements for enclosing electric conductors and equipment to minimize unauthorized access to such equipment be modified to refer to only those areas which are accessible to the public Requires conductive fences around substations to be grounded Power Generation v Addresses guarding of energized parts  Fences screens, partitions or walls This section provides additional requirements and related work practices for power generating plants  Entrances locked or attended Special Conditions w  Warning signs posted  Live parts greater than 150 volts to be guarded or isolated by location or be insulated  Enclosures are to be according to the 1987 NESC Sections llOA and 124A1 and in 1993 NESC  Requires guarding of live parts except during an operation and maintenance function when guards are removed barriers must be installed to prevent employees in the area from contacting exposed live parts Requires employees who do not work regularly at the substation to report their presence Requires information to be communicated to employees during job briefings in accordance with Section \(c of the Rules Comments U APPA and EEI provide comments as follows Both believe that some older substations \(and power plants would not meet NESC as This paragraph proposes special conditions that are encountered during electric power generation, transmission and distribution work including the following Capacitors  Requires individual units in a rack to be short circuited and the rack grounded  Require lines with capacitors connected to be short circuited before being considered deenergized Current transformer secondaries may not be opened while energized and must be bridged if the CT circuit is opened Series street lighting circuits with open circuit voltages greater than 600 volts must be worked in accordance with Section q\or t and the series loop may be opened only after the source transformer is deenergized and isolated or after the loop is bridged to avoid open circuit condition Sufficient artificial light must be provided where insufficient naturals illumination is present to enable employee to work safely A2-17 


US Coast Guard approved personal floatation devices must be supplied and inspected where employees are engaged in work where there is danger of drowning Required employee protection in public work areas to include the following  Warning signs or flags and other traffic control devices  Barricades for additional protection to employees  Barricades around excavated areas  Warning lights at night prominently displayed Lines or equipment which may be sub to backfeed from cogeneration or other sources are to be worked as energized in accordance with the applicable paragraphs of the Rules Comments w APPA submits the following comments regarding this Special Conditions section Recommends that the wording regarding capacitors be modified to include a waiting period for five minutes prior to short circuiting and grounding in accordance with industry standards for discharging of capacitors For series street light circuits, recommends that language be added for bridging to either install a bypass conductor or by placement of grounds so that work occurs between the grounds Recommends modification of the section regarding personal floatation devices to not apply to work sites near fountains decorative ponds swimming pools or other bodies of water on residential and commercial property Definitions x This section of the proposed Rules includes definitions of terms Definitions particularly pertinent to understanding the proposal and which have not previously been included are listed as follows Authorized Employee  an employee to whom the authority and responsibility to perform a specific assignment has been given by the employer who can demonstrate by experience or training the ability to recognize potentially hazardous energy and its potential impact on the work place conditions and who has the knowledge to implement adequate methods and means for the control and isolation of such energy CZearance for Work  Authorization to perform specified work or permission to enter a restricted area Clearance from Hazard  Separation from energized lines or equipment Comments x The following summarizes the changes in some of the definitions which APPA recommends Add to the definition for authorized employee It the authorized employee may be an employee assigned to perform the work or assigned to provide the energy control and isolation function  Recommends that OSHA modify the definition for a line clearance tree trimmer to add the word qualified resulting in the complete designation as a qualified line clearance tree trimmer Recommends that OSHA modify the definition of qualified employee" to remove the word construction from the definition since it is felt that knowledge of construction procedures is beyond the scope of the proposed rule resulting in APPA's new A2-18 I 


wording as follows more knowledgeable in operation and hazards associated with electric power generation transmission and/or distribution equipment Recommends that OSHA add a definition for the word practicable and replace the word feasible with practicable wherever it appears in the proposed regulations and that practicable be further defined as capable of being accomplished by reasonably available and economic means OTHER ISSUES Clothing OSHA requested comments on the advisability of adopting requirements regarding the clothing worn by electric utility industry employees EEI has presented comments which indicates research is underway prior to establishing a standard for clothing to be worn by electric utility employees However EEI's position is that this standard has not developed to the extent that it could be included in the OSHA Rules Both APPA and EEI state that they would support a requirement that employers train employees regarding the proper type of clothing to wear to minimize hazards when working in the vicinity of exposed energized facilities Grandfathering Due to the anticipated cost impact on the utility industry of the proposed Rules requiring that existing installations be brought to the requirements of the proposed Rules both APPA and EEI propose that the final Rules include an omnibus grandfather provision This provision would exempt those selected types of facilities from modification to meet the new rules EEI states that if the grandfathering concept is incorporated that electric utility employees will not be deprived of proper protection They propose that employers be required to provide employees with a level of protection equivalent to that which the standard would require in those instances in which the utility does not choose to modify existing facilities to comply with the final standard Rubber Sleeves OSHA requests comments from the industry on whether it would be advisable to require rubber insulating sleeves when gloves are used on lines or equipment energized at more than a given voltage EEI states its position that utilities should continue to have the option of choosing rubber gloves or gloves and sleeves to protect employees when it is necessary to work closer to energized lines than the distances specified in the clearance tables Preemuting State Laws EEI requests that the final Rules be clear in their preempting state rules applicable to the operation and maintenance work rules for electric power systems. This is especially critical since some states now have existing laws which are more stringent than the proposed OSHA Rules Examples are 1 in California and Pennsylvania where electric utility linemen are prohibited from using rubber gloves to work on lines and equipment energized at more than certain voltages and 2 in California and Connecticut where the live line bare hand method of working on high voltage transmission systems is prohibited One utility Pacific Gas  Electric has obtained a variance from the California OSHA to perform live line bare-hand transmission maintenance work on an experimental basis Coiiflicts Between the Rilles and Part 1926 Subpart V Since many of the work procedures in construction work and operation and maintenance work are similar and difficult to distinguish between EEI requests that the final order be clear in establishing which rule has jurisdiction over such similar work areas A2-19 v 


IMPACTS ON COSTS AND ASSOCIATED BENEFITS In its introduction to the proposed rules OSHA has provided an estimate of the annual cost impact on the electric utility industry for the proposed des of approximately 20.7 million OSHA estimates that compliance with this proposed standard would annually prevent between 24 and 28 fatalities and 2,175 injuries per year The utilities which have responded to this proposed standard through their respective associations have questioned the claims both of the magnitude of the cost involved and the benefit to the industry in preventing fatalities and lost-time injuries Both EEI and APPA feel that the annual cost which OSHA estimates are significantly lower than would be realized in practice Factors which APPA and EEI feel were not properly addressed include the following OSHA has not accurately accounted for cost of potential retroactive impacts including retrofitting and modifying existing installations and equipment OSHA has not consistently implemented performance based provisions in proposed rules  many portions require specific approaches which would require utilities to replace procedures already in place with new procedures Estimates were based on an average size investor-owned utility of 2,800 employees and an average rural cooperative of 56 employees, which are not applicable to many smaller systems such as municipal systems OSHA has not adequately addressed the retraining which would be necessary with modifying long-established industry practices to be in accordance with the OSHA rules EEI claims that OSHA's proposed clearance requirements would not allow the use of established maintenance techniques for maintaining high voltage transmission systems and thus would require new techniques For an example of the cost which is estimated to be experienced as a result of the new Rules one of the EEI member companies has estimated that approximately 20,000 transmission towers would need to be modified to accommodate the required step bolts in the Rules at an estimated cost of 6,200,000 Additionally this same company estimates that the annual cost of retesting live line tools for its estimated 1,000 tools would be 265,000 Additionally, both EEI and APPA question the additional benefits which OSHA claims would result from implementation of the new Rules APPA questions the estimates of preventing an additional 24 to 28 fatalities annually and 2,175 injuries per year in that it fails to account for the fact that the industry has already implemented in large part safety measures which are incorporated in the Rules EEI and APPA also point out that many preventable injuries cannot be eliminated despite work rules enforcement and safety awareness campaigns since many such accidents which result in fatalities are due to employee being trained but not following the employer's training and policies PRESENT STATUS OF RULES According to information received from the OSHA office in February 1993 the final Rules are to be published no later than July 1993 and possibly as soon as March 1993 OSHA closed their receipt of comments in March 1991 and no further changes in the rules are thought possible A2-20 


CONCLUSION The OSHA 1910.269 which proposes to cover electric utility operation and maintenance work rules affects a multitude of working procedures as are summarized in this paper It is not possible at the present time to assess the final structure of the Rules as may be proposed in 1993 or subsequent years Since the comments from the utility associations APPA and EEI were made following the initial release of the proposed OSHA Rules in 1989 a significant amount of time has elapsed where other events have occurred which may affect the form of the final Rules The 1993 NESC went into effect in August 1992 and includes some of the requirements to which the commenters objected For example a significant requirement in the Part 4 of the 1993 NESC requires that rubber gloves be utilized on exposed energized parts of facilities operating at 50 to 300 volts This requirement is in conflict with EEl\222s proposed change to the OSHA Rules which would still allow working such secondary facilities without the use of rubber gloves Electric utilities are advised to review the January 31 1989 proposed operation and maintenance Rules as summarized in this paper and to review their procedures which would be affected by application of the Rules Many of the procedures proposed in the Rules provide valuable guidance in electric utilities\222 operation and maintenance activities Where the cost impact is not significant, it is recommended that utilities consider implementing such procedures in expectation of the Rules being published in the next few months Also it would be appropriate for electric utilities to review the 1993 edition of the NESC since there are portions of the Rules which have resulted in changes in the NESC These changes mainly occur in Part 4 Rules for the Operation of Electric Supply and Communications Lines and Equipment The concerns which the commenters have addressed regarding the cost impact and the resulting benefits experienced as a result of the promulgation of the Rules are real ones and must be addressed in the final Rules As a result this paper cannot present a conclusion regarding the full impact of the Rules The development of such Rules continue to be an ongoing matter and will undoubtedly require later analysis when the final rules are published A2-21 


