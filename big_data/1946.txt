html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">p Abstract  A haplotype is a DNA sequence that is inherited from one parent. They are especially important in the study of complex diseases since they contain more information than genotype data, so the next high priority phase in human genomics involves the development of a full Haplotype Map of human genome [1]. However, obtaining haplotype data is technically difficult and expensive. One of the computational methods for obtaining haplotype data from genotype data is the pure parsimony criterion, an approach known as Haplotype Inference by Pure Parsimony \(HIPP an NP-hard problem. We present a new preprocessing method which drastically decreases the number of relevant haplotypes Several algorithms need to preprocess data; for big problem instances this key procedure is even more important than the process. This preprocessing was eventually tested on real and simulated data applying a tabu search, and the performance of the resulting algorithm showed it to be competitive with the best actual solvers  I. INTRODUCTION In many diploid organisms, such as humans, there are two copies of almost all chromosomes, one inherited from the mother and the other inherited from the father. Mixed data from both chromosomes in the 23 pairs is called genotype while each copy of a chromosome is called haplotype \(note that we are talking about humans Humans share about 99.9% of the DNA sequence, so usually researchers focus on the mutations. The most common mutations among haplotypes are the Single Nucleotide Polymorphisms, a genetic variation which involves just one nucleotide. A SNP is a single nucleotide site where exactly two \(out of four occur in a large percent of the population. We call these values alleles, using 0 to denote wild type and 1 to denote the mutant type allele The haplotype information is more relevant than genotype information since we are able to trace the structure of human populations more accurately and improve our ability to map  This work has been partially supported by the Saiotek and Research Groups 2007-2012 \(IT-242-07 Basque Government 06815-C02-01 and Consolider Ingenio 2010 - CSD2007-00018 projects Spanish Ministry of Science and Innovation in computational biomedicine \(Carlos III Health Institute   disease genes. Due to technology limitations, haplotype information is harder to obtain than genotype data instruments can identify whether the individual is heterozygous at a site \(both haplotypes have the same value or homozygous \(alleles are different obtaining haplotype data from genotype data is called haplotype inference \(HI proved to be efficient for this task. There are several approaches to this problem, combinatorial, statistical, etc but they all need a genetic model. Reviews about the haplotype inference problem can be found in [2], [3 The pure parsimony criterion for haplotype inference was proposed by Hubbell [4], who also showed that this approach is NP-hard, and its biological meanings were illustrated in [5], [6]. It can be formulated as follows: Given a set G of n genotypes, each of length m, find a set H of 2n h a p l o t y p e s  s u c h  t h a t  f o r  e a c h  g e n o t y p e  g i    G  t h e  p a i r   h 2 i   h2i-1 H is minimum HIPP is based on the biological fact that the number of haplotypes is vastly smaller than the number of possible haplotypes. It is also demonstrated by biological experiments that human chromosomes in a population have block structure, where no or few recombinations can occur 


block structure, where no or few recombinations can occur in each block. So methods for haplotype inference are mostly concerned with the analysis of specific blocks in the population, assuming that the recombination level is so low that it can be supposed to be 0. There are several papers and implementations of the HIPP: TIP [5], RTIP [5], PolyIP, [7 8] HybridIP [9] \(integer linear programming formulations SHIPs [10] \(SAT based algorithm  branch and bound  genetic algorithm  RPoly [13] and NRPoly [14] \(pseudo boolean The input for the HI inference problem is a n x m matrix which corresponds to the same m SNP  s data of n members of a population. We use the notation proposed by He and Zelikovsky [15]. Under this notation genotypes are ternary arrays {0, 1, 2}. In this matrix, gi,j is set to 0 if individual i at site j is homozygous with allele 0; gi,j is set to 2 if the population member is homozygous with allele 1 at position j; gi,j is 1 when the individual i is heterozygous at site j which means that one of its parents had allele 0 and the other allele 1. Haplotypes are binary vectors which represent the sequence of a single chromosome. We use this notation A new Preprocessing Procedure for the Haplotype Inference Problem Ekhine Irurozki, Jos  A. Lozano Intelligent Systems Group, Department of Computer Science and Artificial Intelligence  University of the Basque Country 1320978-1-4244-2959-2/09/$25.00 c  2009 IEEE which is not the standard one, because under it the genotype gi,j is explained by the sum of two haplotypes, gi,j = h2i,j + h2ii  j   1  j  m   W e  m a k e  n o  d i s t i n c t i o n  b e t w e e n  m a t e r n a l  a n d  paternal haplotypes and refer to these haplotypes simply as parents of a genotype. For a genotype with p ambiguous heterozygous pairs is 2 p-1 An example of resolving pairs for some genotypes is shown in table I    Some HI solvers create solutions by choosing different explaining pairs of haplotypes, also named resolutions, for each genotype, so their resolving pairs must be generated before running the algorithm. For each genotype we label the resolutions in its associated list by 1, 2, ..., ri, where ri is the resolution size of gi. A solution is an array of length n where n is the number of genotypes in the input matrix where the value at position i, si, indicates that the current resolving pair chosen for genotype gi is the pair labelled w i t h  s i   1  i   The input for the preprocessing is the input matrix, the genotypes, and the output is a set of resolving pairs of haplotypes for each genotype in the input. The trivial preprocessing just generates every pair of haplotypes that can explain the input matrix, but the number of pairs is exponential with the number of ambiguous positions in the genotype. For many current data sets the large number of haplotypes generated make it impractical to solve. Two different and totally independent preprocessing methods are implemented. The first one is based on that used by Gusfield  s RTIP: the main differences are that it makes a small reduction in the number of haplotypes and uses a more efficient representation. The second is a new preprocessing method which vastly reduces the number of haplotypes and pairs. A tabu search is also implemented so the results of both procedures can be compared II. RTIP-BASED PREPROCESSING RTIP is a more practical formulation for TIP, the first integer programming approach for the HIPP problem \(in fact TIP is considered a conceptual formulation due to its 


TIP is considered a conceptual formulation due to its limitations the number of constraints generated by the preprocessing of RTIP can be much smaller. The idea is the following: if a haplotype pair consists of two haplotypes that are both part of no other haplotype pair, then there is no need to consider this pair in the program. RTIP does not consider any haplotype pair where both elements have coverage 1 \(the coverage is the number of genotypes that a haplotype can solve but if the reduced formulation is created by first generating all the resolving pairs and then removing some of them, the work could still make the approach impractical, and will take even longer than the previous formulation An efficient way of generating only haplotypes of coverage 2 or higher is looking for, for every pair of genotypes in the input, which haplotypes can participate in both genotypes This operation is called intersection, and it is carried out as follows Let gi, gj be two genotype vectors. To identify the h a p l o t y p e s  i n  g i    g j   b o t h  v e c t o r s  m u s t  b e  s c a n n e d  f r o m  l e f t  to right; if a site occurs with a 0 in one haplotype and a 2 in t h e  o t h e r   t h e n  t h e  i n t e r s e c t i o n  i s     i f  a  s i t e  o c c u r s  w i t h  a  1  in one vector and 0 or 2 in the other, then set that site to 0 if the first was a 0 or a 1 if the first was a 2. Then, if there are k remaining sites, where both gi and gj contain 1  s, there are exactly 2 k distinct haplotypes in the intersection, and we generate them by setting those k sites to 0 or 1 in every possible way Once the haplotypes in gi, gj have been generated, they must be registered in the resolution list for both genotypes paired with their complementary for each genotype. The following is the main difference between the preprocessing of RTIP \(and probably most of RTIP-based preprocessing algorithms  RTIP registers the new haplotypes and their complementary for each genotype  Our method registers the new haplotypes, but their complementary will only be registered if they has coverage 2 or more  The pair in which one element has coverage 1 is called  incomplete pair  and it is made up of the haplotype which has coverage 2 or higher and a special character    If these haplotypes are ever needed they can be quickly calculated under this notation a genotype is the sum of its resolving haplotypes comparisons. For example when evaluating a solution: this process counts the number of distinct haplotypes in a given solution, so when finding the special character    there is no need to compare it to other haplotypes because it will definitely not appear in any other resolution So while RTIP-based preprocessing methods do not generate pairs in which both haplotypes have coverage 1, we don  t generate haplotypes that have coverage 1. Both methods generate the same number of pairs, but not the same number of haplotypes III. SET-BASED PREPROCESSING One weak point of most HI solvers is that they cannot deal with large problem instances. In this section we TABLE I EVERY RESOLUTION FOR THE GENOTYPES  Genotype Resolving pairs 221120 \(110110, 111010 110010, 111110 201002 \(100001, 101001 211120 \(100010, 111110 100110, 111010 


100110, 111010 101110, 110010 Under this notation, a genotype is explained by the sum of two haplotypes  2009 IEEE Congress on Evolutionary Computation \(CEC 2009 introduce a new preprocessing method to generate haplotype pairs discarding some irrelevant data which will never be part of the optimal solution. Some of the solvers in the literature are exponential size algorithms \(TIP, RTIP HAPAR, GAHAP starting the process with the smallest possible number of resolutions is crucial in order to find the optimal solution in less time for big problem instances A. Preliminary and notation First of all, we introduce a definition: the compatibility base \(or only base represent a set of haplotypes. The haplotypes included in a base are the result of setting the positions with the ambiguous character    with 0  s and 1  s in every possible way. Thus, for a base with p ambiguous positions, there are 2 p haplotypes. With this representation the intersection of two genotypes generates a compatibility base. Let, at the end of the preprocessing step \(performed by the procedure in the previous section, RTIP-based to genotypes g0 and g1 be g0: \(00110 00111 01110 01111 g1: \(00110 00111 01110 01111  and the intersection of both genotypes, Cb01 = 0?11 Characters 1 and 0 represent, as before, that the individual is homozygous, and character    means that there is an ambiguous position. Instead of extending all the haplotypes contained in the base, this situation can be represented as follows g0: \(0?11 Cb01 g1: \(0?11 Cb01  In this section the notation is more compact. The pair lists associated with each genotype can now contain haplotype pairs and base pairs. To distinguish one from the other, haplotype names will always begin with a lower case  h  and bases with a capital letter We will now illustrate a common problem in the preprocessing of RTIP. Let ga, gb be two genotypes in the input matrix with a non-empty intersection, the base Cbab which is calculated as in the previous section and happens to have p ambiguous positions. Once the preprocessing step is finished, it occurs that the members of Cbab have coverage 2 and they are always part of incomplete pairs \(paired with coverage 1 haplotypes resolving pair for these two genotypes, assuming that the same haplotype is selected for ga and gb, there is no criterion for choosing one pair and not other in Cbab. They won  t appear in the resolution of any other genotype, thus, the number of distinct haplotypes will be the same for any pair in Cbab. The problem gets worse as p increases, because the number of haplotypes in a compatibility base with p ambiguous sites is 2 p  B. Set operations and properties Bases are sets of haplotypes. Before continuing let us see how to apply some set operations to bases  Membership relation. A is a subset of B if all the haplotypes in base A are also in base B. That happens if  \(i every position of B with no ambiguous characters has the same value in A, \(ii ambiguous in B and \(iii 


ambiguous in B and \(iii position in B that has not an ambiguous value in A. For instance: B = 10???; A = 1001  Intersection. Two bases have a non-empty intersection if a site in which one base happens to have a 1 and the other one a 0 does not occur and they are not equal. The haplotypes in the intersection are a subset of both initial bases  Subtraction. If A = 1010? is a subset of B = 1??0? the bases that represent the set of haplotypes which belong to B a n d  d o  n o t  b e l o n g  t o  A  a r e  1   0    1 0 1 0      1 1 0 0        1 1 1 0     Complementary bases. Two bases, A, B, are complementary for a genotype g if for every haplotype in A there is one haplotype in B with which to make a complete pair to explain genotype g. Note that both bases must have the same number of ambiguous characters     same positions. So the sum of A = 1?00? and B = 1?10? is genotype g = 21101 C. Preprocessing structures For our preprocess we must group haplotypes so groups are paired off with each other. Since, as shown before, a subtraction can generate lots of sets, we need another representation A pair is made of two different set of bases. Each member of a pair has a main base, b0,  and \(optionally, if they exist some other bases b1, b2, ..., bk, called subtraction bases, that are subsets of the first one. The set of haplotypes included in a member of the pair are the ones which belong to the main base and do not belong to any of the subsets b1, ..., bk. The complementary of every base in a member for the genotype it explains, can be found in the other member of the pair, so the complementary of every haplotype in a member is in the other member, as shown in table II   D. Preprocessing steps The process of obtaining the minimum number of haplotype pairs given a set of genotypes is divided in seven steps First. For every pair of genotypes in the input, generate the intersection as explained in the previous section. If the TABLE II EXAMPLE OF A RESOLUTION FOR GENOTYPE 1110  1 st member 2 nd member b0 1??0 0??0 b1 11?0 00?0 b2 1?00 0?10 Haplotypes included 1010 0100 The set of haplotypes included in this resolution are the ones that belong to the set b0 but are not part of the subtraction sets b1 or b2  1322 2009 IEEE Congress on Evolutionary Computation \(CEC 2009 intersection is non-empty, add this base to the set S initially empty genotypes Second. Close set S under intersections. Every time a new base is added to the set its complementary for every genotype in its coverage is also added Third. Select for each genotype gi the subset of bases, si   S  w h i c h  c a n  s o l v e  g i   Fourth. Create a pair with every base in si \(note that for each base its complementary for the genotype is also in the set si the base as b0 and its subsets in si as b1, ..., bk. All its complementary bases are in the other member of the pair 


Fifth. Create a new graph with the elements in all the set pair lists of the genotypes. The nodes are the members of the pairs in the set pair lists and two nodes A, B are joined by an edge iff A and B both form a resolving pair in the list for a genotype Sixth. Generate haplotype pairs from the graph. In every connected component choose a node, and for this generate a haplotype. Starting with this node, visit all the edges using the next algorithm  X: current node x: set of chosen haplotypes in X for each adjacent node Y if edge E from A to B has not been visited yet set E as visited add to y the haplotype of Y which pairs with one of x if it does not already exist  The output of this preprocessing method has the same form as the RTIP-based method, but the number of haplotypes and pairs can be much smaller E. Cuts Given a solution to a HIPP problem where a genotype gi is explained by the pair of haplotypes \(hi, hj genotype gi may also be explained by the pair of haplotypes hj, hi number of solutions and consequently reduces the search space. In practice, this kind of symmetry is eliminated by adding a constraint to the model which guarantees that elements in a pair of haplotypes or bases are lexicographically ordered Another cut can be made that reduces the number of pairs in the set pair list and the time of generating haplotypes from base pairs. If a base A has two or more subtraction bases and A is equal to the union of those subsets, then there is no haplotype in A that is not included in any of the subset bases. Thus, it is not necessary to include it in the list since there will be no haplotype to generate in the next step In a base-pair, not every subset of b0 appears as its subtraction base; a base bi is included as b1, ..., bk if it is a s u b s e t  o f  b 0  a n d  t h e r e  i s  n o  b a s e  b h  t h a t  b 0    b h    b i    Once again not every haplotype is generated; if a member of a base pair has coverage 1 there is no need to keep its data registered. By just labelling them with a special character the memory used can be decreased and we avoid making lots of comparisons IV. HIPP VIA TABU SEARCH The tabu search is a metaheuristic procedure which makes use of historical data to guide the search to different regions of the search space. It is divided in two processes. In the intensification process a short-term memory is used in order to find the optimal solution in the proximity of the current solution. It stores attributes that become tabu active, they cannot appear in any solution, for a definite number of iterations. In the diversification process the search is moved to unexplored regions of the search space A. Neighbourhood systems This algorithm has three neighbourhood systems, but before explaining let us illustrate a property. When, from a given solution s1 a new solution s2 is made by changing the resolution of only one genotype, s2 will have at best 2 haplotypes fewer than the initial one, s1, and at worse 2 more. At best the genotype selected was first explained by two haplotypes that were no part of the current solution of any other genotype and the new resolving pair in s2 for this genotype consist of two haplotypes that are in the current solution in the resolution of any other genotype NS1. The neighbour solutions applying the neighbourhood system NS1 for a given solution s1 are the ones that result from randomly choosing a genotype, g, with uniform probability and starting from a pair of its resolution 


uniform probability and starting from a pair of its resolution list \(also randomly selected haplotypes \(except the current resolution by two haplotypes that are already in the current solution is found. If this pair does not exist, the new resolving pair for this genotype will be the first one found among those having the best evaluation of the fitness function NS2. This neighbourhood system has almost the same process, but the way of selecting the target genotype is different. Here, the probability of selecting a genotype in a given solution is inversely proportional to the sum of the number of genotypes in which both of its current resolving haplotypes participate NS3. This system is used in the diversification procedure Neighbour solutions of a given solution are the result of selecting a genotype \(as in NS2 pair of its resolution list. In order to select a pair the algorithm will look for the incomplete pair that has the first element of the order-vector \(a structure generated in the diversification procedure that will be explained later in this section pair is inversely proportional to the distance of that element to the beginning of the order-vector. If that element is not chosen, not found or if it is not part of an incomplete pair the search will restart with the second element of the ordervector and so on. This neighbourhood system gives higher probability to those haplotypes in the vector  s first positions B. Initial solution The first step for any tabu search algorithm is to generate an initial solution. Our algorithm creates it by randomly selecting a resolving pair for each genotype in the input Although this is probably not a good solution, the algorithm 2009 IEEE Congress on Evolutionary Computation \(CEC 2009 moves quickly to better solutions We use a multistart approach: if the search finishes without finding the minimum number of haplotypes \(which is known from the beginning solution and the search begins again C. Intensification There are two procedures implemented to intensify the search 1 The neighbourhood systems used here are NS1 and NS2 They are used alternatively starting with NS1. A short-term memory is also used. Every time a solution is changed the genotype and the index of the pair leaving the solution are stored, so they become tabu active attributes for the next NT iterations. The solution returned by NS1 or NS2 \(the best one among the neighbours only if is not tabu, regardless of whether the evaluation of the fitness function is worse 2 In this procedure, which makes a kind of refinement process for the solution returned by the Best All function, a solution is represented as a graph: haplotypes included in the current solution are represented by nodes and two nodes hi, hj, are joined by an edge, e, iff the resolving pair for genotype ge in the current solution is \(hi, hj re-explain some of the genotypes with other haplotypes already in use, not one by one, but the genotypes in a whole connected component. This means that after creating the graph, one connected component must be selected somehow to re-explain its genotypes, so there must be a heuristic to evaluate each component and its goodness. We have chosen 1 CCvalue = \(gn/A 4B+C 1  where A is the number of haplotypes in the connected component, B the number of genotypes explained by complete pairs, C the number of incomplete pairs in the 


complete pairs, C the number of incomplete pairs in the same connected component and gn is B + C, the total number of genotypes. Thus, when haplotypes are used more than once the final value is better: the bigger the value, the better the component The process is the following: once values are calculated select the worst connected component and try to re-explain every genotype in it. This new solution will become the current one iff it has a better evaluation of the fitness function. This process is repeated while there is one or more connected component which the algorithm has not tried to reassign Looking for new resolving pairs for a connected component can be another optimization problem. In which order are genotypes selected? And concerning each genotype which among all its possible explanations is chosen? Here genotypes are selected with uniform probability. For a genotype, if it has a resolving pair in which both haplotypes are used in any other connected component, it is assigned For any other case, all their resolutions are scanned and some of them are stored  Complete pairs in which one haplotype is in the current solution. This is the most complex case. Let the pair \(h1 h2 pair is selected as the new resolution, h2 will also be introduced to the solution. These are stored because it is possible to find a group of genotypes which have a resolution that share haplotype h2. If this group of pairs exists, those resolutions which share h2 are selected. If it does not exist, select any of the stored resolutions for each genotype. Then, start searching again  Incomplete pair in which the haplotype is in the current solution. If no pair is stored in the group described above select as the current resolution any pair in this group If none of the previous are found, the priorities are  Complete pair in which no haplotype is in the current solution  Incomplete pair in which the haplotype is not in the current solution. These pairs have the lowest priority; only selected if there is no pair that belongs to the described groups D. Diversification Although the algorithm explained in the previous section can yield good results, it cannot escape from local optimal solutions. This procedure tries to move to a different region of the search space in order to find a global optimal solution which means that the already visited regions of the search space must be registered, and since the amount of memory needed to record all this information can be huge, only those attributes which are probably part of good solutions are registered: those haplotypes that are part, anytime, of complete pairs \(those which can be shared by multiple genotypes In the preprocessing procedure for each haplotype, the number of complete pairs in which it participates is stored \(if it does participate in any complete pair memory structure is filled with this static data and the number of times visited in the previous intensification procedure \(dynamic information of an intensification iteration vector in a probabilistic way, then builds a solution in a deterministic way \(setting for each genotype the pair of its list which has the first element of the vector in a complete pair neighbourhood system NS3. The new solution generated with this system becomes the current solution with a probability that also varies if the evaluation of the fitness function is better or worse than the previous solution V. EXPERIMENTAL FRAMEWORK A. Data sets In order to compare this algorithm, we tested it with some 


In order to compare this algorithm, we tested it with some common instances in the literature [10] \(provided by Brown and Harrower in this data set  Uniform instances. Generate a set of haplotypes using the ms program, remove the repeated ones and randomly select two haplotypes as an explanation for each of the n 1324 2009 IEEE Congress on Evolutionary Computation \(CEC 2009 genotypes. There are 200 uniform instances  Non-uniform instances. Generate a set of 2n haplotypes using the ms program and randomly select two haplotypes as an explanation for each of the n genotypes without removing the repeated ones. There are 90 non-uniform instances in the dataset  Hapmap instances. Real sequences obtained from the HapMap project [1]. 24 insances Uniform and non-uniform instances were generated with ms [16], a program for generating haplotype samples B. Parameter settings The values of the parameter in the tabu search are the following: maxDiv \(maximum number of iterations in the diversification procedure maximum number of iterations in the intensification procedure maximum number of iterations in the Best All procedure number of genotypes in the input; maxGlo \(maximum number of  total iterations tabu active for 2n iterations, tabu period. Since the tabu search is a non-deterministic procedure, the time results given are the average of 10 executions. It is not usual that the algorithm iterates maxGlo times without finding the minimum number of haplotypes, but if that happens the algorithm starts again by generating a new initial solution C. Results Tables III, IV and V show the results of the executions of the tabu search algorithm using the two described preprocessing procedures. These three tables are divided in three sections each: the first and second refer to the information about the two preprocessing methods, RTIPbased and Set-based. For each one the number of haplotypes generated is given, the preprocessing time, the tabu search time \(TS reduction that is made with the set-based preprocessing in the number of haplotypes and the total time: if A is the value of the preprocessing of RTIP and B  the value of the setbased preprocessing, then the reduction is given as \(B-A So a negative value means that RTIP value is better, 0 means they are the same and a positive value means that set-based is better Due the large amount of uniform and non-uniform instances, the results of those having the same number of genotypes, sites and recombination level are grouped and the average results are given. The group named M_N is formed by instances of M SNP  s data over N individuals. Table III shows the results of the execution of the algorithm with the uniform instances. Uniform instances are small and, maybe not the best examples, since as far as standard simulation goes they do not make a lot of sense [17]. As seen in table III, the number of haplotypes generated by set-based preprocessing is always less than or equal to the number of haplotypes generated by the RTIP-based one. The total time needed to solve the problem is always smaller with RTIPbased preprocessing. This difference comes from the computational time spent in the preprocessing step. This is particularly serious in the case of instances that have been generated by recombination. The set-based preprocessing  TABLE III UNIFORM INSTANCES RTIP based preprocessing Set based preprocessing Time Time 


Time Time  haplo types Prepr. TS Total haplo types Prepr. TS Total Reduction hapls Total time reduction 100_30 376 75 32 107 137 407 56 462 1,75 -0,77 10_50 19 27 2 29 19 55 8 63 0,00 -0,54 10_50 10_50 10_50 30_50 95 41 6 46 86 407 6 414 0,11 -0,89 30_50 30_50 30_50 50_30 113 31 6 37 88 261 19 281 0,28 -0,87 50_50 178 60 22 82 144 1.433 11 1.444 0,24 -0,94 75_30 198 36 58 94 114 326 13 340 0,73 -0,72       TABLE IV NON-UNIFORM INSTANCES RTIP based preprocessing Set based preprocessing Time Time   haplotypes Prepr. TS Total  haplotypes Prepr. TS Total Reduction hapls Total time reduction 100_30 256 619 34 653 110 409 11 420 1,34 0,55 10_50 12 124 3 126 12 27 1 28 0,01 3,57 30_50 53 513 293 806 52 257 220 477 0,01 0,69 50_30 78 315 8 323 60 167 3 171 0,31 0,89 50_50 148 784 196 980 112 782 1.089 1.871 0,33 -0,48 75_30 194 430 22 450 105 310 166 476 0,85 -0,06  2009 IEEE Congress on Evolutionary Computation \(CEC 2009 method is slower as the recombination level in the dataset increases. The fact that the genotypes have been generated by recombination makes it very hard to close the set of bases by intersection. As mentioned before, haplotype inference studies usually select data from blocks where recombination probability is almost 0, so the instances in our experiments with recombination level 4, 16 and 40 are not even realistic but were tested because they are a kind of standard in the literature The results of the non-uniform instances are shown in table IV. Non-uniform instances are a little harder than uniform instances. Taking into consideration the number haplotypes generated by each preprocessing method we can see that the difference gets bigger as the problem size grows being zero or almost 0 for the smallest instances. It is also worth noticing that the time spent by the tabu search when data is preprocessed with the set-based preprocessing is lower than that of the RTIP preprocessing for 78 of the 90 instances The hapmap instances are considered hard; the execution 


information of all of them is shown in table V. The number of sites of the hapmap instances is denoted by the last two characters of the name of the instance and the number of genotypes varies between 5 and 68. Only one of the 24 instances is aborted because it exceeds the given time limit Experiments show that is the same instance that SHIP and RPoly abort. Although the execution time is not reduced for every instance, the total time for the 21 solved instances with RTIP-based preprocessing is 558 seconds and the total preprocessing time using set-based procedure \(note that 23 or 24 instances are solved reducing the search space, the time needed for the tabu search is reduced for all except three instances \(note that this process is non-deterministic Our proposed preprocessing method makes a difference between aborting an instance and computing it in a  TABLE V HAPMAP INSTANCES RTIP based preprocessing Set based preprocessing haplotypes # haploty pes  haplo types Prepr. TS Total haplo types Prepr. TS Total Reduct hapls Time reduct t_chr10_CEU_30 380 1.218 2.679 3.897 195 1.365 603 1.968 0,95 0,98 t_chr10_CEU_50 786 552 9 561 178 372 6 378 3,42 0,48 t_chr10_CEU_75 488 105 3 108 20 25 1 26 23,40 3,15 t_chr10_HCB_30 118 208 128 336 117 226 2 228 0,01 0,47 t_chr10_HCB_50 229 120 2 122 156 238 1 239 0,47 -0,49 t_chr10_HCB_75 87.870 69.127 545 69.672 991 4.167 8 4.175 87,67 15,69 t_chr10_JPT_30 256 316 3 319 149 207 51 258 0,72 0,24 t_chr10_JPT_50 98 78 1 79 25 24 1 25 2,92 2,16 t_chr10_JPT_75 Too large   1.315 11.483 411 11.894 t_chr10_YRI_30 2.257 1.765 4.221 5.986 905 5.958 2.425 8.383 1,49 -0,29 t_chr10_YRI_50 3.411 1.669 866 2.535 551 4.017 9 4.026 5,19 -0,37 t_chr10_YRI_75 167.171 370.887 2.976 373.863 2.827 197.411 1.450 198.861 58,13 0,88 t_chr21_CEU_30 103 206 2 208 33 34 1 35 2,12 4,94 t_chr21_CEU_50 18.074 13.335 69.528 82.863 1.279 10.376 15 10.391 13,13 6,97 t_chr21_CEU_75 Too large   3.317 85.667 188 85.855 t_chr21_HCB_30 46 118 1 119 36 90 0 90 0,28 0,32 t_chr21_HCB_50 6 54 0 54 6 17 0 17 0,00 2,18 t_chr21_HCB_75 4.157 1.264 26 1.290 752 4.601 186 4.787 4,53 -0,73 t_chr21_JPT_30 127 214 2 216 95 205 4 209 0,34 0,03 t_chr21_JPT_50 136 148 2 150 112 102 2 104 0,21 0,44 t_chr21_JPT_75 9.277 7.000 38.281 45.281 2.470 63.757 2.518 66.275 2,76 -0,32 t_chr21_YRI_30 92 139 8 147 72 427 1 428 0,28 -0,66 t_chr21_YRI_50 1.826 826 12 838 493 1.378 5 1.383 2,70 -0,39 t_chr21_YRI_75 Too large   Too large    TABLE VI NUMBER OF INSTANCES SOLVED BY EACH ALGORITHM   Sites Recombination level Genotypes RTIP Poly Hybrid HAPAR SHIPs SetTabu 10 - 50 15/15 15/15 15/15 15/15 15/15 15/15 10 4 50 15/15 14/15 14/15 15/15 15/15 15/15 10 16 50 15/15 6/15 6/15 15/15 15/15 15/15 30 - 50 6/15 4/15 3/15 15/15 15/15 15/15 50 - 30 0/50 12/50 13/50 50/50 50/50 50/50 75 - 30 0/10 2/10 2/10 8/10 10/10 10/10 Uniform 


Uniform 100 - 30 0/10 0/10 1/10 9/10 10/10 10/10 10 - 50 15/15 14/15 14/15 15/15 15/15 15/15 30 - 50 11/15 1/15 2/15 15/15 15/15 15/15 50 - 30 3/15 0/15 1/15 12/15 15/15 15/15 75 - 30 2/15 0/15 0/15 4/15 15/15 15/15 NonUniform 100 - 30 1/15 0/15 0/15 4/15 15/15 15/15 Hapmap 30:75 - 5:68 0/24 12/24 12/24 13/24 23/24 23/24 Total 10:100 0:16 5:68 83/229 80/229 83/229 190/229 228/229 228/229 1326 2009 IEEE Congress on Evolutionary Computation \(CEC 2009 reasonable time for big instances without recombination. For small instances RTIP-based method is usually quicker and the number of haplotypes generated is almost the same as the number of haplotypes generated by the RTIP-based procedure. This is a deterministic process so execution times are quite similar Although the tabu search process was firstly implemented to test the preprocessing procedures it has proved to be an effective algorithm for the HIPP problem. The refinement process in the intensification phase is probably the strongest procedure in the tabu search; this is the procedure that, in most cases, returns the optimal solution Table V shows the result of the total number of instances solved by our algorithm another five solvers, which can be found in the reference section. Our algorithm, which was implemented in Java, was tested with a time limit of 10,000 seconds on a MacBook 2.1 with an Intel Core 2 Duo processor up to 2.16GHz and 3GB RAM, although, JVM  s maximum heap size \(on Mac OS X five solvers were tested by Lin  e and Marques-Silva  [10 with a time limit of 10,000 seconds, using a 1.9GHz Athlon XP. However, using different machines to test the solver does not mean that the number of solved instances can be increased, since, when solved, the executions of our algorithm never took longer than 4 minutes  VI. CONCLUSIONS We have presented a new preprocessing algorithm for the HIPP problem coupled with a tabu search. This new preprocessing is based on manipulating subsets of haplotypes instead of haplotype units. The experimental results show that in the hardest instances this new preprocessing step can dramatically decrease the time spent by the whole algorithm \(preprocessing + tabu search fact is due to the reduction in the search space provoked by the new preprocessing. On the other hand, the new preprocessing does not work properly in instances with recombination. However, we can think about the two proposed preprocessing algorithm as complementary procedures, because the weaknesses of the set-based procedure \(recombination and small instances solved by the RTIP-based procedure As mentioned before, our aim was to develop a new preprocessing method to deal with large problem instances but, since the implemented tabu search has proved to be an effective algorithm, new functionalities should be developed  VII. ACKNOWLEDGEMENTS We want to thank D. Brown and I. Harrower for providing us the instances for our experiments REFERENCES 1] International HapMap Project, http://www.hapmap.org 2] Bonizzoni,P., Vedova,G.D., Dnodi,R., and Li,J  The haplotyping problem: an overview of computational models and solutions   Journal of Computer Science and Technology, 18\(6 3] Gusfield, D  An overview of combinatiorial methods for haplotype inference  Lecture notes in computer science, Springer, vol 2983/2004, pp. 599-600, 2004 


4] Hubbell, E. Personal communication, August 2000 5] Gusfeld, D  Haplotype inference by pure parsimony  In E. Chavez R. Baeza-Yates and M. Crochemore, editors, 14th Annual Symposium on Combinatorial Pattern Matching \(CPM'03 volume 2676, pp. 144-155, 2003 6] Wang, L., and Xu, Y  Haplotype inference by maximum parsimony Bioinformatics  19\(14  1780, 2003 7] Halld  rsson, B.V., Vafna, V., Edwards, N., Lippert, R., Yooseph, S Istrail, S  A Survey for Determining Haplotypes  Computational Methods for SNPs and Haplotype Inference  proc DIMACS/RECOMB Satellite Workshop, pp. 26-47, 2004 8] Brown, D.G. and Harrower, I.M  A new integer programming formulation for the pure parsimony problem in haplotype analysis   Proc. Fourth Ann. Workshop Algorithms in Bioinformatics, pp. 254265, 2004 9] Brown, D., Harrower, I  Integer programming approaches to haplotype inference by pure parsimony  IEEE/ACM Transactions on Computational Biology and Bioinformatics, vol 3, no. 2, 2006 10] Gra  a, A., Marques-Silva, J., Lynce, I., Oliveira, A.L  Efficient haplotype inference with boolean satisfiability  National conference on Artificial Intelligence \(AAAI 11] Wang, L., and Xu, Y  Haplotype inference by maximum parsimony   Bioinformatics 19\(14  1780, 2003 12] Rui-Sheng Wang, Xiang-Sun Zhang, Li Sheng  Haplotype inference by pure parsimony via genetic algorithm  International Symposium on OR and Its Applications, 2005 13] Graca, A. S., Marques-Silva, J., Lynce, I. and Oliveira  A. efficient haplotype inference with pseudo-boolean optimization  Algebraic Biology, July 2007, Hagenberg, Austria, 2007 14] Graca, A. S., Marques-Silva, J., Lynce, I. and Oliveira  A efficient haplotype inference with combined CP and OR techniques  CPAIOR 2008, 2008 15] He, J., Zelikovsky, A. Proc Fourth Ann  Linear reduction for haplotype inference  Workshop algorithms in Bioinformatics, pp 242-253, 2004 16] Hudson R.R  Generating samples under a Wrigth-Fisher neutral model of genetic variation  Bioinformatics 18, pp. 337-338, 2002 17] I. Harrower. Personal communication, 2008 2009 IEEE Congress on Evolutionary Computation \(CEC 2009 pre></body></html 


1] P. Alvarez, J. Banares, and J. Ezpeleta, ?Approaching Web Service Coordination and Composition by Means of Petri Nets: the Case of the Nets-within-Nets Paradigm?, ICSOC 2005, LNCS 3826, pp.185-197, 2005 2] M. ter Beek, A. Bucchiarone, and S. Gnesi, ?Web Service Composition Approaches: From Industrial Standards to Formal Methods?, Second International Conference on Internet and Web Applications and Services \(ICIW?07 Computer Society, 2007 3] X.N. Feng, Q. Liu, and Z. Wang, ?A Web Service Composition Modeling and Evaluation Method Used Petri Net?, LNCS Volume 3842/2006, pp. 905-911, SpringerVerlag, 2006 4] H. Foster, S. Uchitel, J. Magee, and J. Kramer, ?Modelbased verification of Web Service Compositions?, 18th IEEE International Conference on Automated Software Engineering, pp. 152- 161, 2003 5] X. Fu, T. Bultan, and J.W. Su, ?Analysis of Interacting BPEL Web Services?, WWW2004, pp. 17-22, New York USA, May 2004 6] J.D. Ge, H.Y. Hu, P. Lu, H. Hu, and J. L  Translation of Nets Within Nets in Cross-Organizational Software Process Modeling?, SPW 2005, LNCS 3840, pp. 60-375 Springer-Verlag, 2005 7] Group for program system, faculty of Information Technique University Dortmund, ?PDDL?, http://ls5www.cs.uni-dortmund.de/~edelkamp/ipc-4/pddl.html 8] R. Hamadi and B. Benatallah, ?A Petri Net-based Model for Web Service Composition?, Fourteenth Australasian Database Conference \(ADC2003 CRPIT, Vol. 17, pp. 191-200, 2003 9] S. Hinz, K. Schmidt, and C. Stahl, ?Transforming BPEL to Petri Nets?, BPM 2005, LNCS 3649, pp. 220?235, Springer-Verlag, 2005 10] H. Kang, X.L. Yang, and S.M. Yuan, ?Modeling and verification of Web Services Composition based on CPN 2007 IFIP International Conference on Network and Parallel Computing-Workshops, pp. 613-617, 2007 11] H.M. Kim, A. Sengupta, and J. Evermann, "MOQ: Web Services Ontologies for QOS and General Quality EvaluaFigure 6. XML net for Web service selection Figure 7. Filter Schema for QoS-aware WS selection owl:Ontology owl:Class rdfs:subClassOf owl:Restriction rdf :resource : "\\s+#costUSCent owl:onProperty owl:maxCardinality   rdf:RDF Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 tions", European Conference on Information Systems \(ECIS 2005 12] S. Klink, Y. Li, and A. Oberweis, "INCOME2010 - a Toolset for Developing Process-Oriented Information Systems Based on Petri Nets", International Workshop on Petri Nets Tools and APplications \(PNTAP 2008, associated to SIMUTools 2008 March 2008 13] O. Kluge, ?Petri nets as a Semantic Model for message Sequence Chart Specifications?, proceedings of INT 2002 pp. 138-147, 2002 14] K. Lenz and A. Oberweis, ?Inter-Organizational Business Process Management with XML Nets?, H. Ehrig, W Reisig, G. Rozenberg, H. Weber \(Eds gy for Communication Based Systems, LNCS 2472, pp. 243263, Springer-Verlag, 2003 15] K. Lenz and A. Oberweis, "Workflow Services: A Petri Net-Based Approach to Web Services", Int. Symposium on Leveraging Applications of Formal Methods, pp. 35-42, Pa 


Leveraging Applications of Formal Methods, pp. 35-42, Paphos/Cyprus, November 2004 16] L. Lin and I.B. Arpinar, ?Discovery of Semantic Relations between Web Services?, IEEE International Conference on Web Services \(ICWS?06 17] Q. Lin, J.D. Ge, H. Hu, and J. Lu, ?An Approach to Model Cross-Organizational Processes using Object Petri net?, 2007 IEEE Congress on Services \(SERVICES 2007 pp. 146-152, July 2007 18] H. Ludwig, A. Keller, A. Dan, R. P. King, and R Franck, ?Web Service level Agreement \(WSLA Specification version 1.0?, IBM, 2003 19] N. Lohmann, P. Massuthe, C. Stahl, and D. Weinberg Analyzing Interacting BPEL Process?, S. Dustdar, J.L. Fiadeiro, and A. Sheth \(Eds 32, Springer-Verlag, 2006 20] S.A. Mcllraith and T.C. Son, ?Adapting Golog for Composition of Semantic Web Services?, 8th International Conference on Knowledge Representation and Reasoning KR2002 21] S.A. Mcllraith, T.C. Son, and H.L. Zeng, ?Semantic Web Services?, IEEE Intelligent Systems, March/April 2001 16\(2 22] N. Milanovic and M. Malek, ?Current Solution for Web Service Composition?, IEEE Internet Computing, NovemberDecember 2004 23] S. Narayanan and S.A. Mcllraith, ?Simulation, Verification and Automated Composition of Web Service?, 11th International World Wide Web Conference, Honolulu, Hawaii, USA, May 2002 24] The Organization for the Advancement of Structured Information Standards \(OASIS Process Execution Language Version 2.0?, 11 April, 2007 http://docs.oasis-open.org/wsbpel/2.0/wsbpel-v2.0.pdf 25] S.R. Ponnekanti and A. Fox, ?SWORD: a Developer Toolkit for Web Services Composition?, 11th International World Wide Web Conference, Honolulu, Hawaii, USA, May 2002 26] Z.Z. Qian, S.L. Lu, and L. Xie, ?Colored Petri Nets Based Automatic Service Composition?, 2007 IEEE AsiaPacific Services Computing Conference, pp. 431-438, 2007 27] C. Ouyang, E. Verbeek, W.M.P. van der Aalst, S. Breutel1, M. Dumas1, and A.H.M. ter Hofstede1, ?Formal semantics and analysis of Control flow in WS-BPEL?, BPM center Technical Report, BPM-05-15, 2005 28] J.H. Rao and X.M. Su, ?A Survey of Automated Web Service Composition Methods?, SWSWPC 2004, LNCS 3387, pp. 43-54, Springer-Verlag, 2005 29] J.H. Rao, P. Kuegas, and M. Matskin, ?Application of Linear Logic to Web Service Composition?, 1st International Conference on Web Services, Las Vegas, USA, June 2003 30] J.H. Rao, P. Kuegas, and M. Matskin, ?Logic-based Web Services Composition: From Service Description to Process Model?, 2004 International Conference on Web Services, pp.446-453, San Diego, USA, July 2004 31] M. Sgroi, A. Kondratyev, Y. Watanabe, L. Lavagno and A. Sangiovanni-Vincentelli, ?Synthesis of Petri Nets from Message Sequence Charts Specifications for Protocol Design?, Conference on Design, Analysis, and Simulation of Distributed Systems, pp. 193-199, 2004 32] W.M.P. Van der Aalst, "The Application of Petri Nets to Workflow Management", The Journal of Circuits, Systems and Computers, 8\(1 33] The World Wide Web Consortium \(W3C Semantic Markup for Web Services?, 22 November, 2004 http://www.w3.org/Submission/OWL-S 34] The World Wide Web Consortium \(W3C vices Choreography Description Language Version 1.0?, 17 December, 2004, http://www.w3.org/TR/2004/WD-ws-cdl10-20041217 35] The World Wide Web Consortium \(W3C vice Choreography Interface \(WSCI 


http://www.w3.org/TR/wsci 36] The World Wide Web Consortium \(W3C vice Modeling Language \(WSML http://www.w3.org/Submission/WSML 37] The World Wide Web Consortium \(W3C vice Modeling Ontology \(WSMO http://www.w3.org/Submission/WSMO 38] J. Yang and M.P. Papazoglou, ?Web Component: A Substrate for Web Service Reuse and Composition?, Proc 14th Conf. Advanced Information Systems Eng. \(CAiSE 02 LNCS 2348, pp. 21?36, Springer-Verlag, 2002 39] Y.P. Yang, Q.P. Tan, and Y. Xiao, ?Verifying Web Services Composition Based on Hierarchical Colored Petri Nets?, IHIS?05, pp. 47-53, Bremen, Germany, 2005 40] Y.P. Yang, Q.P. Tan, Y. Xiao, J.S. Yu, and F. Liu, ?Exploiting Hierarchical CP-Nets to Increase the Reliability of Web Services Workflow?, Symposium on Applications and the Internet \(SAINT?06 41] X.C. Yi and K.J. Kochut, ?Process Composition of Web Services with Complex Conversation Protocols: a Colored Petri Nets Based Approach?, Conference on Design, Analysis, and Simulation of Distributed Systems, pp.141-148, 2004 42] D. Zhovtobryukh, ?A Petri Net-based Approach for Automated Goal-Driven Web Service Composition?, SIMULATION, Vol. 83, Issue 1, pp.33-63, January 2007 43] C. Zhou, L.T. Chia, and B.S. Lee, "Web Services Discovery with DAML-QoS Ontology", International Journal of Web Services Research, vol. 2: no. 2: pp. 43-66, 2005   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


  17 Mission Phase Relevant Archit ecture Informat ion Purpose Funct ion Mat urit y Pr oduct s DODAF M odel Re f e r e nce N ot e s Preliminary System Design Integrated Risk List Cross  functional list of risks compiled across integrated product team PDR Delivery Consolidated Ri sk Li st FFP1A Operational Environment M atrix documenting how test requirements that have been levied are satisfied with test \(at both the component and system levels Identification of the method that w ill be used to verify the requirement Identification of any potential non-conformances  Initial Delivery Environmental Te st  Verification Matrix FFP7 This will show the capability of the SV to withstand various environments \(i.e. launch vehicles System Interface Control Documentation This will be a suite of documents, the mission plan s hould identify critical system boundaries that reuqire a formal interface control document M inimum Criteria:  SV - Ground and Payload to SV ICD initial drafts must be compl e t e Other pertinent ICDs:  LV - Spacecraft, Component interface ICD Initial Delivery Interface Control Documentation SV 1  SV 6 Schedule Program Driving Schedule Requirements Key schedule driven technical decisions Giver receiver relationships that span different program elements Li v i n g Document Intgrated Milestone Schedule PV2 System Sub-system Design Specifications Partial Preliminary understanding of system subsystem design Allocation of required system functions to configuration items Demonstration of how system requirements are satisfied by design Initial Delivery PDR De si gn  Presentation SV 5 Open System Trades Desription organized by susbsystem of open design trades and decsions that need to be completed Each trade should have an owner and assocaited due dates that are aligned with program constriants Li v i n g Document Trade  Decision tracking matrix FFP5 Technical Performance Measures Demonstrate design peformance to critical program requirements outlined within the requirements document Initial Delivery Technical performance budget SV 7 Values in the budget should be compared to industry standards for a given maturity in the devleopment De t ai l e d De si gn System  Design Specifications Detailed description of "to be"  system subsystem design Allocation of required system functions to configuration items Demonstration of how system requirements are satisfied by design Final Delivery CDR De si gn  Presentation SV 4  SV 5 Note: Reference Lesson 11 - Need to look at some views and diagrams that would be useful for every subsystem Integrated Risk List Cross  functional list of risks compiled across integrated product team CDR Delivery Consolidated Ri sk Li st FFP1A Operational Environment Matrix documenting how test requirements that have been levied are satisfied with test \(at both the component and system levels Identification of the method that w ill be used to verify the requirement Identification of any potential non-conformances  Final Delivery Environmental Te st  Verification Matrix FFP7 Note: previous delivery s houl d have defined how requirements would be satisfied for long lead components.  This delivery would address all remaiing compents and system levels System Interface Control Documentation This will be a suite of documents, the mission plan s hould identify critical system boundaries that reuqire a formal interface control document M inimum Criteria:  SV - Ground and Payload to SV ICD initial drafts must be compl e t e Other pertinent ICDs:  LV - Spacecraft, Component Interface ICD Final Delivery Interface Control Documentation SV 1  SV 6 Schedule Program Driving Schedule Requirements Key schedule driven technical decisions Giver receiver relationships that span different program elements CDR De l i v e r y Intgrated Milestone Schedule PV2 Integration Prodcution Plan List of all components under procurement and their expected and need dates List should include all piece parts, miscellaneous mat ls, connectors and required ground support equipment Initial Delivery Sy st e m Pa r t s  Li st  FFP6 Technical Performance Measures Demonstrate design peformance to critical program requirements outlined within the requirements document Final Delivery Technical performance budget SV 7 Values in the budget should be compared to industry standards for a given maturity in the devleopment Open System Trades Desription organized by susbsystem of open design trades and decsions that need to be completed Each trade should have an owner and assocaited due dates that are aligned with program constriants Li v i n g Document Trade  Decision tracking matrix FFP5   


 LNCRITIC 0.884 \(.005 0.362 352 0.593 053  CRPRO -0.007 \(.306 0.012 183 0.002 798  CRCON 0.010 \(.291 0.013 230 0.019 095  Model fit F p value 24.900 lt;.0001 11.110 lt;.0001 5.940 lt;.0001 Adjusted R2 0.559 0.553 0.320 p &lt; .10 p &lt; .05 Notes: p values are in parentheses  4.5. South Korean versus American market  In terms of the effect of WOM, we find no discernable difference in the motion picture markets of South Korea and the United States. Volume of WOM is positively correlated to the following week?s revenue in both markets, and valence of WOM is not significant The effect of critical reviews, however, did not concur While the literature on the American market data reports that positive critical reviews are positively related to box office revenue[21, 34], the results on the Korean market was different. There could be several reasons for this. First, South Korea and the United States have different sources for critical reviews, and the sources may have different impacts on moviegoers Second, the characteristics of critics might be different i.e., Korean critics may prefer movies that are considered less commercial or artistic than American critics  5. Conclusion  WOM and critical reviews both are important attributes that influence box office revenue in the motion picture industry. In this study, six hypotheses related to this issue were set up and tested. Data was collected on the motion picture industry of South Korea by using several websites that provide content and statistical data about movies. Finally, data on 118 movies was collected and the movies were categorized into two groups based on the distributors of the movies If the distributor of a movie was one of the major distributors in South Korea, that movie was categorized into mainstream movies, and if not then the movie was categorized into non-mainstream movies. As expected mainstream movies had much higher box office revenue and volume of WOM than non-mainstream Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 movies. In the case of the volume of critical reviews 


movies. In the case of the volume of critical reviews however, there was no big difference between mainstream and non-mainstream movies. WOM and critical reviews were usually positive H1 and H2 tested the relationship between WOM and weekly box office revenue, and the results supported the hypotheses. The volume of WOM was positively related to weekly box office revenue, while the valence of WOM had no significant effect. H3 H4a, and H4b tested the impact of critical reviews, and the results also supported the hypotheses except H4b The volume and valence of critical reviews had no consistent significances to weekly box office revenue H3 H4b. Table 7 showed that the number of critical reviews was statistically significant to aggregate box office revenue \(H4a for the attitude of critical reviews \(H4b the result more detail, an additional test was performed using only those factors related to critical reviews as independent variables. The result of the additional test supported H4, but the signs were reversed, i.e. positive critical reviews had minus signs, and negative critical reviews had plus signs. This reversed signs imply that the preference of critical reviewers is very similar to that of normal moviegoers. H5s and H6s tested the different effects of WOM and critical reviews on mainstream and non-mainstream movies. The result failed to determine that WOM give different impact on mainstream and non-mainstream movies, so H5a and H5b were rejected. H6, however, was supported, i.e the effects of critical reviews were different for mainstream and non-mainstream movies. There were no significant relationships between critical reviews and aggregate box office revenue in mainstream movies. For non-mainstream movies, however, the volume of critical reviews and the percentage of negative critical reviews were significant. Nonmainstream movies have fewer sources from which consumers can get information, and this might explain the results The above findings lead to several managerial implications. First, producers and distributors of movies could forecast weekly box office revenue by looking at previous weeks? volume of WOM. It does not matter what attitude people have when they spread WOM, the important factor is its volume. Therefore producers and distributors need to develop an appropriate strategy to manage WOM for their movies For example, the terms related to WOM marketing such as buzz and viral marketing are easily found Second, for the distributors who usually distribute less commercial and more artistic movies, and consequently have a smaller market compared to the major distributors, critical reviews can impact their movies box office revenues in a significant way. There are usually fewer sources for information for nonmainstream movies than mainstream movies, and so small efforts could leverage the outcomes. Finally, for those who are dealing with mainstream movies, the finding that the valence of WOM and critical reviews do not have significant relationship with box office revenue can have certain implications. Particularly, the attitude of critical reviews showed reversed effects Therefore, they may need to concentrate on other features rather than attitude of moviegoers or critical reviews, such as encouraging moviegoers to spread WOM This study contributes to the understanding of the motion picture industry, especially the relationship between box office revenue and WOM including critical reviews. There are existing studies that already 


critical reviews. There are existing studies that already dealt with similar issues, but this study has some differentiated features compare to prior studies. First the data used in this study was collected from South Korea, while most of the relevant studies usually focus on the North American market. This helps to provide the opportunity to understand the international market especially the Asian market, even though South Korea is a small part of it in terms of the motion picture industry. Second, movies were categorized to two groups, i.e. mainstream and non-mainstream and this study attempted to determine how WOM impacts these categories differently by testing several hypotheses In this study, there are also several limitations that could be dealt with in future research. First, using box office revenue as a dependent variable is more meaningful for distributers rather than producers. Due to there is close correlation between box office revenue and number of screens, one of producers? main concerns is how many screens their movies can be played on. Moreover, DVD sales are also important measurement for success of movies these days, and so it also could be a dependent variable. Therefore, it could be possible to give more fruitful managerial implications to various players in the motion picture industry by taking some other dependent variables Second, in this study, movies were categorized simply as mainstream and non-mainstream movies, but there could be further studies with diverse techniques of movie categorizations. For example, it would be possible to study the varying influence of WOM or critical reviews on different genres or movie budgets Third, an interesting finding of this study is that positive critical reviews could have negative relationship with box office revenue while negative critical reviews could have positive relationship. This study tried to provide a reasonable discussion on the issue, but more studies could be elaborate on it  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 References  1] Dellarocas, C., The Digitization of Word of Mouth Promise and Challenges of Online Feedback Mechanisms Management Science, 2003. 49\(10 2] Bone, P.F., Word-of-mouth effects on short-term and long-term product judgments. Journal of Business Research 1995. 32\(3 3] Swanson, S.R. and S.W. Kelley, Service recovery attributions and word-of-mouth intentions. European Journal of Marketing, 2001. 35\(1 4] Hennig-Thurau, F., et al., Electronic word-of-mouth via consumer-opinion platforms: What motivates consumers to articulate themselves on the internet? Journal of Interactive Marketing, 2004. 18\(1 5] Fong, J. and S. Burton, Electronic Word-of-Mouth: A Comparison of Stated and Revealed Behavior on Electronic Discussion Boards. Journal of Interactive Advertising, 2006 6\(2 6] Gruen, T.W., T. Osmonbekov, and A.J. Czaplewski eWOM: The impact of customer-to-customer online knowhow exchange on customer value and loyalty. Journal of Business Research, 2006. 59\(4 7] Garbarino, E. and M. Strahilevitz, Gender differences in the perceived risk of buying online and the effects of receiving a site recommendation. Journal of Business Research, 2004. 57\(7 8] Ward, J.C. and A.L. Ostrom, The Internet as information minefield: An analysis of the source and content of brand information yielded by net searches. Journal of Business Research, 2003. 56\(11 


9] Goldsmith, R.E. and D. Horowitz, Measuring Motivations for Online Opinion Seeking. Journal of Interactive Advertising, 2006. 6\(2 10] Eliashberg, J., A. Elberse, and M. Leenders, The motion picture industry: critical issues in practice, current research amp; new research directions. HBS Working Paper, 2005 11] S&amp;P, Industry surveys: Movies and home entertainment 2004 12] KNSO, Revenue of Motion Picture Industry 2004 Ministry of Culture, Sports, and Tourism, 2004 13] Duan, W., B. Gu, and A.B. Whinston, Do Online Reviews Matter? - An Empirical Investigation of Panel Data 2005, UT Austin 14] Zhang, X., C. Dellarocas, and N.F. Awad, Estimating word-of-mouth for movies: The impact of online movie reviews on box office performance, in Workshop on Information Systems and Economics \(WISE Park, MD 15] Mahajan, V., E. Muller, and R.A. Kerin, Introduction Strategy For New Products With Positive And Negative Word-Of-Mouth. Management Science, 1984. 30\(12 1389-1404 16] Moul, C.C., Measuring Word of Mouth's Impact on Theatrical Movie Admissions. Journal of Economics &amp Management Strategy, 2007. 16\(4 17] Liu, Y., Word of Mouth for Movies: Its Dynamics and Impact on Box Office Revenue. Journal of Marketing, 2006 70\(3 18] Austin, B.A., Immediate Seating: A Look at Movie Audiences. 1989, Wadsworth Publishing Company 19] Bayus, B.L., Word of Mouth: The Indirect Effects of Marketing Efforts. Journal of Advertising Research, 1985 25\(3 20] Faber, R.J., Effect of Media Advertising and Other Sources on Movie Selection. Journalism Quarterly, 1984 61\(2 21] Eliashberg, J. and S.M. Shugan, Film critics: Influencers or predictors? Journal of Marketing, 1997. 61\(2 22] Reinstein, D.A. and C.M. Snyder, The Influence Of Expert Reviews On Consumer Demand For Experience Goods: A Case Study Of Movie Critics. Journal of Industrial Economics, 2005. 53\(1 23] Gemser, G., M. Van Oostrum, and M. Leenders, The impact of film reviews on the box office performance of art house versus mainstream motion pictures. Journal of Cultural Economics, 2007. 31\(1 24] Wijnberg, N.M. and G. Gemser, Adding Value to Innovation: Impressionism and the Transformation of the Selection System in Visual Arts. Organization Science, 2000 11\(3 25] De Vany, A. and W.D. Walls, Bose-Einstein Dynamics and Adaptive Contracting in the Motion Picture Industry Economic Journal, 1996. 106\(439 26] Bagella, M. and L. Becchetti, The Determinants of Motion Picture Box Office Performance: Evidence from Movies Produced in Italy. Journal of Cultural Economics 1999. 23\(4 27] Basuroy, S., K.K. Desai, and D. Talukdar, An Empirical Investigation of Signaling in the Motion Picture Industry Journal of Marketing Research \(JMR 2 295 28] Neelamegham, R. and D. Jain, Consumer Choice Process for Experience Goods: An Econometric Model and Analysis. Journal of Marketing Research \(JMR 3 p. 373-386 29] Lovell, G., Movies and manipulation: How studios punish critics. Columbia Journalism Review, 1997. 35\(5 30] Thompson, K., Film Art: An Introduction. 2001 McGraw Hill, New York 31] Zuckerman, E.W. and T.Y. Kim, The critical trade-off identity assignment and box-office success in the feature film industry. Industrial and Corporate Change, 2003. 12\(1 


industry. Industrial and Corporate Change, 2003. 12\(1 27-67 32] KOFIC, Annual Report of Film Industry in Korea 2006 Korean Film Council, 2006 33] Sutton, S., Predicting and Explaining Intentions and Behavior: How Well Are We Doing? Journal of Applied Social Psychology, 1998. 28\(15 34] Basuroy, S., S. Chatterjee, and S.A. Ravid, How Critical Are Critical Reviews? The Box Office Effects of Film Critics Star Power, and Budgets. Journal of Marketing, 2003. 67\(4 p. 103-117  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 





