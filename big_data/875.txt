  1 Understanding Earthquake Fault Systems using QuakeSim Analysis and Data Assimilation Tools  Andrea Donnellan, Jay Parker, Margaret Glasscoe, Robert Granat Jet Propulsion Laboratory, Califor nia Institute of Technology 4800 Oak Grove Drive Pasadena, CA  91109 818-354-4737 Andrea.Donnellan@jpl.nasa.gov  John Rundle Department of Physics, University of California, Davis One Shields Avenue Davis, CA, 95616  Dennis McLeod, and Rami Al-Ghanmi University of Southern California 
Mail Code 0781, 3651 Trousdale Parkway Los Angeles, CA 90089  Lisa Grant University of California, Irvine Irvine, CA 92697  Abstract 227We are using the QuakeSim environment to model interacting fault systems. One goal of QuakeSim is to prepare for the large volumes of data that spaceborne missions such as DESDynI w ill produce. QuakeSim has the ability to ingest distributed hete rogenous data in the form of InSAR, GPS, seismicity, and fault data into various earthquake modeling applications, automating the analysis when possible. Virtual California simulates interacting 
faults in California. We can compare output from long timehistory Virtual California runs with the current state of strain and the strain history in California. In addition to spaceborne data we will begin assimilating data from UAVSAR airborne flights over the San Francisco Bay Area the Transverse Ranges, and the Salton Trough. Results of the models are important for understanding future earthquake risk and for providing decision support following earthquakes. Improved models require this sensor web of different data sources and a modeling environment for understanding the combined data 
1,2  T ABLE OF C ONTENTS  1  I NTRODUCTION 1  2  F EDERATED D ATABASE 2  3  M ODELING AND S IMULATION A PPLICATIONS 
3  4  F ORECASTING 4  5  C ONCLUSIONS 6  6  A CKNOWLEDGEMENTS 6  R EFERENCES 7  B IOGRAPHIES 
7  1  1 978-1-4244-2622-5 09/$25.00 \2512009 IEEE 2 IEEEAC paper #1100, Version 4, Updated December 10, 2008 1  I NTRODUCTION  Traditional earthquake science is done by domain experts in individual, disjointed efforts and has typically been groundbased. As an example, crustal deformation observations conventionally come from triangulation, trilateration, or 
strain meters. More recently G PS networks have been used to measure crustal deformation. InSAR measurements provide detailed images of surface displacements. Large volumes of systematic measurements will be made with the advent of UAVSAR and DESDynI, one of the recommended Decadal Survey missions. Both DESDynI and UAVSAR carry L-band interferometric radar instruments and to produce images of surface deformation Crustal deformation modeling can include analytic solutions, finite element models, or elastic and viscoelastic models. Fault mechanics includes knowledge of fault frictional properties, fault nucleation, and rupture propagation. Integrating these models and the various 
datasets will enhance our understanding of earthquake processes The advent of web infrastructure and applications allows the integration of various scales of models to improve understanding of earthquake processes and mechanics. Data constrain the models; hence data sources also need to be integrated into the computational infrastructure Developing prototype syst ems now using UAVSAR, will prepare us for routinely anal yzing data from DESDynI. We use a service architecture explo iting a mix of Grid and Web 


  2 Table 1. Application descriptions, data sources, and how they are compared Application Purpose Data Source Compared To Virtual California Interacting fault model Faults, friction Earthquakes GeoFEST Finite element deformation model Faults GPS and InSAR surface deformation Disloc Surface displacements from fault motion Faults GPS and InSAR surface deformation Simplex Inversion for fault motion from deformation data InSAR and GPS surface deformation Fault data constrain model RIPI Seismic pattern analysis Seismicity Earthquake faults RDAHMM Time series analysis GPS Earthquake, aquifer sources  2.0/Cloud technologies to support computational and data systems. Descriptions of datasets and their interrelationships are essential to support effective interfaces for users, including data discovery, delivery, visualization, and integration. The QuakeSim federated database QuakeTables, interconnects data sources providing information that is then accessed by the modeling applications. The increased volumes of data provide an opportunity to carry out pattern analysis and study earthquakes from a nonlinear dynamical perspective QuakeSim is a project to integrate both real-time and archival sensor data with high-performance computing applications for data mining and assimilation to improve earthquake forecasts. The multi-scale nature of earthquakes requires integrating many data types and models to fully simulate and understand the earthquake process. QuakeSim focuses on modeling the interseismic process through various boundary element, finite element, and analytic applications, which run on various platforms ranging from desktops to high-end computers. Making these data available to modelers is leading to significant improvements in earthquake forecast quality and thereby mitigating the danger from this natural hazard Modeling earthquake fault systems enables us to understand how earthquake faults interact, how stress is transferred between faults, and how an earthquake on one fault will delay or advance the likelihood of an earthquake on other faults. QuakeSim incorporat es several approaches to modeling these systems \(Table 1\rtual California is a boundary element software application that models interactions between vertical strike-slip faults. GeoFEST is finite element software that computes the stress and strain field from arbitrary faults within a mesh. GeoFEST can model faults of any geometry heterogeneous elastic or viscoelastic crustal material properties. Analytic tools Disloc and Simplex calculate surface deformation or fault parameters in an elastic ha lf space. Pattern analysis algorithms have proved useful for analyzing time series GPS\and spatial \(seismicity\a. The latter looks promising for earthquake forecasting 2  F EDERATED D ATABASE  Modeling interacting fault sy stems and surface deformation due to fault slip requires specifications of faults as input to the models. Through a web-services based portal QuakeSim provides global access to reference models of faults and fault data as well as the analysis tools. Federation of the data records and provides portal-based access to diverse data types. Currently, QuakeTables houses a comprehensive set of paleoseismic and fault data that can be ingested into QuakeSim applications. The database includes processed InSAR data and is being populated with processed California interferograms, courtesy of Paul Lundgren, as they become available QuakeSim applications are used to model fault activity such as rate of strain accumulation or offset related to earthquakes over a finite fault segment. The modeler is interested in the general fault characteristics, such as geometry and average rate of slip with an associated uncertainty. Paleoseismic data and results are typically reported in scientific publications and there is no standard format or method for this reporting. It is a challenge to convert data, particularly those collected and reported by a variety of means into standard data for modeling applications. For many faults there are multiple interpretations The purpose of QuakeTables is to standardize data for QuakeSim modelers and allow th e modeler to further refine interpretations about faults  W e  have devel oped ontologies for the different types of data, such as paleoseismic fault data, and InSAR data. QuakeTables does 


  3 not house one single, self-consistent, fault model for California. Rather than defining a single reference model for faults or other data QuakeTables houses published data sources. Multiple different interpretations and data sets allows the user to access a self-consistent set of faults for their model and to be able to trace a fault segment recorded in the database back to the original reference. This allows the modeler to test various fault interpretations and leave the option for uploading a new fault interpretation based on refined analysis. An important capability of the QuakeTables design is its capability to store data from different data sources and keep it in its original format along with any calculated or derived datasets based on this original set 3  M ODELING AND S IMULATION A PPLICATIONS  Disloc and Simplex Disloc and Simplex are both based on Okada\222s  formulation for calculating surface displacements from dislocations in an elastic ha lf-space. Disloc is a forward model for calculating surface deformation from prescribed slip on an arbitrary number of faults.  Simplex is an inverse algorithm residual minimization algorithm using disloc to calculate fault motions and errors based on surface deformation measurem Simplex is limited to homogeneous elastic models, and so we are developing a correcti on step, wherein the Simplex estimate of fault slip automatically becomes the basis of a suite of detailed finite element simulation including known materials variation. This suite of results will indicate the best first-order correction to the fault parameters. The goal is a broadly adaptable integrat ed system of precision surface deformation monitoring, combined with a modeling system that incorporates processes at multiple scales. This will allow definition of a baseline model of regional and global deformation processes, to be continuously compared with sensor observations for automatic early detection of unusual events. Although this feature is not yet part of QuakeSim, it illustrates the goal of interfacing modeling tools of different scales and with different features, to improve the overall model of fault system deformation and interactions Virtual California \(VC Virtual California  i s a t opol ogi cal l y real i s t i c boundary  element numerical simulation of earthquakes occurring on the fault systems of California. It includes all the major strike-slip faults in California and has now been extended to depth-dependent boundary elements and dipping faults. VC is an efficient, parallel object oriented C++ numerical code that runs on NASA computers Columbia and JPL computer COSMOS using MPI-II protocol s. VC can also compute associated surface displacemen ts through time. From the related simulation, InSAR interferograms can be computed for use in analyzing and interpreting signals from real interferograms obtained via radar satellites such as DESDynI. This process of comparing simulated interferograms with real interferograms will allow us to study questions relating to physics of earthquakes such as s1\ failure process of major earthquakes on complex fault systems; 2\ming and statistics of major earthquakes on complex fault systems; and 3\gin of space-time correlations between major earthquakes Another major application of VC simulations lies in earthquake forecasting. In weat her forecasting, current and past observational data are routinely assimilated into numerical simulations to produce ensemble forecasts of future events in a process termed \223model steering.\224 We have developed a similar approach \(Van Aalsburg et al 2008\hat is motivated by analyses of previous 30-year forecasts of the Working Group on California Earthquake Probabilities \(Field, 2008 systematically comparing simulation to observed data the variability of paleoseismic and historic data, a series of spatial probability density functions \(PDFs\can be computed that describe the probable locations of future large earthquakes. These forecasts yield fault-based loca tions for the next earthquake as well as most probable locations for earthquakes during the next 30 years Virtual California produces a very large synthetic seismic record. Systematic observations over a long time period provide an opportunity to observe emergent behavior and fault interactions in the system. Examining 40,000 years of VC data and calculating the correlation of events on an 223initiating\224 fault segment with subsequent events on a second fault element produces a correlation score matrix that shows the relative amount of correlation between events on two elements \(Figure 1\been expanded to include processing of data at different time window lengths. Examining correlation score matrices at different time window lengths can provide insight into the relationships between faults at different time scales The events on the Eastern California Shear Zone typically precede, but do not follow events on the southern San Andreas fault GeoFEST GeoFEST is a finite element program for modeling fault motion in a mechanically realist crust. It includes nonlinear rheology, slip on curved and wrinkled faults, and production of images of Coulomb St Anom al i e s can be detected and assessed with less approximation in the modeling. Stress transfer can be estimated for hazard assessment 


  4  Figure 1 Log of correlation score matrix 400 yr time window.  Raw score matrix is saturated at values around zero, so a log function is applied to highlight interesting features.  Analys is above features 59 faults \(639 elements\ include the \223creeping\224 section of the San Andreas fau lt because of computational considerations GeoFEST is being used to investigate the effects of large earthquakes over time.  The magnit 1906 San Francisco earthquake produced a rupture of 2\2268 m rupturing the northern third of the San Andreas fault It has been questioned whether there is still a detectable strain signature in the present-day geodetic data. Studies to date with GeoFEST point to the importance of developing realistic models of crustal deformation \(Figure 2\ore complex \(realistic\models show postseismic effects of the 1906 earthquake of 1 mm/yr rather than the 2\2265 mm/yr for the less complex models \(Figure 3 4  F ORECASTING  The Pattern Informatics \(PI\Intensity \(RI\and RIPI methods use online seismicity catalogs to generate space-time forecast maps. Systematic and ongoing real-time tests of these forecasts are posted on the NASA/JPL quakesim web site. A real-time test of the original method was published in Rundle et al and a t e st of an updated and modified method as published in Holliday et al  The great m a jori t y of t h e eart hquakes have occurred on or near a colored anomaly, or \223hotspot\224 on that map. The location of the recent M5.4 Chino Hills earthquake was successfully fo recast by both maps, but the more recent M 5.1 Ludlow earthquake  \(Figure 4\At present we update versions of a forecast map on the QuakeSim web site as publicati ons occur.  Eventually we will publish continuously updated forecasts. In the last twelve months eight earthquakes above magnitude 5 have occurred in identified hotspots \(Figure 4\dentified hotspots make up only 1.2% of the total map area of the 


  5    Figure 2 Model geometry for complex 1906 earthquake postseismic de formation models. QuakeSim portal tools were used to generate initial and subsequent meshes for input into GeoFEST viscoelastic simulation software   Figure 3 Fault parallel displacement for a: left panel: uniform slip model, middle panel: variable slip on the fault, and right panel: multiple segment San Andreas fault with plate boundary conditions. Residual velocities become smaller as the model becomes more complex forecast or the state of Califor nia. Approximately half the total boxes have at least 1 M>3 earthquake in them Therefore for the to tal active area, it would be forecast area of 2.4%. The approach is to minimize the forecast area which is essentially the false alarm rate, while still detecting all the large earthquakes \(maximizing the hit rate\The mean forecast error is the average distance that a M>5 earthquake occurs from a 11 km pixel box boundary. Most of this error is due to offshore earthquake # 6, which is about 50 km off the nearest red pixel. Without that earthquake, the error would be less than 5.5 km, half of one pixel box size. Additional earthquakes will allow us to optimize the forecast area Earthquake forecasts and models with time frames of years rather than decades and much smaller spatial scales on the order of 10 km rather than 100 km scales are now possible The models are based on patterns of seismicity at present Similar pattern recognition techniques will be used on the radar interferograms from DESDynI when they become available.  The methodology requires systematic data, which DESDynI will produce. Current InSAR missions do not produce systematic crustal deformation measurements   


  6  Figure 4 Recent successful forecast of the M 5.4 Chino Hills earthquake of July 29, 2008  5  C ONCLUSIONS  QuakeSim model applications are showing the importance of modeling the complexity and detail of fault systems Faults interact with each ot her, and the order in which earthquakes occur impacts the order of subsequent earthquakes.  Developing models to match observed data requires realistic models.  Efficiency is gained by the access of the data by models such as of QuakeTables through QuakeSim. Results from the Pattern Informatics method and Virtual California indicate that seismicity and crustal deformation produce detectable patterns that can be used to infer earthquake hazard. Con tinued modeling will indicate patterns of crustal deformation that may be key to improved earthquake forecasting.  Devel oping these prototypes now is important for being prepared to maximize the use of InSAR crustal deformation data from DESDynI and to develop a routine operational data analysis system.  UAVSAR will be a valuable testbed for developing a prototype system for DESDynI, but will also be important for filling in data gaps in response to earthquakes or other events 6  A CKNOWLEDGEMENTS  We\222d like to thank the many other QuakeSim contributors These include JPL staff members Charles Norton for the adaptive mesh refinement and porting and optimization of software to the Columbia and Cosmos systems, and Margaret Glasscoe for testing the portal using science applications and developing the web pages, and Harout Nazerian for developing the web pages and testing the portal. Galip Aydin, John Youl Choi, and Zhigang Qi, at Indiana University have also contributed to development of the QuakeSim portal. Gleb More in at UC Davis has worked on the development of Virtual California including porting to the Columbia computer syst em. Lorena Medina corrected and validated the QuakeTables da tabase at UC Irvine. Terry Tullis at Brown University and Nick Beeler from the USGS develop and maintain the PARK application. This work was carried out at the Jet Propulsion Laboratory, California Institute of Technology under contract with NASA from the Earth Science and Technology Office, and at University of Southern California, Indiana University, NASA Ames University of California Davis, and University of California 


  7 Irvine, and Brown University under subcontract with the Jet Propulsion Laboratory R EFERENCES  1. Grant, L.B., et al A Web Services-Based Universal Approach to Heterogene ous Fault Databases Computing in Science and Engineering Special Issue on MultiPhysics Modeling, 2005. 7\(4\ p. 51-57 2. Okada, Y Surface deformation due to shear and tensile faults in a half-space Bulletin of the Seimological Society of America, 1983. 75\(4\ p. 1135-1154 3 Lyzenga, G.A., W.R. Panero, and A. Donnellan The Influence of Anelastic Surface Layers on Postseismic Thrust Fault Deformation Journal of Geophysical Research, 2000. 105: p. 3151-4157 4 Rundle, P.B., et al Virtual California: Fault model frictional parameters, applications Pure and Applied Geophysics, 2006. 163\(1819-1846 5. Rundle, J.B., et al A simulation-based approach to forecasting the next great San Francisco earthquake  Proceedings of the National Academy of Sciences, 2005 102: p. 15,363-15,367 6. Van Aalsburg, J., et al A feasibility study of data assimilation in numerical simulations of earthquake fault sytems Physics of the Earth and Planetary Interiors, 2007 163\(149-162 7 Parker, J., et al Geophysical Finite Element Simulation tool \(GeoFEST\: algorithms and validation for quasistatic regional faulted crust problems Pure and Applied Geophysics, 2008. 165: p. 497-521 8 Wald, D.J., H. Kanamori, and D.V. Helmberger Source Study of the 1906 San Francisco Earthquake Bulletin of the Seimological Society of America, 1993. 83: p. 9811019 9. Ellsworth, W.L Earthquake History 1769-1989 in The San Andreas Fault System R.E. Wallace, Editor. 1990 U.S. Geological Survey Professional Paper. p. 152-187 10. Lawson A.C The California Eart hquake of April 18 1906 in Report of the State Earthquake Investigation Commission 1908, Carnegie Institution of Washington 11. Rundle, J.B., et al Self-organization in leaky threshold systems: The influence of near-mean field dynamics and its implications for earthquakes, neurobiology, and forecasting Proceedings of the National Academy of Sciences, 2002. 99: p. 2514-2521 12. Holliday, J.R., et al A RELM Earthquake Forecast Based on Pattern Informatics Seismological Research Letters 2007. 78: p. 87-93 B IOGRAPHIES  Andrea Donnellan is the QuakeSim principal investigator at NASA\222s Jet Propulsion Laboratory and is a research professor at the University of Southern California. Donnellan uses GPS and InSAR satellite technology coupled with high performance computer models to study earthquakes, plate tectonics, and the corresponding movements of the earth's crust. She has been a geophysicist at JPL since 1993 She received a B.S. from the Oh io State University in 1986 with a geology major and mathematics minor. She received her M.S. and Ph.D. in geophysics from Caltech's Seismological Laboratory in 1988 and 1991 respectively Donnellan received an M.S. in Computer Science from the University of Southern California in 2003. She held a National Research Council Postdoctoral Fellowship at NASA Goddard Space Flight Center. Donnellan was a Visiting Associate at the Se ismological Laboratory at Caltech from 1995 to 1996. In 1996 Donnellan received the Presidential Early Career Award for Scientists and Engineers, in 2003 the Women in Aerospace Award for Outstanding Achievement, and in 2006 she was the MUSES of the California Science Center Foundation Woman of the Year Jay Parker is a Senior Scientist in the Geodynamics and Space Geodesy group of the Jet Propulsion Laboratory a NASA center administered by the California Institute of Technology. His graduate research used computer simulations to explain mesospheric ionization response to solar flares and dynamic instabilities. Dr Parker's research subjects at the Jet Propulsion Laboratory include a variety of topics in remote sensing analysis and modeling These include supercomputing algorithms for electromagnetic scattering and radiation, satellite geodesy and finite element simulation for earthquake-related deformation, and ocean sensing through GPS signal reflection. He is currently th e software engineer and coinvestigator for the QuakeSim project, which has developed a solid Earth science framework including a variety of simulation and analysis tools. He also develops the SEASCRAPE software system for high-fidelity simulation and parametric retrieval of atmospheric infrared spectrometry at Remote Sensing Analysis Systems, Inc. of Altadena CA. Dr. Parker is a member of the American Geophysical Union and co-chair of the Data Understanding 


  8 and Assimilation working group of the APEC Cooperation for Earthquake Simulation Robert Granat is a senior member of the technical staff in the Data Understanding Systems Group at NASA\222s Jet Propulsion Laboratory. He received a B.S in engineering and applied science from Caltech in 1996, an M.S. in electrical engineering in 1998 from University of California, Los Angeles, and a Ph.D. in electrical engineering with an emphasis in signal processing from UCLA. He works on the application of hidden Markov models to the study earthquakes and spacecraft systems John Rundle is the Director of the Computational Science and Engineering Center at the Univers ity of California at Davis, and is Director of the California Institute for Hazard Research of the University of California.  He was educated at Princeton University \(BSE 1972\he University of CaliforniaLos Angeles \(MS, 1973, PhD, 1976\ His research is focused on understanding the dynamics of earthquakes through numerical simulations; pattern analysis of complex systems; dynamics of driven nonlinear Earth systems; and adaptation in general complex systems  He has published over 200 papers in the peer-reviewed literature Dennis McLeod is currently Professor of Computer Science at the University of Southern California, and Director of the Semantic Information Research Laboratory. He received his Ph.D., M.S and B.S. degrees in Computer Science and Electrical Engineering from MIT.  Dr McLeod has published widely in the areas of data and knowledge base systems, fede rated databases, database models and design, and ontologies. His current research focuses on dynamic ontologies, user-customized information access, database semantic heterogeneity resolution and interoperation; personalized information management environments; information management environments for geoscience and homeland security information, crisis management decision support systems and privacy and trust in information systems Rami Al-Ghanmi is a PhD student in the Computer Science Department at the University of Southern California. He received his MS in Computer Science from the USC \(2006\S in Computer Engineering from King Fahd University of Petroleum & Minerals Dhahran, Saudi Arabia \(2002\s currently working with Professor Dennis McLeod at the Semantic Information Research Group at USC. His research interests are Semantic Web Se rvices and Ontology-based Federation of scientific data Lisa Grant is an associate professor of public health at University of California Irvine.  She studies environmental problems from a geologic perspective with emphasis on natural hazards. The primary objective of her research program is to identify active faults and to quantify their potential for generating large earthquakes by discovering their history of earthquake production over geologic time intervals. The record of previous large earthquakes on active faults is one of the best indicators of future earthquake activity. The results of her research on earthquake occurrence patterns are applied for earthquake forecasting, land-use planni ng, building design, risk assessment, disaster preparedness planning, and public education about the earthquake threat.  Grant received a B.S. in environmental Earth science from Stanford in 1985 an M.S. in both environmental engineering and science and geology in 1989 and 1990 respectively and a Ph.D. in geology and geophysics from Caltech in 1993. She serves on the board of directors of the Southern California Earthquake Center, and is an associate director of the California Hazards Research Institute 


  9  


result of actions by individuals and units within the organization. These actions of assimilation, in turn are stimulated or discouraged by an organizational milieu of norms, values, and rules. To the extent that assimilation of IT lies in the arena of rational choice of individuals, organizations may be able to foster assimilation by providing appropriate incentives Third, the quality of information resource management is positively associated with organizational assimilation of IT. The capacity of IT department in providing end-user support and enduser participation in IT project planning continue to Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6   be important in promoting the organizational assimilation of IT   6. References  1] A. Northrop, W. Dutton and K. Kraemer, ?The Management of Computer Applications in Local Government? Public Administration Review, 42\(3 2] A. Pinsonneault, and K.  L. Kraemer, ?Middle Management Downsizing: An Empirical Investigation of the Impact of Information Technology,? Management Science, 43\(5 3] A. Pinsonneault, and K. L. Kraemer, ?Exploring the Role of Information Technology in Organizational Downsizing: A Tale of Two American Cities Organization Science, 13\(2 4] B. Bozeman, and S. Bretschneider, ?Public Management Information Systems: Theory and Prescription,? Public Administration Review, Special Issue 1986, pp. 475-487 5] Beyer, J. and H. M. Trice, Implementing Change Alcoholism Polices in Work Organization. New York: Free Press, 1978 6] C. Ranganathan, Jasbir S. Dhaliwal, and Thopmson S. H. Teo, ?Assimilation and Diffusion of Web Technologies in Supply-Chain Management: An Examination of Key Drivers and Performance Impacts International Journal of Electronic Commerce 9\(1 pp.  127-161 7] Choi, Heungsuk, ?Infomatizing Korean Localities Coordination by the Central Leadership,? International Review of Public Administration 3\(2 8] Choi, Heungsuk, ?Reforming Government with Information Technology in South Korea,? Asian Journal of Political Science 11\(1  9] D. Chatterjee, R. Grewal, and V. Sambamurthy Shaping Up for E-Commerce: Institutional Enablers of the Organizational Assimilation of Web Technologies,? MIS Quarterly 26\(2 10] D. T. Bugler, and S. Bretschneider, ?Technology Push or Program Pull: Interest in New Information Technologies within Public Organizations,? In Public Management: The State of Art, Barry Bozeman \(ed Jossey-Bass, 1993 11] D. Davis,  ?Perceived Usefulness, Perceived Ease of Use, and User acceptance of Information Technology MIS Quarterly 13, 1989, pp. 319-339 12] D. Davis, R.P. Bagozzi, and P.R. Warshaw, ?User Acceptance of Computer Technology: A Comparison of Two Theoretical Models,? Management Science 35 1989,pp.  982-1002 13] F. Hull, and J. Hage, ?Organizing for Innovation Beyond Burns and Stalker?s Organic Type,? Sociology 16 1989, pp. 546-577 


14] H. Rebecca, ?An Information Infrastructure for Innovative Management of Government,? Public Administration Review 54\(6 15] H. Russell, and S. R. Carrico, ?Developing Capabilities to Use Information Strategically,? MIS Quarterly 12\(1 16] J. Perry and K. L. Kraemer,  ?Chief Executive Support and Innovation Adoption,? Administration and Society, 12\(2 17] J. R. Kimberly, and M. J. Evanisko Organizational, Environmental and Individual Impacts on Innovation,? Academy of Management Journal 24, 1981 pp. 689-713 18] M. Igbaria, ?End-User Computing Effectiveness: A Structure Equation Model,? Omega 18\(6 652 19] M. Jae Moon,  Application of Information Technology in State Procurement Management Pricewaterhouse Coopers Endowment for the Business of Government, 2002 20] M. Jae Moon and S. Bretschneider, ?Does Perception of Red Tape Constrain IT Innovativeness in Organizations: Unexpected Results from Simultaneous Equation Model and Implications,? Journal of Public Administration Research and Theory. 12\(2 291 21] Nedovic-Budic, Zorica and D. Godschalk, ?Human Factors in Adoption of Geographic Information Systems GIS Administration Review, 56\(6 22] Peled, Alon, ?Centralization or Diffusion? Two Tales of Online Government,? Administration and Society 32\(6 23] P. D. Collins, Hage, J. and Hull, F.M Organizational and technological predictors of change in automaticity. Academy of Management Journal 31\(3 pp.512?543 September 1988 24] Quinn, R. E. and Kimberly, J.R., Paradox, planning and perseverance: Guidelines for managerial practice, in Managing Organizational Transitions \(Kimberly, J. and Quinn, R. Eds 25] R. B. Cooper, and Zmud, R.W. IT implementation research: A technological diffusion approach. Management Science 36\(2  26] R. L. Daft, ?Bureaucratic versus Nonbureaucratic Structure and the Process of Innovation and Change,? in S Bachrach ed. Research in the Sociology of Organization vol. 1: 129-166. Greenwich, Conn.: JAI Press, 1982 27] Rainey, H. G, "Organization Structure, Design Technology, and Information Technology," In Understanding and Managing Public Organizations, C.A Jossey-Bass, 2003 28] R. G. Fichman and C. F. Kemerer,  ?The Assimilation of Software Process Innovation: An Organizational Learning Perspective,? Management Science 43\(10 29] Rogers, E. M, Diffusion of Innovations, New York The Free Press, 1995 30] S. Seneviratne, ?Information Technology and Organizational Change in the Public Sector,? In Information Technology and Computer Applications in Public Administration: Issues and Trends. Garson, David ed Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7   31] Seog-Yong, Kim et al.,  ?Public Employee?s Attitudes on Application of a Learning Organization to Public Sector,? Korean Public Administration Quarterly 17\(4 


17\(4 32] Seung-Hwan Myeong, ?The perceived impacts of information technologies on budget tasks in county government: Focusing on budget managers? perception and budget tasks? environments,?  Doctoral dissertation Syracuse University, 1996 33] Simon, H.  A, Models of Discovery, Dordrecht Holland: R. Reidel., 1977 34] Specht, Pamela,  ?The Impact of IT on Organization Performance in the Public Sector,? In Handbook of Public Information Systems. Garson, David ed 35] Trice, H. M., and Beyer, J. M. The Cultures of Work Organizations. Upper Saddle River, NJ: Prentice Hall 1993 36] V. Grover, and M. D. Goslar, ?The initiation adoption, and implementation of telecommunications technologies in U.S. organizations,? Journal of Management Information Systems 10\(1 163 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 pre></body></html 


managed. Although the provided methodology is flexible with respect to the level of detail, the decision to develop highly detailed RPBBs causes higher efforts for documenting processes. Further refinements of the methodology could address this issue by identifying typical sequences of components patterns basis of a semantic standardization of RPBBs using the comprehensive basis of 200 process models for the public sector. Consequently, such patterns will enable standardization of process fragments, too, and support a more efficient modeling \(cp. analogy-based construction [14 identification of typical process patterns is required Concluding previous discussions, the main strength of RPBB-based process documentation is the semantic standardization of models. This not only enables distributed modeling by heterogeneous user groups but also allows for the automation of analyses such as pattern-based mechanisms for plausibility and consistency check, performance measurement identification of organizational weaknesses and potentials for optimization \(organizational technology-driven, etc into other \(e.g. more technical or user-dependent notations and views [26]. However, automation approaches obviously require technical support Hence, a realization in form of a web-based prototype is currently progressing. On the one hand this prototype supports distributed process modeling with RPBBs implementing the idea of stepwise specialization via dynamic dialogues \(guided modeling semantic reasoning enables the pattern-based identification of organizational deficiencies weaknesses technologies The prototypical implementation allows for flexible modifications of existing RPBBs and for the extension with additional RPBBs and weakness pattern. It enables comprehensive testing and evaluation of the described approaches. Furthermore it supports the transfer of the developed methodologies to other domains and further associated adaptations and refinements. The underlying methodological foundation and the prototypical implementation thereby represent a rigorous as well as relevant contribution to the field of collaborative process modeling 6. Acknowledgement  The work published in this paper is partially funded by the European Commission \(E.C the PICTURE STREP. It does not represent the view of E.C. or the project consortium  7. Bibliography  1] O. Glassey: "A Case Study on Process Modelling Three Questions and Three Techniques", Decision Support Systems, Vol. 44, 2008, pp. 842-853  2] P. Green, M. Rosemann: "Integrated Process Modeling An Ontological Evalutation", Information Systems, Vol 25, 2000, pp. 73-87  3] B. Curtis, M.I. Kellner, J. Over: "Process Modeling Communications of the ACM, Vol. 35, 1992, pp. 75-90  4] B. List, B. Korherr: "An Evaluation of Conceptual Business Process Modelling Languages", in: Proceedings of the 21st Annual ACM Symposium on Applied 


of the 21st Annual ACM Symposium on Applied Computing \(SAC2006 ACM Press, New York, 2006, pp. 1532-1539  5] S. Lippe, U. Greiner, A. Barros: "A Survey on State of the Art to Facilitate Modelling of Cross-Organisational Business Processes", in: XML4BPM 2005, Proceedings of the 2nd GI Workshop XML4BPM - XML Interchange Formats for Business Process Management at 11th GI Conference BTW 2005. Gesellschaft f  r Informatik \(GI Bonn, Karlsruhe 2005, pp. 7-21  6] T.W. Malone, K. Crowston: "The interdisciplinary study of coordination", ACM Computing Surveys \(CSUR Vol. 26, 1994, pp. 87-119  7] C. Legner, K. Wende: "The Challenges of InterOrganizational Business Process Design - A Research Agenda", in: Proceedings of the 15th European Conference on Information Systems \(ECIS2007 Gallen, Switzerland. University of St. Gallen, St. Gallen 2007, pp. 106-118  8] O. Hanseth, E. Jacucci, M. Grisot, M. Aanestad Reflexive standardization: Side effects and complexity in standard making", MIS Quarterly, Vol. 30, 2006, pp. 564581  9] S.L. Schneberger, E.R. McLean: "The complexity cross: implications for practice", Communications of the ACM, Vol. 46, 2003, pp. 216-225  10] H.G. Rainey, R.W. Backoff, C.H. Levine: "Comparing Public and Private Organizations", Public Administration Review, Vol. 36, 1976, pp. 233-244  11] K.L. Schiflett, M. Zey: "Comparison of Characteristics of Private Product Producing Organizations and Public Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 Service Organizations", The Sociological Quarterly, Vol 31, 1990, pp. 569-583  12] L. Baacke, P. Rohner, R. Winter: "Aggregation of Reference Process Building Blocks to Improve Modeling in Public Administrations", in: Electronic Government, 6th International EGOV Conference, Proceedings of ongoing research, project contributions and workshops. Trauner Druck, Regensburg, Germany 2007, pp. 149-156  13] R. Sch  tte, T. Rotthowe: "The Guidelines of Modeling An Approach to Enhance the Quality in Information Models", in: ER 1998, pp. 240-254  14] J. vom Brocke: "Design Principles for Reference Modelling. Reusing Information Models by Means of Aggregation, Specialisation, Instantiation, and Analogy in: Fettke, P., Loos, P. \(eds Business Systems Analysis. Idea Group Publishing Hershey, PA, USA 2007, pp. 47-75  15] O. Thomas: "Understanding the Term Reference Model in Information Systems Research: History Literature Analysis and Explanation", in: Proceedings of the Workshop on Business Process Reference Models BPRM 2005 International Conference on Business Process Management BPM  16] P. Fettke, P. Loos, J. Zwicker: "Business Process Reference Models: Survey and Classification", in 


Reference Models: Survey and Classification", in Proceedings of the Workshop on Business Process Reference Models \(BPRM 2005 the Third International Conference on Business Process Management \(BPM  17] W.M.P. van der Aalst, A. Dreiling, F. Gottschalk, M Rosemann, M.H. Jansen-Vullers: "Configurable Process Models as a Basis for Reference Modeling", in Proceedings of the Workshop on Business Process Reference Models \(BPRM 2005 the Third International Conference on Business Process Management \(BPM  18] R. Knackstedt, C. Janiesch, R. T.: "Configuring Reference Models - An Integrated Approach for Transaction Processing and Decision Support", in Proceedings of the 8th International Conference on Enterprise Information Systems \(ICEIS 2006 135-143  19] F. Gottschalk, W.M.P. van der Aalst, M.H. JansenVullers: "Configurable Process Models - A Foundational Approach", in: Becker, J., Delfmann, P. \(eds Modeling - Efficient Information Systems Design Through Reuse of Information Models. Physica Verlag, Heidelberg 2007, pp. 59-77  20] A. Dreiling, M. Rosemann, W.M.P. van der Aalst, W Sadiq, S. Khan: "Model-Driven Process Configuration of Enterprise Systems", in: Wirtschaftsinformatik 2005 eEconomy, eGovernment, eSociety. Physica-Verlag Heidelberg 2005, pp. 687-706  21] C. Szyperski, D. Gruntz, S. Murer: Component Software: Beyond Object-Oriented Programming. Addison Wesley Longman, ACM Press New York 2002  22] D. Kratzig, K. B  nke, D. Slarna: Enterprise SOA Service-Oriented Architecture - Best Practices. Prentice Hall Professional Technical Reference, Upper Saddle River NJ 2004  23] T.H. Davenport, J.E. Short: "The New Industrial Engineering: Information Technology and Business Process Redesign", Sloan Management Review, Vol. 1990 pp. 11-27  24] S.T. March, G.G. Smith: "Design and natural science research on information technology", Decision Support Systems, Vol. 15, 1995, pp. 251-266  25] A.R. Hevner, S.T. March, J. Park, S. Ram: "Design Science in Information System Research", MIS Quarterly Vol. 28, 2004, pp. 75-101  26] L. Baacke, R. Fitterer, T. Mettler, P. Rohner: "A Methodology for ICT Impact Analysis Based on Semantic Process Models", in: Electronic Government, 7th International EGOV Conference, Proceedings of ongoing research, project contributions and workshops. Trauner Druck, Turin, Italy 2008, pp. 1-8  27] S. Brinkkemper: "Method Engineering: Engineering of Information Systems Development Methods and Tools Information and Software Technology, Vol. 38, 1996, pp 275-280  28] PICTURE \(Process Identification and Clustering for Transparency in Reorganising Public Administrations www.picture-eu.org, Specific Targeted Research Project 


STREP European Commission, 2006-2009  29] PICTURE: "Process Building Block Specification Deliverable 1.7 access 2007-04-10  30] S.A. White: Introduction to BPMN. IBM Corporation 2004  31] L. Baacke, R. Fitterer, P. Rohner: "Measuring Impacts of ICT on the Process Landscape of Public Administrations", in: Proceedings of the 3rd International Conference on e-Government \(ICEG2007 Quebec, Montreal, Canada 2007, pp. 21-30    Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


2 1 0 00                4 G ro w th 1  1 3 1 0 9 2 0 2 3 0 10  0 56                So ci ode m og ra ph ic c ha ra ct er is tic s 


s 5 A ge y ea rs   21 7 8 7 3 9 0 01 0 22  0 1 4 0 0 8              6 G en de r i s fe m al e2   0 2 4  0 0 6 0 0 2 0 00 0 0 3 0 10    


           7 C ur re nt ly n ot w or ki ng 2  0 0 5  0 0 8 0 04 0 04 0 0 1 0 16  0 16             8 C ur re nt ly in e du ca tio n2   0 6 


6 7  0 01 0 1 9 0 08  0 03 0 6 8 0 0 7 0 3 2           9 C ur re nt ly w or ki ng 2  0 2 8  0 03 0 18  0 1 1 0 0 3 0 64  0 00 0 1 4 0 8 9   


        10 E du ca tio n ac hi ev ed 3  3 5 7 1 5 2  0 04 0 02 0 2 1 0 1 2 0 16  0 02 0 1 6 0 13  0 0 6         11 D is pe ns ab le in co m e   


  21 0 9 2 72 7  0 14  0 0 1 0 09  0 08  0 2 0 0 00 0 0 4 0 18  0 1 6 0 0 1        In te rn et u sa ge                     


  12 A ct iv e in te rn et u sa ge 1  0 0 2 0 9 6 0 2 1 0 25  0 11  0 12  0 10  0 0 4 0 05  0 0 8 0 0 5 0 0 1 0 12        13 H ou rs o nl in e h ou rs 


rs   2 6 5 3 0 3  0 04 0 12  0 1 1 0 0 3 0 40  0 0 7 0 0 7 0 4 7 0 5 3 0 07  0 1 1 0 07       14 W illi ng ne ss to p ay 1  1 8 3 0 6 3  0 03 0 10 


10  0 07  0 08  0 0 2 0 0 4 0 0 1 0 01  0 00 0 0 5 0 14  0 04 0 05      G am e sp ec ifi c va ria bl es                      15 T en 


ur e w ee ks   2 8 2 3 5 2 0 2 6 0 31  0 0 9 0 01 0 12  0 0 4 0 02 0 0 9 0 0 9 0 07  0 02 0 13  0 08  0 0 4    16 C ro ss o ve r on o ffl in e 4  0 1 5 


5 1 1 1 0 1 9 0 11  0 13  0 18  0 2 0 0 1 4 0 0 7 0 14  0 1 1 0 0 4 0 08  0 15  0 0 5 0 01 0 07    17 S at is fa ct io n1   18 7 5 1 3 16  0 18  0 00 


00 0 44  0 52  0 1 4 0 0 3 0 02 0 07  0 0 9 0 1 4 0 10  0 08  0 0 6 0 09  0 0 1 0 13   18 C om m itm en t1  0 6 2 0 8 3 0 3 1 0 13  0 37  0 39  0 0 7 


7 0 0 6 0 02 0 03  0 0 4 0 1 3 0 14  0 17  0 0 5 0 09  0 07  0 19  0 58  S ou rc e O w n ca lc ul at io n N ot e N  1 3 89 o bs er va tio ns S ig ni fic an ce le ve ls 


ls  p  0 05 S D  S ta nd ar d de vi at io n 1 5 po in t L ik er t s ca le ra ng in g fro m 2 to 2  2 du m m y va ria bl e 3 o rd in al v ar ia bl e ra ng in g fro m v oc at io na l e du ca 


tio n to P h D 4 n um be r o f c on ta ct s   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 


