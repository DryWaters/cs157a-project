MANET Mining Mining Temporal Association Rules Ahmad Jabas Computer Science and Engineering Department Osmania University Hyderabad 500 007 India aojabas@hotmail.com Dr Rama M Garimella Communication Research Center International Institute of Information Technology Hyderabad 500 032,India rammurthy@iiit.ac.in Prof S Ramachandram Computer Science and Engineering Department Osmania University Hyderabad 500 007 India schandram@gmail.com Abstract A wireless ad hoc network MANET is a collection of autonomous nodes or terminals that communicate with each other by forming a multihop radio network and maintaining connectivity in a decentralized manner MANET is characterized by a rapidly changed topology As a result packets select different multi-hops paths to reach their destinations Even though packets are somehow independent from each other there are still some hidden relations patterns among them These relations may be utilized to give useful information to different MANET protocols In this work association rules technique is applied in time domain to obtain these hidden relations keywords Mobile Ad Hoc Networks\(MANET Data Mining MANET Mining Priori Algorithm 1 Introduction MANET needs efﬁcient distributed algorithms to determine network organization However most of the algorithms  including broadcasting multicasting unicasting electing a leader determining viable routing paths delivering messages and keys  are not well-deﬁned problems Accordingly MANET algorithms are still immature and there is no doubt that the unpredictable topology mobility is the main reason of this immaturity One way to enhance MANET performance is to disclose the hidden characteristics patterns of MANET and to utilize them in the distributed algorithms This can be achieved by analyzing the MANET trafﬁc in a tempo-spatial domain In this work a new framework is proposed Applying the association rules to the packets trafﬁc in MANET with respect to time This technique pave the way for the other MANET algorithms Fosca and Dino g a v e general idea about the geographic knowledge discovery The rest of the paper is organized as following The second section describes the common aspects between MANET mining and data mining The third section explains the proposed framework The fourth section shows the simulation and the results The fth section provides a conclusion for the paper and the future work 2 MANET Mining and Data Mining A Vision of Convergence Data mining among transactions is similar to a large extent to MANET mining among packets for the following reasons 1 Each transaction in data mining is a set of items attributes Each packet in MANET mining passes 
2008 International Symposium on Parallel and Distributed Processing with Applications 978-0-7695-3471-8/08 $25.00 © 2008 IEEE DOI 10.1109/ISPA.2008.66 765 
2008 International Symposium on Parallel and Distributed Processing with Applications 978-0-7695-3471-8/08 $25.00 © 2008 IEEE DOI 10.1109/ISPA.2008.66 765 
2008 International Symposium on Parallel and Distributed Processing with Applications 978-0-7695-3471-8/08 $25.00 © 2008 IEEE DOI 10.1109/ISPA.2008.66 765 
2008 International Symposium on Parallel and Distributed Processing with Applications 978-0-7695-3471-8/08 $25.00 © 2008 IEEE DOI 10.1109/ISPA.2008.66 765 


N 1 N 2 N 3 N 4 N 5 N 6 N 7 N 8  1 1 1 0 1 1 0 0  1 1 1 0 1 1 0 0  1 1 1 0 1 1 0 0  1 1 1 0 1 1 0 0  1 1 1 1 0 1 0 1  1 1 1 1 0 1 0 1  1 1 1 1 0 1 0 1  1 1 1 1 0 1 0 1  1 1 1 1 0 1 0 1  1 1 1 1 0 1 0 1  Table1.TrafﬁcTable through a set of hops nodes 2 Data mining is applicable to a database with a large number of transactions MANET mining is applicable to a trafﬁc with a large number of packets 3 The purpose of mining association rules in a database is to discover all rules that have support and conﬁdence predictability greater than or equal to the userspeciﬁed minimum support and minimum conﬁdence The problem of mining association rules in MANET is to discover the most likely patterns such as routes among packets and the cooperated nodes 4 Each frequent set in data mining maps different packets with common hops along the path in MANET mining Nowadays databases became a mature technology and massive collection and storage of data became feasible at increasingly cheaper costs Accordingly the powerful methods for discovering knowledge from data go beyond the limitations of traditional statistics machine learning and database querying MANET mining is described next section 3 MANET Mining 3.1 Introduction Beside Bramer’s notation  n e w structures and operations are deﬁned in this section Nodes in MANET mining need not read from a database Each packet keeps track of the path it is following Whenever the packet goes through some node it adds that node’s identity to the list of nodes traveled As soon as the destination receives the packet it gets that list The destination node add this list entry or transaction to a table This table is called in this work the trafﬁc table The Trafﬁc table contains all packets lists received by a node Thus each node gets a sample of the trafﬁc  N 9 N 10 N 11 N 12 N 13 N 14 N 15  1 0 0 0 1 1 0  1 0 0 0 1 1 0  1 0 0 0 1 1 0  1 0 0 0 1 1 0  1 0 0 0 1 0 0  1 0 0 0 1 0 0  1 0 0 0 1 0 0  0 0 0 0 1 1 1  0 0 0 0 1 1 1  0 0 0 0 1 1 1 Table 2 Trafﬁc Table Contd Figure 1 MANET at time t0 in MANET to construct its own trafﬁc table without any request from any other node In other words nodes do not add new trafﬁc to MANET they just capture passively the passing packets and check their lists Now nodes exploit trafﬁc tables by applying data mining techniques to get useful information The trafﬁc table can be condensed by coding the presence of each node with one bit 3.2 Demonstration on a Simple MANET This section explains how association rules mining is applied to MANET Association rules seek to identify what things go together Consider the s cenario in Fig 1 There are 15 nodes in this Ad hoc network The node N 1 is the source and the node N 2 is the destination For simplicity a small number of packets is considered The source sends 4 packets through the path N 1 N 3 N 5 N 6 N 9 N 14 N 13 N 2   Then the topology changes Nodes N 5 and N 14 leave Node N 4 and N 8 enter and the source sends three packets through the path N 1 N 3 N 4 N 6 N 9 N 8 N 13 N 2   Again the 
766 
766 
766 
766 


topology changes Node N 9 leaves Nodes N 14 and N 15 join and the next three packets will follow the path N 1 N 3 N 4 N 6 N 15 N 14 N 8 N 13 N 2  Nowthe trafﬁc table in both the source and destination will be as in tables 1 and 2 Mining association rules is applied to this trafﬁc table Assume that the support s  70  Then L 7  MFS   N 1 N 2 N 3 N 6 N 9 N 13 N 14  The set of all frequent sets is L  L 1 002 L 2 002 L 3 002 L 4 002 L 5 002 L 6 002 L 7 The nodes in each frequent set are correlated with support=70 Note that any subset of the maximum frequent set is a frequent set according to the downward closure property From now onwards it is enough to calculate the MFS 4 Simulation and Results New metric is deﬁned in this section It is deﬁned as follows Correlation 025 ratio  s   of N odes in M F S  s   of nodes involved in routing The higher ratio indicates better correlation among nodes Before simulation mining and analysis of the trafﬁc the metric is applied to the demonstration example for illustration From the previous section the MFS of the trafﬁc table in tables 1 and 2 with support s  70 is  N 1 N 2 N 3 N 6 N 9 N 13 N 14   The number of nodes in MFS is 7 and the number of nodes involved in routing in this trafﬁc table is 11 i.e columns that contain at least one 1 value Correlation 025 ratio  with s  70  7 11 In this example there are few nodes with few packets That means the route from the source to the destination does change few nodes only In real scenario more number of nodes may communicate for longer time this results in huge number of packets trafﬁc with low Correlation 025 ratio This is because the routes are completely changed after some time In other words the number of common nodes decreases with time passing Accordingly the Correlation 025 ratio decreases One way to improve the Correlation 025 ratio is to mine the MANET trafﬁc for a short time interval 002 in seconds order For closer view the trafﬁc table is roughly divided in the demonstrative example into two equal tables trafﬁcs i.e two periods of time The rst ve tuples in the table occurred in the rst interval and the rest occurred in the second interval By mining both of these trafﬁc tables with the same support the MFSs results are MFS 1   N 1 N 2 N 3 N 5 N 6 N 9 N 13 N 14  Parameter value Number of the nodes 100 Routing protocol AODV DSDV DSR Mobility model Random way point Pause time 1 second Radio transmission range 250 meters Channel capacity 1 mbps Data ow constant bit rate Data packet size 512 bytes Node placement random Terrain area 1500m*1500m Simulation time 120 second Propagation model Two Ray Ground Speed 5 10 50 m/sec Table 3 The Simulation Parameters MFS 2   N 1 N 2 N 3 N 4 N 6 N 8 N 13  Correlation 025 ratio 1 002  70  8 10 Correlation 025 ratio 2 002  70  7 10 Both of these ratios are higher than the ratio taken for one trafﬁc table For the accuracy purpose in the time is divide into intervals slices Each interval 002=2 and 3 sec The Correlation 025 ratio is calculated for each interval Then the mean of these Correlation 025 ratios is calculated The MANET trafﬁc generated by NS2  2  7 and the u sed parameters in the simulator are shown in the Table 3 In this work several parameters with their effects on MANETs mining are studied Figures 3 4 and 5 show the effect of the time interval 002 on the Correlation 025 ratio with s  70  80 and 90 respectively for the routing protocol AODV  Figures 6 and 7 show the effect of the support s on the ratio with 002=02 and 03 sec respectively for the routing protocol AODV  Figures 8 9 and 10 show the effect of the time interval 002 on the Correlation 025 ratio with s  70  80 and 90 respectively for the routing protocol DSDV  Figures 11 and 12 show the effect of the support s on the ratio with 002=02 and 03 sec respectively for the routing protocol DSDV  Figures 13 14 and 15 show the effect of the time interval 002 on the Correlation 025 ratio with s  70  80 and 90 respectively for the routing protocol DSR  Figures 16 and 17 show the effect of the support s on the ratio with 002 02 and 03 sec respectively for the routing protocol DSR  The gures bring out high Correlation-ratios for the protocols especially for the protocol DSDV which has higher Correlation 025 ratio  
767 
767 
767 
767 


Figure 2 MANET stack with mining 5 Conclusions and Future Work Despite the high mobility of the nodes in up to 50 m/sec the proposed framework is still able to discover the hidden relations among nodes and proves that the trafﬁc of MANET is a raw material for mining This work is a basis for different algorithms in MANET such as routing protocols security power conservation etc Figure 2 give the position of MANET mining in the Ad Hoc stack The layers above the MAC layer may get beneﬁt  either directly or by the cross layer from the proactive MANET mining results at any time with very reasonable storage-cost bit-wise and with free of trafﬁc-cost Also the off-theshelf data mining techniques come into the picture to save the computation-cost References  M  B ramer  Principles of Data Mining  Springer Verlag Berlin Heidelberg 2007  K  F all The ns Manual  The VINT Project University of California Feb 11,2007  F  G iannotti and D  P edreschi Mobility Data Mining and Privacy  Springer Verlag Berlin Heidelberg 2008  M  G reis Tutorial for the Network Simulator NS2   D  L Olson and D  Delen Advance Data Mining Techniques  Springer Verlag Berlin Heidelberg 2008  A  K  P ujari Data Mining Techniques  Universities Press India 2001  M  C  S tuart K urk o wski T rac y Camp Manet s imulation s tudies The current state and new simulation tools Mobile Computing and Communications Review  9\(4 2005 5 10 15 20 25 30 35 40 45 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1   Speed Ratio Protocol:AODV, s:70 002 02sec Protocol:AODV, s:70 002 03sec  Protocol:AODV, s:70 002 04sec  Figure 3 The effect of the time interval 002 with s  70 on the ratio for the routing protocol AODV  5 10 15 20 25 30 35 40 45 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1   Speed Ratio Protocol:AODV, s:80 002 02sec Protocol:AODV, s:80 002 03sec  Protocol:AODV, s:80 002 04sec  Figure 4 The effect of the time interval 002 with s  80 on the ratio for the routing protocol AODV  5 10 15 20 25 30 35 40 45 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1   Speed Ratio Protocol:AODV, s:90 002 02sec Protocol:AODV, s:90 002 03sec  Protocol:AODV, s:90 002 04sec  Figure 5 The effect of the time interval 002 with s  90 on the ratio for the routing protocol AODV  
768 
768 
768 
768 


5 10 15 20 25 30 35 40 45 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1   Speed Ratio Protocol:AODV, s:70 002 02sec Protocol:AODV, s:80 002 02sec  Protocol:AODV, s:90 002 02sec  Figure 6 The effect of the support s with time interval 002=02 sec on the ratio for the routing protocol AODV  5 10 15 20 25 30 35 40 45 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1   Speed Ratio Protocol:AODV, s:70 002 03sec Protocol:AODV, s:80 002 03sec  Protocol:AODV, s:90 002 03sec  Figure 7 The effect of the support s with time interval 002=03 sec on the ratio for the routing protocol AODV  5 10 15 20 25 30 35 40 45 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1   Speed Ratio Protocol:DSDV, s:70 002 02sec Protocol:DSDV, s:70 002 03sec  Protocol:DSDV, s:70 002 04sec  Figure 8 The effect of the time interval 002 with s  70 on the ratio for the routing protocol DSDV  5 10 15 20 25 30 35 40 45 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1   Speed Ratio Protocol:DSDV, s:80 002 02sec Protocol:DSDV, s:80 002 03sec  Protocol:DSDV, s:80 002 04sec  Figure 9 The effect of the time interval 002 with s  80 on the ratio for the routing protocol DSDV  5 10 15 20 25 30 35 40 45 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1   Speed Ratio Protocol:DSDV, s:90 002 02sec Protocol:DSDV, s:90 002 03sec  Protocol:DSDV, s:90 002 04sec  Figure 10 The effect of the time interval 002 with s  90 on the ratio for the routing protocol DSDV  5 10 15 20 25 30 35 40 45 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1   Speed Ratio Protocol:DSDV, s:70 002 02sec Protocol:DSDV, s:80 002 02sec  Protocol:DSDV, s:90 002 02sec  Figure 11 The effect of the support s with time interval 002=02 sec on the ratio for the routing protocol DSDV  
769 
769 
769 
769 


5 10 15 20 25 30 35 40 45 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1   Speed Ratio Protocol:DSDV, s:70 002 03sec Protocol:DSDV, s:80 002 03sec  Protocol:DSDV, s:90 002 03sec  Figure 12 The effect of the support s with time interval 002=03 sec on the ratio for the routing protocol DSDV  5 10 15 20 25 30 35 40 45 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1   Speed Ratio Protocol:DSR, s:70 002 02 Protocol:DSR, s:70 002 03  Protocol:DSR, s:70 002 04  Figure 13 The effect of the time interval 002 with s  70 on the ratio for the routing protocol DSR  5 10 15 20 25 30 35 40 45 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1   Speed Ratio Protocol:DSR, s:80 002 02 Protocol:DSR, s:80 002 03  Protocol:DSR, s:80 002 04  Figure 14 The effect of the time interval 002 with s  80 on the ratio for the routing protocol DSR  5 10 15 20 25 30 35 40 45 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1   Speed Ratio Protocol:DSR, s:90 002 02 Protocol:DSR, s:90 002 03  Protocol:DSR, s:90 002 04  Figure 15 The effect of the time interval 002 with s  90 on the ratio for the routing protocol DSR  5 10 15 20 25 30 35 40 45 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1   Speed Ratio Protocol:DSR, s:70 002 02sec Protocol:DSR, s:80 002 02sec  Protocol:DSR, s:90 002 02sec  Figure 16 The effect of the support s with time interval 002=02 sec on the ratio for the routing protocol DSR  5 10 15 20 25 30 35 40 45 50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1   Speed Ratio Protocol:DSR, s:70 002 03sec Protocol:DSR, s:80 002 03sec  Protocol:DSR, s:90 002 03sec  Figure 17 The effect of the support s with time interval 002=03 sec on the ratio for the routing protocol DSR  
770 
770 
770 
770 


                                                                                                                 
456 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efﬁcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efﬁciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efﬁciency When the clustering model is available it is a signiﬁcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques Sufﬁcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efﬁcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efﬁcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conﬁdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conﬁdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conﬁdence The sufﬁcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efﬁcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efﬁciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufﬁciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70–81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207–216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487–499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145–154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146–153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9–15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597–600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern Classiﬁcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155–162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512–521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1–12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265–278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283–304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476–482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559–563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220–231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13–24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10–17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549–550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188–201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259–283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909–921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305–345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432–444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conﬁdence Intelligent Data Analysis  9\(4\:381–395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49–73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407–419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1–12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki Efﬁciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642–658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483–490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194–203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344–353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efﬁcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103–114 1996 
618 
618 


