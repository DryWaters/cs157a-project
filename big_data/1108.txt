Grid-based Data Mining in Real-life Business Scenario Tianchao Li Institut f\374r Informatik, Technische Universit\344t M\374nchen, German\y tianchao.li@in.tum.de Toni Bollinger, Nikolaus Breuer, Hans-Dieter Wehle IBM Development Laboratory B\366blingen, Germany toni.bollinger,hdwehle,nbreuer}@de.ibm.com Abstract This paper presents a Grid-based distributed and parallel data mining system targeting a real-life application scenario typical in the business realm \205 franchise supermarket basket analysis. Following a layered design of three tiers, this system enables parallel association rule mining on a farm of Grid servers, offers a standard service interface for custom applications, and provides a friendly user portal. The work presented in this paper reveals specific requirements for applying Grid-based data mining in the business realm, which is helpful for the design and implementation of a generic Grid-based data mining system 1. Introduction Data mining, which targets the goal of retrieving information automatically from large data sets, is one of the most important business intelligence technologies Because of its high computational intensiveness and data intensiveness, data mining serves a good field of application for Grid technology The idea of data mining on the Grid is not new, but it has become a hot research topic only recently. The number of research efforts up to now is still quite limited \(for a short summary see [1 systems, such as NASA\220s Information Power Grid [2 TeraGrid [3] and Discovery Net [4] are either utilizing non-standard data mining techniques, or restricted to a special domain in the scientific realm The implementation of a Grid-based data mining system targeting real-life application scenarios in the business realm will reveal the importance of specific requirements that are not so evident in the scientific realm, and will contribute to a generic design and implementation The rest of the paper is organized as follows. Section 2 describes a typical application scenario in the real world business realm, the analysis of which reveals some specific requirements for implementing such a Grid-based data mining system. Section 3 describes the system infrastructure, which follows a layered design of three tiers \205 Grid tier, service tier, and client/portal tier Focusing on a general description of the parallel mining task submission workflow, Section 4 summarizes the design and implementation. Section 5 discusses future developments and the paper concludes with section 6 2. A Real-life Business Application Scenario 2.1. Franchise Supermarket Basket Analysis A typical application scenario of Grid-based data mining in real-life enterprises is presented in Figure 1 where there is a franchise supermarket formed by headquarter, regional branches and distributed member stores Figure 1 Franchise-supermarket scenario Each store collects transaction data by scanning bar codes at the till when customers buy products. Most stores rely on regional branch to store their transaction data, while some stores have local databases. For the Boston Store  Paris Store New York Store Miami Store 
Germany Branch Data Center 
Berlin Store 
Headquarter 
Chicago Store A merica Branch Data Center Stuttgart Store Munich Store 
Proceedings of the IEEE/WIC/ACM Internati onal Conference on Web Intelligence \(WI\22204 0-7695-2100-2/04 $ 20.00 IEEE 


sake of safety or convenience, the transaction data for certain stores are optionally replicated in several different data storages. Each regional branch or member stores have their own choice of data mining software and deploys the software locally. The headquarter employ data mining to analyze the transactional data of selected member stores The franchise supermarket basket analysis scenario described above, though specific, is typical for Grid based data mining in the business realm. Another example situation that follow similar scenario is the cooperation among several financial organizations. They need to share data patterns relevant to the data mining task, but they do not want to share the data since they are sensitive 2.2. Rational Grid-based Approach The application scenario described above reveals some important properties which advocate the adoption of a Grid-based data mining approach First, security is crucial for distributed data mining in enterprises. The business data are so sensitive that they should in no case be accessed without proper privileges Second, Grid-based data mining system for enterprises should be able to cope with the heterogeneity of not only computers, operating systems, networks, but also that from the different sources of data and different data mining software Third, in the highly competitive business environment, enterprises tend to be more flexible and dynamic. As a result, data mining systems in enterprises face stronger demand on the capability of processing data that are dynamically organized. In the franchise supermarket scenario, it often happens that new stores are opened for business expansion and non-profitable stores are closed With all above properties, a Grid-based approach becomes the natural choice, although there could be other similar techniques that are capable of achieving the same goal 2.3. Scenario Specific Requirements The application scenario described above also reveals specific requirements that are crucial for the design and implementation of the Grid-based data mining system It is common practice for Grid applications to decide on program shipping versus data shipping, i.e. to move program to the place of data or to move data to the place of program. However, neither of these two approaches is suitable for application scenarios specified above The inadequacy of data shipping is mainly attributed to security and efficiency considerations. With data shipping, when the business data is transmitted through the network and copied to other sites, access to the business data only with proper privileges can hardly be guaranteed. Data mining is highly data intensive, thus the network can often be a bottleneck for a distributed data mining system, which results in efficiency degradation Enterprises tend to use standard data mining techniques from commercial data mining software rather than domain specific knowledge extraction techniques from self-implemented software. The complexity and differences in installation and configuration procedures of those software also inadequate program shipping especially in cases that data mining programs are tightly coupled with the database Instead of data shipping and program shipping, it should be more appropriate to ship the data pattern of each data partition rather than the data, and to ship script for invoking the program rather than the whole executable 3. System Infrastructure The system follows a layered design \(Figure 2 where the infrastructure is divided into three tiers in general \205 Grid tier, service tier, and client/portal tier [5 Figure 2 System infrastructure 3.1. Grid Tier The underlying Grid tier is composed of a set of heterogeneous compute nodes. The business data is partitioned and deployed on each server as databases Software enabling data mining on local partial of data are installed and configured on each Grid node. Grid middleware enables a secure, coordinated and dynamic  data mining script-template repository Grid Middleware Service Tier Service Interfaces Grid Tier Grid-enabled data mining manager Result Visualization Module New Task Module Common Parts Server Browser Module Task Browser Module Client/Portal Tier Proceedings of the IEEE/WIC/ACM Internati onal Conference on Web Intelligence \(WI\22204 0-7695-2100-2/04 $ 20.00 IEEE 


resource sharing among Grid nodes. Custom information providers are implemented and deployed to provide additional information about configurations and data content 3.2. Service Tier The service tier implements a Grid-enabled data mining system and provides standard service interfaces The Grid-enabled data mining system consists of a data mining script template repository and a Grid-enabled data mining manager. The data mining script template repository stores the scripts and script templates for all kinds of data mining software deployed on the servers The script templates are transformed into actual mining scripts in an adaptive way. The Grid-enabled data mining manager controls the process of distributed data mining on the Grid servers until finally the result of data mining is retrieved 3.3. Client/Portal Tier The Grid-enabled data mining services can be accessed with custom applications that are either programmed or generated with tools according to the Web service interface definitions. For casual users that do not intend to have custom implementation of service clients, a portal is implemented to provide easy access The portal provides a user interface for the Grid system with which the users submit new data mining tasks browse the statuses of tasks, and explore the visualization of data mining results. The portal is implemented as a J2EE compliant application, so that it is accessible by the end users simply through a Web browser 4. Parallel Mining on Grid Servers The market basket analysis is based on association rule mining, one of the major standard data mining techniques. Based on algorithms like Apriori algorithm 6], this mining technique is applicable for an \215embarrassingly parallel\216 paradigm, where a task can be divided into small independent jobs. These independent jobs can then be distributed on the farm of Grid servers as is assigned by a scheduler A little bit complexity comes from the fact that the result returned by data mining software usually only contains association rules that satisfy the requirements specified in mining parameters \205 minimal support minimal confidence, maximal rule length etc. This might lead to an inaccuracy in the result of merging: It may happen that even if an association rule satisfies the conditions for minimal support and confidence on the whole dataset, these conditions are not fulfilled on one specific server. In this case, this rule would not be in the result set of this server such that we will not be able to get the corresponding confidence and support values for this rule on this server. As a result, the confidence and support for this association rule after merging will not be exact, even though it can be quite close to the exact value Corrections are necessary to be made in order to get accurate results while stick to the same parallelization paradigm. This is made though additionally executing small pieces of mining jobs that will do some makeup works to complement the missing information. Trading accuracy with performance, this \215accurate\216 mode emphasis more on availability and accuracy, rather than efficiency as it is in the case of a \215rough\216 mode Figure 3 Parallel mining on Grid servers Figure 3 shows the general process of submitting association rule mining task on the Grid. The mining manager starts a mining task with scheduling, when the scheduler matches the mining task to available servers in an optimal way. The scripts for the mining task are prepared separately for each destination server, adapted to the specific data mining software on that server These scripts inherit data mining parameters from the mining request, especially the requirement of minimal support, minimal confidence, and maximal rule length Grid jobs are then submitted to the Grid servers for performing data mining on local partition of data. The contents of Grid job results, i.e. the result of data mining on Grid servers, are read into memory and are merged together. Unlike \215rough\216 mode, which accepts the inexact result of simple merging, the \215exact\216 mode tries to Rough mode  Schedule Prepare Scripts Submit Jobs Read Results Exact mode Merge Results Detect Incomplete Rules Prepare Makeup Scripts Submit Makeup Jobs Read Makeup-Job Results Merge Makeup-Job Results Generate PMML Proceedings of the IEEE/WIC/ACM Internati onal Conference on Web Intelligence \(WI\22204 0-7695-2100-2/04 $ 20.00 IEEE 


make corrections. Rules that possibly suffer from the absence of information are detected. Scripts for makeup mining jobs, each with very small sets of items, are prepared while removing the requirements for minimal support and confidence. The makeup mining jobs are submitted, and the results are collected. A makeup merge is performed to update the association rule parameters from the initial merge For detailed discussion of some specific issues of design and implementation, especially those for the custom information providers and brokers, task scheduling, adaptive script preparation, and XML-based message formats based on PMML [7] etc, please reference the paper by Li and Bollinger [8 5. Further Developments The application scenario discussed in this paper utilizes the association rule mining. Algorithms for this mining technique are applicable for a coarse-grained parallelism, where a job is divided into small independent sub-jobs. For other data mining techniques such as clustering, classification, and regression, studies should be carried out to find appropriate algorithms for a similar parallelism While the current implementation is based on Globus Toolkit 2 \(GT2  Globus Toolkit 3 \(GT3 out. From implementation perspective, the resource management services and the information services offered by GT2 and GT3 keeps much consistency, thus the migration of the Grid client and Grid servers are relatively easy. For the service interface, migrating from stateless and persistent Web service to stateful and transient Grid service demands however a moderate work. Despite the effort in the migration, offering a Grid service instead of Web service can greatly simplify the implementation of the Grid portal and other custom client applications that consume the Grid-based data mining service. With Web service, the Grid portal has to maintain a long running process \(running as a message-driven EJB keep track of it. With Grid service, this long running process can be broken into several very short processes each responsible separately for invoke the service, to get the status or result of the service invocation 6. Conclusions Real-world application scenarios in business realm have specific requirements that are not evident in scientific realm. The design of a generic Grid-based data mining system should take full consideration of application scenarios in both realms A real-life application scenario typical in the business realm, the franchise supermarket basket analysis has been discussed. A Grid-based data mining system has been implemented targeting this scenario, the infrastructure of which follows a layered design. With underlying custom information providers and brokers optimistic task scheduling, adaptive script preparation and scenario specific parallelization with \215embarrassingly parallel\216 paradigm, the system offers service interface for a distributed and parallel Grid-based data mining system References 1]Cannataro, M.; Talia, D.: The Knowledge Grid Communications of the ACM Vol. 46, No. 1, pp89-93, Jan. 2003 2] Hinke, T.; Novotny, J.: Data Mining on NASA\220s Information Power Grid Proc. 9th IEEE International Symposium on High Performance Distributed Computing Pittsburgh Pennsylvania, Aug. 1-4, 2000 3]Conover, H. et.al.: Data Mining on the TeraGrid, Poster Presentation Supercomputing Conference 2003 Phoenix, AZ Nov. 15, 2003 4 000\373 ur 000\376 in, V. et.al.: Discovery Net: Towards a Grid of Knowledge Discovery Proc. 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining Edmonton, Alberta, Canada, pp658-663, 2002 5] Li, T.: A Grid-based  Business Intelligence Infrastructure \205 Concept, Design and Prototype Implementation of Business Intelligence Grid Master Thesis, Technische Universit\344t M\374nchen 2003 6] Agrawal, R.; Srikant, A.: Fast algorithms for mining association rules Proc. 20th International Conference on Very Large Data Bases pp487-499, 1994 7] Data Mining Group, PMML 2.0 Specification http://www.dmg.org/v2-0/GeneralStructure.html 8] Li, T.; Bollinger, T.:  Distributed and Parallel Data Mining on the Grid Proc. 7th Workshop Parallel Systems and Algorithms Augsburg, Germany, Mar. 26, 2003 9] Globus Toolkit, http://www.globus.org Proceedings of the IEEE/WIC/ACM Internati onal Conference on Web Intelligence \(WI\22204 0-7695-2100-2/04 $ 20.00 IEEE 


a method to figure them out because they partly depend 0.836 844 can be divided into two sorts by support and there is can obtain higher efficiency when the dimension of data is high Meanwhile for different data can implement Reference 1 R Agrawal T Imielinski and A Swami Mining Association Rules Between Sets of Items in Large Databases Proceedings of the ACM SIGMOD Conference on Management of data 207-216 May 1993 2 R Agrawal and R Srikant Fast Algorithms for Mining Association Rules In Proc VLDB 1994 pp 487-499 3 C Liu H Lu J X Yu W Wang X Xiao AFOPT An Efficient Implementation of Pattern Growth Approach In SDM 2003 4 Hao Huang Xindong Wu and Richard Relue Association Analysis with One Scan of Databases University of Vermont Computer Science Technical Report CS-02-3 2002 5 Taorong Qiu Xiaoqing Chen Qing Liu Houkuan Huang An Algorithm of Association Rules Extracting Based on Granular Computing and Its Application May 2005 6 J.S Park M.S Chen and P.S Yu An Effective Hashbased Algorithm for Mining Association Rules SIGMOD'95 San Jose CA May 1995 7 A Savasere E Omiecinski and S Navathe An Efficient Algorithm for Mining Association Rules in Large Databases VLDB'95 432-443 Zurich Switzerland 8 H Toivonen Sampling Large Databases for Association Rules VLDB'96 134-145 Bombay India Sept 1996 9 S Brin R Motwani J D Ullman and S Tsur Dynamic Itemset Counting and Implication Rules for Market Basket Analysis SIGMOD'97 Tucson Arizona May 1997 10 Pei-qi Lin Zeng-zhi Li Yin-lung Zhao Effective Algorithm of Mining Frequent Itemsets for Association Rules Proceedmgs of the Third International Conference on Machine Leaming and Cybemetics Shanghai 26-29 August 2004 6 Conclusion In this paper the HDO Apriori algorithm is 1-4244-0605-6/06/$20.00 C IEEE Time s 700 2 626 600 Figure 1 The comparison of runtime at different minimum supports between the original Apriori and the improved Apriori From Figure 1 we can know that with the reduction of the minimum support the efficiency of the HDO Apriori algorithm _ I 26  300 4 3 31251 original Apriori 200 228 100 2.48 PlgnisPot*2 1 75 2 nrmusupport=0.252 3 splog minsupport*A*2 rnimusupp or 0.084 nursupp or on efficiency 5.3 Discussions For different datasets the proper values of minimum support to show the advantages of the HDO Apriori algorithm on the number of frequent itemsets Hence for the ratio R has increased just 5 500 2 561 2.48 441 I 5  improvzed Apriori 400 we have not found can change correspondingly from can also be obtained are different but on the internal characteristic of data For example assume some types of data can influence just a little a little the efficiency changes hardly On the contrary if the supports of data distribute averagely the influence of the minimum support can be evident Hence the HDO algorithm will be surely applicable to the latter types of data In contrast when the dimension of data is high the advantage of the HDO Apriori algorithm is evident no matter what the internal characteristic of data is That's why our algorithm is high-dimension oriented But if we want to precisely predict how much the efficiency can be improved at given data for the influence of the differences in data further research is also required proposed to update the classical Apriori algorithm Through pruning candidate itemsets by the infrequent itemsets with lower dimension the present algorithm can reduce the redundancy while generating subitemsets and verifying them in the database Validated by the experiments it we still need further research to find methods to estimate how much improvement the HDO Apriori algorithm a lower level to a higher level than that of the classic Apriori algorithm Verified by the experiments when the dimension of data is high the efficiency of the HDO algorithm is greatly improved And also if the minimum support is small enough the comparative improvement a large gap in the values of the supports between the two sorts of data So a small change of the minimum support within the gap 


5 uniform membership functions and its definitive interval is bounded within [0.05,0.15 With linguistic minimum support, the process of finding the set of large itemsets proceeds as illustrated next. Assume the linguistic minimum support value is given as  Low  First, this value is transformed into a fuzzy set of minimum support, namely \(0.05, 0.075, 0. l as shown in Figure 4. Second, the fuzzy weighted set of the given minimum support is calculated. Finally, the weighted support of each item or itemset is compared to the fuzzy weighted minimum support by fuzzy ranking. If the weighted support is equal to or greater than the weighted minimum support, then the corresponding itemset is considered large Fig. 3 Membership functions and base variablesof attribute ir poni  SUP Fig. 4 Membership functions of the minimum support for the fitness function 4.1. CHROMOSOME ENCODING Our target in using GAS is to cluster the values of quantitative attributes into fuzzy sets with respect to a given fitness evaluation criteria. For this purpose, each individual represents base values of membership functions of a quantitative attribute in the database. In our experiments, we used membership functions in triangular shape because it is in general the most appropriate shape To illustrate this, consider a quantitative attribute ik and assume it has 3 corresponding fuzzy sets Membership functions for, attribute ik and their base variables are shown in Figure 3. Each base variable takes finite values. For instance, the search space of base value bi lies between the minimum and maximum values of attribute ik, denoted mW4 The search intervals of all the base values and intersection point Rib of attribute ik are bf. : [min\(D D Rj, : [min\(Di b: : [min\(D b: : [ R , ,  m a W bl :[min\(D D So, based on the assumption of having 3 fuzzy sets per attribute, as it is the case with attribute ik, a chromosome consisting of the base lengths and the intersection points is represented as: b ~ b ~ R i , b i : b ~ b ~ ~ b , : R i ~ b ~ b ~  . . b ~ b ~ R i - b ~ b We use real-valued coding, where chromosomes are represented as floating point numbers and their genes are the real parameters  Ihese chromosomes form the input to the fitness function described in the next section 4.2. FITNESS EVALUATION The fitness function measures the goodness of an individual in a given population. It is one of the key issues to a successful GA; simply because the main task in a GA is to optimize a fitness function. Consequently the fitness function should be carefully set, by 4.3. SELEC~ION PROCESS During each generation, individuals with higher fitness values survive while those with lower fitness values are destroyed. In other words, individuals who are strong according to parent selection policy are candidates to form a new population. Parent selection mimics the survival of the best individuals in the given population Many selection procedures are currently in use However, Holland  s original fitness-proportionate selection is one of the simplest selection procedures [ l  I So, we used this selection policy in our experiments Let firness\(x,t r the fitness of individual x and the average fitness of the population during evolution phase t .  Then, the usage value of individual x as a parent is: rsr \(x , t xJ 1 After selecting chromosomes with respect to the evaluation function, genetic operators such as, crossover and mutation, are applied to these individuals 


and mutation, are applied to these individuals Crossover refers to information exchange between individuals in a population in order to produce new individuals. The idea behind the crossover operation utilized in our study is as follows. It takes as input 2 individuals, selects random points, and exchanges the subindividuals behind the selected points. Since the length of the chromosomes is long, the multi-point crossover strategy has been used with the crossover points determined randomly On the other hand, mutation means a random change in the information of an-individual. It is very important 113 for populations. It is an operation that defines a local or global variation in an individual. Mutation is traditionally performed in order to increase the diversity of the genetic information. Otherwise, after several generations, the diversity of the chromosomes decreases and some chunks of the chromosomes may end up being the same for all population members and the information they contain may not evolve further. A probability test determines whether a mutation will be carried out or not. ?he probability of mutation depends on the following condition average fitness of new generation &lt;average fitness of old generation Since the initial population can be a subset of all possible solutions, an important bit of each chromosome may be inverted, i.e., 0 appears as 1 or vice versa Crossover may not solve this and mutation is inevitable for the solution Finally, after generating each individual in the initial population, the executed GA includes the following steps Algorithm 4.1 \(Generating Association Rules 1 2 3 4 5 6 7 8 9 Using the given membership functions about item importance, transform each linguistic term, which reflects the importance of item i,, I l k  c m ,  into a fuzzy set wt of weights Specify population size N and generate initial chromosomes According to the current chromosome, transform the quantitative value f ,A of each item it in each transaction f j ,  1 5 j 5 n , into a fuzzy set f Calculate the fuzzy weighted support of each item fizzy set pair \( j k 9 f Compute the weighted fuzzy set of the given minimum support value as Find the large itemsets based on the weighted fuzzy set of the given minimum support value Evaluate each chromosome with respect to the already specifie.6 fitness function Perform selection, crossover and mutation If not end-test then go to Step 3 otherwise return the best chromosome WMinS=S.\(the weight of 10. Generate all possible association rules from each identified large weighted fuzzy itemset 11. From the rules generated in step 10, identify strong rules based on the specified fuzzy weighted confidence 12. From the rules identified in step 11, decide on interesting association rules by calculating the interestingness value for each strong rule Algorithm 4.1 employs CA to return interesting 


Algorithm 4.1 employs CA to return interesting association rules. The process considers fuzzy importance of items and involves fuzzy weighted support and confidence. This algorithm has been implemented and tested, the results are presented next in Section 5 5. EXPERIMENTAL RESULTS We used real-life dataset and conducted some experiments to assess the effectiveness of the CA-based fuzzy weighted mining approach presented in this paper All of the experiments were performed using a Pentium 111, 1.4GHz CPU with 512 MB of memory and running Windows 2000. As experimental data, we used lOOK transactions dataset taken from the adult data of United States census in 2000. In the experiments, we have used 6 quantitative attributes, each with three corresponding fuzzy sets. Finally, we have used three linguistic intervals for which random linguistic weights have been generated namely \(Important, Very-Important Ordinary Important Unimportant, Ordinary 0-1 and UI-0, respectively 500 2 400 300 &lt; 200 1 0  1 100 Very Low Medium High Very Low High Minimum Support Fig. 5 Number of large itemsets for linguistic terms of minimum support M i n .  cont A Fig. 6 Membership function for nunimum confidence The first experiment tests, for the above three different linguistic weight intervals, the correlation between expressing minimum support in linguistic terms and the number of large itemsets produced. The obtained results are reported in Figure 5, which shows that the number of large itemsets decreases as a function of the linguistic minimum support yI 100 d, 80 1 60 I 5 40 0 B 20 i o Very Low Medium High Very LOW High MininumConfidencc Fig. 7 Number of interesting rules for different linguistic terms of nunimumconfidence; nunimum support fixed as  nuddle   In the second experiment, the minimum support is fixed at the linguistic value  middle  and we tested, for the three linguistic weight intervals, the effect of using linguistic te rm to express minimum confidence, as shown in Figure 6, on the number of generated interesting 114 association rules. The achieved results are reported in Figure 7. The obtained results do meet our expectations i.e., more rules are generated for higher weights However, the number decreases, for all cases, as the linguistic confidence threshold increases 140 120 TI00 80 60 2 40 20 0 0 20 40 60 80 100 Number ofTransactions \(K 


Fig. 8 Runtime for GAS to find fuzzy sets for the three linguistic intervals The last experiment is dedicated to investigate the performance for the three linguistic intervals. In particular, we examined how the performance varies with the number of transactions. This is reflected in Figure 8 which shows the runtime as we increase the number of input records from 10K to 100K, for the three different cases. The results plotted in Figure 8 show that the method scales quite linearly for the census dataset used in the experiments 6. CONCLUSIONS In this paper, we proposed a clustering approach to solve the problem of interval partitioning in favor of the maximum number of large itemsets based on linguistic minimum support and confidence. The main achievement of the proposed approach is employing GAS to dynamically adjust and optimize membership functions which are essential in finding interesting weighted association rules from quantitative transactions, based on support and confidence specified as linguistic terms Compared to previous mining approaches, the proposed approach directly manages linguistic parameters, which are more natural and understandable to humans. Results of the experiments conducted on a real life census dataset demonstrated the effectiveness and applicability of the proposed approach r31 r41 r51 REFERENCES R. Agrawal, T. Imielinski and A. Swami  Mining association rules between sets of items in large databases  Proc. of ACM SIGMOD, pp.207-216, 1993 W.H. Au and K.C.C. Chan  An Effective Algorithm for Discovering Fuzzy Rules in Relational Databases  Proc C.H. Cai, et al  Mining Association Rules with Weighted Items  Proc. of IDEAS, pp.68-77, 1998 K.C.C. Chan and W.H. Au  Mining Fuzzy Association Rules  Proc. of ACM CIKM, pp.209-215, 1997 B.C. Chien, ZL. Lin and T.P. Hong  An Efficient Clustering Algorithm for Mining Fuzzy Quantitative Association Rules  IFSA World Congress and NAFIPS International Conference, Vo1.3, pp.1306-1311,2001 A.W.C. Fu, et al  Ending Fuzzy Sets for the Mining of Association Rules for Numerical Attributes  Proc. of the OfIEEE-FUZZ, pp.1314-1319,1998 115 International Symposium of Intelligent Data Engineering and Learning, pp.263-268, Oct. 1998 D.E. Goldberg, Genetic Algorithms in Search Optimization, and Machine Learning, Addison-Wesley Reading, MA, 1989 S. Guha, R. Rastogi and K. Shim  CURE: An Efficient Clustering Algorithm for Large Databases  Information Systems, Vo1.26, No.1, pp.35-58,2001 A. Gyenesei  A Fuzzy Approach for Mining Quantitative Association Rules  TUCS Technical Report No.336 2000 K. Hirota and W. Pedrycz  Linguistic Data Mining and Fuzzy Modelling  Proc. of IEEE-FUZZ, pp.1448-1496 1996 J.H. Holland, Adaptation in Natural and Artificial Systems, The MIT Press, Cambridge, MA, MIT Press edition, 1992. First edition: University of Michigan Press 1975 T.P. Hong, C.S. Kuo and S.C. Chi  A fuzzy data mining algorithm for quantitative values  Proc. of the International Conference on Knowledge-Based Intelligent Information Engineering Systems, pp.480483, 1999 T.P. Hong, C.S. Kuo and S.C. Chi  Mining Association 


Rules from Quantitative Data  Intelligent Data Analysis Vo1.3, pp.363-376, 1999 T. P. Hong, M. J. Chiang and S. L. Wang  Mining from Quantitative Data with Linguistic Minimum Supports and Confidences  Proc. of IEEE-FUZZ, pp. 494-499,2002 H. Ishibuchi, T. Nakashima and T. Yamamoto  Fuzzy Association Rules for Handling Continuous Attributes   Proc. of IEEE International Symposium on Industrial Electronics, pp. 1 1 8 - 12 1, 200 1 M. Kaya, R. Alhajj, F. Polat and A. Arslan  Efficient Automated Mining of Fuzzy Association Rules  Proc. of DEXA, 2002 C.M. Kuok, A.W. Fu and M.H. Wong  Mining fuzzy association rules in databases  SIGMOD Record, Vol. 17 No.1, pp.41-46, 1998 B. Lent, A. Swami and J. Widom  Clustering Association Rules  Proc. of IEEE ICDE!, pp.220-23 1 1997 R.J. Miller and Y. Yang  Association Rules over Interval Data  Proc. ofACM SIGMOD, pp.452-461, 1997 R. Ng and J. Han  Efficient and effective clustering methods for spatial data mining  Proc. of VLDB, 1994 W. Pedrycz  Fuzzy Sets Technology in Knowledge Discovery  Fuzzy Sets and Systems, 98, pp.279-290 1998 R. Srikant and R. Agrawal  Mining quantitative association rules in large relational tables  Proc. of ACM W. Wang and S.M. Bridges  Genetic Algorithm Optimization of Membership Functions for Mining Fuzzy Association Rules  Proc. of the International Conference on F m y  Theory &amp; Technology, pp. 13 1-134,2000 R.R. Yager  Fuzzy Summaries in Database Mining   Proc. of the Conference on Artificial Intelligence for Application, pp.265-269, 1995 S .  Yue, el al  Mining fuzzy association rules with weighted items  Proc. of IEEE SMC Conference pp.1906-1911,2000 L.A., Zadeh  Fuzzy Sets  Information and Control W. Zhang  Mining Fuzzy Quantitative Association Rules  Proc. of IEEE ICTRI, pp.99-102, 1999 SIGMOD, pp.1-12, 19 V01.8, pp.338-353, 1965 pre></body></html 


efficiency then AOFI. However utilization of fuzzy concept hierarchies provides more flexibility in reflecting expert knowledge and so allows better modeling of real-life dependencies among attribute values, which will lead to more satisfactory overall results for the induction process. The drawback of the computational cost may additionally decline when we notice that, in contrast to many other data mining algorithms, hierarchical induction algorithms need to run only once through the original \(i.e. massive dataset. We are continuing an investigation of computational costs of our approach for large datasets ACKNOWLEDGMENT Rafal Angryk would like to thank the Montana NASA EPSCoR Grant Consortium for sponsoring this research REFERENCES 1] J. Han , M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann, New York, NY 2000 2] J. Han, Y. Cai, and N. Cercone  Knowledge discovery in databases: An attribute-oriented approach  Proc. 18th Int. Conf. Ver y Large Data Bases, Vancouver, Canada, 1992, pp. 547-559 3] J. Han  Towards Efficient Induction Mechanisms in Database Systems  Theoretical Computing Science, 133, 1994, pp. 361-385 4] J. Han, Y. Fu  Discovery of Multiple-Level Association Rules from Large Databases  IEEE Trans.  on KD E, 11\(5 5] C.L. Carter, H.J. Hamilton  Efficient AttributeOriented Generalization for Knowledge Discovery from Large Databases  IEEE Trans. on KDE 10\(2 6] R.J. Hilderman, H.J. Hamilton, and N. Cercone  Data mining in large databases using domain generalization graphs  Journal of Intelligent Information Systems, 13\(3 7] C.-C. Hsu  Extending attribute-oriented induction algorithm for major values and numeric values   Expert Systems with Applications , 27, 2004, pp 187-202 8] D.H. Lee, M.H. Kim  Database summarization using fuzzy ISA hierarchies  IEEE Trans . on SMC - part B, 27\(1 9] K.-M. Lee  Mining generalized fuzzy quantitative association rules with fuzzy generalization hierarchies  20th NAFIPS Int'l Conf., Vancouver Canada, 2001, pp. 2977-2982 10] J. C. Cubero, J.M. Medina, O. Pons &amp; M.A. Vila  Data Summarization in Relational Databases through  Fuzzy Dependencies  Information Sciences, 121\(3-4 11] G. Raschia, N. Mouaddib  SAINTETIQ:a fuzzy set-based approach to database summarization   Fuzzy Sets and Systems, 129\(2 162 12] R. Angryk, F. Petry  Consistent fuzzy concept hierarchies for attribute generalization  Proceeding of the IASTED Int. Conf. on Information and Knowledge Sharing, Scottsdale AZ, USA, November 2003, pp. 158-163 13] Toxics Release Inventory \(TRI available EPA database hosted at http://www.epa.gov/tri/tridata/tri01/index.htm The 2005 IEEE International Conference on Fuzzy Systems790 pre></body></html 


the initial global candidate set would be similar to the set of global MFIs. As a result, during the global mining phase the communication and synchronization overhead is low  0 2 4 6 8 1 0 Number of Nodes Figure 5. Speedup of DMM 4.4.2 Sizeup For the sizeup test, we fixed the system to the 8-node con figuration, and distributed each database listed in Table 2 to the 8 nodes. Then, we increased the local database sire at each node from 45 MB to 215 MB by duplicating the initial database partition allocated to the node. Thus, the data distribution characteristics remained the same as the local database size was increased. This is different from the speedup test, where the database repartitioning was per formed when the number of nodes was increased. The per formance of DMM is affected by the database repartitioning to some extent, although it is usually very small. During the sizeup test, the local mining result of DMM is not changed at all at each node The results shown in Figure 6 indicate that DMM has a very good sizeup property. Since increasing the size of local database did not affect the local mining result of DMM at each node, the total execution time increased just due to more disk U 0  and computation cost which scaled almost linearly with sizeup 5 Conclusions In this paper, we proposed a new parallel maximal fre quent itemset \(MFI Max-Miner \(DMM tems. DMM is a parallel version of Max-Miner, and it re quires low synchronization and communication overhead compared to other parallel algorithms. In DMM, Max Miner is applied on each database partition during the lo 0 45 90 135 180 225 270 Amwnt of Data per Node \(ME Figure 6. Sizeup of DMM cal mining phase. Only one synchronization is needed at thc end of this phase to construct thc initial global candi date set. In the global mining phase, a top-down search is performed on the candidate set, and a prefix tree is used to count the candidates with different length efficiently. Usu ally, just a few passes are needed to find all global maximal frequent itemsets. Thus, DMM largely reduces the number of synchronizations required between processing nodes Compared with Count Distribution, DMM shows a great improvement when some frequent itemscts are large \(i.e long patterns employed by DMM for efficient communication between nodes; and global support estimation, subset-infrequency based pruning, and superset-frequency based pruning are used to reduce the size of global candidate set. DMM has very good speedup and sizeup properties References I ]  R. Agrawal and R. Srikant  FdSt Algorithms for Mining As sociation Rules  Pmc. o f f h e  ZOrh VLDB Conf, 1994, pp 487499 2] R. Agrawal and I. C. Shafer  Parallel Mining of Association Rules  IEEE Trans. on Knowledge and Dura Engineering Vol. 8, No. 6, 1996, pp. 962-969 3] R. I. Bayardo  Efficient Mining Long Patlems from Databases  Proc. ofrhe ACM SIGMOD Inf  l Conf on Man ogemenr ofDara, 1998, pp. 85-91 4] S.  M. Chung and J. Yang  A Parallel Distributive Join Al gorithm for Cube-Connected Multiprocessors  IEEE Trans on Parallel and Disrribured Systems, Vol. 7, No. 2, 1996, pp 127-137 51 M. Snir, S. Otto. S. Huss-Lederman, D. Walker, and J. Don gana, MPI: The Complete Reference, The MIT Press, 1996 


gana, MPI: The Complete Reference, The MIT Press, 1996 6] R. Rymon  Search through Systematic Set Enumeralion   Pmc. of3rd Inr  l Con$ on Principles of Knowledge Repre sentation and Reasoning, 1992, pp. 539-550 507 pre></body></html 


sketch-index in answering aggregate queries. Then Section 5.2 studies the effect of approximating spatiotemporal data, while Section 5.3 presents preliminary results for mining association rules 5.1 Performance of sketch-indexes Due to the lack of real spatio-temporal datasets we generate synthetic data in a way similar to [SJLL00 TPS03] aiming at simulation of air traffic. We first adopt a real spatial dataset [Tiger] that contains 10k 2D points representing locations in the Long Beach county \(the data space is normalized to unit length on each dimension These points serve as the  airbases  At the initial timestamp 0, we generate 100k air planes, such that each plane \(i uniformly generated in [200,300], \(ii, iii destination that are two random different airbases, and iv  the velocity direction is determined by the orientation of the line segment connecting its source and destination airbases move continually according to their velocities. Once a plane reaches its destination, it flies towards another randomly selected also uniform in [0.02, 0.04 reports to its nearest airbase, or specifically, the database consists of tuples in the form &lt;time t, airbase b, plane p passenger # a&gt;, specifying that plane p with a passengers is closest to base b at time t A spatio-temporal count/sum query has two parameters the length qrlen of its query \(square number qtlen of timestamps covered by its interval. The actual extent of the window \(interval uniformly in the data space \(history, i.e., timestamps 0,100 air planes that report to airbases in qr during qt, while a sum query returns the sum of these planes  passengers. A workload consists of 100 queries with the same parameters qrlen and qtlen The disk page size is set to 1k in all cases \(the relatively small page size simulates situations where the database is much more voluminous specialized method for distinct spatio-temporal aggregation, we compare the sketch-index to the following relational approach that can be implemented in a DBMS. Specifically, we index the 4-tuple table lt;t,b,p,a&gt; using a B-tree on the time t column. Given a count query \(with window qr and interval qt SELECT distinct p FROM &lt;t,b,p,a&gt WHERE t?qt &amp; b contained in qr The performance of each method is measured as the average number of page accesses \(per query processing a workload. For the sketch-index, we also report the average \(relative Specifically, let acti and esti be the actual and estimated results of the i-th query in the workload; then the error equals \(1/100 set the number of bits in each sketch to 24, and vary the number of sketches The first experiment evaluates the space consumption Figure 5.1 shows the sketch index size as a function of the number of sketches used \(count- and sum-indexes have the same results more sketches are included, but is usually considerably smaller than the database size \(e.g., for 16 signatures, the size is only 40% the database size 0 20 40 60 80 


80 100 120 140 160 8 16 32 number of sketches size \(mega bytes database size Figure 5.1: Size comparison Next we demonstrate the superiority of the proposed sketch-pruning query algorithm, with respect to the na  ve one that applies only spatio-temporal predicates. Figure 5.2a illustrates the costs of both algorithms for countworkloads with qtlen=10 and various qrlen \(the index used in this case has 16 sketches also illustrate the performance of the relational method which, however, is clearly incomparable \(for qrlen?0.1, it is worse by an order of magnitude we omit this technique Sketch-pruning always outperforms na  ve \(e.g., eventually two times faster for qrlen=0.25 increases with qrlen, since queries returning larger results tend to set bits in the result sketch more quickly, thus enhancing the power of Heuristics 3.1 and 3.2. In Figure 5.2b, we compare the two methods by fixing qrlen to 0.15 and varying qtlen. Similar to the findings of [PTKZ02]4 both algorithms demonstrate  step-wise  growths in their costs, while sketch-pruning is again significantly faster The experiments with sum-workloads lead to the same observations, and therefore we evaluate sketch-indexes using sketch-pruning in the rest of the experiments 4 As explained in [PTKZ02], query processing accesses at most two paths from the root to the leaf level of each B-tree regardless the length of the query interval Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE sketch-pruning naive relational 0 100 200 300 400 500 600 700 800 900 0.05 0.1 0.15 0.2 0.25 number of disk accesses query rectangle length 300 0 100 200 400 500 600 1 5 10 15 20 number of disk accesses query interval length a qtlen=10 b qrlen=0.15 Figure 5.2: Superiority of sketch-pruning \(count As discussed in Section 2, a large number of sketches reduces the variance in the resulting estimate. To verify this, Figure 5.3a plots the count-workload error of indexes 


using 8-, 16-, and 32- sketches, as a function of qrlen qtlen=10 error \(below 10 it increases slowly with qrlen used, however, the error rate is much higher \(up to 30 and has serious fluctuation, indicating the prediction is not robust. The performance of 16-sketch is in between these two extremes, or specifically, its accuracy is reasonably high \(average error around 15 much less fluctuation than 8-sketch 32-sketch 16-sketch 8-sketch relative error 0 5 10 15 20 25 30 35 0.05 0.1 0.15 0.2 0.25 query rectangle length relative error 0 5 10 15 20 25 30 35 1 5 10 15 20 query interval length a qtlen=10, count b qrlen=0.15, count relative error query rectangle length 0 5 10 15 20 25 0.05 0.1 0.15 0.2 0.25 relative error query interval length 0 5 10 15 20 25 30 1 5 10 15 20 c qtlen=10, sum d qrlen=0.15, sum Figure 5.3: Accuracy of the approximate results The same phenomena are confirmed in Figures 5.3b where we fix qrlen to 0.15 and vary qtlen 5.3d \(results for sum-workloads number of sketches improves the estimation accuracy, it also leads to higher space requirements \(as shown in Figure 5.1 Figures 5.4a and 5.4b show the number of disk accesses for the settings of Figures 5.3a and 5.3b. All indexes have almost the same behavior, while the 32-sketch is clearly more expensive than the other two indexes. The interesting observation is that 8- and 16-sketches have 


interesting observation is that 8- and 16-sketches have almost the same overhead due to the similar heights of their B-trees. Since the diagrams for sum-workloads illustrate \(almost avoid redundancy 32-sketch 16-sketch 8-sketch number of disk accesses query rectangle length 0 50 100 150 200 250 300 350 400 0.05 0.1 0.15 0.2 0.25 number of disk accesses query interval length 0 50 100 150 200 250 300 350 1 5 10 15 20 a qtlen=10 b qrlen=0.15 Figure 5.4: Costs of indexes with various signatures Summary: The sketch index constitutes an effective method for approximate spatio-temporal \(distinct aggregate processing. Particularly, the best tradeoff between space, query time, and estimation accuracy obtained by 16 sketches, which leads to size around 40 the database, fast response time \(an order of magnitude faster than the relational method average relative error 5.2 Approximating spatio-temporal data We proceed to study the efficiency of using sketches to approximate spatio-temporal data \(proposed in Section 4.1 as in the last section, except that at each timestamp all airplanes report their locations to a central server \(instead of their respective nearest bases maintains a table in the form &lt;time t, plane p, x, y&gt;, where x,y with parameters qrlen and qtlen distinct planes satisfying the spatial and temporal conditions. For comparison, we index the table using a 3D R*-tree on the columns time, x, and y. Given a query, this tree facilitates the retrieval of all qualifying tuples, after which a post-processing step is performed to obtain the Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE number of distinct planes \(in the sequel, we refer to this method as 3DR method introduces a regular res  res grid of the data space, where the resolution res is a parameter. We adopt 16 sketches because, as mentioned earlier, this number gives the best overall performance Figure 5.5 compares the sizes of the resulting sketch indexes \(obtained with resolutions res=25, 50, 100 the database size. In all cases, we achieve high compression rate \(e.g., the rate is 25% for res=25 evaluate the query efficiency, we first set the resolution to the median value 50, and use the sketch index to answer workloads with various qrlen \(qtlen=10 


workloads with various qrlen \(qtlen=10 size \(mega bytes database size 0 20 40 60 80 100 120 140 160 25 50 100 resolution Figure 5.5: Size reduction Figure 5.6a shows the query costs \(together with the error in each case method. The sketch index is faster than 3DR by an order of magnitude \(note that the vertical axis is in logarithmic scale around 15% error observations using workloads with different qtlen Finally, we examine the effect of resolution res using a workload with qrlen=0.15 and qtlen=10. As shown in Figure 5.6c, larger res incurs higher query overhead, but improves the estimation accuracy Summary: The proposed sketch method can be used to efficiently approximate spatio-temporal data for aggregate processing. It consumes significantly smaller space, and answers a query almost in real-time with low error 3D Rsketch number of disk accesses query rectangle length 1 10 100 1k 10k 0.05 0.1 0.15 0.2 0.25 16 14% 15 15% 13 relative error number of disk accesses query interval length 1 10 100 1k 10k 1 5 10 15 20 16 15% 15% 12% 11 relative error a qtlen=10, res=25 b qrlen=0.15, res=25 0 500 1000 1500 2000 2500 25 50 100 number of disk accesses resolution 20% 15% 14 relative error c qrlen=0.15, qtlen=10 


c qrlen=0.15, qtlen=10 Figure 5.6: Query efficiency \(costs and error 5.3 Mining association rules To evaluate the proposed algorithm for mining spatiotemporal association rules, we first artificially formulate 1000 association rules in the form \(r1,T,90 with 90% confidence i randomly picked from 10k ones, \(ii in at most one rule, and \(iii Then, at each of the following 100 timestamps, we assign 100k objects to the 10k regions following these rules. We execute our algorithms \(using 16 sketches these rules, and measure \(i  correct  rules divided by the total number of discovered rules, and \(ii successfully mined Figures 5.7a and 5.7b illustrate the precision and recall as a function of T respectively. Our algorithm has good precision \(close to 90 majority of the rules discovered are correct. The recall however, is relatively low for short T, but gradually increases \(90% for T=25 evaluated in the previous sections, the estimation error decreases as the query result becomes larger \(i.e., the case for higher T 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 precision HT 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 recall HT a b Figure 5.7: Efficiency of the mining algorithm Summary: The preliminary results justify the usefulness of our mining algorithm, whose efficiency improves as T increases Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE 6. Conclusions While efficient aggregation is the objective of most spatio-temporal applications in practice, the existing solutions either incur prohibitive space consumption and query time, or are not able to return useful aggregate results due to the distinct counting problem. In this paper we propose the sketch index that integrates traditional approximate counting techniques with spatio-temporal indexes. Sketch indexes use a highly optimized query algorithm resulting in both smaller database size and faster query time. Our experiments show that while a sketch index consumes only a fraction of the space required for a conventional database, it can process 


required for a conventional database, it can process queries an order of magnitude faster with average relative error less than 15 While we chose to use FM sketches, our methodology can leverage any sketches allowing union operations Comparing the efficiency of different sketches constitutes a direction for future work, as well as further investigation of more sophisticated algorithms for mining association rules. For example, heuristics similar to those used for searching sketch indexes may be applied to improve the brute-force implementation ACKNOWLEDGEMENTS Yufei Tao and Dimitris Papadias were supported by grant HKUST 6197/02E from Hong Kong RGC. George Kollios, Jeffrey Considine and were Feifei Li supported by NSF CAREER IIS-0133825 and NSF IIS-0308213 grants References BKSS90] Beckmann, N., Kriegel, H., Schneider, R Seeger, B. The R*-tree: An Efficient and Robust Access Method for Points and Rectangles. SIGMOD, 1990 CDD+01] Chaudhuri, S., Das, G., Datar, M., Motwani R., Narasayya, V. Overcoming Limitations of Sampling for Aggregation Queries. ICDE 2001 CLKB04] Jeffrey Considine, Feifei Li, George Kollios John Byers. Approximate aggregation techniques for sensor databases. ICDE, 2004 CR94] Chen, C., Roussopoulos, N. Adaptive Selectivity Estimation Using Query Feedback. SIGMOD, 1994 FM85] Flajolet, P., Martin, G. Probabilistic Counting Algorithms for Data Base Applications JCSS, 32\(2 G84] Guttman, A. R-Trees: A Dynamic Index Structure for Spatial Searching. SIGMOD 1984 GAA03] Govindarajan, S., Agarwal, P., Arge, L. CRBTree: An Efficient Indexing Scheme for Range Aggregate Queries. ICDT, 2003 GGR03] Ganguly, S., Garofalakis, M., Rastogi, R Processing Set Expressions Over Continuous Update Streams. SIGMOD, 2003 HHW97] Hellerstein, J., Haas, P., Wang, H. Online Aggregation. SIGMOD, 1997 JL99] Jurgens, M., Lenz, H. PISA: Performance Models for Index Structures with and without Aggregated Data. SSDBM, 1999 LM01] Lazaridis, I., Mehrotra, S. Progressive Approximate Aggregate Queries with a Multi-Resolution Tree Structure. SIGMOD 2001 PGF02] Palmer, C., Gibbons, P., Faloutsos, C. ANF A Fast and Scalable Tool for Data Mining in Massive Graphs. SIGKDD, 2002 PKZT01] Papadias,  D., Kalnis, P.,  Zhang, J., Tao, Y Efficient OLAP Operations in Spatial Data Warehouses. SSTD, 2001 PTKZ02] Papadias, D., Tao, Y., Kalnis, P., Zhang, J Indexing Spatio-Temporal Data Warehouses ICDE, 2002 SJLL00] Saltenis, S., Jensen, C., Leutenegger, S Lopez, M.A. Indexing the Positions of Continuously Moving Objects. SIGMOD 2000 SRF87] Sellis, T., Roussopoulos, N., Faloutsos, C The R+-tree: A Dynamic Index for MultiDimensional Objects. VLDB, 1987 TGIK02] Thaper, N., Guha, S., Indyk, P., Koudas, N Dynamic Multidimensional Histograms 


SIGMOD, 2002 Tiger] www.census.gov/geo/www/tiger TPS03] Tao, Y., Papadias, D., Sun, J. The TPR*Tree: An Optimized Spatio-Temporal Access Method for Predictive Queries. VLDB, 2003 TPZ02] Tao, Y., Papadias, D., Zhang, J. Aggregate Processing of Planar Points. EDBT, 2002 TSP03] Tao, Y., Sun, J., Papadias, D. Analysis of Predictive Spatio-Temporal Queries. TODS 28\(4 ZMT+01] Zhang, D., Markowetz, A., Tsotras, V Gunopulos, D., Seeger, B. Efficient Computation of Temporal Aggregates with Range Predicates. PODS, 2001 ZTG02] Zhang, D., Tsotras, V., Gunopulos, D Efficient Aggregation over Objects with Extent PODS, 2002 Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE pre></body></html 


