Adaptation of Apriori to MapReduce to Build a Warehouse of Relations Between Named Entities Across the Web Jean-Daniel Cryans Software engineering and IT Dept Ecole de technologie superieure Montreal, QC, Canada Email: jdcryans@apache.org Sylvie Ratte Software engineering and IT Dept Ecole de technologie superieure Montreal, QC, Canada Email: Sylvie.Ratte@etsmtl.ca Roger Champagne Software engineering and IT Dept Ecole de technologie superieure Montreal, QC, Canada Email: Roger.Champagne@etsmtl.ca AbstractThe Semantic Web has made possible the use of the Internet to extract useful content, a task that could necessitate an infrastructure across the Web. With Hadoop, a free implementation of the MapReduce programming paradigm created by Google, we can treat these data reliably over hundreds of servers. This article describes how the Apriori algorithm was adapted to MapReduce in the search for relations between entities to deal with thousands of Web pages coming from RSS feeds daily. First, every feed is looked up five times per day and each entry is registered in a database with MapReduce. Second, the entries are read and their content sent to the Web service OpenCalais for the detection of named entities. For each Web page, the set of all itemsets found is generated and stored in the database. Third, all generated sets from first to last, are counted and their support is registered Finally, various analytical tasks are executed to present the relationships found. Our tests show that the third step, executed over 3,000,000 sets, was 4.5 times faster using five servers than using a single machine. This approach allows us to easily and automatically distribute treatments on as many machines as are available, and be able to process datasets that one server, even a very powerful one, would not be able to manage alone. We believe that this work is a step forward in processing semantic Web data efficiently and effectively Keywords-web mining; association rules; Apriori algorithm 


MapReduce paradigm I. INTRODUCTION Most of the data mining algorithms written during the last decade have been designed to execute on a single computer In the Semantic Web era, it is becoming increasingly easy to extract information from the Web, provided that we can store these data which no longer reside on a single machine. We must therefore find ways to parallelize the processing on several machines, in order to increase both storage capacity and treatment. This study proposes an adaptation of the search algorithm Apriori to the MapReduce programming paradigm to deal with news feeds to find the relations between named entities. This new algorithm, called AprioriMR, will use the knowledge contained on the Internet to reveal links between cities, people, objects, monuments or other tangible concepts The rest of the article is organized as follows. Section II explains the background of this research. Section III describes the objectives of the adaptation. Section IV presents the tools used to obtain the data and process them. Section V explains how data are processed and how the Apriori algorithm was adapted. Section VI presents the results of the study. Section 7 puts this study into perspective with respect to other, similar proposals II. BACKGROUND This study is part of the Semantic Web effort to make information available to users and machines. In it, we seek to more precisely extract the relationship between named entities in news feeds. The news comes from companies known for the quality of their content, such as newspapers and television stations that publish text online. This is to ensure: 1 way; and 2 the information published is correct. These properties are discussed in more detail in Section IV As it is currently impossible to make useful queries on the content stored only on the Web, we must do this on our own infrastructure. The data for large numbers of pages cannot be contained on a single machine, because their volume easily exceeds the size of the largest hard disks. Even if this were possible, the loss of that single machine would make the content unavailable for a time. Furthermore, the 


serial processing of these data would be limited by the speed of the computer components. This is why software such as Hadoop [1], which facilitates this type of processing, have been designed. Hadoop provides a distributed file system that can be used on more than a thousand machines and an implementation of the MapReduce programming paradigm 2], invented by Google, which allows data to be processed reliably on these machines. At the time of writing no implementation of Apriori within Hadoop exists. While the itemset counting is trivial in Map/Reduce, the real challenge of the algorithm is in the candidate selection implementation At the end of the study, the system created should be able to take a named entity and find other entities that are 2010 Second International Conference on Advances in Databases, Knowledge, and Data Applications Unrecognized Copyright Information DOI 10.1109/DBKDA.2010.34 185978-0-7695-3981-2/10 $26.00  2010 IEEE very closely linked to it on the basis of Web content. This will necessarily lead to the disambiguation of entities \(the word apple, for example, can refer to a fruit, a computer company, or a multimedia corporation also be possible to determine the strength of the relationships between several entities III. OBJECTIVES This study has three objectives: 1 learning algorithm of association rules [3] to the MapReduce paradigm [4]; 2  a large number of articles from the Internet in a distributed manner; and 3  batch processing environment, as a warehouse capable of responding to requests in real time Adapting the Apriori algorithm to MapReduce is necessary to deal with sets of transactions that cannot be stored in RAM on a single computer. This adjustment will help treat millions of items from the Web several times a day and obtain results equivalent to those of the original algorithm Since Apriori is not designed to run on a cluster of machines we must first demonstrate whether or not this is feasible. It should also be mentioned that the MapReduce framework imposes a minimum cost in terms of time for each execution. Consequently, we must determine the performance 


loss caused by the use of MapReduce on data that can be processed on a single machine Furthermore, the feasibility of using OpenCalais for very large sets of articles must be shown, because this is the tool used to find the named entities. We must then be able to send tens of thousands of articles per day to this Web service and not be constrained by the response time, which is more than one second. In other words, a method must be properly designed to use OpenCalais in a distributed manner HBASE is a database inspired largely by Bigtable [10 which allows Google to store terabytes of structured data in a scalable way. However, it does not offer a means of interaction with the SQL database language, as it is not relational, but it is nevertheless compatible with MapReduce We must then validate that HBASE will be able to store millions of articles from the Web along with the itemsets that will be extracted. In addition, we must ensure that we can quickly retrieve specific information despite the lack of relationships between tables, in order to serve a website and deal with batch processing simultaneously IV. DATA AND SOFTWARE The first source of data for this study was the thousands of RSS feeds of newspapers and television stations which publish articles online. Each RSS feed contains a series of recent articles that are described by title, abstract, first lines of the text, date, and, sometimes, attached files \(images etc indirectly to an HTML page containing an article described rdf:Description rdf:about http://d.opencalais.com/comphash-1 64136b2b-cb4e-36ac-9f32-f58f4c1f1c8a rdf:type rdf:resource http://sopencalais.com/1/type em/e/Company c:name>British Airways </c:name c:nationality>British</c:nationality rdf:Description Figure 1. Example of a resource in RDF format by a title, subtitle, and text, but also by general elements of each site \(menus, advertising, and links to related articles The use of RSS feeds has several advantages. First, the use of a Web crawler, which is often difficult to configure, is 


no longer necessary, since it only takes one library to read the RSS and another to give the HTML of a URL. Also it helps to have a high level of control over which pages will be covered, because, without a proper and expensive configuration, crawlers such as Nutch [11] and Heritrix [12 have to analyze both images and link forms, recovering a large number of pages in the process that have no value in the search for well-written articles and valid content The second data source is OpenCalais, a Web service which makes it possible to find named entities in a text The result is presented by default as a Resource Description Framework \(RDF resources that come from the Web For example, the resource http://d.opencalais.com/er/geo/city ralg-geo1/b3719a18-c511-51a8-b3f9f8480b3b6e48.html is a reference to the City of New York in the United States An RDF response contains many resources, as shown in Figure 1, where British Airways has a URI, a name, a type and a nationality. It attributes change depending on the type of response Using OpenCalais allows us to respond easily to the problem of recognition of named entities, but there are several constraints. First, articles sent to the service cannot have more than 100,000 characters, otherwise an error is returned. Second, a maximum of 40,000 queries per day and four requests per second can be sent to the service. Third the submitted texts must be very well built \(title, subtitle and text is because the emphasis is on accuracy and not on recall Despite these limitations, it is much more economical to use OpenCalais than to develop our own library for named entity detection The choice of Apache Hadoop \(which contains an implementation of MapReduce and a distributed file system 186 and HBASE is justified by the high level of scalability and availability they offer. It thus becomes possible to store several terabytes of data and process them easily and reliably, since the software stack is resistant to engine failures and mistakes. The versions we used were Hadoop 


0.19.1 and HBASE 0.19.3 A cluster of ten computers was used to store data with Hadoop and HBASE. The master node was composed of a processor running at 1.8 GHz, with 2 GB of RAM and two disks as mirrors. The other nine slave nodes had the same processor and 1 GB of memory, with two 80 GB disks used independently by the distributed file system, for a total of approximately 1 TO. A cluster of five computers \(each with a 2.4 GHz processor and 1 GB of memory process the data with MapReduce. The master process of MapReduce was on the same master node of the first cluster Such a configuration is not consistent with MapReduces principle of proximity, because normally all slave processes are found on all slave nodes. But, with one processor and little memory, it is not possible to combine the processes without competing for resources V. METHODS Thanks to these tools and data sources, it is possible to acquire data, as shown in Figure 2. Several times a day, the following operations are performed in sequence 1 URLs of new articles that will be saved in HBASE with a label indicating that they are new and should therefore be treated \(see 1a and 1b in Figure 2 2 consists of going through the HBASE table of all articles to download the HTMLs of the new articles. It then communicates with OpenCalais to get the RDF filter entities with a relevance index of 2 \(which is too low remaining entities \(except the empty set the Powerset, which will be forwarded to the Reduce phase. This last phase will record this new set of itemsets in the table of articles and mark the page as read. As the task is going through the full table of items, it also notes the final number of articles with itemsets, which will become the total number of transactions \(see 1a, 2b and 2c in Figure 2 In order not to limit the reading speed of pages to the number of servers \(in this case, five reads three pages at a time. After each page is read there is a down time of five seconds, as a courtesy to 


the site analyzed. Moreover, since OpenCalais limits the number of articles processed per second to four it is likely that one or more processes will receive an error from the Web service. When this occurs, a random waiting time of between 0.1 and 10 seconds Figure 2. Process Data Acquisition Process is observed before trying again, so as not to reproduce the same problem indefinitely 3 articles and forwards each of the itemsets individually to the Reduce phase, which will in turn obtain the number of occurrences of each of the itemsets. Only itemsets will be saved, the number of which, divided by the total number of transactions, called support, is greater than a predefined metric set when configuring the task \(see 3a and 3b in Figure 2 4 aggregates \(see, in particular, Section VI the data for a search engine \(see 4 in Figure 2 VI. RESULTS The data acquisition task has been performed regularly for a month on a growing number of RSS feeds, and has produced more and more data every day. At the time of writing, about 1,500 feeds are cycled every day, which helped to analyze more than 100,000 web pages. Approximately 20% of the pages covered were either unavailable \(HTTP 400 or 503 errors, for example or too big, or their encoding was not well specified. From these pages, over 50,000 named entities were discovered and the number of itemsets not filtered reached more than 3 million. The disk space required to contain these data is approximately 30 GB uncompressed, but replicated three times, so the total space required is about 90 GB. Two data computers failed during testing, but this had no impact on the results The adaptation of the Apriori algorithm to MapReduce to make the processing task extensible was verified by running the third step of the method on a different number of machines, from one to five. The number of machines in the data cluster was not changed, as this would have influenced the maximum flow. Between each run, HBASE was rebooted to clear statements in memory and in order to benefit from 


the same optimizations of the Sun Java Virtual Machine each time. The Map number was established at 40 for all 187 Figure 3. Execution time \(sec machines treatments and the Reduce number was equivalent to the number of machines. The results are shown in Figure 3. We can see that moving from one to five machines reduces the execution time by a factor of 4.5. This is because each Map can draw information from one machine at a time, which leaves the rest of the cluster data dormant The reason why the execution with five machines is not exactly five times faster is the quality of the data distribution in HBASE: it can easily happen that two Maps are using the same data machine at the same time, inducing a slowdown The Reduce phase with a single machine is also faster because it resides on a virtual machine and discovers the location of the data only once. When using five machines each of them must find where to place the data in HBASE The experiment demonstrates that it is indeed possible to use OpenCalais in a Web-crawling task, but we must be able to handle the errors returned. As described in Section V, it is useless to resend our requests directly to the Web service immediately, or to wait for a fixed period of time prior to doing so. It is best to choose a random time between shipments. Another issue is the occurrence of unexpected errors. Sometimes, for example, after several attempts, the service does not respond within the maximum period of 15 seconds, while other pages are handled at the same time without a problem on other servers. In addition to the random wait, experience shows that we must also perform a maximum number of sends and then give up, in order not to lose too much time on the treatment of a single page The final validation was to ensure that HBASE was able to contain millions of elements in addition to being able to respond to requests despite the lack of a relationship between tables and a different data model, while dealing with batch processing. With millions of elements, we tried to filter none of the itemset at the third stage, and therefore to write the 3 million items in an HBASE table. While the treatment was successful, it took much longer, and had the disadvantage of slowing down the analytical tasks that 


followed. The first task to run at that point was the detection of the itemsets with the largest supports for each cardinality from one to twelve the itemset table in full. Its execution time is approximately 25% higher for about 130 times more elements, which is normal, given that scanning an HBASE table usually takes less than a millisecond per line. Despite these additional data, the website developed in [7] is not slower, since each request, which is performed by a row key and a column key is very fast, no matter how many rows are involved \(one to a billion VII. DISCUSSION Our work is similar to that of [5], whose algorithm processes uncertain data flows. Flows are characterized by a number of transactions containing items, and must be treated immediately because they are no longer available for reuse In addition, items are not distributed evenly in each stream which means that an item that is uncommon at one moment may become very common later on. Uncertain data are characterized by an existential uncertainty that indicates the degree of certainty that each item will be in each transaction which is not the case for typical transactional databases where this uncertainty does not exist. Their flows for data distribution resembles in a way to our RSS stream, since it is uneven, the difference being that we keep the stream available so that it is always possible to process it again Since it is not necessary to provide a limited-stream window to users, all the available information is used. The existential uncertainty is almost identical to the degree of relevance provided by OpenCalais: if an entity is not relevant to the article, it is probably part of the noise that surrounds it. At this time, the support calculation does not take into account the relevance of the entities, because we think that, beyond a certain level, confidence is sufficient to include the entity. In addition, levels of support are so low that filtering them with the help of this probability would only reduce their number still more Aflori and Craus implementation [13] of Apriori used grid technologies in order to solve the scalability problem caused by large datasets. Their design is composed of a suite of services used on top of the Globus Toolkit in order to discover the associations rules in geographically 


distributed databases. Their goal is different than ours as we only wanted to mine association rules in a single database hosted on a local dedicated grid of machines. Their services somehow resembles to MapReduce as they create temporary tasks on machines in order to run Apriori but the actual processing is serialized The work of Dean and Ghemawat [4] on MapReduce at Google shows that a software framework for distributed processing over thousands of machines is feasible and can be reliable. The paradigm allows for parallel treatment of different domains without resorting to complicated logic The Map phase of each task takes as input a set of keys and values on which treatment will be performed to create other keys and other values. When this phase is completed, all values are grouped according to their key. These groupings 188 are then forwarded to the Reduce phase, which performs further processing to create other keys and values. Each Map/Reduce phase is separated into sub-tasks distributed on the machines of the cluster, and, if one of them is missing its sub-tasks are automatically redirected to other machines in a transparent manner. It is also possible to make further adjustments to MapReduce, such as writing directly into a compatible database \(Bigtable at Google, or HBASE in Open Source The use of this paradigm can easily make a serialized processing task extensible. Since the Map/Reduce writing tasks are not dependent on the number of machines, the same code can be used on five or a thousand machines. In order to process documents on the Web, it is very advantageous to adapt Apriori. The tasks described in this study are consistent with the model described in [4]. The distribution potential is used to read multiple feeds simultaneously on multiple machines VIII. CONCLUSION Adapting the Apriori algorithm to MapReduce is a valid solution to the problem posed by the amount of data to be processed on the Web. The use of a cluster of machines to distribute the treatment has helped to optimize RSS streammining, and allows the complete processing of multiple streams \(reading the articles and sending them to OpenCalais 


mining between named entities in a sample of 3 million itemsets stored in HBASE, and could handle even more data Moreover, if, at some point, the treatments become too long it will be possible to add new resources without modifying the algorithm. This study serves as the basis for the dynamic website developed in [7], which takes the processed data and presents it to users. Future work could focus on cleaning up Web pages to remove advertising and static content \(such as a menus the one proposed by Leung and Hao [5] could also be used to present only current and up-to-date information. Finally our preliminary research on the Apriori algorithm opens the door to the adaptation of other more efficient algorithms e.g. AprioriTid, AprioriHybrid [14] amoung many others Regarding this extension, it would be interesting to see how far the support calculation phase \(our task 3 be accelerated REFERENCES 1] A. Bialecki, A. Murthy, C. Douglas, D. Cutting, D. Das D. Borthakur, E. Soztutar, A. Gates, J. Kellerman, M. Konar N. Daley, O. Natkovich, O. OMalley, P. Hunt, M. Stack C. Taton, and T. White. \(2009  able: http://hadoop.apache.org/core/, 2010/15/01 2] H.C. Yang, A. Dasdan, R.-L. Hsiao, and D. Parker, Mapreduce-merge: simplified relational data processing on large clusters, in ACM SIGMOD International Conference on Management of Data, 2007, pp. 1029  1040 3] R. Agrawal, T. Imielinski, and A. Swami, Mining association rules between sets of items in large databases, in ACM SIGMOD International Conference on Management of Data 1993, pp. 207  216 4] J. Dean and S. Ghemawat, Mapreduce: simplified data processing on large clusters, Commun. ACM, vol. 51, no. 1, 2008 pp. 107113 5] C. Leung, and B. Hao, Mining of Frequent Itemsets from Streams of Uncertain Data, in IEEE International Conference on Data Engineering, 2009, pp. 16631670 6] C. Aflori and M. Craus. Grid implementation of the Apriori algorithm, in Advances in Engineering Software, Vol 38, 2007 pp. 295300 7] J.-D. Cryans, Prototypage dun engin de recherche de relations entre des entites nommees, Projet de fin detudes, Ecole 


de technologie superieure, 2009, 35 p 8] N. Joffe, J.-D. Cryans, B. Duxbury, J. Kellerman, A. Purtell M. Stack, and R. Rawson. \(2009  http://hbase.org, 2010/15/01 9] OpenCalais. \(2009  able: http://www.opencalais.com, 2010/15/01 10] F. Chang, J. Dean, S. Ghemawat, W. Hsieh, D. Wallach M. Burrows, T. Chandra, A. Fikes, and R. Gruber, Bigtable A distributed storage system for structured data, ACM Transactions on Computer Systems \(TOCS Article 4 11] A. Bialecki, M. Cafarella, J. Charron, D. Cutting, O. Gospodnetic, D. Guney, P. Kosiorowski, D. Kubes, C.A. Mattmann S. Siren, and J. Xing. \(2009  able: http://lucene.apache.org/nutch/, 2010/15/01 12] Multiple authors \(2009  http://crawler.archive.org/, 2010/15/01 13] C. Aflori, and M. Craus, Grid implementation of the Apriori algorithm, in Engineering Software, Volume 38, Issue 5, May 2007, pp. 295300 14] R. Agrawal, H. Mannila, R. Srikant, H. Toivonen A.I. Verkamo, Fast discovery of association rules, in U.M. Fayyad, G. Piatetsky-Shapiro , P. Smyth, R. Uthurusamy eds Press, 1996, pp. 307328 189 


Judging by common sense, the High threshold is defined as follows: for sections with one lane , if there exist 2 or more cars in 10 meters, then it is a High volume situation. On the other hand, the middle threshold is defined as follows: for sections with one lane, if there exist more than 1, and less than 2 cars in 10 meters, it is a Middle volume situation. The remains will be discretized to Low traffic volume situation The cars are randomly generated based on the values of the pre-defined OD\(OringinJDestination for every OD pair is changing during the execution as shown in Fig.6 lY l Jl .? 15 co ? 10 Xl 5 Q O .r:: 0 o 50 100  150 T:in e 4n ilutes Fig. 6. Change of OriginiDestination values in simulations B. Simulation Result The parameter setting of the proposed data mining during the evolution is shown in Table IV. The total number of rules stored in the rule pool is shown in Fig.7. Each round has the same number of generations of 50 and the chosen set size for AAM is 100[14 TABLE IV PARAMETER SETTING FOR EVOLUTION Items Values Number of judgment nodes 100 Number of processing nodes 10 N umber of attributes 500 Number of consequents 1500 Number of time points 120 Minimum confidence value 0.9 Minimum chi-squared value 6.63 Minimum support value 0.05 40000 Ul 35000 !Il " 30000 '" .... "0 25000 20000 ? '" 15000 j9 10000 0 5000 E 


r r  r r l o 100 200 300 400 500 ROlU1ds Fig. 7. Total number of rules extracted In order to check the effectiveness of the extracted rules we tested the classification accuracy of the proposed method using the classifier with competition The average prediction accuracy is shown in Table V. The accuracy is defined in the following: if the traffic prediction result of the section at time t is "Low" and the real traffic of this section at time t is exactly "Low", then the accuracy is 100%. The low/middle/high accuracy means the accuracy when the real traffic is low/middle/high, respectively Table V shows that the method with Accuracy Validation can improve the overall accuracy than the method without Accuracy Validation. The Middle volume of the traffic network can not be predicted accurately because the middle situations do not appear frequently enough to generate an enough number of association rules TABLE V AVERAGE PREDICTION ACCURACY FOR TESTING DATABASE Method Prediction Accuracy Overall Low Middle High With Accuracy Validation 84.82 85.84 57.71 89.71 Without Accuracy Validation 82.64 83.57 56.57 87.13 Longer step prediction is explored by studying the 2-step, 3step and 4-step prediction, where n-step means the prediction of the traffic volume at n time points later. It's results are shown in Fig. 8. It is shown from Fig.8 that the prediction accuracy decreases as the number of prediction step increases but the increase of the number of steps does not affect the overall accuracy so much, so the proposed method can do relative stable prediction even if the number of prediction steps Increases The ratio of the usable rules to the total number of rules in each prediction step is shown in Fig.9, which describes that 


how the number of the usable rules for prediction decreases by the increase of the prediction steps, considering the condition that the time delay between the antecedent part and consequent part should be bigger or equal to the prediction step. In another word, as the prediction step increases, the number of the usable rules decreases 86 if G 84 82 80 f-------?-""":__---------1 0:: 78 r-------------">.-======---1 :? 76 -------------------j 74 72 4 n-6tep I?w ihAcCUlacyValiiarnn -w ihoutAcCUlacyValiiarnn I Fig. 8. Overall accuracy for 2-step, 3-step and 4-step prediction The proposed method cannot extract all the rules meeting the given definition of importance since it uses the fixed 2254 120 II 100 80 "'<Ie o 60 ?? ? 0: 40 20 0 N Step Fig. 9. Average percentage of usable rules for each prediction step number of rules for each class of the consequent attributes but the result shows that the ability to extract important rules is sufficient enough for the purposes. The mechanism of Accuracy Validation shows more stable performances as shown in Fig. 8 V. CONCLUSION In this paper, an association rule mining method using GNP with Accuracy Validation mechanism has been proposed. It is clarified from simulations that the proposed method can extract important time-related association rules for each class of the consequent attributes efficiently. What's more important is that these rules can be used to decide to which class the time related data belong accurately. From simulations, we have also 


found that the proposed method can be used in traffic volume prediction problems. Further improvements of the proposed method is studied in terms of applying the proposed method to real world navigation systems REFERENCES I Vehicle Routing and Congestion Predictions for Real-time Driver Guid ance, Transportation Research Records, 1408, Transportation Research Board, Washington D.C., pp. 66-74, 1993 2 October 17, 1989 3 network predictions, In Proc. of the Conference of Canadian Society of Civil Engineers, Sherbrooke, Quebec, June, pp. 331-339, 1997 4 dynamic traffic networks with joint route and departure time choice. In Proc. of the 84th Annual Meeting of the Transportation Research Board Washington, DC., 2005 5 Springer, 2002 6 Related Sequential Association Rule Mining and Traffic Prediction", In Proc. of the IEEE Congress on Evolutionary Computation 2009, 2009/5 7 gorithm: Genetic Network Programming\(GNP Reinforcement Learning", Evolutionary Computation, MIT press, Vol. IS No.3, pp.369-398, 2007 8 Multiagent Models Based on Symbiosis", IEEE Trans. on Syst., Man and Cybernetics - Part B -, Vol.36, No.1, pp.179-193, 2006 9 Deck Elevator Group Supervisory Control System Using Genetic Network Programming, IEEE Trans. on Systems, Man and Cybernetics, Part C, Vol 38, No. 4, pp. 535-550, 200817 10 University of Michigan Press, 1975 11] D. E. Goldberg, Genetic Algorithm in search, optimization and machine learning, Addison -Wesley, 1989 12 means of natural selection, Cambridge, Mass., MIT Press, 1992 13 Programs, Cambridge, Mass.: MIT Press, 1994 


14 Association Rules Mining with Attribute Accumulation Mechanism and its Application to Traffic Prediction", Journal of Advanced Computational Intelligence and Intelligent Informatics, Vol. 12, No. 5, pp. 467-478 200817 15 Information and Communication System", Vehicle Navigation and Infor mation Systems Conference, 1993 2255 


intend to expand this work to involve some interesting features in each stage prediction and evaluate it on many datasets   REFERENCES  1] F. Ricardo, N. Ana, M. Paula, B. Gleidson, R. Fabiano ODE: Ontology-based software Development Environment, Proceedings of the IX Argentine Congress on Computer Science, pp. 1124-1135, 2003 2] E. Mendes, B. A. Kitchenham. Further comparison of cross-company and within-company effort estimation models for Web applications. In: Proc. 10th IEEE International Software Metrics Symposium, Chicago USA, pp.348-357, 2004 3] B. Boehm, R. Valerdi. Achievements and Challenges in Software Resource Estimation, Proceedings of ICSE 06 Shanghai, China, pp. 74-83,  2006 4] K. Molokken, M. Jorgensen. A review of software surveys on software effort estimation, Proceedings of International Symposium on Empirical Software Engineering \(ISESE 2003 5] M. Jorgensen, K. Molokken-Ostvold. How large are software cost overruns? A review of the 1994 CHAOS report, Information and Software Technology, Vol. 48 issue 4. PP. 297-301, 2006 6] X. Huanga, D. Hob, J. Rena, L. F. Capretz. Improving the COCOMO model using a neuro-Fuzzy approach Applied Soft Computing, Vol.7, issue 1, pp. 29-40, 2007 7] L. Briand, T. Langley, I. Wieczorek. A replicated assessment and comparison of common software cost modeling techniques, Proceedings of the 22nd international conference on Software Engineering, 2000 8] S.-J Huang, N. H. Chiu. Optimization of analogy weights by genetic algorithm for software effort estimation Information and Software Technology, Vol. 48, issue 11 pp. 1034-1045, 2006 9] Z. Xu, T. M. Khoshgoftaar. Identification of Fuzzy models of software cost estimation, Fuzzy Sets and Systems, Vol. 145, issue 1, pp. 141-163, 2004 10] R. Pressman. Software Engineering: practitioner 


approaches, McGraw Hill, London, 2004 11] M. Boraso, C. Montangero, H. Sedhi. Software cost estimation: an experimental study of model performance Universita di Pisa, Italy, 1996 12] Y. Wang, Q. Song, J. Shen., 2007. Grey Learning Based Software Stage-Effort Estimation. International Conference on Machine Learning and Cybernetics, pp 1470-1475, 2007 13] S. G. MacDonell, M. J. Shepperd. Using prior-phase effort records for re-estimation during software projects Ninth International, Software Metrics Symposium, pp 73- 86, 2003 14] M .C Ohlsson, C. Wohlin. An Empirical Study of Effort Estimation during Project Execution, Sixth International Software Metrics Symposium \(METRICS'99 1999 15] N. H. Chiu,,S. J. Huang.  The adjusted analogy-based software effort estimation based on similarity distances Journal of Systems and Software, Vol. 80, issue 4, pp 628-640, 2007 16] P. Sentas, L. Angelis, I. Stamelos, G.  Bleris. Software productivity and effort prediction with ordinal regression Information and Software Technology, Vol. 47, issue 1 pp. 17-29, 2005 17] E. Mendes, N. Mosley. Comparing effort prediction models for Web design and authoring using boxplots Australian Computer Science Communications,  Vol. 23 Issue 1, pp. 125-133, 2001 18] E. Mendes, N. Mosley, I. Watson. A comparison of casebased reasoning approaches, Proceedings of the 11th international conference on World Wide Web, pp. 272280, 2002 19] Q. Zhao, S. S. Bhowmick. Association Rule Mining: A Survey  http://citeseer.ist.psu.edu/734613.html, 2003 20] S. Morisak, A. Monden, H. Tamada. An Extension of association rule mining for software engineering data repositories, Information Science Technical Report NAIST, 2006 21] Q. Song, M. Shepperd.  M. Cartwright, C. Mair. Software defect association mining and defect correction effort prediction, IEEE transaction on software engineering Vol. 32, No.2, pp. 69-82, 2006 


22] R. Agrawal, T. Amielinski, A. Swami. Mining association rule between sets of items in large databases Proceedings of the ACM SIGMOD International Conference on Management of Data, pp. 207-216, 1993 23] M-J. Huang, Y-L. Tsou, S-C. Lee.  Integrating Fuzzy data mining and Fuzzy artificial neural networks for discovering implicit knowledge, J. Knowledge-Based Systems, Vol.19 \(6 24] ISBSG International Software Benchmarking standards Group, Data repository release 10, Site http://www.isbsg.org, 2007 25] L. Zadeh. Toward a theory of Fuzzy information granulation and its centrality in human reasoning and Fuzzy logic. J. Fuzzy sets and Systems 90, pp. 111-127 1997 26] I. H. Witten, E. Frank. Data Mining: Practical machine learning tools and techniques, 2nd Edition, Morgan Kaufmann, San Francisco, 2005   256 


encountering a related term, i.e. IC\(c e intuition behind the use of the negative likelihood is that the more probable a term to appear, the less information it conveys. All these features show that Jiangs measure tends to be more general and more appropriate for evaluating nontaxonomically related terms. Indeed, a high score of the relatedness measures suggests a strong relationship between terms Nevertheless, all relatedness measures have limitations because they assume that all the semantic content of a particular term is modeled by semantic links in WordNet Consequently, in many situations, truly related terms obtain a low scores even though their belongings to a certain category of tags, e.g., jargon tags Additionally, when measuring the quality of an automatically knowledge acquisition results, the typical measures used in Information Retrieval are Recall, Precision and F-Measure. However, computing Recall and F-Measure requires the availability of a Gold Standard. Hence, we will only compute the Precision which speci?es to which extent the non-taxonomic relationships is extracted correctly. In this case, the ratio between the correctly extracted relations i.e., their relatedness measures is greater than or equal to a minimum threshold, and the whole number of extracted ones is computed. Thus, we have Precision Total correctly selected entities Total selected entities 12 http://search.cpan.org/dist/WordNet-Similarity 13 A term refers to a tag subject or a tag object C. Evaluation of non-taxonomic relationships Only a percentage of the full set of non-taxonomic relationships \(89 is caused by the presence of non standard terms which are not contained in WordNet and, in consequence, cannot be evaluated using WordNet-based relatedness measures. Fig. 5 depicts the evaluation results of the extracted non-taxonomic relationships against their relatedness measures High relatedness score \(88 17% of the extracted relationships, as most of terms are strongly related with respect WordNet Null Scores were obtained for 5% of the extracted 


relationships. Analyzing this case in more detail, we have observed that the poor score is caused in many situations by the way in which Jiangs distance metric works. This latter completely depends on the distance between two terms based on the number of edges found on the path between them in WordNet. In consequence this measure returns a value that does not fully represent reality. For example, on the one hand, Jiangs distance metric returns a null value for the relationship between insurance and car, even though the ?rst is a commonly related to the second, i.e., car involved insurance Finally with a minimal Jiangs distance metric threshold, set to 46%, the computed precision of correctly extracted relationships candidates is equal to 68.8 An example of extracted non-taxonomic relationships is depicted in Table V where each relation describes the subject tag, e.g., tool, the predicate, e.g is being developed within, and the object tag, e.g mesh. Fig. 4 represent a fragment output of the extracted ontological structure where each concept de?nes a set of similar and synonym tags and labels, i.e., mentions has been, revealed, caused and is created with describe the predicates of the non-taxonomic relationships between terms Due to the limitations observed by the automatic evaluation procedure and the lack of gold standards containing non-taxonomic relationships, we have examined the extracted non-taxonomic relationships from a linguistic point 377 Top space      distance     quad great     groovy nifty caused address      addresses extension      quotation   reference  references extensions        referenz     source      refrence sources    rfrences    quotations research    search     searching searchs open-source     open_source 


opensource linux aim     design     designer      designers patern    project     patterns     projekte projects web+design    web_design webdesign internet       internetbs net          web network      networking networks      web discussion     news       password word      words community      communities is_created_with mentions revealed has_been Figure 4. A fragment output of the extracted ontological structure of view. This qualitative evaluation can bring some interesting insights about the kind of results one can expect Invalid relations are extracted: Even though a relation such as music cities skill is considered as correct one since tag subject, tag object and predicate are correctly extracted. From a semantic point of view, this relation has no meaning. Hence, a higher precision is expected Figure 5. Summary of non-taxonomic evaluation measure Table V EXAMPLES OF EXTRACTED NON TAXONOMIC RELATIONSHIPS Subject Predicate Object search has been reference reference mentions search tool is being developed within mesh security added encoding search revealed reference java provides library by performing the sense analysis on complete relations An ambiguity in the extracted predicates between terms is observed: Hence, same relations are redundant since they use a synonym predicates between terms, e.g java provides library and java yields library. Thus we expect that the redundancy removal within extracted relations will be of bene?t for the improvement of the 


obtained results VI. CONCLUSION AND FUTURE WORK The extraction of non-taxonomic relationships from folksonomies is to the best of our knowledge is the least tackled task within ontology building from folksonomy. This is why there is a need of novel and general purpose approaches covering the full process of learning relationships. In this paper, we introduced a new approach called NONTAXFOLKS that starts by pre-processing tags aiming at getting a set of frequent tagsets corresponding to an agreed representation Then, they are used to retrieve related tags using external resources such as WordNet. Thanks to the particular structure of triadic concepts, it allows grouping semantically related tags by considering the semantic relatedness embodied in the different frequencies of co-occurences among users, resources and tags in the folksonomy. Thereafter we introduced an algorithm called NTREXTRACTION for extracting non-taxonomic relationships between pair of tags picked from the triadic concepts. In summary, our approach uses several well known techniques \(such as formal concept analysis or association rule discovering the social bookmaring environnement in order to propose a new way of extracting labeled non-taxonomic relationships between tags. Currently, we are investigating the following topic concerning the discovered predicates between two terms. Indeed, in order to avoid relationships redundancy and thus a redundancy in the builded ontology. One can try to classify them into prede?ned semantic classes, detect synonyms, inverses, etc. A standard classi?cation of verbs could be used for this purpose, adding additional information about the semantic content, e.g., senses, verb types, thematic roles, etc., of predicates relationships 378 REFERENCES 1] J. Pan, S. Taylor, and E. Thomas, Reducing ambiguity in tagging systems with folksonomy search expansion, in Proceedings of the 6th Annual European Semantic Web Conference \(ESWC2009 2] V. S. M. Kavalec, A. Maedche, Discovery of lexical entries for non-taxonomic relations in ontology learning, in Proceedings of the SOFSEM 2004, LNCS, vol. 2932, 2004, pp 249256 


3] L. Specia and E. Motta, Integrating folksonomies with the semantic web, in Proceedings of the 4th European Semantic Web Conference \(ESWC 2007 Innsbruck, Austria, vol. 4519, June 2007, pp. 624639 4] P. Mika, Ontologies are us: A uni?ed model of social networks and semantics, in Proceedings of the 4th International Semantic Web Conference \(ISWC2005 3729, Galway, Ireland, June 2005, pp. 522536 5] P. Schmitz, Inducing ontology from ?ickr tags, in Proceedings of the Workshop on Collaborative Tagging \(WWW 2006 Edinburgh, Scotland, May 2006 6] M. Zhou, S. Bao, X. Wu, and Y. Yu, An unsupervised model for exploring hierarchical semantics from social annotations, in Proceedings of the 6th International Semantic Web Conference and 2nd Asian Semantic Web Conference ISWC/ASWC2007 Korea, vol. 4825, November 2006, pp. 673686 7] C. Schmitz, A. Hotho, R. Jaschke, and G. Stumme, Mining association rules in folksonomies, in Proceedings of the 10th IFCS Conference \(IFCS 2006 2006, pp. 261270 8] A. Hotho, A. Maedche, S. Staab, and V. Zacharias, On knowledgeable unsupervised text mining, in Proceedings of Text Mining Workshop, Physica-Verlag, 2003, pp. 131152 9] A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme, Information retrieval in folksonomies: Search and ranking, in The Semantic Web: Research and Applications, vol. 4011 Springer, 2006, pp. 411426 10] F. Lehmann and R. Wille, A triadic approach to formal concept analysis, in Proceedings of the 3rd International Conference on Conceptual Structures: Applications, Implementation and Theory. Springer-Verlag, 1995, pp. 3243 11] R. Jaschke, A. Hotho, C. Schmitz, B. Ganter, and G.Stumme Discovering shared conceptualizations in folksonomies Web Semantics: Science, Services and Agents on the World Wide Web, vol. 6, pp. 3853, 2008 12] A. Mathes, Folksonomies - cooperative classi?cation and communication through shared metadata, Graduate School of Library and Information Science, University of Illinois Urbana-Champaign, Tech. Rep. LIS590CMC, December 2004 13] H. Lin, J. Davis, and Y. Zhou, An integrated approach 


to extracting ontological structures from folksonomies, in Proceedings of the 6th European Semantic Web Conference ESWC 2009 vol. 5554, 2009, pp. 654668 14] M. Szomszor, H. Alani, K. OHara, and N. Shadbolt, Semantic modelling of user interests based on cross-folksonomy, in Proceedings of the 7th International Semantic Web Conference \(ISWC 2008 15] G.Begelman, P. Keller, and F.Smadja, Automated tag clustering: Improving search and exploration in the tag space, in Proceedings of the the Collaborative Web Tagging Workshop WWW 2006 16] R. Jaschke, A. Hotho, C. Schmitz, B. Ganter, and G. Stumme TRIAS - an algorithm for mining iceberg tri-lattices, in Procedings of the 6th IEEE International Conference on Data Mining, \(ICDM 2006 2006, pp. 907911 17] C. Borgelt, Ef?cient implementation of APRIORI and ECLAT, in FIMI, COEUR Workshop Proceedings, COEURWS.org, vol. 126, 2003 18] J. Tang, H. Leung, Q. Luo, D. Chen, and J. Gong, Towards ontology learning from folksonomies, in Proceedings of the 21st international jont conference on Arti?cal intelligence IJCAI 2009 20892094 19] L. Ding, T. Finin, A. Joshi, R. Pan, R. Cost, Y. Peng P. Reddivari, V. Doshi, and J. Sachs, Swoogle: A search and metadata engine for the semantic web, in Proceedings of the 13th ACM Conference on Information and Knowledge Management, ACM Press, 2004, pp. 652659 20] A. Hliaoutakis, G. Varelas, E. Voutsakis, E. Petrakis, and E. E Milios, Information retrieval by semantic similarity, International Journal on Semantic Web and Information Systems IJSWIS 21] G. Pirro, M. Ruffolo, and D. Talia, Secco: On building semantic links in peer to peer networks, Journal on Data Semantics XII, LNCS 5480, pp. 136, 2009 22] C. Meilicke, H. Stuckenschmidt, and A. Tamilin, Repairing ontology mappings, in Proceedings of the International Conference AAAI 2007, Vancouver, British Columbia, Canada 2007, pp. 14081413 23] S. Ravi and M. Rada, Unsupervised graph-based word sense 


disambiguation using measures of word semantic similarity in Proceedings of the International Conference ICSC 2007 Irvine, California, USA, 2007 24] H. G. A. Budanitsky, Semantic distance in wordnet: an experimental application oriented evaluation of ?ve measures in Proceedings of the International Conference NACCL 2001 Pittsburgh, Pennsylvania, USA, 2007, pp. 2934 25] J. Jiang and D. Conrath, Semantic similarity based on corpus statistics and lexical taxonomy, in Proceedings of the International Conference ROCLING X, 1997 379 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


