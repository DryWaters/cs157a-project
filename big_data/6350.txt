 
     
       
   
  
 
 
  


            


        


               


            


                   


 Algorithm 1 Association rule similarity method Require association rules r 1  2 1 dependencies 1  2  list    2 for event  r 1  2 do 3 link 1  2  list    4 for value  event do 5 link 1  2   attributes 1  attributes 2   link 1  2   6 end for 7 dependencies 1  2  sort  link 1  2   dependencies 1  2 8 end for 9 result 1  2   10 for link 1  2  dependencies 1  2 do 11 if link 1  2  result 1  2 then 12 result 1  2  link 1  2  link 1  2 1 13 else 14 result 1  2  link 1  2  1 15 end if 16 end for 17 final  list    18 count  0 19 for link 1  results 1 do 20 if link 1  results 2 then 21 m 1  min results 1  link 1  results 2  link 1  22 m 1  max results 1  link 1  results 2  link 1  23 final  final  min m 1   max m 2  24 delete result 2  link 1 25 else 26 final  final  0 27 end if 28 count  count 1 29 end for 30 for link 1  sequence   results 1   count  do 31 final  final  0 32 end for 33 for link 2  results 2 do 34 final  final  0 35 end for lative dierence of 12  22  0  54 Thatspeciìclinkisthen removed from the dependency list of the comparison workload If the link is not found then a penalty of 0 is applied These 0s directly reduce the similarity value During this loop the number of evaluated links is counted The nal step penalises leftover rules that were not used in the previous step From the starting workload r 1 all leftover links are penalised by adding 0s to the result set as counted from the number of previously evaluated links Any leftover links from workload r 2 are also directly penalised by adding 0s to the result set The method nishes with a list of relative dierences and penalties The algorithm returns with the similarity measure m a  r 1 r 2   mean  final ycalculating the arithmetic mean over all elements in the multi-set 4.3 Evaluation We evaluate the association rule method with two synthetic data sets each having 5 descriptive attributes 4.3.1 Methodology Again each data set is split into 8 distinct segments that are compared to each other yielding a sample size of 28 comparisons For one data set the workloads are created with round-robin selection of 1024 distinct values for each attribute per segment repeated 2 16 times This creation guarantees that dependencies stay consistent and should result in m a  1 by the association rule method The other data set selects uniformly distributed random values between 0 and 1024 for each attribute repeated 2 16 times Random data provides the practical lower limit of the method Again the evaluation of the method is subject to random number generation as mentioned in section 3 4.3.2 Results As expected the association rules found in the roundrobin workload scored a similarity value of m a 1 The method thus is able to nd consistent repeatable behaviour perfectly On the random uniform workload though the method scored a similarity value of m a 0 Avalueof0 was slightly unexpected because we assumed uniform random data to produce at least some usable rules However the apriori algorithm did not consider any of the random attribute dependencies as frequent enough Once we reduced the number of distinct values from 1024 to only 16 the apriori algorithm was able to nd 3 to 5 rules This reduction resulted in a low similarity value of m a 0  13 However note that xed values for support and conìdence are responsible for this result As we see later this problem does not exist in large-scale workloads as the possible permutations of attributes and events produce a much larger sample space As we are not aware of any other method that automatically quantiìes the dependencies of association rules we have no ground truth or competitors against which we can compare our methodês eectiveness With a practical result of 0  13  m a  1 we are quite conìdent that the method is applicable to many workloads because it covers the possible range of similarity values to a high degree Nevertheless while the association rule measure on its own is not a good choice for a similarity measure it does play the key role in improving the results attainable by our wavelet method 5 COMPOUND MEASURE We combine the wavelet measure and the association rule measure into a compound measure and evaluate its accuracy on operational workloads and with a hypothesis test 5.1 Combining the two methods The combination is based on the concept of transfer functions  where outputs of one measure are weighted according to the outputs of another measure This was shown to be superior to simple linear combinations of measures  I n our case we deìne our transfer function to weight the coefìcients of the wavelet transform before the intermediary result is calculated The idea is that coecients that involve events with attributes from the association rule measure should be weighted according to the inîuence of the attribute Finding these events is trivial because the length of the wavelet is given and the involved attributes were already found via the association rule measure A simple match on event and attribute can thus trigger the weighting and use the weighting value of an attribute f attribute  final  m a  t 1 DWT X 1 w min l 1 l 2   f attribute t 2 DWT X 2 w min l 1 l 2   f attribute 5 These t 1 t 2 arethenusedinsteadof x 1 x 2 to nish the wavelet method calculation However we call this result then our compound similarity measure 0 m 1 


 Data set Events Attr Intvl Intvl AuverGrid 404176 7 12 months DAS-2 1124772 7 22 months Gridê5000 1020195 7 31 months LCG 188041 4 11 days NorduGrid 781370 5 38 months SHARCNET 1195242 4 13 months DQ2 965519 7 24 hours Table 1 Overview of the GWA and DQ2 workloads 5.2 Methodology We evaluate the accuracies of all methods against two scheduling metrics mean waiting time and mean execution time  The premise of this evaluation is as follows if input workloads are similar then the performance metrics that evaluate the output of the system that is executing the workloads should be similar as well An elegant way of doing that is to use scheduling metrics b ecause they can b e principal targets for performance optimisation in a system Two workloads that produce the same waiting times or the same execution times can be considered similar in performance evaluation Many more dierent scheduling metrics can be used if necessary e.g mean resource utilisation or link throughput but for this evaluation we narrow ourselves to mean waiting time  W andmeanexecutiontime  E  The objective is to minimise the error between a similarity measure and the output metrics For example two workloads that dier by 0.3 in their relative waiting times should also yield a similarity value of 0.3 The normalised errors between two workloads w 1 w 2 are thus error  W   similarity w 1 w 2      W w 1   W w 2    error  E   similarity w 1 w 2      E w 1   E w 2    6 We use all the available workloads provided by the Grid Workloads Archive GWA  as w e ll as a w orkload from DQ2  T he GW A w orkloads a re pro vided i n a common trace format for the following large-scale distributed systems AuverGrid Distributed ASCI Supercomputer 2 DAS2 Gridê5000 LHC Computing Grid LCG NorduGrid and SHARCNET These workloads all stem from scientiìc computational traces DQ2 workload stems from scientiìc distributed data management traces N ote t hat D Q2 uses LCG resources but its traces are sampled independently and separately from the LCG traces in the GWA Table 1 gives an overview of the contents of the data sets that store the workload It shows the number of events the number of attributes and the segments per data set DQ2 generates as many events in one third of a single day as the other systems in multiple years It is because of this imbalance that we decided to keep the number of events roughly equivalent and not the actual time spent in the workload Note that the actual number of events is not relevant because the association rule method will only consider the most important constituent events and attributes Additionally the original workload traces contained many dierent attributes but we are only using attributes with a cardinality greater than one i.e multiple dierent values of the attribute are used throughout the workload The inclusion of attributes that stay constant during the workload would yield no additional information to the attribute analysis i.e trivial association rules and such information could be modelled statically anyway The apriori algorithm thus takes care of selecting proper attributes and events and the analyst does not need to select sets of attributes manually Lastly we only use attributes that are part of the workload deìnition e.g submission time or user identiìcation  We are not using output attributes like completion time because they are not part of the workload they are the result of the workload being executed by the system We split each data set into distinct segments that we compare against each other thus yielding sample sizes between 55 and 703 comparisons depending on the number of intervals in each workload In total there are 1874 possible permutations Each workload segment is then separated into two distinct parts a time series signal for the submission time attribute and a list of attribute tuples from descriptive attributes The time series signal is created by aggregating the count of submission times in the workload into cells appropriate for the workload We chose hourly daily and monthly cells as indicated in table 1 with the corresponding number of segments We obtain all possible error  W and error  E by calculating all similarity measures as well as  W  E for all segments in the workloads from the GWA that have this information This includes AuverGrid DAS-2 Gridê5000 SHARCNET In DQ2 workload the  W  E are staging and transfer times respectively and are calculated in exactly the same way 5.3 Results Figure 5 shows the error results over all evaluated workloads Figure 5a focuses on the errors of all methods against  W  There the compound measure is more accurate than all competing methods The interesting observation is that both constituent methods have a higher error but their combination as a compound measure reduces error drastically Especially the association rule measure cannot be used on its own Against the best case i.e the moving average the improvement in accuracy is 12.2 Against the worst case i.e the discrete Fourier transform the improvement is 24.5 This shows how the inclusion of a method for descriptive attributes is highly relevant in addition to time-dependent attributes and should not be ignored The wavelet method however is responsible for reducing standard deviation between 4.6 and 10.4 showing the advantage of a multi-resolution approximation free approach Figure 5b focuses on the errors of all methods against  E  Here the improvement in accuracy for the compound method is only 3.8 against the best case i.e again the moving average and 20.8 against the worst case i.e the Kalman lter Also the improvement in standard deviation is slightly less with 0.3 to 7.6 It is interesting though that the ltering methods also have a slight advantage in this case We have the following explanation for these results The waiting time is largely dependent on the number of already existing events in a system Therefore the speciìcs of a new event are important for its proper scheduling e.g which resource to use These speciìcs are stored in the descriptive attributes and can be found by the association rule method Like a scheduler uses this additional information to make an informed choice where and when to execute the event so does the association rule method evaluate the importance of the attributes This is less obvious for the execution time 


                    NRMSE Moving Average Exp Smoothing Kalman Filter Fourier Transform Wavelet MK-Wavelet Association Rules C ompound Measure 0.0 0.2 0.4 0.6 0.8 1.0 normalised error a Similarity error against Mean Waiting Time                    NRMSE Moving Average Exp Smoothing Kalman Filter Fourier Transform Wavelet MK-Wavelet Association Rules Compound Measure 0.0 0.2 0.4 0.6 0.8 1.0 normalised error b Similarity error against Mean Execution Time  Figure 5 Normalised errors of all methods against mean waiting time and mean execution time Lower is better Method  p value  p   0  50  25 0  125 NRMSE 0.38 510 224 3 Mov Avg 0.22 1120 504 22 Exp Smooth 0.36 488 252 1 Kalman 0.30 693 360 12 Fourier 0.36 589 252 7 Wavelet 0.21 1412 522 65 MK-Wavelet 0.22 1344 504 68 Ass Rules 0.25 640 450 5 Compound 0.20 1510 558 182 Table 2 Two-sample Anderson-Darling test per method for all 1874 pairwise permutations of segments Results are the number of times the null hypothesis could not be rejected where scheduling already took place and the descriptive attributes are less useful However the timing of the event is more important which is properly modelled through the wavelet methods Consequently the wavelet methods have less error and less variance than all other methods in this case A wavelet-based method biased by a transfer function is therefore a good choice in both the waiting and execution time cases In the case of our scientiìc workloads the association rule measure only has a signiìcant impact when the descriptive attributes inîuence the system We can conìrm this for DQ2 workload from our own operational experience 5.4 Hypothesis test We complement the accuracy evaluation with a statistical hypothesis test The two-sample Anderson-Darling test can validate if the empirical distribution functions of two independent samples follow from the same distribution It is a non-parametric k-sample test that does not make assumptions about the distribution function and is therefore an ideal candidate for cases without ground truth Our null hypothesis is that both samples come from the same distribution i.e the similarity error is minimal We prepare all 1874 workload segments by normalising their time-dependent attribute to a relative start of zero Additionally we replace all values in the descriptive attributes into unique numerical representations for easy calculation of an attributeês empirical distribution Then we evaluate all segments with each available method and present the results in table 2 For each method we give the mean P-value of the test and the number of times we could not reject the null hypothesis for a given statistical signiìcance   We want this number to be as close as possible to the maximum of 1874 The higher the number the more often the particular method identiìed two segments as similar which corresponds to a lower error for the similarity measure The results conìrm our previous performance metrics evaluation At a signiìcance of  0  5 the results show four good candidates our compound measure the two wavelet methods and the moving average These results are also true for error  W and error  E in the metrics evaluation The same observation still holds at  0  25 but only at  0  125 we can see the clear advantage of the compound measure Whereas all other methods mostly reject the null hypothesis the compound measure still provides an acceptable result at about 11 with 182 non-rejects Again as in the metrics evaluation the association rule method by itself is seemingly useless but in combination with the wavelet method provides a signiìcant boost to error reduction 6 CONCLUSIONS The performance evaluation of a system strongly depends on the input workload If fundamentally dierent workloads are used for evaluations then conclusions drawn from the results are likely to be non-representative Therefore we propose a compound measure to quantify the similarity between two workloads This compound measure comprises two independent methods the rst one to analyse the timedependent attribute in the workload and the second one to analyse the descriptive attributes in the workload The rst method uses the discrete wavelet transform to derive and compare components that describe the periodic time and frequency behaviour of the time-dependent attribute The novel idea of this approach is that we take advantage of the property of the inverse discrete wavelet transform that guarantees that the original signal can be reconstructed from the scaled coecients This idea improves upon exist 


ing work because the approach is free from any assumptions on the structure of the attribute and does not have to rely on statistical approximations We evaluate the approach using two synthetic data sets to establish the upper and lower bounds of the covered similarity space We nd that we can cover the whole similarity space and that we are only constrained by the random number generation To validate the method against this constraint we investigate the speciìc inîuence of the random number generation by using the method as a test of randomness Additionally operational systems usually exhibit large amounts of noise in their workload therefore we validate the method against noise as well Our ndings show that it is highly resistant to noise whereas other commonly employed methods yield to the law of large numbers or fail completely Additionally the method can consistently identify dissimilar behaviour as well even though it is not as good at that task which is only partially true for other commonly employed methods The second method uses association rule analysis to identify and quantify the relationship between descriptive attributes of the workload The novel idea of this approach is to eliminate the use of values and instead focus on the attributes themselves and therefore rank the relative usage of the attribute That way the most important building blocks of the workload can be compared directly The algorithm is described and then evaluated using synthetic data sets to establish the upper and lower bounds of the covered similarity space This time we nd that we are only constrained by the amount of learnt rules which follows from the used rule learning algorithm At least 16 rules need to be available to cover the whole similarity space when the apriori algorithm is used The method itself however does not require a speciìc rule learning algorithm We then present our compound measure that can address problematic workload characteristics We use the transfer function concept to weight speciìc events in the waveletmethod based on relative attribute dependencies Then we conduct an empirical study to evaluate all methods on operational workload from seven large-scale distributed systems Two important characteristics stand out First the time and frequency behaviour is surprisingly well-modelled with a wavelet method Second the association rule by itself is seemingly useless However the inclusion of descriptive attributes improves accuracy when determining similarity of workloads We show that the compound measure improves upon existing work by evaluating it against two important scheduling metrics mean waiting time and mean execution time The analysis of descriptive attributes for the similarity in addition to the analysis of the time-dependent attribute yields a compound similarity measure that can improve accuracy by 24.5 At the same time the standard deviation can be reduced by 10.4 We conìrm these results with an independent statistical hypothesis test a twosample Anderson-Darling statistic and show the advantage of the compound measure even at higher signiìcance levels of  0  125 with only 11 dierence Our results strongly reassure our initial expectations that the compound measure is a good choice for large-scale and data-intensive systems that have to deal with enormous amounts of events and previously unknown dependencies in their workload Furthermore even though we only used scientiìc workloads in our evaluations we did not observe any constraint that would limit the use to scientiìc workloads 7 FUTURE WORK As a result of this work we can continue to evolve our simulation eort for DQ2 We will use the similarity measure to evaluate the results of our future workload models to speciìcally address problems like data transfer cycles distributed le caching or popularity-based deletion However the measure can still be improved If it is suspected that dependencies change over time then the rate of change needs to be investigated Extending the association rule measure with time-evolving graphs may prove to be appropriate for such cases Additionally we only used the Haar wavelet as suggested by previous work We are interested in investigating the inîuence of dierent types of wavelets on the analysis of non-periodic burst behaviour An implementation of the similarity measure including already pre-processed versions of the used workloads from the evaluation can be downloaded from our website  8 ACKNOWLEDGEMENTS We are grateful to the following teams for providing traces via the Grid Workloads Archive The AuverGrid team the DAS-2 team the Gridê5000 and OAR teams the HEP eScience Group at Imperial College London for the LCG traces the NorduGrid team and J Morton and C Chrush for the SHARCNET traces 9 REFERENCES 1 R A g r a w a l T I m i e l i  nski and A Swami Mining association rules between sets of items in large databases ACM SIGMOD Record  22\(2 June 1993  L  B ergroth H Hak o nen and T  R aita A surv ey of longest common subsequence algorithms In 7th International Symposium on String Processing Information Retrieval  page 39 A Coru na ES Sept 2000 IEEE Computer Society  C  Borgelt and R  K ruse Induction of asso ciation rules apriori implementation In 14th Conference on Computational Statistics  pages 395Ö400 Berlin DE 2002 Physica  M  B ranco Distributed data management for large scale applications  PhD thesis School of Electronics and Computer Science University of Southampton UK Nov 2009  M  B ranco E Zalusk a D de Roure M  L assnig a nd V Garonne Managing very large distributed data sets on a data grid Concurrency and Computation Practice and Experience  22\(11 Aug 2010  R  G  B ro wn D  E ddelbuettel a nd D Bauer Dieharder A random number test suite http://www.phy.duke.edu rgb/General/dieharder.php Oct 2009  L  C  C arrington M Laurenzano A Sna v e ly  R  L  Campbell Jr and L P Davis How well can simple metrics represent the performance of hpc applications In ACM/IEEE International Conference for High Performance Computing Networking Storage and Analysis  Seattle WA USA 2005 ACM  G  C asale N Mi a nd E Smirni C WS a mo del-driv e n scheduling policy for correlated workloads In ACM SIGMETRICS International conference on 


measurement and modeling of computer systems  pages 251Ö262 New York NY USA June 2010 ACM  K  P  C han a nd A W.-C F u Ecien t t ime s eries matching by wavelets In 15th International Conference on Data Engineering  pages 126Ö133 IEEE 1999  C Chatìeld The analysis of time series an introduction  CRC Press 1997  I Daub ec hies Ten lectures on wavelets  Society for Industrial and Applied Mathematics 1992  P  A Dinda a nd D R OêHallaron H ost l oad prediction using linear models Cluster Computing  3\(4 2000  D F r eedman and P  D iaconis On the h istogram as a density estimator L2 theory Probability Theory and Related Fields  57\(4 1981  A Graps An in tro duction to w a v e lets IEEE Computational Science  Engineering  2\(2 1995  J Han J P e i Y Yin and R  M ao Mining frequen t patterns without candidate generation A frequent-pattern tree approach Data Mining and Knowledge Discovery  8\(1 2004  T Ho eîer T  S c hneider a nd A Lumsdaine Characterizing the inîuence of system noise on large-scale applications by simulation In ACM/IEEE International Conference for High Performance Computing Networking Storage and Analysis New Orleans LA USA 2010 IEEE Computer Society  A Iosup H Li M  J an S  A no ep C  Dumitrescu L Wolters and D H J Epema The grid workloads archive Future Generation Computer Systems  24\(7 July 2008  A Kramp e  J  L epping a nd W Sieb en A h y brid markov chain model for workload on parallel computers In ACM International Symposium on High Performance Distributed Computing  pages 589Ö596 Chicago IL USA June 2010 ACM  M Lassnig C ERN P H-ADP DDMLAB public website http://cern.ch/ddmlab-public April 2011  M Lassnig T  F ahringer V  G aronne A  M olfetas and M Branco Stream monitoring in large-scale distributed concealed environments In 5th IEEE International Conference on e-Science  pages 156Ö163 Oxford UK Dec 2009 IEEE Computer Society  M Lassnig T  F ahringer V  G aronne A  M olfetas and M Branco Identiìcation modelling and prediction of non-periodic bursts in workloads In 10th IEEE/ACM International Symposium on Cluster Cloud and Grid Computing  pages 485Ö494 Melbourne AU May 2010 IEEE Computer Society  H Li W orkload dynamics on clusters and g rids The Journal of Supercomputing  47\(1 2009  H Li and M  Muskulus Analysis and m o d eling o f j ob arrivals in a production grid ACM SIGMETRICS Performance Evaluation Review  34\(4 Mar 2007  U Lublin and D  G  F eitelson T he w o rkload on parallel supercomputers modeling the characteristics of rigid jobs Journal of Parallel and Distributed Computing  63\(11 Nov 2003  D P  Mandic a nd J A Cham b e rs Recurrent Neural Networks for Prediction Learning Algorithms Architectures and Stability  Wiley 2001  M Matsumoto a nd T Nishim ura M ersenne Twister a 623-dimensionally equidistributed uniform pseudo-random number generator ACM Transactions on Modeling and Computer Simulation  8\(1 Jan 1998  T N Minh L W o lters and D  E p e ma A realistic integrated model of parallel system workloads In 10th IEEE/ACM International Conference on Cluster Cloud and Grid Computing  pages 464Ö473 Melbourne AU May 2010 IEEE Computer Society  J C Mogul Emergen t mis eha v ior v s complex software systems ACM SIGOPS Operating Systems Review  40\(4 Oct 2006  K Mohror a nd K L Kara v a nic Ev aluating similarity-based trace reduction techniques for scalable performance analysis In 22nd Annual International Conference on Supercomputing  page 55 ACM 2009  Q P a n L Zhang G Dai and H  Z hang T w o denoising methods by wavelet transform IEEE Transactions on Signal Processing  47\(12 Dec 1999  P  Ratn F  Mueller B  R  d e Supinski a nd M Sc h u lz Preserving time in large-scale communication traces In 22nd Annual International Conference on Supercomputing  pages 46Ö55 ACM 2008  F W Sc holz and M  A  S tephens K sample anderson-darling tests Journal of the American Statistical Association  82\(399 1978  Z R Struzik and A  S ieb e s Principles of Data Mining and Knowledge Discovery  volume 1704 of Lecture Notes in Computer Science  chapter The Haar Wavelet Transform in the Time Series Similarity Paradigm pages 12Ö22 Springer 1999  E Theresk a and G  R  G anger IR ONMo del Robust performance models in the wild In ACM SIGMETRICS International conference on measurement and modeling of computer systems  pages 253Ö264 Annapolis MD USA June 2008 ACM  F W a silewski P yW a v elets Discrete W a v e let Transform in Python http://www.pybytes.com/pywavelets May 2010  G I W e bb OPUS A n e cien t admissible algorithm for unordered search Journal of Artiìcial Intelligence Research  3:431Ö465 1995  M J Zaki S calable a lgorithms f or asso ciation m ining IEEE Transactions on Knowledge and Data Engineering  12\(3 May 2000 


association rules and decision trees on analysis of diabetes data from the DiabCare program in France stud health technol inform 2002;90:557-61 6] J.Mondelle Simeon and Rober, Hilderman Exploratory Quantitative Contrast Set Mining:A Discretization Approach, 19th IEEE International Conference on Tools with Artificial Intelligence - Vol.2 ICTAI 2007 7] D.Newman, J. S.Hettich, C.L.S. Blake, and C.J. Merz, UCI Repository of machine learning databases,Irvine, CA: University of California, Department of Information and Computer Science.1998 last accessed: 1/10/2009 8] J.Han, and M.Kamber, Data mining: Concepts and techniques, San Francisco: Morgan Kaufmann Publisher, pp.47- 94, 2006 9] Glenn J. Myatt  Making sense of data: A Practical Guide to Exploratory Data Analysisand   Data Mining:Wiley\(2007 10] G. Chen, AND T.Astebro,  How to deal with missing categorical data: Test of a simple Bayesian method, Organ. Res. Methods 6, 3 2003 11] R.Agrawal, T. Imielinski, & A. Swami, Database mining aperformance perspective, IEEE Transactions on Knowledge and Data Engineering, 5\(6 1993 Special issue on Learning and Discovery in Knowledge-Based Databases 12] R.Agrawal, T.Imielinski, & A.Swami,Mining association rules between sets of items in large databases, In Proc. ACM-SIGMOD int. conf. management of data \(SIGMOD93 USA \(pp. 207216 13] Ian H.Witten and Elbe Frank, Datamining Practical Machine Learning Tools and Techniques, Second Edition, Morgan Kaufmann, .San Fransisco, 2005 14] S.Brin, R. Motwani, J.D. Ullman,  & S.Tsur, Dynamic itemset counting andimplication rules for market basket data, Proceedings of the ACM SIGMODInternational Conference on Management of Data pp. 255-264, Tucson, AZ, May 1997,ACM Press 15] M.J.Zaki, S. Parthasarathy, M. Ogihara, & W.Li, W. New algorithms for fast discovery of association rules, Proceedings of the 3rd International Conference on KnowledgeDiscovery and Data Mining \(KDD 1997,AAAI Press 16] B.Liu, W. Hsu, & Y.Ma, Pruning and summarizing the discovered association, Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 125-134 San Diego, CA, August 1999 


17] J.Han, J.Pei, & Y.Yin, Y,  Mining frequent patterns without candidate generation, Proceedings of the ACM SIGMOD International Conference on Management of Data,  Dallas, TX, May 2000 18] Y.Li, & L.Sweeney, Adding semantics and rigor to association rule learning: the GenTree approach, Technical Report CMU ISRI 05-101 2005 19] M.Rangsipan, Structure-Based Rule Selection Framework for Association Rule Mining of Traffic Accident Data, CIS 2006: 231239                                 


           334 


21] S. Baker and S.K. Nayar, A Theory of Catadioptric Image Formation, IEEE International Conference on Computer Vision \(ICCV pp.35-42, Jan, 1998 22] S.K. Nayar, Catadioptric Omnidirectional Cameras, IEEE Conference on Computer Vision and Pattern Recognition \(CVPR 488, Jun, 1997 23] A.Victorino, La commande referencee capteur: une approche robuste au proble`me de navigation, localisation et cartographie simultanees pour un robot dinterieur. PhD thesis, LUniversite de Nice-Sophia Antipolis, Inria Sophia Antipolis, 2002 3524 


ec  d Fig. 5: Computation Performance Comparison Tab. 4: Computation Savings by TOP-MATA K Connect K Retail K Wap La12 50 58.35% 100 0.01% 200 0.83% 23.04 150 55.91% 400 2.65% 400 30.12% 45.38 250 53.61% 700 1.84% 800 20.03% 25.95 350 48.28% 1100 3.95% 1600 13.06% 27.89 450 43.12% 1400 1.48% 3200 6.14% 12.70 550 39.36% 1700 4.00% 6400 5.63% 7.11 Second, Fig. 5 shows the results of four data sets computed by TOP-MATA and TOP-DATA, respectively. As can be seen, in general, TOP-MATA shows a better performance than TOP-DATA. And as the increase of the ? value, the advantage tends to be even more impressive for these four data sets 4.3. The Computation Saving of TOP-MATA As can be seen in the Tab. 4, four data sets, enjoy signi?cant computation savings brought by TOP-MATA. We can conclude that the computation saving is a major factor for the performance of TOP-MATA. That is, compared with TOP-DATA, a higher computation saving implies a much better performance of TOP-MATA. Since this saving is more signi?cant as the increase of the items, TOP-MATA works better for large scale data sets with a large number of items 5. Conclusion In this paper, we studied the problem of searching for top? item pairs with the highest cosine values among all item pairs. Speci?cally, we provided a novel algorithm TOPMATA which employ a Max-First traversal strategy for ef?ciently performing top-? cosine similarity search. Extensive experimental results veri?ed the effectiveness of the algorithms, And TOP-MATA algorithm is superior to TOPDATA for large-scale data sets with multiple items Acknowledgment This research was partially supported by the National Natural Science Foundation of China \(NSFC No. 70901002 and the Ph.D. Programs Foundation of Ministry of Education of China \(No. 20091102120014 


REFERENCES 1] R. Agrawal, T. Imielinski, and A. Swami, Mining association rules between sets of items in large databases, in SIGMOD 1993 2] C. Alexander, Market Models: A Guide to Financial Data Analysis. John Wiley & Sons, 2001 3] W. Kuo, T.-K. Jensen, A. Butte, L. Ohno-Machado and I. Kohane, Analysis of matched mrna measurements from two different microarray technologies Bioinformatics, vol. 18, p. 405C412, 2002 4] H. Xiong, X. He, C. Ding, Y. Zhang, V. Kumar, and S. Holbrook, Identi?cation of functional modules in protein complexes via hyperclique pattern discovery in PSB, 2005 5] J. Han, H. Cheng, D. Xin, and X. Yan, Frequent pattern mining: Current status and future directions DMKD, vol. 15, no. 1, pp. 5586, 2007 6] P.-N. Tan, M. Steinbach, and V. Kumar, Introduction to Data Mining. Addison-Wesley, 2005 7] S. Brin, R. Motwani, and C. Silverstein, Beyond market basket: generalizing association rules to correlations, in SIGMOD 1997, Tucson, AZ, 1997, pp 265276 8] E. Omiecinski, Alternative interestmeasures formining associations, TKDE, vol. 15, pp. 5769, 2003 9] H. Xiong, S. Shekhar, P.-N. Tan, and V. Kumar Exploiting a support-based upper bound of pearsons correlation coef?cient for ef?ciently identifying strongly correlated pairs, in KDD 2004, 2004, pp 334343 10] I. Ilyas, V. Markl, P. Haas, P. Brown, and A. Aboulnaga, Cords: Automatic discovery of correlations and soft functional dependencies, in SIGMOD 2004 2004, pp. 647658 11] J. Zhang and J. Feigenbaum, Finding highly correlated pairs ef?ciently with powerful pruning, in CIKM 2006, 2006, pp. 152161 12] H. Xiong, W. Zhou, M. Brodie, and S. Ma, Top-k correlation computation, JOC, vol. 20, no. 4, pp 539552, 2008 13] S. Zhu, J. Wu, and G. Xia, Top-k cosine similarity interesting pairs search, in 


http://datamining.buaa.edu.cn/TopKCos.pdf 14] M. Zaki, Scalable algorithms for association mining, TKDE, vol. 12, pp. 372390, 2000 


enhance item-based collaborative filtering, in 2nd IASTED International Conference on Information and Knowledge Sharing, Scottsdale, Arizona, 2003 476 2010 10th International Conference on Intelligent Systems Design and Applications 


Basi Association Rles Basi c  Association R u les Association is basically connecting or tying up occurrences of Association is basically connecting or tying up occurrences of events Ol dib t f ilt t O n l y d escr ib e se t s o f s i mu lt aneous even t s Cannot describe patterns that iterate over time e g  itemset a  0  b  0  g    Eg If you sense higher data rates on the downlink than normal AND New Route generated Implies high chances of Intrusion AND New Route generated Implies high chances of Intrusion Associative IDS for NextGen Frameworks Dr S Dua LA Tech 20 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


