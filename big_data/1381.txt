Visual Interface for Online Watch ing of Frequent Itemset Generation in Apriori and Eclat Aniket Mahanti 002 and Reda Alhajj 002  003 002 Dept of Computer Science 003 Dept of Computer Science University of Calgary Global University Calgary Alberta Canada Beirut Lebanon Abstract This paper describes an interactive graphical user interface tool called Visual Apriori that can be used to study two famous frequent itemset generation algorithms namely Apriori and Eclat Understanding the functional behavior of these two algorithms is critical for students taking a data mining course and Visual Apriori provides a hands-on environment for doing so Visual Apriori relies on active participation from the user where one inputs a transactional database and the tool produces a tree-based frequent itemset generation animation for the algorithm chosen Visual Apriori ides an effortless learning experience by featuring user-friendly and easy to understand controls keywords Animation Apriori association rules data mining Eclat frequent itemset generation Visual Apriori 1 Introduction The notion of association rules is an important concept in knowledge discovery in databases Association rule mining can provide pertinent information about relationships among various objects based on how frequently they occur in a database  The 223rst application o f a ssociation r ule mining is market basket analysis Thi s met hod i s employed by store managers to 223nd patterns in consumer buying behavior and identify what combinations of items in the store are sold most frequently Results of such analysis help managers in devising pro\223table sale strategies Other applications of association rule mining include customer pro\223ling fraud detection telecommunications networks failure prediction credit risk analysis etc The 223rst step in association rule mining is the task of 223nding frequent sets of items or itemsets Frequent or large itemsets are itemsets that appear as much or more frequently in the transactional database than the user-de\223ned frequency It is well known that 223nding frequent itemsets in very large databases is computationally expensive and thus is the major focus in association rule mining research This fact rules out the scope of using naive algorithms as these would result in impractical computation times in case of large databases Two famous algorithms namely Apriori 1 2 and Eclat  7   u s e s ophis ticated approaches to 223 n d frequent itemsets  a b bc bd cd abc abd acd bcd abcd ab ac c ad d a b c b c d d Figure 1 Pre\223x tree for four distinct items Both algorithms use a top down approach in traversing through the pre\223x tree of the items in a database 3  A pre\223x tree also referred to as 215tree\216 is a graphical representation of the power set of items under consideration To understand in more details about the properties of pre\223x trees let us consider an example Figure 1 shows a pre\223x tree for four distinct items a  b  c and d  The root node is designated as level one while the level for any child node is calculated as one plus the level of its parent node Itemsets are arranged according to t heir cardinality with all itemsets of cardinality one being placed at the root node At a certain level the itemsets having a common pre\223x are arranged according to a 223xed order in the node In the example an alphabetical ordering is used To create a child node for an itemset it is simply combined with the itemsets on its right hand side So the child node for itemset a at level one is Proceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


created by combining it with b  c and d to yield ab  ac and ad  The labels n the edges ow e common pre\223x for e itemsets in the child node Understanding association rule mining is a critical task for students taking a data mining course Using the treebased approach for 223nding frequent itemsets in a transactional database is the easiest method for learning the basics of Apriori and Eclat algorithms In this paper we present Visual Apriori an interactive graphical user interface tool that allows the user to input a transactional database and visualize the tree-based frequent itemset generation process in case of the above-mentioned algorithms Visual Apriori allows students to actively participate in the animation process by providing easy to use features that gives them control over every step of the animation The remainder of this paper is organized as follows Section 2 provides background information about association rules and frequent itemset generation Also provided in Section 2 is a brief description of Apriori and Eclat algorithms along with a discussion on their main differences Section 3 describes Visual Apriori and gives a detailed account of its functionality and the various features available Section 4 provides additional information about the tool such as system requirements and validation procedure Section 5 illustrates the use of the tool and highlights its usability characteristics through an example Section 6 summarizes the paper and provides concluding remarks 2 Background 2.1 De\223nitions An association rule can be formally stated in terms of the following statements a Consider a set of n distinct items that is represented by I Let D be the database on which association rule mining is to be applied Each tuple in D consists of sets of items T such that T 002 I  An association rule can be de\223ned as an implication of the form X 003 Y where X Y 004 I are sets of items called itemsets and X 005 Y  006  X and Y are called antecedent and consequent of the association rule respectively b The support for an itemset is the number of tuples wherein the items in an itemset appear together in D  Support of an association rule X 003 Y  can be expressed as n  X 002 Y  n  D  where n  X 007 Y  represents number of tuples containing both X and Y and n  D  is the total number of tuples in D  Support is a measure of the statistical signi\223cance of an association rule c The con\223dence for an association rule X 003 Y  can be expressed as n  X 002 Y  n  X  where n  X  is the number of tuples in X Con\223dence is a measure of the strength of an association rule Usually support and con\223dence are expressed in percentage 2.2 Frequent Itemset Generation The basic two-step algorithm behind 223nding association rules between itemsets given I  D  threshold support  s  and threshold con\223dence  002  can be described as follows 1   a Find all itemsets in D with support b s  b Generate association rules from the itemsets that satisfy a and have a con\223dence b 002  The 223rst step of the algorithm 223nds what are called large or frequent itemsets Those itemsets that do not satisfy a are labeled small or infrequent itemsets Our focus is on frequent itemset generation since this step of association rule mining takes up most of the time and computing resources Two well-known algorithms that do this task are Apriori and Eclat Apriori and Eclat differ from each other in the sense that they use different techniques for g the tree and for calculating the support of e itemsets 2.2.1 Apriori Apriori is a breadth-\223rst search algorithm where the tree is traversed one level at a time Apriori employs a two-pass y for 223nding frequent itemsets for all levels but level one In e 223rst pass a list of candidate itemsets are generated at each level and any itemsets that are supersets of infrequent itemsets are pruned The second pass involves calculating the support values for the remaining itemsets and doing a further pruning of those itemsets which have a support ss than e r-de\223ned threshold The support for the itemsets is calculated by e ither counting the number of transactions in the database for each itemset or by traversing the transactions in stead and counting the itemsets contained in each transaction 3 2.2.2 t Eclat is a depth-\223rst search algorithm where itemsets with the same pre\223x are traversed 223rst Starting from the leftmost node at level one Eclat traverses down the tree and prunes itemsets that have a lower support than the threshold If all nodes having common pre\223x are processed then Eclat backtracks to the next level and continues processing itemsets with a different pre\223x Eclat calculates support for itemsets by maintaining a transaction list for each item 3 This way the transaction database is only required once for Proceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


counting the support at the 223rst level Support for the subsequent levels is calculated by means of intersecting the transactions list of parent itemsets which were combined to form the new child node 3 Description of Visual Apriori Visual Apriori allows users to input a transaction database and a threshold support e in percentage and visualize an animation showing how the tree is built depending on their choice of algorithm The Visual Apriori interface consists of three main parts corresponding to a menu bar an animation display area and a status bar The tool menu bar located at the top of the program window has 223ve menu items which are as follows File  Action  Algorithm  Statistics and Help  All the menu items and their corresponding options have keyboard shortcuts designed for better usability The functionality and usage of the options offered by these menu items is discussed in the successive sections The animation display area is the central part of the program window where all the animations are shown The status bar is located at the bottom of the main window and displays nformation regarding what input 223le has been loaded e animation state and the algorithm and support e chosen Four animation states are possible which are as follows 200 Animation Running  The tool is running an animation showing the tree being drawn for the algorithm chosen and consequently being pruned according to the support value 200 Animation Paused  The animation has been temporarily stopped Pausing the animation allows the user to review the tree generation process 200 Animation Complete  The tool has 223nished drawing the tree 200 Animation Stopped  The user prematurely stopped e animation Note that the animation state is reset when a new input 223le is loaded 3.1 File Menu The File menu provides four options to the user provide e input  Load File  enter the threshold support  Support  save the tree as an image  Save Tree  and quit the program  Exit  The input to Visual Apriori is a in text 223le that contains a 1  0 212 matrix where each row represents a transaction and each column represents a distinct item Figure 2 Sample input 223le for Visual Apriori abcdef 110110 110111 000010 011011 Figure 2 shows an example input 223le This 223le depicts a transaction database with six distinct items and four transactions Note that the entries are space separated The header for the matrix is only provided for readability purposes and should not be included in e actual input 223le as the tool automatically assigns identi\223ers to the items in an alphabetical order A one in the input matrix signi\223es the presence of an item while a zero means otherwise For example the 223rst transaction in the input 223le contains items a  b  d and e  If the input 223le is not in the correct format the tool responds with an error message Such a message is shown when the matrix in the input 223le has characters other than ones and zeros the matrix is incomplete or when the 223le does not contain a matrix at all Once the input 223le has been accepted by the tool the user needs to enter the support value and choose the algorithm after which the tool would start showing the tree generation animation The support value has to be a positive integer value between 10 and 90 inclusive The user has the option of saving the tree either as a Bitmap BMP or Joint Photographic Experts Group JPEG image when the animation has been completed 3.2 Action Menu The action menu provides the user with the ability to control the animation There are four options available in this menu namely Pause  Back  Resume and Stop The Pause option allows the user to temporality stop the animation from progressing further This gives the r the 224exibility of reviewing how the algorithms work and take notes without missing any step that would have been executed if the animation were continuously running The Back option is only active in the Animation Paused state This option provides the user with additional control over the animation process If the user somehow missed an animation step she could pause the animation and by clicking on the Back option each time she would be able to go back one step Clicking on e Resume option allows the animation to continue The Stop option completely halts the animation This option is particularly helpful when the user has accidentally entered the wrong support value and wants to use a different support or algorithm for the same input Proceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


3.3 Algorithm Menu The Algorithm menu allows the user to choose from Apriori and Eclat for the tree generation animation Once an input 223le has been loaded the user has the freedom to use any algorithm and support e f one\220s choosing 3.4 Statistics Menu The atistics menu has two options available namely Current Statistics and Final Statistics The Current Statistics option displays a small window showing three representative indicators which change as the tree grows and is pruned at each level These indicators numerically re\224ect the current state of the animation process and provide pointers to the user as to what is happening The three indicators are as follows Current Level  Itemsets Pruned and Itemsets Processed The Current Level indicator describes the level at which the node processing is taking place The Itemsets Pruned indicator shows how many itemsets have been deleted hile the Itemsets Processed indicator keeps a count of the total number of itemsets drawn By default the Current Statistics box is visible when the animation is in progress however the user has the ability to hide it by clicking on the Current Statistics optioninthe atistics menu Clicking on the same option again brings back the window This option is only active when the animation is in the Animation Paused or Animation Running state Once the tool has 223nished drawing the tree the results of applying the frequent itemset generation algorithms are displayed in the Final Statistics dialog box This g box lists the frequent itemsets ordered according to their size the height of the tree drawn the total number of itemsets processed the number of frequent itemsets and the time to calculate the tree The input parameters are also listed for completeness 3.5 Help Menu The Help menu has three options namely Help Contents  License and About Visual ri  When clicked on e Help Contents option a pop-up window opens up which provides information on how to use the tool The help contents are organized in Hypertext Markup Language HTML format for easy navigation License and About Visual Apriori show the terms of use and copyright informan about e tool 4 General Information 4.1 System Requirements Visual Apriori is written in Java It has been developed to run on any operating system installed with the Java 2 Platform Standard Edition Java Runtime Environment and/or Java Standard Development Kit For hardware requirements at least 64 MB RAM and 10 Megabytes of disk storage capacity is desirable The tool is available as an executable Java Archive JAR 223le and can be started by double clicking on the 223le icon in case of Windows and Mac OS X For a Unix-based environment running X windows the tool can be started by typing the command java jar Visual_Apriori.jar in the shell from e source directory 4.2 Validation The purpose of validation is to ensure that e animations and results produced by Visual Apriori are correct The validation process establishes con\223dence in the tool and makes sure that it aptly simulates the Apriori and Eclat algorithms The validation of Visual Apriori was achieved in two ways In the 223rst approach Visual Apriori was tested on a small input of six distinct items and 223fty transactions Each step executed by the tool in the animation of the two algorithms was followed by manual veri\223cation to ensure that the tool behaved in the manner expected of it The second approach included verifying the frequent itemsets results reported by Visual Apriori to the output produced by another implementation of the Apriori algorithm 6  5 Visual Apriori Demonstration In this section we demonstrate the tree-based animation process using Visual Apriori when applying the Apriori algorithm on a database Suppose a student has a transaction database composed of 6 distinct items and 100 transactions She/he is terested in 223nding the itemsets that appear at least 30 times in the database She/he also wants to know how does Apriori calculate the frequent itemsets in this case The student starts Visual Apriori and loads the database by accessing the Load File option from e File menu After loading the 223le she/he enters a support value of 30 into the Support dialog box Having done that she/he proceeds to choose Apriori from the Algorithm menu As soon as the algorithm is selected Visual Apriori starts drawing the tree on the screen Figure 3 shows a summarized version of the different stages in the animation process The tool starts at the root node by 223lling in the support values for itemsets with cardinality one A succinct repreProceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


a Filling in the support values for itemsets in the second level after candidate generation b Pruning supersets of infrequent itemsets in the third level before calculating the support values c Filling in the support values and pruning itemset\(s having support less than the user-de\223ned threshold d Displaying the results after completion of the tree Figure 3 Different stages in the tree generation animation for an input with 6 distinct items and 100 transactions using Apriori threshold support=30 Proceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


sentation of the pre\223x tree is used in the tool where at any level the complete itemset can be determined by adding to the current identi\223er in the node all the preceding pre\223xes i.e labels of the edges leading to e root node The support values for e itemsets are posted next to their names in e nodes Observe that no itemsets are pruned at the 223rst level because all of them have a higher support than the threshold support value of 30 Also note how the Current Statistics counter keeps track of the animation process After completing the 223rst level the tool moves to the second level Figure 3\(a shows the candidate itemsets being created at the second level The tool 223rst creates the nodes for all candidate itemsets of cardinality two without calculating their support This fact is represented by the tool showing a 215?\216 next to the itemset identi\223ers Apriori wants to reduce the number of nodes for which it has to calculate the support by applying the rule any superset of an infrequent itemset is also infrequent 1 As there were no infrequent items ets i n the 223rst level the tool does not prune any itemsets and 223lls in the support values for all of them at the second level Now the tool prunes ae  de and df because they have a support of 20 which is less than the threshold support value This situation is shown using red crosses over the itemsets See Figure 3\(b After pruning the infrequent itemsets at the second level the tool creates the candidate nodes at the third level From the pruning done at the second level we know that any superset of ae  de or df is also infrequent Applying this theory itemsets abe  ace  ade  adf  bde  bdf  cde and cdf are pruned Here the pruning is shown using blue crosses over the itemsets to distinguish them as supersets of infrequent itemsets Next the support values for the remaining itemsets are calculated and 223lled in Observe that the support of itemset cef is lower than the threshold support and is thus deleted Figure 3\(c depicts this scenario Just like before at the fourth level the tool again creates the candidate nodes All itemsets except abcd and abcf are pruned because they were supersets of pruned itemsets in the previous levels No itemsets are deleted after 223lling in the support for the two remaining itemsets In a similar fashion all the itemsets are pruned at the last level This way the tree animation state changes to Animation Complete and e Current Statistics counter is replaced by the Final Statistics dialog box Figure 3\(d shows the 223nal tree along with the results The Final Statistics dialog box shows frequent itemsets ordered according to increasing cardinality values It also shows that Visual Apriori took minimal time to do the calculations for drawing the resulting tree of height three 6 Conclusions and Future Work This paper presented a graphical user interface tool that visualizes tree-based frequent itemset generation for Apriori and Eclat algorithms Features like Pause  Back  Resume and Stop ensure that the users are active participants in controlling the animations In addition the tool through its Current Statistics window and status bar illustrate the animation process by displaying indicators that depict the status of the tree The results of the animation are arranged in an easy to understand format in the Final Statistics dialog box Overall Visual Apriori provides an interactive and hands-on learning experience for students We plan to expand Visual Apriori to include more frequent itemset generation algorithms such as the frequentpattern tree FP-tree structure algorithm 4  W e a lso in tend to integrate association rule mining options into the tool References 1 R  A g r a w al T  I mielin sk i an d A  S w a mi M i n i n g Asso ciatio n Rules between Sets of Items in Large Databases In Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data  pages 207\205216 ACM Press 1993  R  A gra w al and R  S r i kant  F ast Al gori t h ms for M i n i n g A ssociation Rules In Proc of 20th International Conference on Very Large Databases  pages 487\205499 Morgan Kaufmann 12\20515 1994 3 C  B or gel t  E f 223 c i e nt I m pl ement at i ons of Apr i or i a nd E c l a t  In Proceedings of the IEEE ICDM Workshop on Frequent Itemset Mining Implementations  Melbourne USA November 2003 4 J  H an J P e i  and Y  Y i n  M i n i n g F r equent Pat t e r n s w i t h out Candidate Generation In Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data  pages 1\20512 ACM Press 2000 5 J ia wei H an an d M ich e lin e K amb e r Data Mining Concepts and Techniques  Morgan Kaufmann 2000 6 S  Y ib in  A p r io ri Imp lemen t atio n  Un i v ersity of Regina ril 2000 Available at http www2.cs.uregina.ca 002 hamilton/courses 831/notes/itemsets/itemset prog1.html  7 M  Z ak i S P a rth asarath y  M  O g i h a ra a n d W  Li N e w Algorithms for Fast Discovery of Association Rules In Proceedings of 3rd International Conference on Knowledge Discovery in Databases  pages 283\205296 Newport USA August 1997  Q  Z hao and S  B ho wmi ck Associ at i o n R ul e M i n i ng A Survey Technical report Nanyang Technological University 2003 Available at http://www.cais.ntu.edu.sg 002 qkzhao/pdf/ARS.pdf  Proceedings of the Fourth International Conferenc e on Machine Learning and A pplications \(ICMLA\22205 0-7695-2495-8/05 $20.00 \251 2005  IEEE 


3: get Pk and Pk+l from listPartitions with index 4: A f- {apk} Ilbegin of partition 5: END f- apk+1 Ilend of partition 6: EXI:= false 7: A f- A 8: while \(!EXIT 9: ifllA'l1 &lt;= MSthen 10: if IIA'II = MS then 11: output frequent closed itemsets 12: end if 13: A f- next candidate of frequent closed itemset 14: else 15: output frequent closed itemsets 16: A f- generate next frequent closed itemsets 17: end if 18: if END E A when searching the next closure then 19: EXIT := true 20: end if 21: end while 4.3 An example With our approach, we can dynamically generate parti  tions according to the size and density of the data, and our need. In figure 4, we demonstrate how to generate different partitions for the same data Using the data context of Figure 3, we show below an example \(see Figure 4 need to generate the frequent ordered context. Given the min-support M S = 2, a2 and ag are merged as a2. As support \(a5 frequent ordered context is: as, a6, a7, a4, a3, a2, al   And then, we can give a value to the parameter to deter  mine the partitions, for example, when P P = 0.5 we get 3 partitions: [aI, a7[, [a7, as[ and [as, as]. When PP = 0.6 there are 4 partitions to generate: [aI, a4[, [a4, a6 [, [a6, as and [as, as]. When PP = 0.8,6 partitions are generated aI, a3[, [a3, a4[, [a4, a7[, [a7, a6[, [a6, as[ and [as, as When P P = 0.0 there is only one partition: [aI, as At the end, using algorithm 2, we find separately all fre  quent closed item sets in each partition. For the example of 3 partitions: [aI, a7[, [a7, as[ and [as, as], the frequent closed itemsets of each partition can be generated by algo  rithm 2 \(in Figure 4, Ci is the label of frequent closed item  sets. We get 4 frequent closed itemsets in [aI, a7[: {ad with 8 objects, {a2' al} with 5 objects, {a3, al} with 5 ob  jects, {a3, a2, ad with 2 objects, {a4' ad with 4 objects a4' a3, adwith 3 objects 5 Experimental results We have implemented PFC algorithm in Java. The experiments are performed on a PIII900 computer with 512Mo RAM and Windows XP system. We have tested our algorithm on some datasets of the UCI repository and the worst case. The worst case means the case when the sizes of  and each attribute is verified by n - 1 different objects, each object possesses n - 1 different attributes In order to supply the performance references of our al  gorithm, CLOSET + is tested on the same data. This isn't a real comparison, because 1 but CLOSET + in C++. C++ can improve performance than Java. 2 certain time to generate frequent ordered context. If the size of data is large, it will increase time-space cost. However the comparison with CLOSET + algorithm shows that our algorithm is better for large and dense data Table 1 shows some examples of our experimental re  sults on real data. The results demonstrate CLOSET+ out  perform PFC in most case. But if the size of item set is large and min-support is low, PFC is faster than CLOSET +. From table 1, the results of audiology data, lung-cancer data and soybean-large data show that PFC is faster than CLOSET For the worst case, PFC is faster than CLOSET +. In 


For the worst case, PFC is faster than CLOSET +. In table 2, the data name shows the size of data, for exam  ple, worst16 means the worst case data with 16 objects and items. It's hard to generate frequent closed itemsets for the worst case, so we only test for high min-support. Symbol I" means the algorithm needs very long time to gener  ate frequent closed itemsets, so we didn't get the result Here we demonstrate the number of partitions and maxi  mum time for one partition for PFC In summary, PFC is efficient for large dense data, and is faster than CLOSET + when the number of items is much higher than the number of objects PP=O.O PP=O.S PP=0.6 PP=0.8 Total partitions: I Total partitions: 3 Total partitions: 4 Total partitions: 6 Partition 1 [al ,  as1 #Partition l [al, a7[ #Partition I [al , a4[ #Partition 1 [ al, a3 CO: al [8J CO: al [81 CO: al [81 CO: al [8J CI : a2, al [SJ Cl : a2, al [Sl CI : a2, al [SJ Cl : a2, al [Sl C2 : a3, al [Sl C2 : a3, al [SJ C2 : a3, al [SJ #Partition2 [ a3, a4 C3 : a3, a2, al [2J C3 : a3, a2, al [21 C3 : a3, a2, al [21 CO: a3, al [SJ C4 : a4, al [4J C4 : a4, al [41 #Partition2 [a4, a6[ Cl : a3, a2, al [21 CS : a4, a3, al [31 CS : a4, a3, al [3J CO: a4, al [4J #Partition3 [ a4, a7 C6 : a7, al [4J #Partition2 [ a7, as[ Cl : a4, a3, al [31 CO: a4, al [4J C7 : a7, a2, al [3J CO: a7, al [41 C2 : a7, al [4J Cl : a4, a3, al [31 C8 : a6, a4, al [31 CI : a7, a2, al [3J C3 : a7, a2, al [3J #Partition4 [ a7, a6 C9 : a6, a4, a2, al [2J C2 : a6, a4, al [31 #Partition3 [ a6, as[ CO: a7, al [4J CIO: a6, a4, a3, al [2J C3 : a6, a4, a2, al [21 CO: a6, a4, al [3J Cl : a7, a2, al [31 C11 : as, a7, al [31 C4 : a6, a4, a3, al [2J C I : a6, a4, a2, al [2J #PartitionS [ a6, as C12: as, a7, a2, al [2J #Partition3 [as, as1 C2 : a6, a4, a3, al [21 CO: a6, a4, al [3J C13 : as, a7, a3, al [21 CO: as, a7, al [31 #Partition4 [as, asJ Cl : a6, a4, a2, al [21 CI : as, a7, a2, al [2J CO: as, a7, al [3J C2 : a6, a4, a3, al [21 C2 : as, a7, a3, al [21 Cl : as, a7, a2, al [21 #Partition6 [as, asJ C2 : as, a7, a3, al [2J CO: as, a7, atf31 Cl : as, a7, a2, al [21 C2 : as, a7, a3, al [2J Figure 4. An example of the different partitions for the same data \(The min-support is 2. The  number in brackets [ ] after each frequent closed itemset means the its support 6 Conclusion In this paper we analyze the search space of frequent closed itemsets and propose a new lattice-based algorithm PFC for mining frequent closed itemsets from data. PFC can generate non-overlapping partitions of the search space and mine frequent closed item sets in each partition. There is no relation between each partition in this algorithm. The partitions only share the same source data. We can deal with any partition independently. So we can apply this algorithm for parallel, distributed, or network computing. PFC shows good performance when dealing with very large data. Pre  liminary experimental results show that PFC outperforms CLOSET + for dense data. It is an efficient algorithm to mine frequent closed itemsets for large and dense data The ongoing research is studying the problem of balance of all partitions. And we will implement this algorithm on a parallel or distributed platform Acknowledgements We are grateful to anonymous reviewers for helpful com  ments. This research benefits from the support of IUT de Lens and the region NordlPas de calais References IJ R. Agrawal, T. Imielinski, and A. N. Swami. Mining as  sociation rules between sets of items in large databases. In Proceedings of the 7993 ACM SIGMOD, pages 207-216 Washington, D.C., 26-28 1993 21 R. Agrawal and R. Srikant. Fast algorithms for mining as  sociation rules. In Proc. 1994 Intl. Conf Very Large Data Bases \(VLDB'94  ber 1994 31 J. Boulicaut, A. Bykowski, and C. Rigotti. Freesets: A con  densed representation of boolean data for the approximation of frequency queries. Data Mining and Knowledge Discov  ery, 7\(1 4J H. Fu and E. Mephu Nguifo. Partitioning large data to 


4J H. Fu and E. Mephu Nguifo. Partitioning large data to scale up lattice-based algorithm. In Proceedings ofICTAI03 pages S37-S41, Sacramento, CA, November 20 03. IEEE Press SJ H. Fu and E. Mephu Nguifo. How well go lattice algo  rithms on currently used machine learning testbeds? In 4emes journees d' Extraction et de Gestion des Connais  sances, pages 373-384, France, 20 04 61 B. Ganter and R. Wille. Formal Concept Analysis. Mathe  matical Foundations. Springer, 1999 7J J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. In W. Chen, J. Naughton, and P. A Bernstein, editors, 2000 ACM SIGMOD Intl. Conference on Management of Data, pages 1-12. ACM Press, OS 2000 Data file Objects Items Min support FCI PFC \(msec msec audiology 26 110 1 30401 1302 8563 soybean-small 47 79 1 3541 516 431 lung-cancer 32 228 1 186092 21381 279689 promoters 106 228 3 298772 120426 111421 soybean-large 307 133 1 806030 357408 364524 dermatogogy 366 130 50 192203 20204 18387 breast-cancer-wis 699 110 1 9860 3529 1131 kr-vs-kp 3196 75 1100 2770846 1823092 483896 agaricus-Iepiota 8124 124 100 38347 34815 1462 connect-4bi.data 67557 126 1000 2447136 1165806 65084 Table 1. Experiments on real data.\(FCI means frequent closed itemsets. msec means milliseconds For Ref., + means PFC is faster than CLOSET Data file Min support FCI PFC \(msec msec Worst16 1 65534 571 470 271 9 Worst17 1 131070 1112 1002 541 9 Worst18 1 262142 2243 2174 1091 9 Worst19 1 524286 4576 4466 2213 10 Worst20 1 1048574 9243 9484 4606 10 Worst25 20 68405 2103 66916 451 11 Worst25 19 245505 6099 1095065 1552 11 Worst25 18 726205 15452 10235287 4486 11 Worst25 17 1807780 33348 / 10755 11 Worst25 15 7119515 102237 / 39296 11 Worst30 25 174436 6980 426964 1302 12 Worst30 20 53009101 1029771 / 344035 12 Worst50 47 20875 1132 1042 422 14 Worst50 45 2369935 227207 / 29102 14 Worst60 57 36050 7320 3205 821 15 Worst60 56 523685 82938 1665715 9123 15 Worst60 55 5985197 772210 / 92102 15 Worst70 68 2485 1102 190 121 15 Worst70 67 57225 18096 9483 1933 15 Worst70 66 974120 242138 / 26398 15 Table 2. Experiments on the worst case data 8] S. Kuznetsov and S. Obiedkov. Comparing performance of algorithms for generating concept lattices. lETAI Special Issue on Concept Lattice for KDD, 14\(2/3 9j E. Mephu Nguifo, M. Liquiere, and V. Duquenne. lETA Special Issue on Concept Lattice for KDD. Taylor and Fran  cis, 2002 IOj N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Efficient mining of association rules using closed itemsets lattices lournal of Information Systems, 24\(1 II j 1. Pei, 1. Han, and R. Mao. CLOSET: An efficient algo  rithm for mining frequent closed itemsets. InACM SIGMOD Workshop on Research Issues in Data Mining and Knowl  edge Discovery, pages 21-3 0, 200 0 12] 1. Wang, 1. Han, and 1. Pei. Closet+: Searching for the best strategies for mining frequent closed itelnsets. In In Pro  ceedings of the Ninth ACM SIGKDD International Confer  ence on Knowledge Discovery and Data Mining \(KDD'03 Washington, DC, USA, 2003 13] M. I. Zaki and C.-I. Hsiao. CHARM: An efficient algorithm for closed item set mining. Technical Report 99-10, Rensse  laer Polytechnic Institute, 1999 


laer Polytechnic Institute, 1999 pre></body></html 


efficiency then AOFI. However utilization of fuzzy concept hierarchies provides more flexibility in reflecting expert knowledge and so allows better modeling of real-life dependencies among attribute values, which will lead to more satisfactory overall results for the induction process. The drawback of the computational cost may additionally decline when we notice that, in contrast to many other data mining algorithms, hierarchical induction algorithms need to run only once through the original \(i.e. massive dataset. We are continuing an investigation of computational costs of our approach for large datasets ACKNOWLEDGMENT Rafal Angryk would like to thank the Montana NASA EPSCoR Grant Consortium for sponsoring this research REFERENCES 1] J. Han , M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann, New York, NY 2000 2] J. Han, Y. Cai, and N. Cercone  Knowledge discovery in databases: An attribute-oriented approach  Proc. 18th Int. Conf. Ver y Large Data Bases, Vancouver, Canada, 1992, pp. 547-559 3] J. Han  Towards Efficient Induction Mechanisms in Database Systems  Theoretical Computing Science, 133, 1994, pp. 361-385 4] J. Han, Y. Fu  Discovery of Multiple-Level Association Rules from Large Databases  IEEE Trans.  on KD E, 11\(5 5] C.L. Carter, H.J. Hamilton  Efficient AttributeOriented Generalization for Knowledge Discovery from Large Databases  IEEE Trans. on KDE 10\(2 6] R.J. Hilderman, H.J. Hamilton, and N. Cercone  Data mining in large databases using domain generalization graphs  Journal of Intelligent Information Systems, 13\(3 7] C.-C. Hsu  Extending attribute-oriented induction algorithm for major values and numeric values   Expert Systems with Applications , 27, 2004, pp 187-202 8] D.H. Lee, M.H. Kim  Database summarization using fuzzy ISA hierarchies  IEEE Trans . on SMC - part B, 27\(1 9] K.-M. Lee  Mining generalized fuzzy quantitative association rules with fuzzy generalization hierarchies  20th NAFIPS Int'l Conf., Vancouver Canada, 2001, pp. 2977-2982 10] J. C. Cubero, J.M. Medina, O. Pons &amp; M.A. Vila  Data Summarization in Relational Databases through  Fuzzy Dependencies  Information Sciences, 121\(3-4 11] G. Raschia, N. Mouaddib  SAINTETIQ:a fuzzy set-based approach to database summarization   Fuzzy Sets and Systems, 129\(2 162 12] R. Angryk, F. Petry  Consistent fuzzy concept hierarchies for attribute generalization  Proceeding of the IASTED Int. Conf. on Information and Knowledge Sharing, Scottsdale AZ, USA, November 2003, pp. 158-163 13] Toxics Release Inventory \(TRI available EPA database hosted at http://www.epa.gov/tri/tridata/tri01/index.htm The 2005 IEEE International Conference on Fuzzy Systems790 pre></body></html 


the initial global candidate set would be similar to the set of global MFIs. As a result, during the global mining phase the communication and synchronization overhead is low  0 2 4 6 8 1 0 Number of Nodes Figure 5. Speedup of DMM 4.4.2 Sizeup For the sizeup test, we fixed the system to the 8-node con figuration, and distributed each database listed in Table 2 to the 8 nodes. Then, we increased the local database sire at each node from 45 MB to 215 MB by duplicating the initial database partition allocated to the node. Thus, the data distribution characteristics remained the same as the local database size was increased. This is different from the speedup test, where the database repartitioning was per formed when the number of nodes was increased. The per formance of DMM is affected by the database repartitioning to some extent, although it is usually very small. During the sizeup test, the local mining result of DMM is not changed at all at each node The results shown in Figure 6 indicate that DMM has a very good sizeup property. Since increasing the size of local database did not affect the local mining result of DMM at each node, the total execution time increased just due to more disk U 0  and computation cost which scaled almost linearly with sizeup 5 Conclusions In this paper, we proposed a new parallel maximal fre quent itemset \(MFI Max-Miner \(DMM tems. DMM is a parallel version of Max-Miner, and it re quires low synchronization and communication overhead compared to other parallel algorithms. In DMM, Max Miner is applied on each database partition during the lo 0 45 90 135 180 225 270 Amwnt of Data per Node \(ME Figure 6. Sizeup of DMM cal mining phase. Only one synchronization is needed at thc end of this phase to construct thc initial global candi date set. In the global mining phase, a top-down search is performed on the candidate set, and a prefix tree is used to count the candidates with different length efficiently. Usu ally, just a few passes are needed to find all global maximal frequent itemsets. Thus, DMM largely reduces the number of synchronizations required between processing nodes Compared with Count Distribution, DMM shows a great improvement when some frequent itemscts are large \(i.e long patterns employed by DMM for efficient communication between nodes; and global support estimation, subset-infrequency based pruning, and superset-frequency based pruning are used to reduce the size of global candidate set. DMM has very good speedup and sizeup properties References I ]  R. Agrawal and R. Srikant  FdSt Algorithms for Mining As sociation Rules  Pmc. o f f h e  ZOrh VLDB Conf, 1994, pp 487499 2] R. Agrawal and I. C. Shafer  Parallel Mining of Association Rules  IEEE Trans. on Knowledge and Dura Engineering Vol. 8, No. 6, 1996, pp. 962-969 3] R. I. Bayardo  Efficient Mining Long Patlems from Databases  Proc. ofrhe ACM SIGMOD Inf  l Conf on Man ogemenr ofDara, 1998, pp. 85-91 4] S.  M. Chung and J. Yang  A Parallel Distributive Join Al gorithm for Cube-Connected Multiprocessors  IEEE Trans on Parallel and Disrribured Systems, Vol. 7, No. 2, 1996, pp 127-137 51 M. Snir, S. Otto. S. Huss-Lederman, D. Walker, and J. Don gana, MPI: The Complete Reference, The MIT Press, 1996 


gana, MPI: The Complete Reference, The MIT Press, 1996 6] R. Rymon  Search through Systematic Set Enumeralion   Pmc. of3rd Inr  l Con$ on Principles of Knowledge Repre sentation and Reasoning, 1992, pp. 539-550 507 pre></body></html 


sketch-index in answering aggregate queries. Then Section 5.2 studies the effect of approximating spatiotemporal data, while Section 5.3 presents preliminary results for mining association rules 5.1 Performance of sketch-indexes Due to the lack of real spatio-temporal datasets we generate synthetic data in a way similar to [SJLL00 TPS03] aiming at simulation of air traffic. We first adopt a real spatial dataset [Tiger] that contains 10k 2D points representing locations in the Long Beach county \(the data space is normalized to unit length on each dimension These points serve as the  airbases  At the initial timestamp 0, we generate 100k air planes, such that each plane \(i uniformly generated in [200,300], \(ii, iii destination that are two random different airbases, and iv  the velocity direction is determined by the orientation of the line segment connecting its source and destination airbases move continually according to their velocities. Once a plane reaches its destination, it flies towards another randomly selected also uniform in [0.02, 0.04 reports to its nearest airbase, or specifically, the database consists of tuples in the form &lt;time t, airbase b, plane p passenger # a&gt;, specifying that plane p with a passengers is closest to base b at time t A spatio-temporal count/sum query has two parameters the length qrlen of its query \(square number qtlen of timestamps covered by its interval. The actual extent of the window \(interval uniformly in the data space \(history, i.e., timestamps 0,100 air planes that report to airbases in qr during qt, while a sum query returns the sum of these planes  passengers. A workload consists of 100 queries with the same parameters qrlen and qtlen The disk page size is set to 1k in all cases \(the relatively small page size simulates situations where the database is much more voluminous specialized method for distinct spatio-temporal aggregation, we compare the sketch-index to the following relational approach that can be implemented in a DBMS. Specifically, we index the 4-tuple table lt;t,b,p,a&gt; using a B-tree on the time t column. Given a count query \(with window qr and interval qt SELECT distinct p FROM &lt;t,b,p,a&gt WHERE t?qt &amp; b contained in qr The performance of each method is measured as the average number of page accesses \(per query processing a workload. For the sketch-index, we also report the average \(relative Specifically, let acti and esti be the actual and estimated results of the i-th query in the workload; then the error equals \(1/100 set the number of bits in each sketch to 24, and vary the number of sketches The first experiment evaluates the space consumption Figure 5.1 shows the sketch index size as a function of the number of sketches used \(count- and sum-indexes have the same results more sketches are included, but is usually considerably smaller than the database size \(e.g., for 16 signatures, the size is only 40% the database size 0 20 40 60 80 


80 100 120 140 160 8 16 32 number of sketches size \(mega bytes database size Figure 5.1: Size comparison Next we demonstrate the superiority of the proposed sketch-pruning query algorithm, with respect to the na  ve one that applies only spatio-temporal predicates. Figure 5.2a illustrates the costs of both algorithms for countworkloads with qtlen=10 and various qrlen \(the index used in this case has 16 sketches also illustrate the performance of the relational method which, however, is clearly incomparable \(for qrlen?0.1, it is worse by an order of magnitude we omit this technique Sketch-pruning always outperforms na  ve \(e.g., eventually two times faster for qrlen=0.25 increases with qrlen, since queries returning larger results tend to set bits in the result sketch more quickly, thus enhancing the power of Heuristics 3.1 and 3.2. In Figure 5.2b, we compare the two methods by fixing qrlen to 0.15 and varying qtlen. Similar to the findings of [PTKZ02]4 both algorithms demonstrate  step-wise  growths in their costs, while sketch-pruning is again significantly faster The experiments with sum-workloads lead to the same observations, and therefore we evaluate sketch-indexes using sketch-pruning in the rest of the experiments 4 As explained in [PTKZ02], query processing accesses at most two paths from the root to the leaf level of each B-tree regardless the length of the query interval Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE sketch-pruning naive relational 0 100 200 300 400 500 600 700 800 900 0.05 0.1 0.15 0.2 0.25 number of disk accesses query rectangle length 300 0 100 200 400 500 600 1 5 10 15 20 number of disk accesses query interval length a qtlen=10 b qrlen=0.15 Figure 5.2: Superiority of sketch-pruning \(count As discussed in Section 2, a large number of sketches reduces the variance in the resulting estimate. To verify this, Figure 5.3a plots the count-workload error of indexes 


using 8-, 16-, and 32- sketches, as a function of qrlen qtlen=10 error \(below 10 it increases slowly with qrlen used, however, the error rate is much higher \(up to 30 and has serious fluctuation, indicating the prediction is not robust. The performance of 16-sketch is in between these two extremes, or specifically, its accuracy is reasonably high \(average error around 15 much less fluctuation than 8-sketch 32-sketch 16-sketch 8-sketch relative error 0 5 10 15 20 25 30 35 0.05 0.1 0.15 0.2 0.25 query rectangle length relative error 0 5 10 15 20 25 30 35 1 5 10 15 20 query interval length a qtlen=10, count b qrlen=0.15, count relative error query rectangle length 0 5 10 15 20 25 0.05 0.1 0.15 0.2 0.25 relative error query interval length 0 5 10 15 20 25 30 1 5 10 15 20 c qtlen=10, sum d qrlen=0.15, sum Figure 5.3: Accuracy of the approximate results The same phenomena are confirmed in Figures 5.3b where we fix qrlen to 0.15 and vary qtlen 5.3d \(results for sum-workloads number of sketches improves the estimation accuracy, it also leads to higher space requirements \(as shown in Figure 5.1 Figures 5.4a and 5.4b show the number of disk accesses for the settings of Figures 5.3a and 5.3b. All indexes have almost the same behavior, while the 32-sketch is clearly more expensive than the other two indexes. The interesting observation is that 8- and 16-sketches have 


interesting observation is that 8- and 16-sketches have almost the same overhead due to the similar heights of their B-trees. Since the diagrams for sum-workloads illustrate \(almost avoid redundancy 32-sketch 16-sketch 8-sketch number of disk accesses query rectangle length 0 50 100 150 200 250 300 350 400 0.05 0.1 0.15 0.2 0.25 number of disk accesses query interval length 0 50 100 150 200 250 300 350 1 5 10 15 20 a qtlen=10 b qrlen=0.15 Figure 5.4: Costs of indexes with various signatures Summary: The sketch index constitutes an effective method for approximate spatio-temporal \(distinct aggregate processing. Particularly, the best tradeoff between space, query time, and estimation accuracy obtained by 16 sketches, which leads to size around 40 the database, fast response time \(an order of magnitude faster than the relational method average relative error 5.2 Approximating spatio-temporal data We proceed to study the efficiency of using sketches to approximate spatio-temporal data \(proposed in Section 4.1 as in the last section, except that at each timestamp all airplanes report their locations to a central server \(instead of their respective nearest bases maintains a table in the form &lt;time t, plane p, x, y&gt;, where x,y with parameters qrlen and qtlen distinct planes satisfying the spatial and temporal conditions. For comparison, we index the table using a 3D R*-tree on the columns time, x, and y. Given a query, this tree facilitates the retrieval of all qualifying tuples, after which a post-processing step is performed to obtain the Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE number of distinct planes \(in the sequel, we refer to this method as 3DR method introduces a regular res  res grid of the data space, where the resolution res is a parameter. We adopt 16 sketches because, as mentioned earlier, this number gives the best overall performance Figure 5.5 compares the sizes of the resulting sketch indexes \(obtained with resolutions res=25, 50, 100 the database size. In all cases, we achieve high compression rate \(e.g., the rate is 25% for res=25 evaluate the query efficiency, we first set the resolution to the median value 50, and use the sketch index to answer workloads with various qrlen \(qtlen=10 


workloads with various qrlen \(qtlen=10 size \(mega bytes database size 0 20 40 60 80 100 120 140 160 25 50 100 resolution Figure 5.5: Size reduction Figure 5.6a shows the query costs \(together with the error in each case method. The sketch index is faster than 3DR by an order of magnitude \(note that the vertical axis is in logarithmic scale around 15% error observations using workloads with different qtlen Finally, we examine the effect of resolution res using a workload with qrlen=0.15 and qtlen=10. As shown in Figure 5.6c, larger res incurs higher query overhead, but improves the estimation accuracy Summary: The proposed sketch method can be used to efficiently approximate spatio-temporal data for aggregate processing. It consumes significantly smaller space, and answers a query almost in real-time with low error 3D Rsketch number of disk accesses query rectangle length 1 10 100 1k 10k 0.05 0.1 0.15 0.2 0.25 16 14% 15 15% 13 relative error number of disk accesses query interval length 1 10 100 1k 10k 1 5 10 15 20 16 15% 15% 12% 11 relative error a qtlen=10, res=25 b qrlen=0.15, res=25 0 500 1000 1500 2000 2500 25 50 100 number of disk accesses resolution 20% 15% 14 relative error c qrlen=0.15, qtlen=10 


c qrlen=0.15, qtlen=10 Figure 5.6: Query efficiency \(costs and error 5.3 Mining association rules To evaluate the proposed algorithm for mining spatiotemporal association rules, we first artificially formulate 1000 association rules in the form \(r1,T,90 with 90% confidence i randomly picked from 10k ones, \(ii in at most one rule, and \(iii Then, at each of the following 100 timestamps, we assign 100k objects to the 10k regions following these rules. We execute our algorithms \(using 16 sketches these rules, and measure \(i  correct  rules divided by the total number of discovered rules, and \(ii successfully mined Figures 5.7a and 5.7b illustrate the precision and recall as a function of T respectively. Our algorithm has good precision \(close to 90 majority of the rules discovered are correct. The recall however, is relatively low for short T, but gradually increases \(90% for T=25 evaluated in the previous sections, the estimation error decreases as the query result becomes larger \(i.e., the case for higher T 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 precision HT 78 80 82 84 86 88 90 92 94 96 5 10 2015 25 recall HT a b Figure 5.7: Efficiency of the mining algorithm Summary: The preliminary results justify the usefulness of our mining algorithm, whose efficiency improves as T increases Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE 6. Conclusions While efficient aggregation is the objective of most spatio-temporal applications in practice, the existing solutions either incur prohibitive space consumption and query time, or are not able to return useful aggregate results due to the distinct counting problem. In this paper we propose the sketch index that integrates traditional approximate counting techniques with spatio-temporal indexes. Sketch indexes use a highly optimized query algorithm resulting in both smaller database size and faster query time. Our experiments show that while a sketch index consumes only a fraction of the space required for a conventional database, it can process 


required for a conventional database, it can process queries an order of magnitude faster with average relative error less than 15 While we chose to use FM sketches, our methodology can leverage any sketches allowing union operations Comparing the efficiency of different sketches constitutes a direction for future work, as well as further investigation of more sophisticated algorithms for mining association rules. For example, heuristics similar to those used for searching sketch indexes may be applied to improve the brute-force implementation ACKNOWLEDGEMENTS Yufei Tao and Dimitris Papadias were supported by grant HKUST 6197/02E from Hong Kong RGC. George Kollios, Jeffrey Considine and were Feifei Li supported by NSF CAREER IIS-0133825 and NSF IIS-0308213 grants References BKSS90] Beckmann, N., Kriegel, H., Schneider, R Seeger, B. The R*-tree: An Efficient and Robust Access Method for Points and Rectangles. SIGMOD, 1990 CDD+01] Chaudhuri, S., Das, G., Datar, M., Motwani R., Narasayya, V. Overcoming Limitations of Sampling for Aggregation Queries. ICDE 2001 CLKB04] Jeffrey Considine, Feifei Li, George Kollios John Byers. Approximate aggregation techniques for sensor databases. ICDE, 2004 CR94] Chen, C., Roussopoulos, N. Adaptive Selectivity Estimation Using Query Feedback. SIGMOD, 1994 FM85] Flajolet, P., Martin, G. Probabilistic Counting Algorithms for Data Base Applications JCSS, 32\(2 G84] Guttman, A. R-Trees: A Dynamic Index Structure for Spatial Searching. SIGMOD 1984 GAA03] Govindarajan, S., Agarwal, P., Arge, L. CRBTree: An Efficient Indexing Scheme for Range Aggregate Queries. ICDT, 2003 GGR03] Ganguly, S., Garofalakis, M., Rastogi, R Processing Set Expressions Over Continuous Update Streams. SIGMOD, 2003 HHW97] Hellerstein, J., Haas, P., Wang, H. Online Aggregation. SIGMOD, 1997 JL99] Jurgens, M., Lenz, H. PISA: Performance Models for Index Structures with and without Aggregated Data. SSDBM, 1999 LM01] Lazaridis, I., Mehrotra, S. Progressive Approximate Aggregate Queries with a Multi-Resolution Tree Structure. SIGMOD 2001 PGF02] Palmer, C., Gibbons, P., Faloutsos, C. ANF A Fast and Scalable Tool for Data Mining in Massive Graphs. SIGKDD, 2002 PKZT01] Papadias,  D., Kalnis, P.,  Zhang, J., Tao, Y Efficient OLAP Operations in Spatial Data Warehouses. SSTD, 2001 PTKZ02] Papadias, D., Tao, Y., Kalnis, P., Zhang, J Indexing Spatio-Temporal Data Warehouses ICDE, 2002 SJLL00] Saltenis, S., Jensen, C., Leutenegger, S Lopez, M.A. Indexing the Positions of Continuously Moving Objects. SIGMOD 2000 SRF87] Sellis, T., Roussopoulos, N., Faloutsos, C The R+-tree: A Dynamic Index for MultiDimensional Objects. VLDB, 1987 TGIK02] Thaper, N., Guha, S., Indyk, P., Koudas, N Dynamic Multidimensional Histograms 


SIGMOD, 2002 Tiger] www.census.gov/geo/www/tiger TPS03] Tao, Y., Papadias, D., Sun, J. The TPR*Tree: An Optimized Spatio-Temporal Access Method for Predictive Queries. VLDB, 2003 TPZ02] Tao, Y., Papadias, D., Zhang, J. Aggregate Processing of Planar Points. EDBT, 2002 TSP03] Tao, Y., Sun, J., Papadias, D. Analysis of Predictive Spatio-Temporal Queries. TODS 28\(4 ZMT+01] Zhang, D., Markowetz, A., Tsotras, V Gunopulos, D., Seeger, B. Efficient Computation of Temporal Aggregates with Range Predicates. PODS, 2001 ZTG02] Zhang, D., Tsotras, V., Gunopulos, D Efficient Aggregation over Objects with Extent PODS, 2002 Proceedings of the 20th International Conference on Data Engineering \(ICDE  04 1063-6382/04 $ 20.00  2004 IEEE pre></body></html 


