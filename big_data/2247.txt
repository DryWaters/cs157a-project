Abstract  This paper analyses several recent treatises on hybridised self organising map SON theory. Each article proposes a solution to expedite the SOM mapping process and provide more accurate results within a shorter response time via hybridisation including utilisation of Bayesian classification techniques an interactive associative search and exploration tool and, the use of a hierarchical organization of tiered SOM\222s with input derived via auto-assoeiative feedfnrwsrd neural network technology In this paper we propose that an amalgamation of SOM and assoeiation rule theory may hold the key to a more generic solution less reliant on 
initial supervision and redundant user interaction The results of clustering stem words from text documents could be utilised to derive association rules which designate the applicability of documents to the user A four stage process is consequently detailed demonstrating a generic example of how a graphical derivation of associations may be derived from a repositow of text documents or even a set of synopses of many such repositories I INTRODUCHON The information age has witnessed a flood of massive digital libraries and knowledge bases which have evolved dynamically and continue to do so Hence there is a well documented necessity for intelligent, concise mechanisms that can organise search and classify such overwhelming well-springs of 
information The more fundamental traditional methods for collating and searching collections of text documents lack the ability to manage such copious resources of data Data mining is the systematic derivation of trends and relationships in such large quantities of data I There is have much recent research in data mining sizeable text data stores for knowledge discovery The self-organising map SOM is a data mining innovation which is purported to have made significant steps in enhancing knowledge discovery in very large textual data stores 2 The latter describes a SOM as a normally unsupervised neural-network A New Approach To Hybrid SOM Implementations For Text Classification Paul Gunther and Phoebe Chen Cooperative Information Systems Research Centre Queensland University of Technology, Brisbane QLD 4001 Australia E-mail: paulm@,ua net.au 
p chen@aut.edu.au process that produces a graph of similarity of the data A finite set of models are derived which approximate the open set of input data Each model is a neural node in the map and a recursive process clusters similar models on this map thus the term self-organising\fundamental benefit of the SOM is that it derives a lower dimensionality mapping in situations of high dimensionality The clusters are arranged in this low dimensional topology to preserve the neighbourhood relations in the high dimensional data 3 This relationship consequently holds true for clusters themselves as well as nodes within a single cluster Whilst self-organising maps have proven a demonstrated usefulness in text classification, the primary deficiency is the time required to initially train them 3 71 Even though the preliminary training can 
be unsupervised, the time required to perform the operation limits the applicability of SOM as a potential solution in real-time searches The scope of this document is to analyse several approaches at hybridising the SOM methodology in an attempt to circumvent the above mentioned deficiency These techniques come om separate research carried out by external researchers and documented in appropriate journals The papers chosen all propose a form of hybridisation of self-organising maps which extends the fundamental SOM concept to include concepts external to the base SOM theory This tenet is closely related to the author\222s own preliminary work and thus of interest In previous work researched, all discuss the importance of pre-processing the text documents for simplification 5-91 An implicit assumption is made that all text documents are in the same 
language Whilst the author will maintain this inherent assumption, it may hold bearing on such pre-processing especially with respect to document sources with documents in different languages or indeed multi-lingual records The common denominator in textual pre-processing follows the two steps of removing stop-words and stemming keywords Stop-words are words which hold little to no intrinsic meaning within themselves and merely serve to connect language in semantic and syntactic structure Secondly stemming is the process of derive the root word fiom words via removal of superfluous prefixes and suffixes which would only serve to produce redundant items in a derived index of terms The underlying process of mapping text documents relies upon indexation of keywords and utilisation of such pre-processing serves to maximise both precision and recall in this process In this paper there are the following sections 
a synopsis of previous, generally related work; a more focussed examination of several SOM hybrid approaches, including comparison against our defined set of criteria the preliminary description of our approach to self-organising map hybridisation once again contrasted on the same set of criteria; and a conclusion covering further research yet to be performed 11 RELATED WORK Whilst there has been abundant literature on SOM theory as mentioned previously we have focussed on the following as preliminary reading SOM\222s are often related to document collation 13 It is in this document that it seems that SOM is first widely purported to be an exceptionally viable method in classifying large collections of text documents and organizations of such into maps Although this document was more 
preliminary and focussed on a more interactive map for visualisation other research provided a corollary with more formal evidence of SOM usage on very large document stores 2 The example provided scaled up the SOM algorithm to derive a map with more than a million node kom input categorised into 500 dimensions Over 6.8 million documents were subsequently mapped Parallelism and several optimising short-cuts were utilised to expedite the initial training Such steps may not be feasible in a more generic solution where the documents may be more loosely formatted and not as uniform in layout and/or contents. Other work published at a similar time 0-7803-7293-W011$17.00 Q 2001 lEEE 968 2001 IEEE International Fuzzy Systems Conference 


proposed utilisation of clustering of the SOM output rather via the SOM itself and directly relates to the papers analysed more comprehensively below I 1 121 Such a hierarchical approach allows for a faster initial solution and then consequent more detailed mappings to drill down and fme tune the data mining This derivation of associations which are fed into the supplementary SOM is the key interest of the authors Furthermore two of the documents analysed more thoroughly in this paper implement a more sophisticated variant of this methodology 4 131 Another proposes a similar concept with an approach designated as multiple self-organising maps 14 The base tenet of the mechanics of applying distance measures to string inputs for a SOM is examined  151 Although more related to symbol parsing rather than true text document mapping it warrants a higher level examination for the overall 223philosophy\224 Conversely other previous work specifically focussed on knowledge acquisition from internet date repositories  161 III COMPARISON OF HYBRIDISED Three recent techniques have warranted a more detailed analysis and comparison utilisation of SOM approximation of probability densities of each class to 223exploit\224 Bayesian theory to minimise the average rate of misclassifications 13 amalgamation of an interactive associative search instigated by the user with SOM theory 3 and development of a hierarchical pyramid of SOM\222s in which a higher level tier allows subsequent drilling down into and derivation of a more fine-tuned map 4 All of these methods share a commonality with respect of utilisation of a hybridised SOM in text-based data mining and information retrieval In our analysis we will cover sub-topics use of hierarchy, level of hybridisation degree of generality and requirement for supervision andor interaction Finally a brief synopsis of the deficiencies of each methodology is provided A APPLICABILITY WITH RESPECT TO CRITERIA Use of Hierarchy Previous research stressed that document collections inherently lend themselves to a hierarchical structure 4 This hierarchy should be directly originate from the subject matter Their research resulted in a mechanism for producing a hierarchical feature map  each element in a SOM expands into a SOM of its own until sufficient detail has been explored This approach allows for user interaction in such that the desired level of detail can by chosen; thereby preventing unnecessary mining sophistication and delay but still maintain the desirable degree of precision Similar in technique the proposal to integrate Bayesian classification maintains the fundamental principal of training an independent SOM for each class derived om the training set The difference is that once the basis for the collection of SOM\222s is formed they are merely kept in an array-like structure and there is no utilisation of a hierarchy other than the two tiers of the classes and the reliant self-organising maps Indeed although the interactive associative search method is relatively simplistic in its explanation of SOM usage it demonstrates a more advanced use of hierarchical principle A preliminary search via contemporary mechanisms would produce a result set The proposed extension would subsequently obtain similar documents derived as near neighbours on a SOM This entails a relatively simple hierarchy between a contemporary solution and SOM technology Level of Hybridisation Both the formal hierarchical SOM pyramid and integration with Bayesian theoty method implement a potential solution via multiple SOMs Cervera\222s implementation utilises neuron proximity in the self-organising map as approximations of the class probability densities Bayesian classification theory is subsequently SELF-ORGANUING MAP applied to minimise error in the classification process and to optimise the resultant clusters The methodology instigated by Merkl ordinarily entails a very highdimensional feature set correlated to the number of index terms required This feature set dictates the vector representation of a text document for mapping It is stated that to overcome this 223vocabulary problem\224 auto-associative feedforward neural network technology is subsequently used to optimise compression of this feature space This reduces patterns of hquently cosccuning words to a single word 21 In contmst other work relied upon an interactive associative search instigated by the user 3 The underlying dimensionality rduction is dependent on word category map construction just like FVEBSOM 13 Conversely some research implemented fundamental information retrieval mechanisms for  stemming superfluous word segments and filtering 223noise\224 stop-words 3 41 Indeed the former document purports that even current Contemporary search engines should readily adopt this principle These two papers also share a commonality with respect to exploiting the ready visualisation abilities of SOM technology cE 5 161 In either implementation scenario the resultant hierarchy of results is presented to the user via the graphical nature of SOM output Degree of Generality While Cervera et al actually documented experiments on sonar signals and audio recognition of vowel sounds hm several speakers, the principles maintained in the relevant research would notably hold true for all aspects of SOM unsupervised learning 13 Likewise although the hierarchical SOM pyramid methodology was originally designed specifically for text document classification the base tenets of constructing a pyramid of hierarchically tiered self organising maps would be equally applicable in other SOM scenarios The interactive associative search hybrid, however is heavily reliant on user interaction and the design principle weighs heavily on this interactivity and visual exploration Whilst the SOM\222s 223natural\224 applicability to visualisation does hold true for other applications the hybridisation aspects of this research is dependent on document map queries There are probably only minor facets which could be utilised if symbol strings such as via  151 were derived Furthermore the actual SOM manufacture is a static process in this implementation where clustering is pre-processed for a finite and discrete suite of documents beforehand Thus there is a possibility that the overall architecture is not generic for all cases of text documents This is the opposite of the design of the other two proposals where the inhtructure would conceptually handle all generic text document clustering situations Requirement for SupervisiodInteraction Each method holds a differing degree of supervisory dependence Although the unsupervised associative search is indeed instigated by a user it has the least in that SOM production is completely unsupervised and an initial pre processing stage is performed before the results are viewed The user intervention is corollary to this, at a later time and it is only then when the user perfonns interactive associative searching that results are tailored.The hierarchical pyramid of SOM\222s is dependent on user interaction to decide which higher level tier requires drilling down into and consequent derivation of a more fine-tuned map However conceptually, the entire process could be also all pre-processed for n tier levels Such a scenario would be infeasible due to constraints on user expectation for timely results or suffer the same deficiency as the unsupervised associative search with respect to only being applicable to a static set of documents from a source The hybridisation with Bayesian theory was inherently designed with a supervised component Production of probability density approximations via Bayesian classification is reliant on supervised training As such although the interaction with users after mapping is not relevant the pre-requisite for an expert to initially train the system is Overall Deficiencies The fundamental inadequacy in the Bayesian classification hybrid is the reliance on a supervised component for expediting the classification In the proposed scenario for dynamic implementation on text-based documents provided on an a priori basis 969 


there is little scope for initial pre-training with a specialised training set provided by an expert Another approach must be fashioned to provide more accurate SOM mappings.The base methodology of an interactive associative search is relatively simplistic and reliant on pre-processing the entire suite of documents to be examined before initiating an interactive associative search with the end user Once again this would be inadequate in the scenario in question of a priori-based documents from unknown data sources and repositories \(such as an internet search e.g A pyramid of hierachical SOM\222s is perceived to be the closest to the target of such ad hoc dynamic interactive searches with the hierarchical organization of SOM\222s The example provided in the experiment nonetheless is not rich enough to provide comprehensive feedback on timeliness in a 223real-world\224 scenario The resultant feature space on a much higher dimensionality caused by a large set of index terms would be time prohibitive 1161 IV A NEW APPROACH IN SOM HYBRIDISATION There many aspects of self-organising map theory which are adaptable to hybridisation and cross-pollination of new ideas In terms of classification of text-based documents such hybridisation must expedite the mapping process yet remain unsupervised It is also most likely that the user will wish to alter the output resolution The inherent barrier to efficient knowledge acquisition in such circumstances is the underlying feature space This resultant very high dimensionality is directly originates from the requirement of a high set of index terms Even with advanced information retrieval stemming and stop-word filtering this will cause both loss of precision accuracy and effect recall Without dimensionality reduction on this feature space, results will furthermore not be available in a timely manner A hierarchically-based solution is desirable but a simple approach of merely forming a pyramid of SOM\222s would not facilitate optimisation of the feature space Pre-processing the document repository/s is of course, not a viable option Further research is required to form another hybrid proposal most likely cross pollinating Mer ideas from other data mining theory A HYBRIDISATION OF SOM TEEORY  ASSOCIATION RULES A generic framework may be proposed in which the concepts of episodes and episode rules are developed 2 These are derived from the base concept of association rules and frequent sets when applied to sequential data Text as sequential data, can be perceived as a sequence of feature vector index pairs where the former is an ordered set of features and the latter information about the position of the word in the sequence Consequently episode rule may be derived with respect to these episode pairs as per association rule theory In our approach, simpIiflmg the paradigm further the clusters derived from a SOM could be equated to frequent itemsets As such, these may, in turn be fed to an association rule based engine which would consequently develop association rules In order to simplify the initial classification process, redundant stop-words would be removed and only the word stems of the text documents considered This helps alleviate the verbosity of natural language and allows rule derivation to take place on the simplest, core level The previous work on episodes and their derived rules was too intrinsically bound to grammatical context and punctuation we wish to develop a more holistic perspective about the overall document context This will in turn derive more fundamental associations rather than similar rules bound by the original grammatical context. Fig 1 demonstrates a itamework overview on how the hybridisation will supplement knowledge discovery in text mining The representation of initial text documents could equally be applicable to either a text datastore or a repository of meta-data on text datastores themselves It is the relation of SOM classification results to association rule derivation which is the fundamental focus, not the underlying source of SOM input. The inherent scope for research relates to the application of two data mining methodologies into a hybridised amalgamation The resultant variant is intended to expedite the text mining process on large document collections cf 4 161 Fig 1 Process Steps in the Amalgamation of SOM and Association Rule Theory B.EWPLE OF PROCESSING To better illustrate the processing which occurs in each step a brief generic example will be provided Preprocessing The initial set of documents may equally represent either a single text document repository or a result set detailing the meta-data derived from a collection of such repositories In the former instance the input to the preprocessing stage I is a set of 223records\224 where each record is a document comprised of previously unparsed text In the latter scenario each record is a synopsis of a text document datastore with meta-data such as repository key terms and/or abstracts etc The actual preprocessing phase reduces the textual component into a set of common stem words 2 This process relies upon relatively standardised information retrieval processes for detection and removal of redundant stop words as well as the discovery of pertinent stem words e.g 3,4 Classification The second stage accepts the stem word sets 2 and utilises them as input to a standard self-organising map of relatively small size It is envisaged that the common two-dimensional SOM will be implemented as there is deemed to be no rationale or circumstances which warrant a more complex model to map to The range of each dimension will be low to expedite clustering as it is by far more likely that the order of magnitude of input sets would be in the order of hundreds rather than thousands In a similar vein to the initial phase the underlying mechanics of processing the inputs relies upon existing SOM technology which is well documented in a multitude of sources The only specialisation made is the extraction of the resultant SOM output array en masse as an input to the next stage 3 The operation of the initial preprocessing and classification steps may occur in parallel as the outputs of the former are merely 223fed\224 to the latter asynchronously Discovery In this processing step, there is a realistic expectation that the contents of each cluster in the input 3 may be equated to frequent itemsets Taking a Euclidean perspective as the basis of supposition for the sake of simplicity the distance measures taken form neighbourhood fimctions are perceived to provide a firm foundation for derivation of association rules In figure 2 below we can see that 970 


Distance is much smaller than Distance Hence there is a quantifiably higher degree of associativity between Cluster and Ckuster than it and Cluster e Distance Cluster ________----__-__  Distance Fig 2 Distances Between Clusters Whilst more formal association rule techniques such as Apriori 26-191 will most likely need to be implemented to derive confidence and support levels it is envisaged that the process may be expedited by examining the distance measures for initial itemset suitability The derived association rules may be represented as two-dimensional vectors designating the key stem words which imply other, related stems 4 Postprocessing The associations derived in the discovery phase 4 may be represented to the user as a mathematical graph This provides feedback on which 223records\224 relate to which others and via what key words Of course links may be incremental in such that Node is indirectly related to Noded via Node see figure 3 w-m Node Fig 3 Associations Represented as Graph Nodes In the latter context of associations for entire text document datastores, it is perceived to be feasible to subsequently 223drill down\224 into a desired repository and perform the same associative process on it discovering more detailed rule derivations via classification of its more thorough set of stem keywords cw hPWCABILITY WITE RESPECT TO CRITERIA In order to contrast our approach it is ideal to analyse it with respect to the criteria we detailed above Use of Hierarchy Overall hierarchy is merely a two tiered approach where the SOM clustering is a foundation for association rule data mining However there is an intrinsic hierarchy to the rules themselves in that the derived associations can lead to subsequent ones For example an association that there is a high degree of support that documents with reference to nests also reference buds can point to a corollary association in which it was discovered that documents referring to buds also referred to trees Level of Hybridisation The amalgamation of association rule data mining theory with that of SOM classification entails a relatively high level of hybridisation Degree of Generaliry Although specifically designed for text mining applications, the initial source of stemmed source tern may be kom a repository of text documents or a datastore of meta-terms relating to multiple text document sources Indeed there is no perceivable limitation on the holistic approach to even non-text based originating sources as long as the rule derivation maintains a base level of 223common sense\224 to the original data Requirement for Supervisiodhteraction There is no perceived requirement for supervision in this model also there is deemed to be little necessity of pre-requisite user interaction except in terms of defining the initial document source At the completion of the postprocessing stage it is feasible that user interaction may occur with the model\222s results If the initial original data source was a pool of meta-data on lower level document stores the user may wish to repeat the process on a lower level of abstraction on the actual text document repositories deemed applicable The inherent methodology utilised in the SOM hybrid is unsupervised, whilst associative user interaction may be introduced to exploit the Underlying architectwe the base model is foreseen to be independent of supervision requirements V CONCLUSION AND FUTURE WORK Existing hybrids of SOM theory are not perceived to effectively address the requirements in data mining text documents: a high degree of generality for generic text mining without the requisite initial supervised training An optimum solution would efficiently flag applicable documents Without a necessity for a phase of preparation via training or high level of user interaction Furthermore, it is exceptionally desirable that multiple levels of abstraction be catered for in which more generic original sources with less granularity may be drilled down into to obtain more focussed dependent repositories The proposed amalgamation of SOM and association rule theories is identified as a feasible model for such text mining applications It is envisaged that the viability of this technique may be proven with respect to efficiency and precision of results Further scope also exists in implementing corollary SOM and association rule theory to expedite the output delivery but the underlying fundamental methodology is viewed to be a sound platform for relatively expeditious and efficient text mining REFERENCES  I]Pmdh P.C et al Associbfion, statistical mathematical andnewal approaches for mining breast cancerpattens Expext System with Applications 1999 17\(3 p 223-232 2]Kohonen T et al SelOrganirorion of a hive Document Collection IEEE Transactions on Neural Network 2000 I l\(3 p 574-585 3]Klose A et al Interactive Text RebievalBaedon Document Simihities Phys Chem Farch A 2000.25\(8 p 649454 4]Merkl D Terl classification with se@rganizing maps Some lessom leu Neumcomputing 1998.21\(1-3 p 61-77 5]Savoy J StaTirtical Inference in Rehieval Effectiveness Evaluation Information Processing and Management 1997.33\(4 p 495-512 6]Rilof\200 E and W Lehnert Information extraction as a basis for high-precision tat chs#?cation ACM Trans on information Systems 1994.12\(3 p 296-333 7]Chang C.-H and C.C Hsu Enabling Concept-Based Relevance Feedback for Information Rebieval on the W IEEE TraasaCtons on Knowledge and Data Engineering 1999 1 l\(4 p 595608 8]O!Donnell R and A Smeaton A Linguistic Approach to Infoonwrion Retrieval in 16th Resenrch Colloquium of the British Computer Society Information Renievol Specialist Group 1996 London Taylor Graham Publishing 9]Srinivasan P et al Vombulq mining for information reirievol mugh sets and fuavsets Information Rocegsmg and Management 2001.37\(1 p 15-38  IO]Kaski S et al WEBSOM  Serfdranizing nqm of cument collections Neummputbg 1998.21\(1-3 p 101-117  1 IIVesanto I and E Alhoniemi Clusterkg of the Self-Organizing Mp IEEE Transactions on Neural Networks 2000 I l\(3 p 586600 IZ]Alahakoon D S.K Halgamuge and B Srinivasan DynamicSerforgMLing Maps with Controlled Growth for Knowledge LJiscovety IEEE Transactions on Neural Networks 2000.1 l\(3 p 601-614  13]Cmera E and A.P. del Pobil Multiple se&rganizing maps A hybridlaming scheme Neurocomputhg 1997 M\(4 p 309-318  14]Wan W and D Fraser Multisource Data Fusion with Mlttple Ser/-Ofganizing Maps EEE Transactions on Geoscience and Remote Sensing 1999.37\(3 p 1344-1 349 Neumcomputing 1998.21\(1-3 p 19-30 Map.and Concept Space Techniques Jqumal of the American Society for Information Science 1998.49\(7 p 582-603 Knowledge and Data Engineering 2000 12\(3 p 372-390 Decision Suppod Systems 1999.27\(3 p 329-341 Databases Information Systdms 2000.25\(5 p 323-343 I SIKohonen T and P Somermo Sel/drganizing maps of symbol strings 16]Chen H et al Internet Browsing andSearching User Evafuafions of Coregory  1 7]Zaki M.J Scalrble Algorithm for Association Mining IEEE Transactions on I BlBoley D et al Pamoning-based clmteringfir Web document categorization 19]Pudi V and J.R Hmitsa Quantifing the Utility of the Pasf in Mining Large 971 


row with numbers of items supported by it As discussed earlier an attribute value in a row may supports exactly one item The calculations are supported by fuzzy querying module. When the data set is ready an extemal application is started with this data set given on input J Calculation of the support for the regular 1 itemsets Extemal application reads input data set and immediately calculates support for the regular 1-itemsets It also records for each I-itemset numbers Ds of rows supporting it Construction of 1-itemsets of type 9 and calculation of their support Only regular 1-itemsets of the support higher than a user-specified threshold are taken into account Number of produced I-itemsets of this type for given attribute depends on the number of fuzzy values defined for it The support is obtained by summing up the support of constituent reular I-itemsets All new 1-itemsets are numbered according to a specific convention Pruning of the set of 1-itemsets All itemsets with the support lower than the support threshold minsup are discarded Additionally also itemsets with the support higher than another threshold an item omit threshold are discarded since items present in almost all records contribute nothing interesting to the produced rules Construction of 1-itemsets of type lo calculation of their support and pruning Both regular and l-itemsets added in the Step 5 are considered we refer to them jointly as simple 1-itemsets Produced I-itemsets are identified with lists of constituent simple 1-itemsets The lists are ordered lexicographically what makes the process of generation more efficient Support is computed for generated itemsets and those below minsup threshold are dropped All itemsets produced so far and passing pruning form now the collection of l-itemsets SETk=2 Generate k-itemsets They are generated from frequent k-1 as in AprioriTid. Pairs of frequent k-1 itemsets of the form AI/\\AAZ~.../\\A and BlhB>~...hBk where A for i=l,..,k-2 are sought Then the new k-itemset of the form AI/\\AZ~.../\\A~.I~~.l is generated In the original algorithm the rules so generated are additionally tested and possibly eliminated before Step 7 On the other hand we add another k-itemset generation limitation namely items Abl and have to correspond to different original attributes. This requirement is obvious if items Atl and Bk are regular. Otherwise, identifying an item 9 or IO with a list set of attributes referred to within it we require that the intersection of these sets is empty Calculate support for all k-itemsets The calculation is based on the recorded numbers ID\222S of rows supporting particular k-1 The similar data on the supporting rows is produced for k-itemsets Pruning of the set of k-itemsets as in Step 6 As the result we obtain the frequent k-itemsets IF the set of k-itemsets is void THEN GOT0 Step 11 SET k k 1 GOTO Step 8 Generate rules from frequent I-itemsets l=l,..,k-1 and output them to the file An external application completes its work 2 Display the results The module of FQUERY for Access regains control reads the results of the external application, decodes numbers of the items comprising produced rules and displays results in a form readable for the user The number of produced rules is usually huge Some counter-measures have to be undertaken as e.g., approaches to concise association rules representation see e.g IS We adopted the following pruning scheme A rule R1 is pruned if there exists another rule R2 such that the three conditions are met simultaneously 1 the antecedent of R2 is a subset of the antecedent of R1 2 the consequent of R1 is a subset of the consequent R2 3 the confidence of R2 is not less than the confidence of R1 This leads to a substantial lossless reduction of the number of rules V CONCLUDING REMARKS We presented the use of fuzzy linguistic summaries as a natural and human consistent tool for data mining We indicated that Zadehs 31 concept of a protoform and a hierarchy of protoforms may conveniently be used to present various linguistic summaries We extended our previous work on the application of association rules mining technique for generation of linguistic summaries We proposed more sophisticated forms of fuzzy association rules Their mining becomes even more complex These applies especially to the concept of linguistically aggregated elementary items 10 Similar concept has been recently proposed by other authors 27 Their AND-OR taxonomies share some expressive power as well as computational complexity with our proposal The latter is addressed in 27 with a strong restrictions on the allowed combinations of items in itemsets. Both approaches are also in some sense 223opposite\224 in 27 a sophisticated taxonomy is assumed as in other classical approaches to linguistic association rules mining cf  while in our it may be to some, limited extent, recovered We show that data mining tools dealt with in the paper based on linguistic summaries can be implemented through 706 The IEEE International Conference on Fuzzy Systems 


our FQUERY for Access a fuzzy querying interface cf Kacprzyk and Zadroiny ll 161 REFERENCES  I Agrawal R and Srikant R Fast algorithms for mining association rules in Proceedings of the 20 International Conference on Very Large Databases Santiago, Chile, 1994 121 Anwar T.M Beck H.W and Navathe S.B Knowledge mining by imprecise querying A classification based system in Proceedings of the International Conference on Data Engineering, Tampa, USA 1992,622-630  31 Bosc P and J Kacprzyk eds Fuzziness in Database Management Systems Physica-Verlag, Heidelberg 1995 141 Bosc P and 0 Piven Fuzzy querying in conventional databases in L.A. Zadeh and J Kacprzyk eds Fuzzy Logic for the Management of Uncenainfy Wiley New York 1992 645-671  51 Bosc P L Lietard and 0 Pivert Quantified statements and database fuzzy querying in P Bosc and I Kacprzyk eds Fuzziness in Database Management Systems Physica-Verlag Heidelberg 1995, 275-308  61 George R and R Srikanth Data summarization using genetic algorithms and fuzzy logic in F Herren and J.L Verdegay eds Genetic Algorithms and Soft Computing Physica-Verlag Heidelberg and New York 1996,599  611  71 Kacprzyk J and P Strykowski Linguislic data summaries for intelligent decision support in R Felix ed Fuzzy Decision Analysis and Recognition Technology for Management Planning and Optimization  Proceedings of EFDAN99 1999 3-12  81 Kacprzyk J and P Strykowski Linguitic Summaries of Sales Data at a Computer Retailer A Case Study Proceedings of IFSA'99 Taipei, Taiwan R.0.C vol I 1999,29-33  91 Kacprzyk 1 and R.R Yager Linguistic summaries of data using fuzzy logic 1nt J of General Systems vol 30 2001 133.154  Kacprzyk 1 R.R Yager and Zadroiny S A fuzzy logic based approach to linguistic summaries of databases Inr J of Applied Mathemnrics and Computer Science vol IO 2000 813-834 I I Kacprzyk J and S Zadroiny FQUERY for Access fuzzy querying for a Windows-based DBMS in P Bosc and J Kacprzyk eds Fuzziness in Database Management Systems Physica-Verlag Heidelberg 1995 415  433 I21 Kacprzyk I and S Zadroiny Implementation of OWA operators in fuzzy querying for Microsoft Access in R.R Yager and I Kacprzyk \(eds The Ordered Weighted Averaging Ope~ators Theory and Applications Kluwer Boston 1997, 293  306 1131 Kacprzyk J and S Zadromy Data mining via linguistic summaries of data An interactive approach in T Yamakawa and G Marsumoto eds Methodologies for the Conception Design and Application of Soy Computing Proc of IIZUKA'98 lizuka, Japan 1998,668-671 I14lKacprzyk I and S Zadroiny On Interactive Linguistic Summarization of Databases via a Fuzzy-Logic-Based Querying Add-on to Microsoft Access in Bemd Reusch ed Computational Intelligence Theory and Applicarions Springer Verlag, Heidelberg 1999, 462-472 ISIKacprzyk 1 and S Zadroiny On combining intelligent querying and data mining using fuzzy logic concepts in G Bordogna and G Pasi eds Recent Research Issues on the Management of Fuzziness in Databases Physica  Verlag Heidelberg and New York 2000 67-81 IbIKacprzyk J and S Zadroiny On linguistic approaches in flexible querying and mining of association rules in H.L Larsen J Kacprzyk S Zadroiny T Andreasen and H Christiansen eds Flexible Query Answering Systems Recent Advances Physica-Verlag Springer-Verlag Heidelberg New York 2001,475-484 17]Kacprzyk J and Zi6lkowski A Database queries with fuzzy linguistic quantifiers lEEE Tramon System Man and Cybernetics SMC  16 1986,474  479 I181 Kryszkiewicz M Concise representation of frequent patterns based on disjunction-free generators in Proc of ICDM'2001 San Jose, IEEE Computer Society Press, 2001 305-312 I91 Lee I.-H and Lee-Kwang H An extension of association rules using fuzzy sets in Proc of 7th IFSA World Conrress 1997 Prague Vol 1 399-402  1201 Mannila H Toivanen H and Verkamo A.I Efficient algorithms for discovering association rules in U.M Fayyad and R Uthurusamy eds Proc of the AAA1 Workshop on Knowledge Discovery in Databases Seattle USA 1994 181 192 1211 Miller R.J and Yang Y Association rules over interval data in Proc of the ACM-SIGMOD Inl'l Conference on the Management of Data, Tucson, USA 1997,452-461 1221 Petry F.E Fuzzy Databases Principles and Applications Kluwer Boston 1996 231 Rasmussen D and R.R Yager \(1997\Fuzzy query language for hypothesis evaluation in Andreasen T H Christiansen and H L Larsen eds Flexible Query Answering Systems Kluwer Boston, 23-43  Srikant R and Agrawal R Mining generalized association rules in Proc of 21 Int'l Conference on Very Large Databases", Zurich, Switzerland 1995 251 Srikant R and Agrawal R Mining quantitative association rules in large relational tables in Proc of the ACM-SIGMOD 1996 Conference on Management of Data Montreal Canada 1996 126lSrikant R Vu Q and R Agrawal Mining association rules with item constraints in Proc of 3rd Int'l Conference on Knowledge Discovery in Databases and Data Mining Newport Beach, USA 1997 I271 Subramanian D.K AnanThanarayana V.S and M Narashima Muny Knowledge-based association rule mining using AND OR taxonomies Knowledge-Based Systems 16,2003,37-45 1281 Yager R.R A new approach to the summarization of data Informarion Sciences vol 28 1982 69-86 1291 Yager R.R On linguistic summaries of data in G Piatetsky Shapiro and W.J Frawley eds Knowledge Discovery in Databases AAA1 Presfie MIT Press, Menlo Park 1991 347  363 301 Zadeh L.A A computational approach to fuzzy quantifiers in natural languages Computers and Maths with Appls 9 1983 I49  184 1311 Zadeh L.A A prototype-centered approach to adding deduction capabilities to search engines  the concept of a protoform BISC Seminar 2002 University of Califomid Berkeley, 2002 707 The IEEE International Conference on Fuzzy Systems 


1 2 3 4 5 6 7 8 9 10 References P. Cabena P Hadjinian R Stadler J Verhees and A Zanasi Discovering Data Mining: From Concept to Implementation Prentice Hall 07458,1997 pp 12 30 N Lavac E Keraunou and B Zupan 223Intelligent Data Analysis in Medicine and Pharmacology.\224 IDAM4P-97 Nagoya Japonska Kluwer Acedemic Publishers 1997 pp 61-67 W Horn 223Artificial Intelligence in Medicine on its Way from Data-Intensive to Knowledge-Intensive\224 Austrian Research Institute for Artificial Intelligence Technical Report TU-2001-01 Vienna Austria Val 23 NO 1,2001 pp 5-12 N Lavrac 223Data Mining in Medicine Selected Techniques and Applications\224 In Proc of the Second Intemational Conference on ne Practical Applications of Knowledge Discovery and Data Mining pp 11-31 S Startchik Geometric and Illumination Invariant Object Representation Application to Content-based Image Retrieval Ph.D Dissertation No 3009 University of Geneva, Switzerland, July 1998 T Zrimaec 223A Medical Image Information System\224 VISIM Workshop Utrecht Netherlands Oct 2001 Paper 2 G.D Magoulas and A Prentza, \223Machine learning in Medical Applications\224 Workshop on Machine Leaming in Medical Applications ACAI-99 1999 pp.53-58 P A Devijer and J Kitter Pattem Recognition A Statistical Approach Prentice-Hall 1982 K Woods Automated Image Analysis Techniques in Digital Mammography Ph.D Thesis, Department of Computer Science and Engineering University of South Florida, December 1994 W Chu I Leong R Taira C Breant 223A Temporal Evolutionary Object Oriented Data Model and its Query Language for Medical Image Management\224 Proceedings of the Id KLDB conference Vancouver Canada 1992 pp.53-64 11 V Megalooikonomou J For L Shen F Makedon Data Mining in Brain Imaging Statistical Methods in Medical Research 2000 pp 359-394 12 0 R Zaiane Resource and Knowledge Discovery fiom the Internet and Multimedia Repositories PhD Thesis School of Computing Science Simon Fraser IJniversity, March 1999 13 M Antonie 0 Zaiane A Coman 223Application of Data Mining Techniques for Medical Image Classification\224 In Proceedings of the Second i\221nternational Workshop on Multimedia Data Mining with ACM SIGKDD Coi?ference \(MDM/KDD\2222001 San Francisco USA Aupst 26,2001 14 J Hipp V Guntzer and G Nakhaeizadeh 223Algorithms for Association Rule Mining. A General Survey and Comparison.\224 ACM SIGKDD Volume 2 Issue 1 July 2000 pp 58 64 15 R Aggrawal and R Srikant 223Fast Algorithms for Mining Association Rules\224 In Proceeding of the 20th International Conference of Very Large Data Eases VLDB Chile 1994 pp 487 499 16 Ginneken B Romeny Statistical Local Texture Analysis Applied to Computer- Aided Diagnosis in Chest Radiography Statistics of Shapes and Textures Copenhagen summer school in computer vision September 4-8 2000 17 B Ginnekan Computer Aided Diagnosis in Chest Radiography PhD thesis, Utrecht University March 2001 pp 14-17 18 C Ordonez C Santana and L Brad 223Discovering Interesting Association Rules in MedicaI Data\224 ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery 2000 pp 78-85 19 C Aggrawal and P Yu 223Mining Large Itemsets for Association Rules\224 In Bulletin of the IEEE Computer Society Technical Committee on Data Engineering V01.2 N0.1~1998 Pg 23-31 20 C Ordonez and E Omiecinski 223Discovering Association rules Based on Image Content\224 In Proceedings of the IEEE Advances in Digital libraries Conference ADL 222993 Pg 38 49 21 K Lee 221\221Intelligent Shape based Image Analysis\224 Department of Computer Science, University of Iowa 55:247 Project 2001 2:2 J.Han J Pei and Y Yin 223Mining Frequent Patterns without Candidate Generation\224 In Proceedings of SIGMOD-2000 Dallas, May 2000,pp 1  12  1187  


3 3.5 fixed-style adap-style Er 4 S1 White burst the transient signal s1 is white and Gaus40 02~45\260\260 10 rn 0 sian with zero mean S2 Single exponentially-decaying sinusoid 30 60 20 X 4050 f S2\(i Ce f cos\(27f i/foq5 33 20  0   260 30   for i I l  M with the phase  randomly chosen from   20   0 2 7 and the frequency f randomly generated in the range 10   10       1 Js 4 1\260 10 1 0 0 0 0.75 0.7~~~~~~~~~~~~~~~~~~~~~~0 0.9 0.85 00 0 0 0 000 ogpq 0 0 0 1 02 1 03 legt le 1 1h02 1033 3 Exponentially-enveloped white burst transient length transient length Figure 9 Exponential case The right plot gives compariS3 i efS si i 34 son of the bias and thresholds used in the new adaptive Page procedure to those of the fixed-style Page scheme tailored to for i 1   M each specific transient length In all cases T 10l6 The agS4 Narrowband burst S4 iS created by passing white gregate SNR SL corresponding to the bias used in the fixed Gaussian noise through a narrowband filter whose bandwidth Page schme is pltted in he left igure.1S 0.3wr and whose center frequency iS chosen randomly 8 10\26065 _F 0~~~~~~~~~~~~~~~~~~~0 bL where bL denotes the bias for the fixed-style comparison only Page designed for length L 4 SIMULATION STUDY Again considering the fixed-style Page scheme first based The purpose of this section is to study the performance of the on the thresholds h's calculated by the FFT approach we above designed adaptive Page test regardless of the transient find from figure 8 that the behavior of the Page test designed signal's form strength and location The signal model is for particular transient lengths L does not provide constant detectability and thus modification according to our adaptive Ho x  w 31 Page procedure is desirable Therefore b and h\(L are ob tained according to 23 and 21 The corresponding fixed H1 x\(n  s\(n n men  n  n  nd bias b and variable threshold h\(L are plotted and compared s\(n  w\(m nS  n  mS  Td to those used in the fixed-style Page tests in figure 9 The 32 performance of the adaptive Page procedure is given in figure 10 and a gratifying detection improvement is observed in which x denotes the observation vector w is white Gausespecially for shorter transient lengths sian noise with zero mean and unit variance the vector s is the transient signal of interest The transients are of short du-1 ration M compared with the observation length N In our 60r 7 r 1.5 simulations we always use N  128 M  30 fs  16  2  and A  0.5 unless stated otherwise The transients are as 2.5 50   2.5  follows 50 10\260 1021o2 100 t analysis fixed-style envelope sient lengh0 0 simulation fixed-styld te envelope betwen7flse0larm0is  l6 Te udatetake theform Pagetestoptiize  anansiena adap style simulation adap style Page 0.4 designed for L200 loo 101 0 2 10 oo 101102103 transient length transient length Figure 8 Detect"ab"ility of exponential transients using fixedFigure 10 Performance of the adaptive Page scheme in style Page procedures designed for various but specific tranExponential transient problem The Pd envelope from the sient length L In all cases Pd 0.8 and the average time normal fixed-style Page procedure and the performance of the between false alarms is T 106  The update takes the form Page test optimized for transient length L=200 are shown for g\(x x 


2 35 04 T 2 0.4 0.2 0.2 provides the best performance over a wide range of transients 10 15 20 25 10 15 20 Unlike the Page detector the transient duration M is required here and Tmax does indeed show some sensitivity as regards c d this parameter as illustrated in figure 11 for transient signal 7  sl Similar relationship between Tmax and M is observed for 0.8 0.8 other types of transients 0.2 0.75 0 Cl 10 15 20 25 10 15 20 0.7  a aggregate Certainly this is not an exhaustive menu of transients but a for v is 2.5 when information about M is completely unavailwide range is covered able In fact several new power-law detectors were developed in 19 for instance we have We apply the adaptive Page detector designed in section 3 the Gaussian shift-in-variance case to the above transient's T a _da Page 0.6 Tma 0.6 Mf-1+i 0.6 l0.6 0.8 0.4 0.4 0.2 X X XN 37 detection The b and h\(L derived according to 21 is 12\(N Z\(Xi  X+\261 3 used in this adaptive detector where Pd  8 T  106 and t N  128 The assumptions on which the adaptive Page proSimilarly by combining 3 contiguous FFT bins we can write cedure is built are those of Si the detector is weakly suited Tf 3 It was found that for most practical transient signals Tf 2 to S3 and would seem to bear little affection for either S2 or and Tf 3 were preferable to Tpl S4 a b To illustrate the performance of our adaptive Page detector we compare it to other detectors In 18 it was found that 0.8  0.8 Nuttall's maximum detector 14 SNR dB aggregate SNR dB 0.65  Figure 12 Detection performances of the adaptive Page scheme The transient duration is M  30 samples different oT 0.6  panels refer to different transient signals with a transient signal sl b transient signal 82 c transient signal 83 and 0.55  d signal 84 0.5  We plot Pd versus the aggregate SNR in figure 12 in which Pf a  10-4 It is noted that the adaptive Page procedure 0.45 provides very close performance to that of the maximum detector in all four situations which is best in all cases as 0.4 0 20 40 60 M 80 100 120 in 18 This is exciting as we recall that M is tuned in the maximum detector and that our adaptive Page test reFigure 11 Detection performances of Tmax vs Al for quires no such prior information It is additionally noted that transient signal sl The true transient duration is M  30 qursnschpirnfmao.Itsadtoalyoedht transient  sina  Th tretasetduaini.l 3 the adaptive Page procedure provides performance superior samples and the aggregate SNR is 18dB The dash-dotted line indicates the best performance of Tmax when true M to even the improved power-law detector Tf12 in most cases is chosen The dotted line indicates the performance of the with the exception of S2 in which the transient is highly naradaptive Page scheme rowband Nuttall's Tmax looks for an increase in empirical variance as 5 SUMMARY does our Page processor In  1 8 it was found that another detector due to Nuttall works particularly well for narrowband The standard Page test is designed to detect a change in distritransient signals this based on the power-law statistic 15 bution amongst a conditionally independent observation pool defined as but works nicely at detecting even transient changes provided N they are of known character The standard Page update has Tp1 N 5 i 36 implicit a fixed negative bias and a detection is recorded i upon passage by this update of a fixed threshold these the In 36 v is an adjustable exponent and the Xi  are bias and threshold are determined by the ambient and tranmagnitude-squared FFT bins corresponding to the observasient distributional models fO and fil Specifically when fO tions x It has been found that the best compromise value and fi are close the bias is light and threshold high and 9 1 0 T with v=2.5 0.4 Tmax max S 


when fo and fi are distinct the Page test uses heavy bias and quence of Random Variables Biometrika Vol 57 No low threshold 1 pp 1-17 1970 6 B Broder and S Schwartz Quickest Detection ProceNotionally a transient signal that is long-and-quiet and one dures and Transient Signal Detection ONR Technical that is short-and-loud ought to have approximately the same Res and Transier 1990 detectability However these two engender very different Report 2 November 1990 Page tests and unfortunately the test designed for one can 7 D Casasent J-S Smokelin A Ye Wavelet and Gabor work quite poorly for the other Consequently in this paper Transforms for Detection Optical Engineering Vol an adaptive Page processor has been developed it uses a con31 No 9 pp 1893-1898 September 1992 stant bias but has a threshold that adaptively changes with the 8 S Del Marco and J Weiss M-band Wavepacket-Based number of samples since the most recent reset Transient Signal Detector Using a Translation-Invariant Wavelet Transform Optical Engineering Vol 33 No The new detector has been studied extensively in the Gaus7 pp 2175-2182 July 1994 sian shift-in-mean and shift-in-variance and also in the expo[9 T Dyson Topics in Nonlinear Filtering and Detecnential shift-in-scale cases It works very well and essentially tion PhD Thesis Princeton University Princeton NJ traces the envelope of performance achievable with the best 1986 Page processor tuned to each transient length the proposal is reasonable but ad-hoc but apparently we hardly could do 10 B Friedlander B Porat Performance Analysis of better Transient Detectors Based on a Class of Linear Data Transforms IEEE Transactions on Information TheTransient detection is interesting because one does not know ory Vol 38 No 2 pp 665-673 March 1992 in advance the sort of transient signal one has to look for it 11 C Han P Willett and D Abraham Some Methods could be narrowband or not it could have a sharp attack or it to Evaluate the Performance of Page's Test as used to could increase slowly and disappear abruptly Many transient Detect Transient Signals IEEE Transactions on Signal detectors are tuned to one type of transient and comparatively Processing pp 2112-2127 August 1999 blind to others What tends to unite transient signals of prac[12 G Lorden Procedures for Reacting to a Change in Distical interest however is that they are an organized agglomtribution Annals o Mathematical Statistics vol 42 eration of energy into contiguous or nearby time samples 1897-1908 1971 Now assuming a unit-normal ambient a transient detector pp 1 that assumes nothing but this local scale-change and one that 13 G Moustakides Optimal Stopping Times for Detectis reasonably insensitive to other characteristics such as specing Changes in Distributions Annals of Statistics vol trum is that based on the Page structure for Gaussian shift14 pp 1379-87 1986 in-variance The adaptive Page test developed here is also 14 A Nuttall Detection Capability of linear-And-Power insensitive to transient length it has here been tested for a Processor for Random Burst Signals of Unknown Locavariety of transient signals for which it is not on the surface tion NUWC-NPT Tech Rep 10,822 August 1997 well-suited and its performance has been found remarkably N o-Law Progood 15 A Nuttall Detection Performance of Power-LwPo good.a cessors for Random Signals of Unknown Location REFERENCES Structure Extent and Strength NUWC-NPT Technical Report 10,751 September 1994 1 D Abraham Asymptotically Optimal Bias for a Gen[16 E Page Continuous Inspection Schemes Biometrika eral Nonlinearity in Page's Test IEEE Transactions vol 41 pp 100115 1954 on Aerospace and Electronic Systems pp 1-8 January 17 B Porat and B Friedlander Performance Analysis of 1996 a Class of Transient Detection Algorithms-A Unified 2 M Basseville and I Nikiforov Detection of Abrupt Framework IEEE Transactions on Signal Processing Changes Theory and Application Englewood Cliffs Vol 40 No 10 pp 2536-2546 October 1992 NJ Prentice Hall 1993 18 Z Wang and P Willett A Performance Study of Some 3 A Shiryaev On Optimum Methods in Quickest DetecTransient Detectors IEEE Transactions on Signal Protion Problems Theory Prob Appl Vol 8 No 1 pp cessing Vol 48-9 pp 2682-2686 September 2000 22-46 1963 19 Z Wang and P Willett All-Purpose and Plug-In 4 M Basseville Edge Detection Using Sequential MethPower-Law Detectors for Transient Signals IEEE ods for Change in Level-Part  Sequential Detection Transactions on Signal Processing November 2001 Of Change in Mean IEEE Transactions on Acoustic 20 P Willett and B Chen A New Sequential Detector Speech and Signal Processing Vol ASSP-29 No 1 for Short Duration Signals Proceedings of ICASSPFeb 1981 98 Seattle WA May 1998 5 D Hinkley Inference About the Change-Point in a Se[21 P Willett and Y Bar-Shalom Track Testing for Single 10 


Targets in Clutter Proceedings of the SPIE Aerosense Now using the above procedure we can calculate hL for tranConference on Signal Processing for Small Targets sient duration L L  1  N given T and thus the timeApril 2000 varying threshold h\(L via 21 and also the performance in terms of Pd of the Page and our adaptive Page tests APPENDIX 2 EVALUATE PERFORMANCE OF ADAPTIVE 1 THE FFT APPROACH TO EVALUATE PAGE TEST PERFORMANCE OF FIXED-STYLE PAGE TEST For the transient change problem modeled as in 10 the runlength metrics 5 and 1 are of less interest than they would be for the permanent change problem Further and perhaps more important given their context in this paper these approximations do not apply at all in the case of a time-varying Uration A Page update Thus given the update rule and the average time-between-false alarms T we employ the FFT approach introduced in 11 to obtain the requisite threshold h that satisfies it and then to get the detection performance Pd Interested readers please refer to 11 for detail since here only a j l brief description is given AYvvfj Consider Page's test as an iterated sequential test ST with 7 n sample ihdex n    sbti-Sb lower and upper thresholds 0 and h Each individual ST is defined as an update rule Figure 13 Illustration of a Page implementation 20 with Z Z.i  g\(x non-zero initial point The change starts at point nm indicated by the dotted line and a decision rule as in 20 The procedure to calculate the probability of detection for the Thus the pdf of Zn is adaptive Page scheme is complicated by the fact that the Page bn Z  fn\(Z  fg Z 38 statistic be non-zero at the start point of a change Figure 13 shows such an example where the transient change begins at where fg is the pdf of the update g\(xn fn-i denotes the point nm and the threshold index i  5 at the start point nm pdf of Zn given that the test has continued to time n and due to the non-zero initialization Since the threshold index  denotes convolution the convolution can be made both i plays an important role in the adaptive scheme a non-zero accurate and quick via a fast Fourier transform FFT Then initialization could result in a different detection decision It we compute is thus necessary to calculate detection probabilities for difOn Z ferent threshold index i corresponding to the start point nm fn\(z f h n  0  z  h 39 Overall under the H1 hypothesis we have JO fn z dz 00 as a direct normalization In a straightforward manner under Pd\(nd S p\(i nd i 42 the Ho hypothesis one can express T as i=l F Eo  F1 N 40 where nd is the transient length and i is the threshold index T n=Z P1\(n corresponding to the start point nd According to the definiwhere Ei N is the expected number of samples to a decision tion the pmf p\(i is decided by the characteristics of the test for hypothesis Hi and pi n Pr\(ST ends at n and decides Hl Ho Under the H hypothesis assuming the standard situation we p\(i Pon i 1 Ho 43 have E 0 Pon nm Ho ndl1 Pd nd 5 Pr detect k resets 41 where Pon n l Ho Pr ST will continue to time-step n I I Ho k=o Under the Ho hypothesis assuming fo z  d\(z with the where nd is the transient length6 update g  we can calculate the pdf fr,\(z H0 according to 39 and thus calculate Pon nm Ho0 correspondingly 61n 11 it was noted that the probability of detection is increased both by latent detections caused by diffusive threshold-crossings after a transient's For each index i to calculate the corresponding Pd md i in end and also by a non-zero CUSUM value at the inception of a transient  Both of these can be accounted for via the direct FFT approach and for 42 we need to study stopping probabilities both for the case details we invite the reader to examine 11 fo Z f&i-1\(Z Ho and for fo z 5\(z  For the case that 11 


fo z fi 1 z Ho we consider a decision rule Z Jane Wang Z Jane Wang received the BSc degree from Tsinghua Univer[h\(n+i-1 stopanddecideH sity China in 1996 and the MS and ZC  h[\(n  i1 c ntine dtest 44 PhD degrees from the University of Con 0 h\(m  i 1 continue test necticut in 2000 and 2002 respectively oc 0 stop and decide Ho all in electrical engineering She spent two years as Research Associate of ElecAd b d ts d rtrical and Computer Engineering DeAnd based on this decision rule 44 we compute the imporpartment and Institute for Systems Research at the Univertant quantities sity of Maryland She is now an Assistant Professor in the Department of Electrical and Computer Engineering at the pO\(m  Pr\(ST ends at n and decides Ho University of British Columbia Her present research interests are in the broad areas of statistical signal processing p n  Pr ST ends at n and decides Hi r4 in Pr\(ST ends at in and decides H1 information security and wireless communications Respectively for the case that fo z  d\(z we consider a decision rule Peter Willett Peter Willett is a Profesh c s and decide H sor of Electrical and Computer EngiZne 0,h cstop 1 neering at the University of ConnectiZn E 1 0,h\(n continue test 45 _ cut Previously he was at the University oo 0 stop and decide Ho of Toronto from which he received his BASc in 1982 and at Princeton University from which he received his PhD in and compute the corresponding quantities p\260\(n and po n 1 H h w e mrt Now using p n and p n and pog\(n and po n we can  E 18.Hha rte,mogterop Now,'using a a  ics about the processing of signals from volumetric arrays calculate Pd nd i as in 41 and finally calculate the overall decentralized detection information theory CDMA learPd nd in 42 It is worth to mention that for theoretical d  ing from data target tracking and transient detection He analysis we use the infinite as the upper bound of i in 42 is a Fellow of the IEEE and is a member of the IEEE Sighowever we use a finite reasonable upper bound in practical nal Processing Society's SAM technical committee He is an calculation For instance in the Gaussian shift-in-mean appliassociate editor both for IEEE Transactions on Aerospace cations we plot p\(i vs i in figure 14 From this figure it is and Electronic Systems and for IEEE Transactions on Sysclear that p\(i decays quickly with the increase of i therefore tems Man and Cyberetics He is a track organizer for it is reasonable to consider the truncated pmf Similar obserRemote Sensing at the IEEE Aerospace Conference 2001vations could be found for the Gaussian shift-in-variance and 2003 and was co-chair of the Diagnostics Prognosis and the Exponential cases System Health Management SPIE Conference in Orlando He also served as Program Co-Chairfor the 2003 IEEE Systems 02 Man  Cybernetics Conference in Washington DC 0.18 0.16  0.14 0.12 a 0.1 0.08 0.06 0.04 0.02 100 101 102 103 Figure 14 The pmf p\(i in the Gaussian shift-in-mean case 12 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


