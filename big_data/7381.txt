Discovering Long Maximal Frequent Pattern Shu-Jing Lin National Chung-Shan Institute of Science & Technology Taoyuan, Taiwan Yi-Chung Chen and Don-Lin Yang Department of Information Engineering and Computer Science Feng Chia University Taichung, Taiwan Jungpin Wu Department of Statistics Feng Chia University Taichung, Taiwan Abstract 227 Association rule mining, the most commonly used method for data mining, has numerous applications. Although many approaches that can find association rules have been developed, most utilize maximum frequent itemsets that are short. Existing methods fail to perform well in applications involving large amounts of data and incur longer itemsets Apriori-like algorithms have this problem because they generate many candidate itemsets and spend considerable time scanning databases; that is, their processing method is bottom-up and layered. This paper solves this problem via a novel hybrid Multilevel-Search algorithm. The algorithm concurrently uses the bidirectional Pincer-Search and parameter prediction mechanism along with the bottom-up search of the Parameterised method to reduce the number of candidate itemsets and consequently, the number of database scans Experimental results demonstrate that the proposed algorithm performs well, especially when the length of the maximum frequent itemsets are longer than or equal to eight. The concurrent approach of our multilevel algorithm results in faster execution time and improved efficiency Keywords 227 data mining; association rule; maximum frequent itemset; long itemset; multilevel search  I  I NTRODUCTION  In data mining or knowledge discovery [1, 2 a n an a l yt i c  method is applied automatically or semi-automatically to look for meaningful rules or patterns in large amounts of data. The main goal of data mining is to extract useful information from huge databases in an effective and efficient ma   Data  mining is very useful in both data management and decision making. Association rule mining 4 h e m o st  f r e q u e n t l y us e d  data mining technique, uses the Apriori algorithm to discover the relationships between various points of data in a database Many mining algorithms are based on the Apriori algorithm as it is simple and straightforward The Apriori algorithm has two phases, for finding frequent itemsets: candidate generation and verification. A frequent itemset is a set of items that appear together in a number of database records and their occurrence frequency meets a pre-defined threshold. To find all frequent m-itemsets for m starting from 1 to the maximal length of frequent itemsets, the algorithm must produce all 2m of its subsets and scan the database m times. With their exponential complexity, Apriori-like algorithms are restricted to only short frequent itemsets To overcome this limitation, this paper proposes the Multilevel-Search algorithm that efficiently extracts the maximal frequent itemsets, where an itemset is maximal frequent when it has no superset that is frequent. Experimental results demonstrate that the proposed approach is most efficient when maximum frequent itemsets are long. This paper  to literature is its novel approach to effectively seeking longer association rules [5 s i n g b o t t om up and top-down searches concurrently with a multilevel approach. The remainder of this paper is organized as follows Section II explores relevant methods related to literature Section III introduces the concept and pseudo code of the proposed Multilevel-Search algo rithm. Section IV presents detailed implementation and explanatory examples. Section V provides comparisons, experimental analysis, and efficiency evaluations, and Section VI contains concluding remarks and future directions for research II  R ELATED W ORK  The number of organizations utilizing mining association rules [6] to discove r useful information is increasing Definition of association rules Let I = {i1, i2,..., ik} be a set of k distinct items. A transaction T is a set of items in I and T I. A transaction T can repr esent items purchased by a customer from a supermarket. A database D is a set of transactions. An itemset is a set X of items \(X I number of items in an itemset is its length. Itemsets of length k are referred to as k-itemsets. A transaction T is said to support an itemset X I if and only if X T. The fraction of transactions in D that support X is the support of X, denoted as support\(X\user can define a minimum support threshold which is a fraction or percentage An association rule has the form R: X Y; X, Y I and X Y The support for rule R is defined as support\(X  Y\. A confidence factor for such a rule \(customarily represented by percentage defined as 100% \327 support\(X  Y\ / support\(X\, and is used to evaluate the strength of an association rule. An itemset is frequent when its support meets the minimum support; otherwise, it is infrequent. A frequent itemset is interesting \(or strong\ when its confidence meets the minimum confidence level This paper focuses on finding the maximal frequent itemsets [7-9  An i t e m s e t is a  m a x i m a l f r e q uen t ite m s e t w h en  it is frequent and no proper superset of it is frequent. The maximal frequent set \(MFS\f all maximal frequent itemsets. To find maximal freque nt itemsets \(M FI\ficiently Lin and Kedem \(2002 l ie d t h e Pincer S ea rch  algorithm that quickly searched for maximal frequent itemsets in two directions: bottom-up and top-down. Denwattana and 136 8th International Conference on Advanced Computational Intelligence\nChi\ang Mai, Thailand; February 14-16, 2016 n978-1-4673-7782-9/16/$31.00 ©2016 IEEE 


Getta \(2001 pro p ose d t h e Para m e teris ed alg o ri th m b a sed  on the Apriori algorithm. Hong et al. \(2009 o proposed a mining algorithm based on the Parameterised algorithm, applied the idea of a bottom-up search with multiple levels to reduce the number of candidate itemsets and the number of database scans to accelerate the process of finding maximal frequent itemsets. The method proposed in this paper mainly integrates concepts from both the Parameterised and Pincer-Search algorithms III  O UR E FFICIENT A LGORITHM FOR D ISCOVERING M AXIMAL F REQUENT I TEMSETS  A  The Concept Informing the Proposed Approach The proposed Multilevel-Search algorithm is based on bottom-up and top-down searches of maximal frequent itemsets of the Pincer-Search and Parameterised algorithms This algorithm can go up or down many levels in one pass First, the Multilevel-Search algo rithm constructs a statistics table and sets the required parameters, as does the Parameterised algorithm. For instance, a parameter n indicates how many lattice levels [1 e tra v ersed a t a t i m e  Parameter k is the start level of the mining process. The next step is to predict the candidate from k to k+n-1 msets and scan the database once to verify them. When the prediction is incorrect, the database is scanned again to correct the error The process continues and works efficiently by using a bidirectional search until the end condition is met. Thus, the Multilevel-Search algorithm can eliminate more redundant itemsets than other methods and also reduces the number of database scans B  No Missing Candidate Itemsets The algorithm starts from the lattice level of the parameter k 2; the value of k is increased by parameter n. Then predicted-frequent k  n 1\msets are produced and used to eliminate lower-level candidate itemsets. Section IV.D describes the process in detail Lemma 1 Pruning of redundant candidate itemsets in the Multilevel-Search algorithm is based on the prediction that a lattice level does not result in any candidate missing from the next level Proof When the prediction is correct, the algorithm finds the correct result without the need to recover any candidate itemset; otherwise, the recovery procedure in Section IV.D is used to make the necessary correction. Here is the end of the proof C  Multilevel-Search Algorithm The definition of symbols MFCI := Maximal frequent candidate itemsets MFI := Maximal frequent itemsets C k Candidate k itemsets  L k Frequent k itemsets  inf Infrequent itemsets RC Remaining candidate itemsets ST Statistics table  n The number of lattice levels traversed at a time minsup The minimum support t m The user specified threshold of itemset m support t l The user specified threshold of transaction length k C Predicted-frequent k-itemsets k C Predicted-infrequent k-itemsets  Algorithm Multilevel-Search algorithm Input: A database and us er-defined parameters minsup, k, n tm , tl  Output: MFI contains all maximal frequent itemsets Results  k 2 Call the Statistics table procedure to generate a statistics table i.e  ST  Scan ST and count supports for ever y 1-item to generate L 1  Join L 1 to generate C 2  MFCI i  i  L 1 MFI  1 while  L k-1   and C k   do  2  Call the Predict_candidates  procedure to generate 3.   predicted-frequent and pr edicted-infrequent itemsets 4.  Scan database and count s upports for predicted-candidate itemsets and MFCI 5  both in the bottom-up and top-down approach   6.  Move frequent itemsets from MFCI to MFI 7 inf infrequent itemsets in candidate itemsets 8.  Update the MFCI if inf   9  in the top-down approach   10 if there is an incorrect prediction 11.    Call the Recovery procedure to generate remaining candidate itemsets RC  12.    Scan database and count supports for RC and MFCI 13  both in the bottom-up and top-down approaches   14.    Move frequent itemsets from MFCI to MFI 15 inf infrequent itemsets in RC  16.    Update the MFCI if inf  17  in the top-down approach   18 end-if  19.  Join frequent n+k-1 sets to generate C n+k  20  in the bottom-up approach   21 C n+k   C n+k  subsets of MFI 22 L n+k-1 frequent n+k-1 sets subsets of MFI 23.  Results := {Results  L k L k+1 L k+n1   subsets of MFI 24 k  k+n  25 end-while  26. MFI := Results MFI 27. return MFI Fig. 1.  The pseudo code of the Multilevel-Search algorithm Here, MFCI is the set of the candidate-to-be-frequent itemsets with the maximal length; MFI is the set of the frequent itemsets with the maximal length; the m-support of tm is the support of an itemset appearing in the transactions of length m t l the maximal transaction length based on the available resource 137 


Fig. 1 shows the pseudo code of the proposed MultilevelSearch algorithm. A detailed description of the algorithm is provided in Section IV, in which various examples are used to explain its implementation IV  T HE I MPLEMENTATION OF THE M ULTILEVEL S EARCH A LGORITHM  Implementation of the Multilevel-Search algorithm has the following four tasks A  Generate a Statistics Table A statistics table \(ST\ilt by scanning the database once \(Fig. 2\e ST records the number of times every 1-item appears in database transactions The ST is then used to find frequent 1-itemsets \(i.e., L 1 ault k value of the k level is 2. The MFCI is the combination of all items in L 1 If L k-1 and C k are not empty, the Predict_ca ndidates procedure is used The range of traversed levels to be predicted is from level k to level k+n-1 and a bottom-up approach is used          Fig. 2. The pseudo code of the statistics table procedure  TABLE I E XAMPLE DB TID Itemset 1 ABCEF 2 ABCEF 3 BCDEF 4 ABCD 5 ABCE 6 ABC 7 ABF 8 ACE 9 BCE 10 BDE  Example 1 To elucidate how the proposed approach evolves from the Parameterised algorithm, Denwattana and  I and II\the use of this dataset, this paper demonstrates that the proposed is more efficient than their approach. Each record in the example databas  A total of six unique items exist in the database with letters A~F. Table II shows the occurrences of each 1-item in transactions of various lengths. The values of m start from the minimal length of a transaction to the maximal length of a transaction \(i.e m 3, 4, and 5\fically, among the ten transactions, five are 3-length, two are 4-length, and three are 5-length  B  Predict Candidate Itemsets The Predict_candidates procedure \(Fig. 3,\ generates predicted-candidate itemsets according to the statistics table L k-1 k-level, and the user-defined parameters. The parameters include the user-specified threshold of 1-item in any length \(i.e tm he user-specified threshold of transaction length \(i.e tl  and the number of levels to tr averse in each pass \(i.e n ere tm is the support of 1-item appearing in transactions of m length and m ranges from the minimal length of a transaction to the tl Additionally tl must be less than or equal to the maximal length of a transaction based on the available resource The Predict_candidates procedure begins its prediction from k level and uses L k-1 to generate C k  When the candidate k itemsets belong to predicted-frequent itemsets, they are marked k C Predicted-infrequent itemsets are marked k C The support calculation for a 1-item appearing in m length transactions is defined as follows  An item x s support value in the m length transactions  nsactions length tra  the of number  Th e nsaction s length tra  in the  appearing  item x of number  Th e m m 1  The calculation scope is from m length transactions to tl length transactions. Qualified items that satisfy tm are combined, becoming MC k itemsets. If C k is a subset of MC k  C k belongs to the predicted-frequent itemsets. Otherwise, it belongs to the predicted-infrequent itemsets. L k is used to produce C k+1 This prediction procedure is repeated until it reaches the k+n-1 a bottom-up search. While the predicted-frequent k+n-1 itemsets are generated at the k+n1 ey are used to eliminate other candidate itemsets at lower levels Example 2 To simplify the discussion, parameters are set as minsup 20 n 3 tm 80%, and tl 5. Based on the statistics table \(Table II\fr equent 1-itemsets are {A}, {B C}, {D}, {E}, and {F}. Thes e 1-itemsets are combined to generate C2 ={{A, B}, {A, C A, D}, {A, E}, {A, F}, {B C}, {B, D}, {B, E}, {B, F}, {C D}, {C, E}, {C, F}, {D, E D, F}, {E, F}}. Thus, MFCI is L 1 A, B, C, D, E, F}. If  TABLE  II S TATISTICS T ABLE   Items 3-length 4-length 5-length Total su p  A 3 2 2 7 B 4 2 3 9 C 3 2 3 8 D 1 1 1 3 E 3 1 3 7 F 1 0 3 4 of m length transactions 5 2 3 10 Algorithm Statistics table procedure Input: A database Output: A statistics table 1  Scan the database to find the support of 1-item appearing in transactions of m length and m ranges from 1 to the maximal length of a transaction 2  For each value of m find the total number of m len g th transactions in the database 138 


 Algorithm Predict_candidates procedure Input: A statistics table ST  L k-1  k level and the user defined parameters t m t l n  Output: Predicted-frequent and predicted-infrequent itemsets from k to k+n-1 level 1  Scan statistics table 2  C k is generated from the L k-1  3  if C k  subsets of MC k ove C k to k C  4  else move C k to k C  5  for i from k+1 to n+k-1  6  C k+1 generated from the k C  7  if C k+1   subsets of MC k+1 ove C k+1 to 1 k C  8  else move C k+1  to 1 k C  9  end-for 10  if predicted itemsets  subsets of   1 nk C en delete Fig. 3. The pseudo code of the Predict_candidates procedure some i infrequent 1-itemsets exist the cardinality of the MFCI would be reduced by i In this case, the top-down search goes down i levels in one pass, as with the Pincer-Search algorithm Initially, the MFI is an empt y set. Next, the prediction procedure finds MC 2 for C 2  to obtain 2 C and 2 C Because no 2 length transaction exists in the example, the process starts from 3 length transactions. Only one 1-item {B} satisfies t m   3 length transactions is  80  80 5 4   Three 1-itemsets {A}, {B}, and C} satisfy the threshold in 4 length transactions \(Table III r-specified value of t l  is 5, so this value is used to consider transactions up to 5length. In the last column of PC 2 all 1-itemsets under different m length transactions are merged as MC 2 A, B, C E, F}. As {A, D} is not a subset of MC 2 it is predicted as infrequent and belongs to 2 C Consequently 2 C A, B A, C}, {A, E}, {A, F}, {B, C B, E}, {B, F}, {C, E}, {C F}, {E, F}} and 2 C A, D}, {B, D}, {C, D}, {D, E}, {D F}}. Then the itemsets in 2 C are joined to generate C 3 A B, C}, {A, B, E}, {A, B, F}, {A C, E}, {A, C, F}, {A, E, F B, C, E}, {B, C, F}, {B, E, F C, E, F}}. These procedures are repeated to obtain 3 C A, B, C}, {A, B, E}, {A, B, F A, C, E}, {A, C, F}, {A, E, F B, C, E}, {B, C, F}, {B, E F}, {C, E, F}} and 3 C is an empty set; and 4 C A, B, C E}, {A, B, C, F}, {A, B, E, F}, {A, C, E, F}, {B, C, E, F and 4 C is an empty set As k 2 n 3, and k  n 1 = 4 in this example, the prediction process has completed a pass of 2 C  2 C  3 C  3 C  4 C and 4 C Since this work wants to find maximal frequent itemsets their sub-itemsets can be prune d from lower-level candidates using the predicted-frequent 4-itemsets and predictedinfrequent 4-itemsets in the top-down approach After eliminating itemsets, the predicted candidate itemsets MFCI, and MFI are generated as follows 2 C  2 C A D}, {B, D}, {C, D D, E}, {D, F 3 C  3 C  4 C A B, C, E}, {A, B, C, F}, {A, B, E F}, {A, C, E, F}, {B, C, E F 4 C and MFCI={A, B, C D, E, F}; MFI C  Update the Maximal Frequent Candidate Itemsets The database is scan ned again to find the actual support of predicted-frequent, predicted-i nfrequent itemse ts, and MFCI If the frequent-candidate itemsets in MFCI are verified after counting their frequencies, they are moved to the MFI. The MFCI is updated according to the infrequent itemsets inf  Then, an examination is requ ired to check whether any incorrect prediction occurs Example 3 After scanning the data base to count support the frequent k itemsets and infrequent itemset inf are found L 2 B, D}, {C, D}, {D, E inf A, D}, {D, F L 3  L 4 A, B, C, E}, {A, B, C, F}, {A B, E, F}, {A, C, E, F B, C, E, F Then, it utilizes the infrequent itemset inf to update the MFCI using the top-down approa ch. Considering {A, D}, the MFCI becomes {{B, C, D, E, F}, {A, B, C, E, F}}. Finally MFCI = {{A, B, C, E, F}, {B C, D, E}} after processing {D E} to generate {B, C, D, E} an d {B, C, E, F}. However, {B, C E, F} is removed from the MFCI because it is a subset of {A B, C, E, F}. Both the MFCI and MFI are updated as follows MFCI={ {A, B, C, E, F}, {B, C, D, E} } and MFI D  Recover Candidate Itemsets There are two prediction error types: \(1 frequent itemset becomes an infr equent itemset after counting its support; and \(2\redicted-infrequent itemset becomes a frequent itemset after counting its support. To rectify these wrongly predicted itemsets, additi onal candidate itemsets must be generated, which are called Remaining Candidate k TABLE  III T HE I TEMSETS S ATISFYING THE 1-I TEM T HRESHOLD  T M  m length  3length trans 4length trans 5length trans Merged candidate itemsets PC 2  B A,B,C B,C,E,F MC 2 A,B,C,E,F PC 3  B A,B,C B,C,E,F MC 3 A,B,C,E,F PC 4  A,B,C B,C,E,F MC 4 A,B,C,E,F PC 5  B,C,E,F MC 5 B,C,E,F 139 


itemsets RC ws the recovery procedure. If any predicted-frequent j itemset i.e  j C rns out to be an infrequent itemset after calculati on, all of its subsets from the k level to j-1 be produced Similarly, if any predicted-infrequent j itemset i.e  j C es a frequent itemset after calculation, al l of its supersets from j+1 evel to k+n-1 ced. After the RC from k level to k+n-1 vel  is  produced, the database is scanned again  to  count the support of the RC and the MFCI for correcting prediction errors When a frequent-candidate itemset in the MFCI is qualified after counting its support, it will be moved to the MFI. The MFCI will be updated according to the inf found in the RC As in Section II, all supersets in an infrequent itemset must be infrequent and all subsets of a frequent itemset must be frequent. These two propertie s are used to produce a new MFCI. By eliminating the found frequent n+k-1 itemsets using the MFI, any subset belongin g to the MFI is not listed in the frequent itemsets of L n+k-1 Then it joins L n+k-1 itemsets to produce the candidate itemsets of C n+k If any itemset of L n+k-1 in the above elimination procedure is deleted, candidate itemsets are then recovere d to produce complete candidate  n+k sets of C n+k Then, the approach uses the MFI to prune C n+k The value k of the k level is increased to n+k It repeats the above steps until L k-1 and C k are empty. The final result of the MFI is then returned  Algorithm Recovery procedure Input: Incorrect predicted-frequent and predicted-infrequent itemsets k level and the user defined parameters n  Output: Remaining candidate itemsets RC  1  Take apart incorrect predict ed-frequent itemsets to generate remaining candidate itemsets RC  2  until k level 3  Join incorrect predicted-infre quent itemsets to generate remaining candidate itemsets RC  4  until k+n-1 level Fig. 4.  The pseudo code of the Recovery procedure  Example 4 From Example 3, some prediction errors are likely because three itemsets B, D}, {C, D}, and {D, E} in 2 C become frequent after suppor t verification. For correction these itemsets are combined to produce RC itemsets as follows 2 RC  3 RC B, C, D}, {B, D, E}, {C, D, E 4 RC B, C, D, E}; MFCI={A, B, C E, F}, {B, C, D, E The database is scanned again to count the support of the RC itemsets and the MFCI is up dated. Thus, MFI = {A, B, C E, F} after support is counted. The infrequent itemsets {C, D E} and {B, C, D, E} are used to update the MFCI. The MFCI then becomes an empty set. The frequent 4-itemsets are combined to yield C 5 A, B, C, E, F}. The MFI is used to eliminate the frequent itemsets which are not maximal candidate 5-itemsets via the top-down approach, yielding the following results L 2  L 3 B, C, D}, {B, D, E L 4  C 5 MFCI={}; MFI = {A, B, C, E, F Final result = {{B, C, D}, {B D, E}, {A, B, C, E, F Because the frequent 4-itemse ts and candidate 5-itemsets are empty, the entire procedure ends here V  E XPERIMENTAL R ESULTS AND D ISCUSSION  A  Experimental Environment Experiments were performed on an Intel R Xeon TM MP CPU at 2.00GHz with 3800 MB RAM running Window s 2000. The programs were developed using Microsoft Visual Basic 6.0 The Pincer-Search and Parameterised algorithms were also implemented for comparison. Th e IBM dataset generator was used to produce test databases Table IV defines parameters Table V lists the parameters used to generate the databases for the experiments. We generated twenty databases in total and each type of database in Tab le V had five databases   The types of maximal frequent itemsets in these databases are mainly short, moderate, and long in length. The short maximal frequent itemsets range at 1~4; the moderate maximal frequent itemsets range at 5~8; and the long maximal frequent itemsets range at 9~12. All are used in the Multilevel-Search, Pincer-Search, and Parameterised algorithms to examine thei r respective performance B  Data Analysis and Efficiency Assessment Three parameters n  t m and t l in the Multilevel-Search algorithm affect effi ciency and execution time. For instance when parameter t m  is set too large, many infrequent itemsets will be found. Conversely, when parameter t m is too small many frequent itemsets will be found. To demonstrate its simplicity and ease-of-use, multiple experiments are performed with variou s parameters settings for the proposed approach without complex analysis of datasets. Suitable settings for the three parameters n  tm and tl are adopted to TABLE  IV D EFINITIONS OF P ARAMETERS  D The number of transactions T The avera g e len g th of transactions L The number of maximal potentiall y frequent N The number of distinct items TABLE  V P ARAMETERS OF D ATABASES  Type of Database T L N D T2.L10.N500.D100K 2 10 500 100000 T8.L10.N500.D100K 8 10 500 100000 T10.L10.N500.D100K 10 10 500 100000 T12.L10.N500.D100K 12 10 500 100000 140 


   perform an efficiency assess ment of target algorithms Consequently, these parameters are set as follows 1  If the average transaction length is 2 n is set to 2 and t m  is 65 2  If the average transaction length is 8~10 n 4 and t m   80 3  The user-specified threshold of transaction length t l  depends on the database used \(Table V In the first set of experiments, T2.L10.N500.D100K databases are used to compare the efficiency of the MultilevelSearch, Pincer-Search, and Parameterised algorithms The average transaction length is 2. Fig. 5 shows that the Multilevel-Search algorithm pe rforms better than the other two algorithms for short maximal frequent itemsets. Here multilevel and multilayer are used interchangeably   The average length of transactions in T8.L10.N500.D100K datasets is 8 \(Fig. 6\e minimal support of 10%, the difference in execution time is not significant between Multilevel-search and Pincer-search algorithms, while the Parameterised algorithm performs poorly. When minimal support increases, fewer candidates of frequent itemsets are found, and the performance of the Multilevel-search and Parameterised algorithms increases. When minimal support is doubled to 20%, execution time of the Parameterised algorithm is less than that of the Pincer-Search algorithm while the Multilevel-search algorithm remains best The average length of transactions in T10.L10.N500 D100K dataset is 10 \(Fig. 7\minimum support is 20 the Multilevel-Search algorithm is over two times faster than the other two algorithms. Due to the efficiency of going up or down multiple levels in one pass, the Multilevel-search algorithm is the overall winner Using the databases T12.L10.N500.D100K \(Fig. 8 experiment results show that th e efficiency of the MultilevelSearch and Pincer-Search algorith ms are similar, as both use the top-down approach to find the maximal frequent itemsets and improve their efficiency Since the goal in this paper is to find efficient solutions for long patterns, the times taken by database scans and the number of candidate itemsets for databases when T is 8, 10 and 12 are examined in detail Experiment results \(Tables VI and VII\ show that the Multile vel-Search algo rithm has the best performance for total times the database is scanned and the total number of candidate itemsets. Although the Pincersearch generates less candidate itemsets in two instances Table VII\e Multilevel-Search algorithm is still the overall  Fig. 8.  Comparison of the average execution time for three algorithms with T12.L10.N500.D100K TABLE  VI T HE A VERAGE T IMES OF D ATABASE S CAN  M INSUP 20 Databases Apriori Pincer search Param eterise d Multile velSearch T2.L10.N500.D100 4 2 3 2 T8.L10.N500.D100 8 5 4 3 T10.L10.N500.D100 11 6 7 4 T12.L10.N500.D100 13 7 7 4  Fig. 7.  Comparison of the average execution time for three algorithms w ith T10.L10.N500.D100K  Fig. 6. Comparison of the average execution time for three algorithms with T8.L10.N500.D100K   Fig. 5. Comparison of the average execution time for three algorithms with T2.L10.N500.D100K 141 


    winner because the extra candidate itemsets come from the recovery procedure and no data scan is involved To assess the performance of these algorithms, this paper compares the average executi on times for three kinds of databases with the minimal support of 20% and the experiments are performed fift een times for each algorithm Table VIII shows the averag e execution time with the transaction lengths of T = 8, 10 and 12. To elucidate the data in Table VIII, Table IX shows clearly that the MultilevelSearch algorithm is 2.32 times faster than the Pincer-Search algorithm and 2.99 times faster than the Parameterised algorithm. Separate data for th e ratio of improvement in terms of transaction lengths of T = 8, 10, and 12 are also presented VI  C ONCLUSIONS AND F UTURE W ORK  This paper proposes a novel Mu ltilevel-Search algorithm to mine efficiently long associat ion rules. Using the concept of predicting frequent and infrequent itemsets by parameter settings, this Multilevel-Search algorithm combines the topdown search mechanism from the Pincer-Search algorithm and bottom-up multilevel searching from the Parameterised algorithm to rapidly identify ma ximal frequent itemsets. After finding the maximal frequent it emsets, the algorithm can also generate all frequent itemsets. Th is method is especially useful when an itemset is long. Experi mental results verify that the proposed algorithm is faster than the Parameterised and Pincer-Search algorithms However, the Multilevel-Sea rch algorithm has room for improvement in its prediction mechanism. The effects of settings for parameters n  t m  t l can be determined with further study. Experiments showed that prediction accuracy declines when too many levels are scan ned in one pass. This also produces a large number of candidate itemsets. Better prediction accuracy will redu ce the number of redundant candidate itemsets and the number of database scans. In the future, it is planned to use a statistical approach to improve the prediction mechanism and apply parallel comp or better performance A CKNOWLEDGMENT  This work was supported in part by the Ministry of Science and Technology, Taiw an, under grants MOST 1042221-E-035-089 and MOST 104-2218-E-035-012  R EFERENCES  1  M. S. Chen, J. Han, and P   IEEE Transactions on Knowledge and Data Engineering vol. 8, no. 6, pp. 866-883, 1996 2   Burlington: Morgan Kaufmann, 2011 3   Itemsets A  Knowledge-Based Systems vol. 20, no. 4, pp. 329335, 2007 4    Proc. of the 20th International Conference on Very Large Data Bases Santiago, 1994, pp. 487-499 5     Knowledge-Based Systems vol. 18, no. 2-3, pp. 99105, 2005 6  Irina Tudor, Association rule mining as a data mining technique BULETINUL universitatii Petrol-Gaze din. Ploiesti, vol. LX, no. 1, page 49-56, 2008 7   for Mining Multiple Proc. of the 3rd International Conference on Innova tive Computing Information and Control \(ICICIC'08 Proceedings 2008, pp. 332-335 8    in Proc. of the 17th International Conference on Data Engineering \(ICDE 2001\ Heidelberg Germany, 2001, pp. 443-452 9  D. Burdick, M. Calimlim, J. Flannick, J. Gehrke, and T   Proc of the 2003 Workshop on Frequent Itemset Mining Implementations FIMI'03 Melbourne, Florida, November 2003 10   search: An Efficient Algorithm for  IEEE Transactions on Knowledge and Data Engineering vol. 14, no. 3, pp. 553-566, 2002 11   or Mining  Proc. of the 12th Australasian Database Conference 2001, pp. 45-51 12  T. P. Hong, C. Y. Ho rng, C. H. Wu, and S   Expert Systems with Applications vol. 36, no. 1, pp. 72-80, 2009 13  Z. Abdullah, T. Herawan, and M  Least Association Rules Using Scalable Trie Journal of the Chinese Institute of Engineers vol. 35, no. 3, pp. 547-554, 2012 14   Big Data Mining with Parallel Co m puting: A Comparison of Distributed and MapReduce Methodologies Master thesis Dept. of Information Management National Central University   Taiwan  2015 TABLE  IX T HE R ATIO OF I MPROVEMENT IN T ERMS OF E XECUTION T IME  Algorithm  minsup 20 The average length of transactions Average ratio 8 10 12 Pincer-Search Multilevel-Search 2.94 2.45 1.57 2.32 Parameterised Multilevel-Search 1.44 4.77 2.78 2.99 TABLE  VII T HE A VERAGE N UMBER OF C ANDIDATE I TEMSETS  MINSUP 20 The average length of transactions Pincersearch Parameterised Multilevel Search 8 258 338 280 10 913 1356 782 12 1117 1778 1421 TABLE  VIII C OMPARISON OF A VERAGE E XECUTION T IME S ECONDS  Algorithm  minsup 20 The average length of Average time 8 10 12 Pincer-Search 1885.48 3155.97 5155.97 3399.14 Parameterised 924.30 6157.23 9157.23 5412.92 Multilevel-Search 642.29 1290.64 3290.64 1741.19 142 


Fig 4 Clustering results in the 2D embedding created by SNE V C ONCLUSION In this paper we presented a study on application of association rule learning clustering and dimension reduction on the darknet trafc Frequent itemsets with respects to probed destination ports are reported in the experiments Some of the signicant association rules discovered in the experiments are proved to correspond to specic known attacks from IoT devices Analysis on the activity-level time series of the attack patterns has reveal emergence of new type of attacks The proposed scheme can help security experts to discover new emerging attacks at their early stage providing valuable hints for further investigation We leave the formulation of a strategic framework to countermeasure the new threats discovered by this approach for future work R EFERENCES  M Co v a C Krue gel and G V igna Detection and analysis of dri v eby-download attacks and malicious javascript code in  ser WWW 10 New York NY USA ACM 2010 pp 281290  L Lu V  Y e gnesw aran P  Porras and W  Lee Blade An attackagnostic approach for preventing drive-by malware infections in  ser CCS 10 New York NY USA ACM 2010 pp 440450  E Cole  1st ed Syngress Publishing 2013  Z Saud and M H Islam T o w ards proacti v e detection of adv anced persistent threat apt attacks using honeypots in  ser SIN 15 New York NY USA ACM 2015 pp 154157  L In v ernizzi S Misk o vic R T orres C Krue gel S Saha G V igna S Lee and M Mellia Nazca Detecting malware distribution in large-scale networks in  2014  T  Ban M Eto S Guo D Inoue K Nakao and R Huang  A study on association rule mining of darknet big data in  2015 pp 17  M Baile y  E Cook e F  Jahanian J Nazario D W atson  The internet motion sensor-a distributed blackhole monitoring system in  2005  T  Ban L Zhu J Shimamura S P ang D Inoue and K Nakao Behavior analysis of long-term cyber attacks in the darknet in  vol 151 no 3 Elsevier 2012 pp 620628 
Proceedings of the 19th International Conference on World Wide Web Proceedings of the 17th ACM Conference on Computer and Communications Security Advanced Persistent Threat Understanding the Danger and How to Protect Your Organization Proceedings of the 8th International Conference on Security of Information and Networks 21st Annual Network and Distributed System Security Symposium NDSS 2014 San Diego California USA February 23-26 2014 2015 International Joint Conference on Neural Networks IJCNN 2015 Killarney Ireland July 12-17 2015 et al NDSS Neural Information Processing 19th International Conference ICONIP 2012 Doha Qatar November 12-15 2012 Proceedings Part V 
40 30 20 10 0 10 20 30 40 40 30 20 10 0 10 20 30 40 5 10 15 20 25 30 
t 
348 


 U Harder  M W  J ohnson J T  Bradle y  and W  J Knottenbelt Observing internet worm and virus attacks with a small network telescope  vol 151 no 3 pp 4759 2006  K Benson A Dainotti K Claf fy  and E Aben Gaining insight into as-level outages through analysis of internet background radiation in  IEEE 2013 pp 447452  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases in  vol 22 no 2 ACM 1993 pp 207216  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in  vol 29 no 2 ACM 2000 pp 112  J Han H Cheng D Xin and X Y an Frequent pattern mining Current status and future directions  vol 15 no 1 pp 5586 Aug 2007  C C Aggarw al and J Han Eds  Springer 2014  C Bor gelt Frequent item set mining   vol 2 no 6 pp 437456 2012  L Kaufman and P  J Rousseeuw   9th ed Wiley-Interscience Mar 1990  C C Aggarw al  2013  L v an der Maaten and G E Hinton V isualizing high-dimensional data using t-sne  vol 9 pp 25792605 2008  K Nakao K Y oshioka D Inoue and M Eto  A no v el concept of network incident analysis based on multi-layer ovservation of malware activities in  2007 pp 267279  D Inoue K Y oshioka M Eto M Y amagata E Nishino J T ak euchi K Ohkouchi and K Nakao An incident analysis system nicter and its analysis engines based on data mining techniques in  2008  O Bilodeau and T  Dupuy  Dissecting linux/moose  2015 A v ailable http://www weli v esecurity com/wpcontent/uploads/2015/05/Dissecting-LinuxMoose.pdf  O Bilodeau Linux/moose endangered or extinct 2015 A v ailable https://www.virusbulletin.com/uploads/pdf/conference slides/2015 Bilodeau-VB2015.pdf 
Electronic Notes in Theoretical Computer Science Computer Communications Workshops INFOCOM WKSHPS 2013 IEEE Conference on ACM SIGMOD Record ACM SIGMOD Record Data Min Knowl Discov Frequent Pattern Mining Data Mining Knowledge Discovery Finding Groups in Data An Introduction to Cluster Analysis An Introduction to Cluster Analysis Journal of Machine Learning Research The 2nd Joint Workshop on Information Security JWIS07 15th International Conference on NeuroInformation Processing of the Asia Pasic Neural Netowrk Assembly ICONIP 2008 
349 


each of which has distinct characteristics compared with the other partitions Such distinct features among the partitions allow FiDoop-DP to efÞciently reduce the number of redundant transactions In contrast a dataset with high dimensionality has a long average transaction length therefore data partitions produced by FiDoop-DP have no distinct discrepancy Redundant transactions are likely to be formed for partitions that lack distinct characteristics Consequently the beneÞt offered by FiDoop-DP for highdimensional datasets becomes insigniÞcant 6.3.2 Data Correlation We set the correlation among transactions i.e corr to 0.15 0.25 0.35 0.45 0.55 0.65 and 0.75 to measure the impacts of data correlation on the performance of the two algorithms on the 8-node Hadoop cluster The Number of Pivots is set to 60 see also Section 6.1 The experimental results plotted in Fig 5c clearly indicate that FiDoop-DP is more sensitive to data correlation than Pfp This performance trend motivates us to investigate the correlation-related data partition strategy Pfp conducts default data partition based on equal-size item group without taking into account the characteristics of the datasets However FiDoop-DP judiciously groups items with high correlation into one group and clustering similar transactions together In this way the number of redundant transactions kept on multiple nodes is substantially reduced Consequently FiDoop-DP is conducive to cutting back both data transmission trafÞc and computing load As can be seen from Fig 5c there is an optimum balance point for data correlation degree to tune FiDoop-DP performance e.g 0.35 in Fig 5c If data correlation is too small Fidoop-DP will degenerate into random partition schema On the contrary it is difÞcult to divide items into relatively independent groups when data correlation is high meaning that an excessive number of duplicated transactions have to be transferred to multiple nodes Thus a high data correlation leads to redundant transactions formed for partitions thereby increasing network and computing loads 6.4 Speedup Now we are positioned to evaluate the speedup performance of FiDoop-DP and Pfp by increasing the number of data nodes in our Hadoop cluster from 4 to 24 The T40I10D 128 blocks dataset is applied to drive the speedup analysis of the these algorithms Fig 6 reveals the speedups of FiDoop-DP a nd Pfp as a function of the number of data nodes The experimental results illustrated in Fig 6a show that the speedups of FiDoop-DP and Pfp linearly scale up with the increasing number of data nodes Such a speedup trend can be attributed to the fact that increasing the number of data nodes under a xed input data size inevitably 1 reduces the amount of itemsets being handled by each node and 2 increases communication overhead among mappers and reducers Fig 6a shows that FiDoop-DP is better than Pfp in terms of the speedup efÞciency For instance the FiDoop-DP improves the speedup efÞciency of Pfp by up to 11.2 percent with an average of 6.1 percent This trend suggests FiDoopDP improves the speedup efÞciency of Pfp in large-scale The speedup efÞciencies drop when the Hadoop cluster scales up For example the speedup efÞciencies of FiDoopDP and Pfp on the 4-node cluster are 0.970 and 0.995 respectively These two speedup efÞciencies become 0.746 and 0.800 on the 24-node cluster Such a speedup-efÞciency trend is driven by the cost of shufßing intermediate results which sharply goes up when the number of data nodes scales up Although the overall computing capacity is improved by increasing the number of nodes the cost of synchronization and communication among data nodes tends to offset the gain in computing capacity For example the results plotted in Fig 6b conÞrm that the shufßing cost Fig 5 Impacts of data characteristics on FiDoop-DP and Pfp Fig 6 The speedup performance and shufßing cost of FiDoop-DP and Pfp 110 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


is linearly increasing when computing nodes are scaled from 4 to 24 Furthermore the shufßing cost of Pfp is larger than that of FiDoop-DP 6.5 Scalability In this group of experiments we evaluate the scalability of FiDoop-DP and Pfp when the size of input dataset dramatically grows Fig 7 shows the running times of the algorithms when we scale up the size of the T40I10D data series Figs 7a and 7b demonstrate the performance of FiDoop-DP processing various datasets on 8-node and 24-node clusters respectively Fig 7 clearly reveals that the overall execution times of FiDoop-DP and Pfp go up when the input data size is sharply enlarged The parallel mining process is slowed down by the excessive data amount that has to be scanned twice The increased dataset size leads to long scanning time Interestingly FiDoop-DP exhibits a better scalability than Pfp Recall that see also from Algorithm 1 the second MapReduce job compresses an initial transaction database into a signature matrix which is dealt by the subsequent process The compress ratio is high when the input data size is large thereby shortening the subsequent processing time Furthermore Fidoop-DP lowers the network trafÞc induced by the random grouping strategy in Pfp In summary the scalability of FiDoop-DP is higher than that of Pfp when it comes to parallel mining of an enormous amount of data 7R ELATED W ORK 7.1 Data Partitioning in MapReduce Partitioning in databases has been widely studied for both single system servers e.g and distributed storage systems e.g BigTable PNUTS[31 The existing approaches typically produce possible ranges or hash partitions which are then evaluated using heuristics and cost models These schemes offer limited support for OLTP workloads or query analysis in the context of the popular MapReduce programming model In this study we focus on the data partitioning issue in MapReduce High scalability is one of the most important design goals for MapReduce applications Unfortunately the partitioning techniques in existing MapReduce platforms e.g Hadoop are in their infancy leading to serious performance problems Recently a handful of data partitioning schemes have been proposed in the MapReduce platforms Xie et al  developed a data placement management mechanism for heterogeneous Hadoop clusters Their mechanism partitions data fragments to nodes in accordance to the nodes processing speed measured by computing ratios In addition Xie et al  designed a data redistribution algorithm in HDFS to address the data-skew issue imposed by dynamic data insertions and deletions CoHadoop is a H a d oop s lightweight extension which is designed to identify relateddataÞlesfollowedbyamodiÞeddataplacement policy to co-locate copies of those related les in the same server CoHadoop considers the relevance among les that is CoHadoop is an optimization of HaDoop for multiple les A key assumption of the MapReduce programming model is that mappers are completely independent of one another Vernica et al  broke such an assumption by introducing an asynchronous communication channel among mappers T his c hannel e nables the m appers to see global states managed in metadata Such situationaware mappers SAMs can enable MapReduce to exibly partition the inputs Apart from this adaptive sampling and partitioning were proposed to produce balanced partitions for the reducers by sampling mapper outputs and making use of obtained statistics Graph and hypergraph partitioning have been used to guide data partitioning in parallel computing Graph-based partitioning schemes capture data relationships For example Ke et al applied a graphic-execution-plan graph EPG to perform cost estimation and optimization by analyzing various properties of both data and computation Their estimation module coupled with the cost model estimate the runtime cost of each vertex in an EPG which represents the overall runtime cost a data partitioning plan is determined by a cost optimization module Liroz-Gistau et al proposed the MR-Part technique which partitions all input tuples producing the same intermediate key co-located in the same chunk Such a partitioning approach minimizes data transmission among mappers and reducers in the shufße phase The approach captures the relationships between input tuples and intermediate keys by monitoring the execution of representative workload Then based on these relationships their approach applies a min-cut k-way graph partitioning algorithm thereby partitioning and assigning the tuples to appropriate fragments by modeling the workload with a hyper graph In doing so subsequent MapReduce jobs take full advantage of data locality in the reduce phase Their partitioning strategy suffers from adverse initialization overhead Fig 7 The scalability of FiDoop-DP and Pfp when the size of input dataset increases XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 111 


7.2 Application-Aware Data Partitioning Various efÞcient data partitioning strategies have been proposed to improve the performance of parallel computing systems For example Kirsten et al  developed two general partitioning strategies for generating entity match tasks to avoid memory bottlenecks and load imbalances Taking into account the characteristics of input data Aridhi et al proposed a novel density-based data partitioning technique for approximate large-scale frequent subgraph mining to balance computational load among a collection of machines Kotoulas et al built a data distribution mechanism based on clustering in elastic regions Traditional term-based partitioning has limited scalability due to the existence of very skewed frequency distributions among terms Load-balanced distributed clustering across networks and local clustering are introduced to improve the chance that triples with a same key are collocated These selforganizing approaches need no data analysis or upfront parameter adjustments in a priori Lu et al studied k nearest neighbor join using MapReduce in which a data partitioning approach was designed to reduce both shufßing and computational costs In LuÕs study objects are divided into partitions using a Voronoi diagram with carefully selected pivots Then data partitions i.e Voronoi cells are clustered into groups only if distances between them are restricted by a speciÞc bound In this way their approach can answer the k-nearest-neighbour join queries by simply checking object pairs within each group FIM for data-intensive applications over computing clusters has received a growing attention efÞcient data partitioning strategies have been proposed to improve the performance of parallel FIM algorithms A MapReducebased Apriori algorithm is designed to incorporate a new dynamic partitioning and distributing data method to improve mining performance This method divides input data into relatively small splits to provide exibility for improved load-balance performance Moreover the master node doesnÕt distribute all the data once rather the rest data are distributed based on dynamically changing workload and computing capability weight of each node Similarly Jumbo adopted a dynamic partition assignment technology enabling each task to process more than one partition Thus these partitions can be dynamically reassigned to different tasks to improve the load balancing performance of Pfp Uthayopas et al  investigated I/O and execution scheduling strategies to balance data processing load thereby enhancing the utilization of a multi-core cluster system supporting association-rule mining In order to pick a winning strategy in terms of data-blocks assignment Uthayopas et al incorporated three basic placement policies namely the round robin range and random placement Their approach ignores data characteristics during the course of mining association rules 8F URTHER D ISCUSSIONS In this study we investigated the data partitioning issues in parallel FIM We focused on MapReduce-based parallel FPtree algorithms in particular we studied how to partition and distribute a large dataset across data nodes of a Hadoop cluster to reduce network and computing loads We argue that the general idea of FiDoop-DP proposed in this study can be extended to other FIM algorithms like Apriori running on Hadoop clusters Apriori-based parallel FIM algorithms can be classiÞed into two camps namely count distribution and data distribution  For the count distribution camp each node in a cluster calculates local support counts of all candidate itemsets Then the global support counts of the candidates are computed by exchanging the local support counts For the data distribution camp each node only keeps the support counts of a subset of all candidates Each node is responsible for delivering its local database partition to all the other processors to compute support counts In general the data distribution schemes have higher communication overhead than the count distribution ones whereas the data distribution schemes have lower synchronization overhead than its competitor Regardless of the count distribution or data distribution approaches the communication and synchronization cost induce adverse impacts on the performance of parallel mining algorithms The basic idea of Fidoop-DPÑgrouping highly relevant transactions into a partition allows the parallel algorithms to exploit correlations among transactions in database to cut communication and synchronization overhead among Hadoop nodes 9C ONCLUSIONS A ND F UTURE W ORK To mitigate high communication and reduce computing cost in MapReduce-based FIM algorithms we developed FiDoop-DP which exploits correlation among transactions to partition a large dataset across data nodes in a Hadoop cluster FiDoop-DP is able to 1 partition transactions with high similarity together and 2 group highly correlated frequent items into a list One of the salient features of FiDoopDP lies in its capability of lowering network trafÞc and computing load through reducing the number of redundant transactions which are transmitted among Hadoop nodes FiDoop-DP applies the Voronoi diagram-based data partitioning technique to accomplish data partition in which LSH is incorporated to offer an analysis of correlation among transactions At the heart of FiDoop-DP is the second MapReduce job which 1 partitions a large database to form a complete dataset for item groups and 2 conducts FP-Growth processing in parallel on local partitions to generate all frequent patterns Our experimental results reveal that FiDoop-DP signiÞcantly improves the FIM performance of the existing Pfp solution by up to 31 percent with an average of 18 percent We introduced in this study a similarity metric to facilitate data-aware partitioning As a future research direction we will apply this metric to investigate advanced loadbalancing strategies on a heterogeneous Hadoop cluster In one of our earlier studies see for details we addressed the data-placement issue in heterogeneous Hadoop clusters where data are placed across nodes in a way that each node has a balanced data processing load Our data placement scheme can balance the amount of data stored in heterogeneous nodes to achieve improved data-processing performance Such a scheme implemented at the level of Hadoop distributed le system HDFS is unaware of correlations among application data To further improve load balancing 112 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


mechanisms implemented in HDFS we plan to integrate FiDoop-DP with a data-placement mechanism in HDFS on heterogeneous clusters In addition to performance issues energy efÞciency of parallel FIM systems will be an intriguing research direction A CKNOWLEDGMENTS The work in this paper was in part supported by the National Natural Science Foundation of P.R China No.61272263 No.61572343 Xiao QinÕs work was supported by the U.S National Science Foundation under Grants CCF-0845257 CAREER The authors would also like to thank Mojen Lau for proof-reading R EFERENCES  M J Zaki Parallel and distribu ted associat ion mining A survey IEEE Concurrency  vol 7 no 4 pp 14Ð25 Oct 1999  I Pramudiono and M Kitsuregawa  Fp-tax Tree structure based generalized association rule mining in Proc 9th ACM SIGMOD Workshop Res Issues Data Mining Knowl Discovery  2004 pp 60Ð63  J De an a n d S Gh e ma wa t M ap re du ce  S i mp l i e d da ta pr o ce s si n g on large clusters ACM Commun  vol 51 no 1 pp 107Ð113 2008  S Sakr A Liu and A G Fayoumi The family of mapred uce and large-scale data processing systems ACM Comput Surveys  vol 46 no 1 p 11 2013  M.-Y Lin P.-Y Lee and S.-C Hsueh Apriori-based frequent itemset mining algorithms on mapreduce in Proc 6th Int Conf Ubiquitous Inform Manag Commun  2012 pp 76:1Ð76:8  X Li n  Mr a pr io ri  As so ci a ti o n ru le s a lg o ri th m ba se d on mapreduce in Proc IEEE 5th Int Conf Softw Eng Serv Sci  2014 pp 141Ð144  L Zhou Z Zhong J Chang J Li J Huang and S Feng Balanced parallel FP-growth with mapreduce in Proc IEEE Youth Conf Inform Comput Telecommun  2010 pp 243Ð246  S Hong Z Huaxuan C Shiping and H Chunyan The study of improved FP-growth algorithm in mapreduce in Proc 1st Int Workshop Cloud Comput Inform Security  2013 pp 250Ð253  M Riondato  J A DeBrabant R Fonseca and E Upfal Parma A parallel randomized algorithm for approximate association rules mining in mapreduce in Proc 21st ACM Int Conf Informa Knowl Manag  2012 pp 85Ð94  C Lam Hadoop in Action  Greenwich USA Manning Publications Co 2010  H Li Y Wang D Zhang M Zhang and E Y Chang PFP Parallel FP-growth for query recommendation in Proc ACM Conf Recommender Syst  2008 pp 107Ð114  C Curino E Jones Y Zhang and S Madden Schism A workload-driven approach to database replication and partitioning Proc VLDB Endowment  vol 3 no 1-2 pp 48Ð57 2010  P Uthayop as and N Benjamas Impact of i/o and execution scheduling strategies on large scale parallel data mining J Next Generation Inform Technol  vol 5 no 1 p 78 2014  I  P r a m u d i o n o a n d M  K i t s u r e g a w a  P a r a l l e l F P g r o w t h o n P C cluster in Proc.Adv.Knowl.DiscoveryDataMining  2003 pp 467Ð473  Y Xun J Zhang and X Qin Fidoop Parallel mining of frequent itemsets using mapreduce IEEE Trans Syst Man Cybern Syst  vol 46 no 3 pp 313Ð325 Mar 2016 doi 10.1109 TSMC.2015.2437327  S Owen R Anil T Dunning and E Friedman Mahout Action  Greenwich USA Manning 2011  D Borthakur  Hdfs architecture guide HADOOP APACHE PROJECT Available  http://hadoop.apache.org/common/docs current/hdfs design.pdf 2008  M Zaharia M Chowdhury M J Franklin  S Shenker and I Stoica Spark Cluster computing with working sets in Proc 2nd USENIX Conf Hot Topics Cloud Comput  2010 p 10  W Lu Y Shen S Chen and B C Ooi EfÞcient proces sing of k nearest neighbor joins using mapreduce Proc VLDB Endowment  vol 5 no 10 pp 1016Ð1027 2012  T Kanung o D M Mount N S Netanya hu C D Piatko R Silverman and A Y Wu An efÞcient k-means clustering algorithm Analysis and implementation IEEE Trans Pattern Anal Mach Intell  vol 24 no 7 pp 881Ð892 Jul 2002  A K Jain Data clustering 50 years beyond k-means Pattern Recog Lett  vol 31 no 8 pp 651Ð666 2010  D Arthur and S Vassilvitskii  k-means  The advantages of careful seeding in Proc 18th Annu ACM-SIAM Symp Discr Algorithms  2007 pp 1027Ð1035  J Leskovec A Rajaraman and J D Ullman Mining Massive Datasets  Cambridge U.K Cambridge Univ Press 2014  A Stupar  S Mich el and R Schen kel Rankred uceÐpr ocessin g k-nearest neighbor queries on top of mapreduce in Proc 8th Workshop Large-Scale Distrib Syst Informa Retrieval  2010 pp 13Ð18  B Bahmani A Goel and R Shinde EfÞcient distributed locality sensitive hashing in Proc 21st ACM Int Conf Inform Knowl Manag  2012 pp 2174Ð2178  R Panigrahy Entropy based nearest neighbor search in high dimensions in Proc 17th Annu ACM-SIAM Symp Discr Algorithm  2006 pp 1186Ð1195  A Z Broder M Charikar  A M Frieze and M Mitzenma cher Min-wise independent permutations J Comput Syst Sci  vol 60 no 3 pp 630Ð659 2000  L Cristofor ARtool Association rule mining algorit hms and tools 2006  S Agrawal V Narasayya  and B Yang Integrating vertical and horizontal partitioning into automated physical database design in Proc ACM SIGMOD Int Conf Manag Data  2004 pp 359Ð370  F Chang J Dean S Ghema wat W Hsieh D Wallach  M  Burrows T Chandra A Fikes and R Gruber Bigtable A distributed structured data storage system in Proc 7th Symp Operating Syst Des Implementation  2006 pp 305Ð314  B F Cooper R Ramakrishn an U Srivastava A Silberstein P Bohannon H.-A Jacobsen N Puz D Weaver and R Yerneni Pnuts Yahoo!Õs hosted data serving platform Proc VLDB Endowment  vol 1 no 2 pp 1277Ð1288 2008  J Xie and X Qin The 19th heterogenei ty in computing workshop HCW 2010 in Proc IEEE Int Symp Parallel Distrib Process Workshops Phd Forum  Apr 2010 pp 1Ð5  M Y Eltabakh Y Tian F  Ozcan R Gemulla A Krettek and J McPherson Cohadoop Flexible data placement and its exploitation in hadoop Proc VLDB Endowment  vol 4 no 9 pp 575 585 2011  R Vernica A Balmin K S Beyer and V Ercegovac Adaptive mapreduce using situation-aware mappers in Proc 15th Int Conf Extending Database Technol  2012 pp 420Ð431  Q Ke V Prabhakar an Y Xie Y Yu J Wu and J Yang Optimizing data partitioning for data-parallel computing uS Patent App 13/325,049 Dec 13 2011  M Liroz-Gis tau R Akbarinia D Agrawal E Pacitti  and P Valduriez Data partitioning for minimizing transferred data in mapreduce in Proc 6th Int Conf Data Manag Cloud Grid P2P Syst  2013 pp 1Ð12  T Kirsten L Kolb M Hartung A Gro H K  opcke and E Rahm Data partitioning for parallel entity matching Proc VLDB Endowment  vol 3 no 2 pp 1Ð8 2010  S Kotoulas E Oren and F Van Harmelen Mind the data skew Distributed inferencing by speeddating in elastic regions in Proc 19th Int Conf World Wide Web  2010 pp 531Ð540  L Li and M Zhang The strategy of mining associat ion rule based on cloud computing in Proc Int Conf Bus Comput Global Inform  2011 pp 475Ð478  S Groot K Goda and M Kitsuregawa  Towards improv ed load balancing for data intensive distributed computing in Proc ACM Symp Appl Comput  2011 pp 139Ð146  M Z Ashra D Taniar and K Smith ODAM An optimiz ed distributed association rule mining algorithm IEEE Distrib Syst Online  vol 5 no 3 p 1 Mar 2004 Yaling Xun is currently a doctoral student at Taiyuan University of Science and Technology She is currently a lecturer in the School of Computer Science and Technology Taiyuan University of Science and Technology Her research interests include data mining and parallel computing XUN ET AL FIDOOP-DP DATA PARTITIONING IN FREQUENT ITEMSET MINING ON HADOOP CLUSTERS 113 


Jifu Zhang received the BS and MS degrees in computer science and technology from the Hefei University of Tchnology China and the PhD degree in pattern recognition and intelligence systems from the Beijing Institute of Technology in 1983 1989 and 2005 respectively He is currently a professor in the School of Computer Science and Technology TYUST His research interests include data mining parallel and distributed computing and artiÞcial intelligence Xiao Qin received the PhD degree in computer science from the University of Nebraska-Lincoln in 2004 He is currently a professor in the Department of Computer Science and Software Engineering Auburn University His research interests include parallel and distributed systems storage systems fault tolerance real-time systems and performance evaluation He received the U.S NSF Computing Processes and Artifacts Award and the NSF Computer System Research Award in 2007 and the NSF CAREER Award in 2009 He is a senior member of the IEEE Xujun Zhao received the MS degree in computer science and technology in 2005 from the Taiyuan University of Technology China He is currently working toward the PhD degree at Taiyuan University of Science and Technology His research interests include data mining and parallel computing  For more information on this or any other computing topic please visit our Digital Library at www.computer.org/publications/dlib 114 IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS VOL 28 NO 1 JANUARY 2017 


