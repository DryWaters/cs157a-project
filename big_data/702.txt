Dynamic Miss-Counting Algorithms Finding Implication and Similarity Rules with Con\002dence Pruning Shinji Fujiwara 003 Jeffrey D Ullman y Rajeev Motwani z Abstract Dynamic Miss-Counting algorithms are proposed which 002nd all implication and similarity rules with con\002dence pruning but without support pruning To handle data sets with a large number of columns we propose dynamic pruning techniques that can be applied during data scanning DMC counts the numbers of rows in which each pair of columns disagree instead of counting the number of hits DMC deletes a candidate as soon as the number of misses exceeds the maximum number of misses allowed for 
that pair We also propose several optimization techniques that reduce the required memory size signi\002cantly We evaluated our algorithms by using 4 data sets i.e Web access logs Web page-link graph News documents and a Dictionary These data sets have between 74,000 and 700,000 items Experiments show that DMC can 002nd highcon\002dence rules for such a large data sets ef\002ciently 1 Introduction Finding implication and similarity rules are two of the most interesting issues in the data mining area Finding implication rules also known as association-rule mining was initially proposed by Agrawal Imielinski and Swami  F i ndi ng s i mi l a ri t y rul e s i s a l s o u s e ful f or v a ri ous kinds of data-mining such as copy detection clustering and 
collaborative 002ltering 5 16 9 12 11 17 Suppose that we have a set of transaction data D that has n transactions and m boolean attributes A 1 A 2 A m  We say A i  A j is an implication rule if the fraction of the transactions that contain both A i and A j among those transactions that contains A i is more than a minimum con\002dence Wealsosay 
A i  A j is a similarity rule if 003 Hitachi Ltd Central Research Laboratory This work was done while the author was on leave at the Department of Computer Science Stanford University y Department of Computer Science Stanford University Supported in part by NSF Grant IIS-9811904 z Department of Computer Science Stanford University Supported in part by NSF Grant IIS-9811904 the fraction of the transactions that contain both A i and A j among those transactions that contains either A i or 
A j is more than a minimum similarity  The goal is to identify all valid rules for a given data Most of algorithms proposed in the past few years are based on support pruning which prunes the attributes that have low frequency 1 2 4  T hi s a pproach e x t r act s association rules with high support i.e high frequency ef\002ciently However they discard low-support items all the time Example 1.1  Suppose that we want to extract similar pages in the Web by analyzing the page-link graph In order to extract pages that have similar sets of page links we transform a page-link graph to a binary matrix whose columns represent source pages and whose rows represent 
destination pages If a page p i has a link to p j  then the column c i for the row r j is set to 1 Using a high-support threshold we can only get similarity rules between pages that have many links such as a directory page We have focused on 002nding high-con\002dence implication and similarity rules without support pruning We have done some work on this issue and earlier developed a family of algorithms such as 
Min-Hash and LocalitySensitive Hashing schemes 7 8 10 Thes e a l gori t h ms us e randomized techniques 13 and t he y can e x t r act s i mi l a r pairs very ef\002ciently However these algorithms still have a chance of yielding false positives and false negatives In this paper we propose a family called Dynamic MissCounting DMC algorithms to avoid both false negatives and false positives Let M be a boolean matrix that represents the data D  M has n rows and m columns Each 
row represents a transaction in D  and in each row a column c i is set to 1 if the transaction has an attribute A i  The DMC algorithm uses a con\002dence pruning technique rather than support pruning The idea of pruning for high-con\002dence pairs is not new Sergey Brin did some unpublished experiments and the paper  l i k e w i s e explored iterative methods for converging on the pairs of columns with highest correlation These methods never look at all pairs of columns and make many passes over 


the data They are very expensive techniques useful for enormous data sets e.g Brin used them to look for correlations among the 100 million or so words that appear on the Web Our methods are useful for sets with somewhat smaller numbers of columns they extract all pairs of columns with a similarity or implication that is above a small threshold and they use only two passes through the data and realistic amounts of main memory The key idea of the DMC algorithm is counting the numbers of rows in which each pair of columns disagree instead of counting the number of hits and deleting a counter as soon as the number of misses exceeds the maximum number of misses allowed for that pair T 1 A 2 A 3 T 2 A 1 A 2 A 3 T 3 A 1 T 4 A 1 A 2 T 5 A 2 A 3 a D b M  0  1  1 1  1  1 1  0  0 1  1  0 0  1  1 r 1 r 2 r 3 r 4 r 5 c 1 c 2 c 3 Figure 1 Data format Example 1.2  Fig 1 is an example of D and M  Suppose that we would like to extract 100%-con\002dence implication rules for this matrix M  In this sample case no miss at all between two columns is allowed When we read r 1 we have to keep two candidates c 2  c 3 and c 3  c 2 Next when we read r 2  we only have to add two more candidates c 1  c 2 and c 1  c 3  We do not have to add candidates such as c 2  c 1 or c 3  c 1  since they have already had 1 miss at r 1  To detect the number of misses from one column to another that has already occurred we maintain a counter for each column giving the number of 1's in that column seen so far Since the c 2 counter is 1 at r 2 we\002ndthat the candidate pairs that are not in the candidate list for c 2 have already had 1 miss which is too many in this simple example Furthermore when we read r 3  we can immediately delete candidates c 1  c 2 and c 1  c 3  since these candidates miss at this row After reading all rows in the matrix M  only one rule c 3  c 2  survives Example 1.3  Consider a column c i that has 100 1's and suppose we want to 002nd implication rules with 85 or more con\002dence In this case the number of misses from c i to any other column must not be more than 15 Therefore we can delete a counter for candidate pairs c i  c j as soon as the number of misses exceeds 15 Furthermore we do not have to add a new counter for c i after we have seen 16 rows in which c i is set to 1 because a column c j that has not yet appeared already has had 16 misses for the rule c i  c j  Note that this algorithm requires as many as m 2 counters in the worst case However in real data such as Webpage-link graphs most pages are linked to ten or so pages while the number of pages is in the millions Therefore the number of counters for most pages will not approach even remotely the number of pages This fact signi\002cantly reduces the memory requirements and computation cost In this paper we also propose several techniques such as row re-ordering memory-explosion elimination 100%rule pruning column-density pruning and maximum-hits pruning each of which contribute to reducing the size of memory signi\002cantly We start to de\002ne our problem in Section 2 In Section 3 we present conventional data mining algorithms and our new algorithm We have also applied many other optimization techniques to our DMC algorithm which we mention in Section 4 We then present a variant algorithm for 002nding similarity rules in Section 5 In Section 6 we describe the data that we used to evaluate algorithms and show the experimental results of algorithms We conclude in Section 7 by discussing some extensions of our work to apply our approach to extract more complicated rules among three or more attributes 2 Problem statement We view the data as a 0  1 matrix M with n rows and m columns Each column represents an 223attribute,\224 and each row represents a 223transaction.\224 De\002ne S i to be the set of rows that have a 1 in column c i  We de\002ne the con\002dence of the rule c i  c j as Conf  c i c j  j S i  S j j j S i j That is the con\002dence of c i and c j is the fraction of rows among those containing a 1 in c i  that contain a 1 in both c i and c j  Note that if j S i j  j S j j then C onf  c j c i   C onf  c i c j   Therefore we consider only rules c i  c j such that j S i j  j S j j or  j S i j  j S j j and i  j  and our goal is to extract all rules that have minconf or more con\002dence where 0  minconf 024 1  We also de\002ne the similarity of a column pair c i  c j as Sim  c i c j  j S i  S j j j S i  S j j That is the similarity of c i and c j is the fraction of rows among those containing a 1 in either c i or c j  that contain 


a 1 in both c i and c j  Note that this de\002nition is symmetric with respect to c i and c j  Our other goal is to extract all combinations of column pairs that have minsim or more similarity where 0  minsim 024 1  3 Algorithms In this section we review conventional algorithms apriori and Min-Hash  Then we overview our new Dynamic Miss-Counting algorithm 3.1 A\255priori algorithm A-priori 1 2 w h i c h u s e s a s upport p runi ng t echni que to reduce the search space is the most famous and one of the most effective algorithms for 002nding association rules It prunes the candidate pairs if the frequency of each column by itself does not exceed the minimum support threshold However there are some cases where support pruning does not work very well For example consider the data in Fig 1 with minimum support minsup  of 50 and minimum con\002dence minconf  of 85 Since all columns have 3 or more 1's in M  no candidate pairs can be pruned by a-priori and it requires as much memory as m  m 000 1  2 counters To reduce the number of counters the DHP 14 algorithm which uses a hash-based technique to prune candidate pairs was proposed This algorithm works well to prune most of the useless counters in some cases while it does not solve the problem in the next paragraph The most signi\002cant problem for these algorithms is that if many columns in the matrix remain after support pruning they must use too many counters in main memory For example our Web-page-link data has about 700,000 columns and even if we prune the pages that have less than 10 1's there still remains 58,000 columns Therefore about 1.7 billion counters would have to 002t in main memory 3.2 Min\255Hash algorithm 7 8 p ropos ed t h e Min-hash algorithm in order to 002nd all similar pairs without support pruning The basic idea in the Min-hash algorithm is to permute the rows randomly and for each column c i  to compute its hash value h  c i  to be the index of the 002rst row under the permutation that has a 1 in that column To avoid physically permuting rows we instead give a random hash number to each row and extract for each column the minimum hash value of any row where the column has a 1 Since for any column pair  c i c j   Prob  h  c i   h  c j   Sim  c i c j   we can estimate similarity by repeating the random min-hashing process k times Note that we can generate all k min-hash values with a single data scan as explained in 8  This algorithm works very well to 002nd almost all truly similar pairs However it can not 002nd all pairs with 100 reliability because there is a small probability of generating false negatives Furthermore generated candidates have to be veri\002ed to con\002rm that they actually satisfy the minimum similarity  0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0  r 1 r 2 r 3 r 4 r 5 r 6 r 7 r 8 r 9 c 1 c 2 c 3 c 4 c 5 c 6 a\atrix  1 c 6 0 2 c 4 1 c 5 0 1 2 0 c 1 c 2 c 3 c 4 c 5 1 c 6 c 5 0 cnt List of \(id, miss b\ounter array before r 4 2 c 6 0 3 c 5 1 1 2 1 c 1 c 2 c 3 c 4 c 5 2 c 6 c 5 0 c 2 0 c 3 0 c 6 0 c 3 1 c\ounter array after r 4 5 5 c 5 1 5 5 5 c 1 c 2 c 3 c 4 c 5 5 c 6 c 2 1 d\inal result Figure 2 DMC Algorithm 3.3 Dynamic Miss\255Counting algorithm Before describing the DMC algorithm in detail let us introduce an example that shows how our algorithm works Example 3.1  Fig 2 is example data that has 9 rows Suppose that we want to extract 80%-or-more-con\002dence implication rules Since each column contains 002ve 1's only one miss per column is allowed Fig 2\(b shows the data structure that 1 Counts number of 1's that have already appeared in each column 2 Keeps lists of candidates and miss counters for these candidates For instance before processing r 4  4 candidate rules are kept in the lists c 2  c 6  c 3  c 4  c 3  c 5 and c 4  c 5  Recall that we only keep candidates such that j S i j  


j S j j or  j S i j  j S j j and i  j  We do not have to keep an entry for c 6  because c 6 has the minimum number or 1's and the largest column ID number Since c 3  c 4 has already missed at r 3  its miss counter is equal to 1 other miss counters are 0 At r 4  we process as follows 017 c 1 Since c 1 appears for the 002rst time every column in r 4 is set as a candidate with c 1  In this example c 2  c 3  and c 6 are placed in the list for c 1  017 c 2  Though c 2 has already appeared in r 1 the c 2 counter is equal to or less than the maximum number of misses allowed which is 1 in this example Therefore we add to the list for c 2 every column in r 4 that has not previously appeared on the list for c 2  which is only c 3 in this case Note that the new candidates have already missed as many times as the c 2 counter's value which is 1 here A candidate that has already been added to c 2 s list but that does not appear in this row should have its miss counter increased there is no such a column in this example 017 c 3 Since c 3 has already appeared in r 2 and r 3 a column that has not yet been added to the candidate list for c 3 has had 2 misses already Therefore we do not have to add new candidates for c 3  We only have to check the candidates in its candidate list and delete a candidate whose miss counter exceeds the maximum misses In this example both c 4 and c 5 get misses and only c 5 survives Fig 2\(c shows the counter array after processing r 4  Fig 2\(d is the 002nal result after reading all rows which shows c 1  c 2 and c 3  c 5 have at least 80 con\002dence Algorithm 3.1  DMC-base 1 Read M and count the number of 1's for each column ones  c i   2 Calculate the maximum number of misses for each column maxmis  c i  b 1 000 minconf  002 ones  c i  c  Clear the counter and candidate list for each column i.e cnt  c i 0 and cand  c i  NULL  3 In the second scan for each row r i of M do a For each column c j that has 1 in a row r i we describe this condition in the form of c j 2 r i  since a row consists of a set of columns process as following Case cnt  c j   0  Create a candidate list for c j by adding all columns c k 2 r i such that ones  c k   ones  c j  or  ones  c k   ones  c j  and k j  Set all miss counters for candidates to 0 Case cnt  c j  024 maxmis  c j   Merge the candidate list with the column list in r i For all columns c k 2 r i  cand  c j  if c k exists only in r i and c k satis\002es the same condition as the above case add c k into the candidate list while initializing the miss counter to cnt  c j  If c k exists only in the candidate list increase the miss counter for c k  Case cnt  c j   maxmis  c j   Merge the candidate list for c j with the column c k 2 r i  For all columns c k 2 cand  c j  if c k does not exist in r i  then increase the miss counter for c k  If the counter exceeds maxmis  c j   then delete c k from the candidate list for c j  b After processing all c j  increase the counter cnt  c j  for each column c j in r i and continue processing the next row If cnt  c j  is equal to ones  c j   then output all rules c j  c k where c k is a column in cand  c j   and release the candidate list 4 Memory optimization The DMC-base algorithm reduces the memory requirement signi\002cantly e.g in the case of Web-pagelink data with pruning of columns with fewer than 10 1's DMC-base requires about 0.33GB memory while apriori requires 6.8GB memory However our algorithm still requires as much as m 2 main memory in the worst case In this section we show some techniques that reduce the amount of main memory 4.1 Row re\255ordering Suppose that r 7 is the 002rst row in the matrix M in Fig 2 We have to create all column pairs as candidates in this case In general the denser the rows that come 002rst the more memory is necessary for the DMC-base algorithm Therefore we should read sparser rows 002rst in order to reduce the memory size required In Example 3.1 the history of the total number of candidates in the candidate lists is 1  4  4  7  9  7  7  6  2 if we read the matrix M in the original order while it is 1  2  3  5  6  8  5  2  2 if we read M in the sparsest\002rst order  r 1 r 3 r 8 r 2 r 5 r 4 r 6 r 9 r 7   This technique works very well especially if the row density of the matrix has a wide distribution For example the Web-access log has a few clients such as Web crawlers that access all pages on the site while most clients access only a few pages This optimization reduces the memory size signi\002cantly e.g in the previous case of Web-page-link data with support pruning we can reduce the memory size from 0.33GB to 0.033GB 


However it is expensive to sort the original data by density Instead of sorting we make buckets corresponding to ranges of row density for each row That is we divide the original data according to the number of 1's in each row with ranges of 2 i  2 i 1   when we scan the data the 002rst time Then in the next scan we read the lower density buckets 002rst Note that the number of buckets is no more than d log 2 m e 1  b\b p a g e-link g ra p h   1 10 100 1000 10000 100000 1000000 0 50000 100000 150000 200000 of rows processed Memory\(KB   1 10 100 1000 10000 100000 1000000 0 50000 100000 150000 200000 250000 of rows processed Memory\(KB a\eb access lo g Figure 3 Memory size for a counter array 4.2 Memory\255explosion elimination Scanning the densest rows last may cause memory explosion at the end of the algorithm Fig 3 shows the memory consumption for Web-access-log and Web-pagelink data without support pruning when extracting 100%con\002dence rules The required memory size explodes at the end of the processing since both data sets have several rows with many 1's To avoid memory explosion we switch the algorithm from the memory-consuming algorithm DMC-base to an algorithm that uses more time but uses less space We assume that there are few rows with many 1's Therefore when and if the memory explosion begins we can read the rest of the rows and create bitmaps for each column in main memory The low-memory algorithm DMC-bitmap  consists of 2 phases In the 002rst phase it cleans up the candidate list for those c j such that cnt  c j   maxmis  c j   and in the second phase it extracts implication rules c j  c k  for those c j such that cnt  c j  024 maxmis  c j   Algorithm 4.1  DMC-bitmap 1 When the memory needed for counters exceeds a threshold and the bitmaps for the rest of the rows r i  i  t t 1 n  can 002t in main memory read the rest of the rows r i from M and create  n 000 t 1 bits of a bitmap bm  c j   for each column c j such that cnt  c j   ones  c j  227that is we do not have to create bitmaps for those columns that have no 1's in the rest of rows In order for this method to work there must be enough memory to maintain the counter array until such time as the bitmaps will 002t in the same memory 2 Phase 1 For each column c j that has a non-NULL candidate list and cnt  c j   maxmis  c j   017 Count the number of misses for c j  c k such that c k 2 cand  c j  by counting the number of 1's in bm  c j   bm  c k   017 If the total number of misses is no more than maxmis  c j   then output c j  c k as a result 017 Free the candidate list for c j  3 Phase 2 For each column c j such that cnt  c j  024 maxmis  c j   017 Initialize hit counter hit  c k  to 0 If cand  c j  is non-NULL set hit  c k  cnt  c j  000 mis  c j c k  for each column c k 2 cand  c j  where mis  c j c k  is the number of misses of c j against c k  017 Count the number of hits in the rest of rows r i  i  t t 1 n   by increasing each hit counter hit  c k  such that c k 2 r i  for each row r i such that r i 2 c j  017 For each column c k such that ones  c k   ones  c j  or  ones  c k  ones  c j  and k>j   if hit  c k  025 ones  c j  000 maxmis  c j  then output c j  c k as a result 4.3 100%\255rule pruning Finding 100%-con\002dence rules is much easier than 002nding less-than-100%-con\002dence rules We do not have to count the number of misses since we can delete a candidate as soon as we 002nd a miss for it Therefore we only have to keep the candidate ID lists in main memory 


Furthermore we can also simplify both the DMC-base and DMC-bitmap algorithms since after 002nding the 002rst 1 on a column c j  there is no chance of adding any candidates into the candidate list cand  c j     1 10 100 1000 10000 100000 1000000 1 10 100 1000 10000 of 1's in a column of columns Web access log Page link Dic tionar y New s Web access lo g Page link News Dictionary Figure 4 Column density distribution Suppose that we want to extract 90 or more con\002dence rules In such a case a column that has fewer than 9 1's must have no miss Therefore if we can extract 100%con\002dence rules 002rst then we only consider those columns that have 10 or more 1's Fig 4 shows the distribution of the number of 1's in each column of four real data sets Since our data has many columns with few 1's this optimization improves the performance 4.4 DMC\255imp algorithm The 002nal algorithm to extract implication rules is following Algorithm 4.2  DMC-imp 1 Read M and count the number of 1's for each column ones  c i   Partition the data into buckets B i  i  0  d log 2 m e  according to the number of 1's for each row 2 Extract 100%-con\002dence rules by using the simpli\002ed DMC-base and DMC-bitmap algorithms Note we can skipStep1of DMC-base and we should scan the sparsest buckets 002rst In Phase 2 of DMC-bitmap we count the number of misses between c j and all c k that have at least one of rows r t r t 1 r n in common with c j  3 Remove columns c j such that ones  c j  024 1  1 000 minconf   4 Extract less-than-100%-con\002dence rules by using the DMC-base and DMC-bitmap algorithms Note that we can skip Step 1 of DMC-base and we should scan the sparsest buckets 002rst In our implementation we switch the algorithm from DMCbase to DMC-bitmap when the number of remaining rows becomes 64 or less and the memory size for the counter array that keeps both miss counters and candidate lists for each column exceeds 50MB Note that even if the memory size exceeds 50MB we do not switch the algorithm if the number of remaining rows is more than 64 regardless of the number of columns 5 Finding similarity rules We can 002nd similarity rules more ef\002ciently than we can implication rules since we can apply two more optimization techniques column-density pruning and maximum-hits pruning  5.1 Column\255density pruning If column c i and c j  such that j S i j\024j S j j have minsim or more similarity the ratio of the number of 1's j S i j  j S j j  must be minsim or more minsim 024 Sim  c i c j  j S i  S j j j S i  S j j 024 j S i j j S j j 024 1 Therefore we can save the memory space for miss counters for those column pairs c i and c j such that j S i j  j S j j  minsim  since we do not have to consider such pairs 5.2 Maximum\255hits pruning Similarity is affected by the number of misses of both c i against c j and c j against c i  while con\002dence depends only on the number of misses of c i against c j where j S i j\024j S j j  Therefore we can prune candidates earlier by considering the maximum number of hits as follows Example 5.1  Consider a matrix M as Fig 5 Since we do not count the number of misses between c i and c j such that j S i j  j S j j  we only maintain a miss counter from c 1 to c 2  If we want to extract 75%-or-more similar column pairs one miss is allowed between c 1 and c 2 At r 2 we initialize the miss counter for  c 1 c 2  to 0 Then at r 4 we check this candidate again 1 We should delete 1 Note that we could have deleted  c 1 c 2  at r 3 where c 2 has 1 but c 1 does not However in order to apply the maximum-hits pruning from a dense column c d to sparse columns c s from c 2 to c 1  in this example we have to check all candidate lists for sparse columns c s  which increases the computation time signi\002cantly Therefore we only apply the maximumhits pruning from a sparse column to dense columns 


 0 1 1 1 0 1 1 1  1 1 1 0   r 1 r 2 r 3 r 4  r x r y  c 1 c 2   3 c 2 0 1 c 1 c 2      Maximum hits = 2 Figure 5 Maximum hits pruning the miss counter for  c 1 c 2   even though both c 1 and c 2 are 1 in r 4  Because cnt  c 1   1 and cnt  c 2   3 before reading r 4  the numbers of remaining 1's for each column are 3 and 2 respectively Therefore we know that there are at most 2 rows with both c 1 and c 2 in the following rows Since this pair has already had 1 hit in r 2  the maximum possible number of hits c hit isatmost 3 and the maximum possible similarity d Sim  c 1 c 2  is c hit   ones  c 1  ones  c 2  000 c hit   5  Let rem  c i  be the number of remaining 1's of c i i.e rem  c i   ones  c i  000 cnt  c i   Then the maximum number of hits with c i and c j such that ones  c i  024 ones  c j  is calculated as follows 017 If rem  c i  024 rem  c j   all remaining 1's of c i can match 1's of c j  Therefore the maximum possible hits c hit is ones  c i  000 mis  c i  017 If rem  c i   rem  c j  then rem  c i  000 rem  c j  of the remaining 1's of c i can not match 1's of c j  Therefore the maximum possible hits c hit is ones  c i  000 mis  c i  000  rem  c i  000 rem  c j   Using c hit  we can calculate the maximum possible similarity d Sim  c i c j  c hit   ones  c i  ones  c j  000 c hit   which can be used for candidate pruning 5.3 DMC\255sim algorithm The overall algorithm to 002nd similar column pairs is following Algorithm 5.1  DMC-sim 1 Same as Step 1 of DMC-imp  2 Extract 100%-similar i.e identical columns We only have to keep candidates such that ones  c i   ones  c j  with no miss To avoid memory explosion we also apply a variant of the DMC-bitmap algorithm The main points that we should modify it are following 017 Compare those columns that have the same number of 1's since we want to extract identical pairs 017 Extract those column pairs c i and c j that have the same bitmap instead of counting the number of 1's in bm  c j   bm  c k   3 Remove columns c j such that ones  c j  024 1  1 000 minsim  000 1  Note that the cut-off threshold is not 1  1 000 minsim   since there might be less-than-100 similar pairs between the columns whose number of 1's are d 1  1 000 minsim  000 1 e and d 1  1 000 minsim  e  4 Extract less-than-100 similar columns by using a variant of DMC-base and DMC-bitmap algorithms The modi\002cation points are following 017 Skip Step 1 of DMC-base and scan the sparsest bucket 002rst 017 Keep candidates such that ones  c i  ones  c j  is minsim or more 017 Discard candidates whenever the maximum possible similarity d Sim  c i c j  islessthan minsim  6 Performance Evaluation We implemented our algorithms DMC-imp and DMCsim  on a Sun Ultra 2  2 002 200 MHz 256MB memory workstation In this section we describe the data sets that we used for our experiments Then we show the experimental results of both algorithms To compare DMC algorithms with a-priori and Min-Hash algorithms we have done an experiment with applying both support pruning and con\002dence pruning Note that support pruning can be applied to DMC and Min-Hash algorithms in the same manner as a-priori  Finally we show sample rules extracted from news articles 6.1 Data sets Table 1 shows the size of the data sets we used in our experiments The meaning of each data set is followings 1 Web access log This data set consists of the log entries from the sun Web server The columns in this case are the URL's and the rows represent distinct 


client IP address that have accessed the server recently An entry is set to 1 if there has been at least one hit for that URL from that particular client IP The data set Wlog  has more than 200,000 rows and 75,000 columns We also prepared the data set WlogP by pruning those columns with 10-or-fewer 1's The pruned data set WlogP has 13,000 columns 2 Web-page-link graph This data represents the URL link graph for the Stanford site which has about 700,000 pages Both columns and rows are the URL's An entry is set to 1 if there is a link from the page p i to the page p j  The rows are p i and the columns are p j in the data plinkF andtherowsare p j and the columns are p i in the data plinkT  Extracting similar columns from plinkF means extracting pages that are referred to by similar sets of pages while extracting from plinkT means extracting pages that have similar sets of links 3 News documents This data News  consists of 84,000 Reuters news documents Each row is a document and each column is a word Stop words  which appear among the documents very frequently are pruned before processing and 170,000 words remain By using implication rules with low-support pruning we can get sets of words each of which is related to a news topic To compare the performance with other algorithms we also prepare a smaller data set NewsP  since other algorithms could not be applied to our data sets in a reasonable execution time on our machine due to its memory size 4 Dictionary This is an on-line version of the 1913 Webster's dictionary that is available through the Gutenberg Project 15  C ol umns are head words words being de\002ned and rows are de\002nition words words used in the de\002nition There are 96,000 head words and 45,000 de\002nition words We can get similar de\002nition words such as brother-in-law and sisterin-law  by extracting similarity rules Table 1 Real data sets Data Rows Columns Wlog 218,518 74,957 WlogP 203,185 13,087 pliknF 173,338 697,824 plinkT 695,280 688,747 News 84,672 170,372 NewsP 16,392 9,518 dicD 45,418 96,540 6.2 Experimental results In this section we discuss our experimental results We use 6 sets of the data 226 Wlog  WlogP  linkF  linkT  News and dicD 226 for evaluating DMC-imp and DMC-sim Wealsouse one small data set NewsP for comparing the performance among the algorithms including a-priori and Min-hash  Fig 6 shows the experimental results The left graphs are for 002nding implication rules and the right graphs are for 002nding similarity rules Fig 6\(a and b plot the execution time with a different threshold Each execution can be 002nished in a reasonable time if we want to extract 85%-ormore rules The execution time decreases lineally according to increasing the threshold in general Fig 6\(c and d shows the details of the execution time for Wlog  The execution time for pre-scanning is small compared to other execution times The execution time for 002nding 100 rules is also small and constant for each threshold while the execution time for 002nding less than 100 rules depends on the threshold The execution times of DMC-imp and DMC-sim for plinkT have a gap between 80 and 75 threshold Fig 6\(e and f show the detail of the execution time In these cases the low-memory algorithm DMC-bitmap  jumps up from 22 to 398 seconds in the case of DMCimp and from 27 to 399 seconds in the case of DMC-sim  respectively The reason why this jump-up occurs is that larger rows that are handled in the DMC-bitmap includes many columns that have frequency 4 Since we can not prune columns with frequency 4 if the threshold is 75 or less the execution time for DMC-bitmap was dramatically increased If we could apply low-support pruning before executing algorithms it would not occur Fig 6\(g and h show the maximum memory size for the counter array that keeps candidate IDs and their misscounters DMC-sim requires much less memory than DMCimp since DMC-sim can use the two additional pruning techniques that we mentioned in Sec 5 Except DMCimp for News with 75%-or-less threshold all executions can 002t in main memory Since we apply the DMC-bitmap algorithms the memory requirement does not explode even as the threshold decreases For example the memory requirement for plinkT does not jump up even if the threshold decreases from 80 down to 70 while the execution time does In order to compare the DMC algorithms to a-priori and Min-Hash  we generated a pruned data set of news documents NewsP  We created this data from 16,392 news documents and pruned the columns with minimum support threshold 35 0.2 and maximum support threshold 3278 20 Since the number of remaining columns was 9518 the counters for all pairs could be 002t in main memory that required 172MB This situation is best for a-priori since 


   0 100 200 300 400 500 600 700 800 65 70 75 80 85 90 95 100 Confidence threshold Execution time \(sec    0 100 200 300 400 500 600 700 800 65 70 75 80 85 90 95 100 Similarity threshold Execution time \(sec Wlog WlogP plinkF plinkT News dicD   0 50 100 150 200 65 70 75 80 85 90 95 100 Confidence Threshold Memory \(MB    0 50 100 150 200 65 70 75 80 85 90 95 100 Similarity Threshold Memory \(MB Wlog WlogP plinkF plinkT News dicD a b g h   e plinkT 0 100 200 300 400 500 600 700 65 70 75 80 85 90 95 100 Confidence threshold Time \(sec   c Wlog 0 100 200 300 400 500 600 700 65 70 75 80 85 90 95 100 Confidence threshold Time \(sec   f plinkT 0 100 200 300 400 500 600 700 65 70 75 80 85 90 95 100 Similarity threshold Time \(sec     d Wlog 0 100 200 300 400 500 600 700 65 70 75 80 85 90 95 100 Similarity threshold Time \(sec 100-bitmap 100%-base 100%-bitmap 100%-base Pre-scaning    i NewsP 0 50 100 150 200 65 70 75 80 85 90 95 100 Confidence threshold Time \(sec DMC-imp a-priori K-Min    j NewsP 0 50 100 150 200 65 70 75 80 85 90 95 100 Similarity threshold Time \(sec DMC-sim a-priori Min-Hash Figure 6 Experimental results 


polgar -> international polgar -> old polgar -> judit polgar -> champion polgar -> youngest polgar -> chess polgar -> kasparov polgar -> men polgar -> highest polgar -> top polgar -> soviet polgar -> players polgar -> federation polgar -> player polgar -> ranked polgar -> grandmaster polgar -> garri  j udit -> soviet j udit -> hungary kasparov -> chess kasparov -> game kasparov -> champion grandmaster -> soviet grandmaster -> champion grandmaster -> chess garri -> chess garri -> kasparov garri -> soviet garri -> championship garri -> champion Figure 7 Sample rules the memory optimization techniques for DMC and MinHash will not improve their performance signi\002cantly Fig 6\(i and j show the execution times of these algorithms The K-Min algorithm is a variant algorithm of Min-Hash  which can extract implication rules instead of similarity rules However it could not extract complete sets of true rules therefore we plotted the execution time when the number of false negatives was less than 10 The other algorithms including Min-Hash for 002nding similarity rules could extract complete sets of true rules In this experiment a-priori is best for 002nding implication rules with 75%-or-less con\002dence threshold and Min-Hash is best for 002nding similarity rules with 70%or-less similarity threshold respectively However the DMC algorithms are best for 002nding both implication and similarity rules with high threshold 6.3 Extracted rules Text-mining by using extracted implication rules with low-support pruning is one of the interesting applications for our algorithms Fig 7 shows sample rules we extracted from News with an 85 con\002dence threshold and with a support pruning less than 5 These rules are extracted by selecting all rules related to keyword Polgar and its successors recursively This set of rules indicates information about Miss Judit Polgar who is 12-years-old has been ranked No 1 in the women's world chess 7 Conclusion and future works We presented two new algorithms DMC-imp and DMCsim  for 002nding implication and similarity rules Our algorithms do not use support pruning but use con\002dence or similarity pruning which reduces the memory size signi\002cantly We also proposed the other pruning techniques row re-ordering  100%-rule pruning  columndensity pruning and maximum-hits pruning  In order to evaluate the performance of the algorithms we used 4 sets of data Web-access logs Web-page-link graph news documents and a dictionary The algorithms have been implemented on a Sun Ultra 2  2 002 200 MHz 256MB memory workstation According to the experimental results our algorithms can be executed in a reasonable time The algorithms that proposed previously can not execute on our data sets since the algorithms required more than 256MB memory Therefore we compared performance by using the News data sets by pruning by using support threshold 35 so that all counters for a-priori can 002t in main memory The comparison results shows that DMC-imp can execute 1.7 times faster than a-priori  and 1.9 times faster than K-Min  and that DMC-sim can execute 5.9 times faster than apriori  and 1.7 times faster than Min-Hash  in case of an 85 threshold The followings are future research topics 017 Our algorithm can not extract rules among more than two columns while a-priori can do so However by grouping similarity and implication rules as showed in Sec 6.3 we can get useful groups of rules among more than two columns This idea can be applied to other data sets which generates more interesting rules 017 The memory requirement for News with less than 80 con\002dence threshold exceeds 256MB To be scalable this algorithm a parallel algorithm based on a divideand-conquer technique such as FDM 6 f or a-priori  is necessary References 1 R  A g r a w al T  I mielin sk i an d A  S w a mi M in in g Association Rules Between Sets of Items in Large Databases In Proceedings of the ACM SIGMOD Conference on Management of Data 1993 pp 207\226 216  R  A gra w al and R  S ri kant  F as t A l gori t h ms for Mining Association Rules In Proceedings of the 20th 


International Conference on Very Large Databases 1994  R  J  B ayardo J r  R  A gra w al and D  G unopul os  Constraint-Based Rule Mining in Large Dense Databases In Proceedings of the 15th International Conference on Data Engineering 1999 pp 188\226197 4 S  B rin  R Mo tw an i J.D Ullman  a n d S Tsu r  Dynamic itemset counting and implication rules for market basket data In Proceedings of the ACM SIGMOD Conference on Management of Data 1997 pp 255\226264  A  B roder  On the r esemblance and c ontainment of documents In Compression and Complexity of Sequences SEQUENCES'97  1998 pp 21\22629  D  W  C heung J  H a n V  T  N g  e t a l  A Fast Distributed Algorithm for Mining Association Rules In Proceedings of Conference on Parallel and Distributed Information Systems 1996 pp 31\22642 7 E  C o h e n  Size-Estimatio n F rame w o rk with Applications to Transitive Closure and Reachability Journal of Computer and System Sciences 55 1997 441\226453 8 E  C o h e n  M Datar  S Fu jiw ara A Gio n i s et al Finding Interesting Associations without Support Pruning In Proceedings of the 16th International Conference on Data Engineering  2000  R O D uda and P E H art Pattern Classi\002cation and Scene Analysis  A Wiley-Interscience Publication New York 1973  A  G i oni s  P  Indyk and R  M ot w a ni  S i m i l a ri t y Search in High Dimensions via Hashing In Proceedings of the 25th International Conference on Very Large Databases 1999 11 D Go ld b e r g  D  Nich o l s B.M Ok i an d D  T erry  Using collaborative 002ltering to weave an information tapestry Communications of the ACM 55 1991 1\226 19  S  G uha R  R as t ogi  a nd K  S h i m  C U R E A n Ef\002cient Clustering Algorithm for Large Databases In Proceedings of the ACM-SIGMOD International Conference on Management of Data 1998 pp 73\22684  R  Mot w ani a nd P  R a gha v a n Randomized Algorithms Cambridge University Press 1995  J  S  P a rk M S  C hen and P S  Y u A n ef fect i v e hash-based algorithm for mining association rules In Proceedings of the ACM SIGMOD Conference on Management of Data 1995 pp 175\226186  P r oj ect G u t e nber g  http:..www.gutenberg.net  1999  N Shi v akumar and H  G arcia-Molina B u ilding a Scalable and Accurate Copy Detection Mechanism In Proceedings of the 3rd International Conference on the Theory and Practice of Digital Libraries  1996  H.R  V a rian and P  R esnick E ds C A C M S pecial Issue on Recommender Systems Communications of the ACM 40 1997 


User Anomaly Description programmer2 logs in from beta secretary logs in at night sysadm logs in from jupiter programmer1 becomes a secretary secretary becomes a manager programmer1 logs in at night sysadm becomes a programmer manager1 becomes a sysadm manager2 logs in from pluto Table 12 User Anomaly Description User Normal Anomaly programmer2 0.58 0.79 0.00 secretary  1  1  0.00 sysadm 0.84 0.95 0.00 programmer1 0.31 1.00 0.04 secretary 0.41 0.98 0.17 programmer1  1  1  0.00 sysadm 0.64 0.95 0.00 manager1 0.57 1.00 0.00 manager2 1.00 1.00 0.00 Table 13 Similarity with User's Own Pro\002le tivities of each time segment am pm and nt We treat the 5th week as the training period during which we compare the patterns from each session to the pro\002le of the time segment We record the normal range of the similarity scores during this week The data in the 6th week has some user anomalies as described in Table 12 For each of the anomalous sessions we compare its patterns against the original user's pro\002le and then compare the resulting similarity score against the recorded normal range of the same time segment In Table 13 the column labeled 223Normal\224 is the range of similarity of each user against his or her own pro\002le as recorded during the 5th week A 1 here means that the user did not login during the time segment in the 5th week The column 223Anomaly\224 is the similarity measure of the anomalous session described Table 12 We see that all anomalous sessions can be clearly detected since their similarity scores are much smaller than the normal range For example when the sysadm becomes programmer see Table 12 his/her patterns have zero matches with the sysadm's pro\002le while for the whole 5th week the pm similarity scores are in the range of 0.64 to 0.95 Unfortunately formal evaluation statistics are not available to determine the error rates of this approach However this initial test indicates a path worthy of future study 6 Related Work Network intrusion detection has been an on-going research area 17  M ore r ecent s ystems e g B ro 18   NFR 6  a n d EMERALD  1 9  a ll mad e e x ten s ib ility th eir primary design goals Our research focuses on automatic methods for constructing intrusion detection models The meta-learning mechanism is designed to automate the extention process of IDSs We share the same views discussed in 20 t h at an ID S s houl d b e b ui l t us i n g s t a ndard components We believe that the operating system and networking community should be responsible for building a robust 223Event\224 box In 10  a l gori t h ms for a nal y zi ng us er s h el l c ommands and detecting anomalies were discussed The basic idea is to 002rst collapse the multi-column shell commands into a single stream of strings and then string matching techniques and consideration of 223concept drift\224 are used to build and update user pro\002les We believe that our extended frequent episodes algorithm is a superior approach because it considers both the association among commands and arguments and the frequent sequential patterns of such associations 7 Conclusions and Future Directions In this paper we outline a data mining framework for constructing intrusion detection models The key idea is to apply data mining programs to audit data to compute misuse and anomaly detection models according to the observed behavior in the data To facilitate adaptability and extensibility we propose the use of meta-learning as a means to construct a combined model that incorporate evidence from multiple lightweight base models This mechanism makes it feasible to introduce new ID components in an existing IDS possibly without signi\002cant re-engineering We extend the basic association rules and frequent episodes algorithms to accommodate the special requirements in analyzing audit data Our experiments show that the frequent patterns mined from audit data can be used as reliable user anomaly detection models and as guidelines for selecting temporal statistical features to build effective classi\002cation models Results from the 1998 DARPA Intrusion Detection Evaluation Program showed our detection models performed as well as the best systems built using the manual knowledge engineering approaches Our future work includes developing network anomaly detection strategies and devising a mechanical procedure to translate our automatically learned detection rules into modules for real-time IDSs A preliminary project in collaboration with NFR has just started 12 


8 Acknowledgments We wish to thank our colleagues at Columbia University Chris Park Wei Fan and Andreas Prodromidis for their help and encouragement References 1 R  A g r a w a l  T  I m i e lin sk i a n d A  S w a m i  M in in g a sso c i a tion rules between sets of items in large databases In Proceedings of the ACM SIGMOD Conference on Management of Data  pages 207\226216 1993 2 P  K  C han a nd S  J S t ol f o  T o w ar d p ar al l e l a nd di st r i b u t e d learning by meta-learning In AAAI Workshop in Knowledge Discovery in Databases  pages 227\226240 1993 3 W  W  C ohen Fast ef f ect i v e r ul e i nduct i on I n Machine Learning the 12th International Conference  Lake Taho CA 1995 Morgan Kaufmann 4 U  F ayyad G P i at et sk yS h api r o and P  S myt h  T he KDD process of extracting useful knowledge from volumes of data Communications of the ACM  39\(11\:27\22634 November 1996 5 K  I l gun R  A K e mmer e r  and P  A  P or r a s S t at e t r a nsition analysis A rule-based intrusion detection approach IEEE Transactions on Software Engineering  21\(3\:181\226 199 March 1995 6 N  F  R  I n c  N etw o rk 003ig h t reco rd er  h ttp www n fr co m  1997 7 V  J acobson C  L e r e s and S  M cC anne t cpdump a v ai l a bl e via anonymous ftp to ftp.ee.lbl.gov June 1989 8 C  K o  G Fin k  a n d K  L e v itt Au to m a te d d e t e c tio n o f v u l nerabilities in privileged programs by execution monitoring In Proceedings of the 10th Annual Computer Security Applications Conference  pages 134\226144 December 1994 9 S  K umar and E  H  S paf f or d A s of t w ar e a r c hi t ect ur e t o support misuse intrusion detection In Proceedings of the 18th National Information Security Conference  pages 194\226 204 1995  T  L a ne and C  E  B r odl e y  S equence m at chi n g a nd l ear ni ng in anomaly detection for computer security In AAAI Workshop AI Approaches to Fraud Detection and Risk Management  pages 43\22649 AAAI Press July 1997  W  L e e a nd S  J S t ol f o  D at a m i n i n g a ppr oaches f o r i nt r u sion detection In Proceedings of the 7th USENIX Security Symposium  San Antonio TX January 1998  W  L ee S  J S t ol f o  a nd K W  Mok Mi ni ng i n a d at a\003 o w environment Experience in intrusion detection submitted for publication March 1999  T  L unt  D et ect i n g i nt r uder s i n comput er syst ems I n Proceedings of the 1993 Conference on Auditing and Computer Technology  1993  T  L unt  A  T amar u F  Gi l h am R  J agannat h an P  N eumann H Javitz A Valdes and T Garvey A real-time intrusion detection expert system IDES 002nal technical report Technical report Computer Science Laboratory SRI International Menlo Park California February 1992  H Manni l a and H  T oi v onen Di sco v e r i ng gener a l i zed episodes using minimal occurrences In Proceedings of the 2nd International Conference on Knowledge Discovery in Databases and Data Mining  Portland Oregon August 1996  H Manni l a  H  T oi v onen and A  I  V er kamo D i s co vering frequent episodes in sequences In Proceedings of the 1st International Conference on Knowledge Discovery in Databases and Data Mining  Montreal Canada August 1995  B M ukherjee L T  Heberlein and K  N  L e v itt Netw ork intrusion detection IEEE Network  May/June 1994  V  Paxon B r o  A syst em f o r d et ect i n g n et w o r k i n t r uder s in real-time In Proceedings of the 7th USENIX Security Symposium  San Antonio TX 1998  P  A P o r r a s a nd P  G Neumann E m er al d E v ent m oni t o r i ng enabling responses to anomalous live disturbances In National Information Systems Security Conference  Baltimore MD October 1997  S  S t ai nf or dC h en C ommon i nt r u si on det ect i o n f r a me w o r k  http://seclab.cs.ucdavis.edu/cidf  S  J S t ol f o  A  L  P r odr omi d i s  S  T sel e pi s W  L ee D W  Fan and P K Chan JAM Java agents for meta-learning over distributed databases In Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining  pages 74\22681 Newport Beach CA August 1997 AAAI Press  S unS of t  Mount ai n V i e w  C A  SunSHIELD Basic Security Module Guide  13 


 30 40 50 60 70 80 90 100 110 120 30 40 50 60 70 80 90 10 0 Execution Time [s Number of Nodes Figure 13 Execution time of HPA program pass 2 on PC cluster 2 Scan the transaction database and count the support count Each processing node reads the transaction database from its local disk 000 itemsets are generated from that transaction and the same hash function used in phase 1 s applied to each of them Each of the 000 itemsets is sent to certain processing node according the hash value For the itemsets received from the other nodes and those locally generated whose ID equals the node\220s ID the hash table is searched If hit its support count value is incremented 3 Determine the large itemset After reading all the transaction data each processing node can individually determine whether each candidate 000 itemset satisfy user-specified minimum support or not Each processing node sends large 000 itemsets to the coordinator where all the large 000 itemsets are gathered 4 Check the terminal condition If the large 000 itemsets are empty the algorithm terminates Otherwise the coordinator broadcasts large 000 itemsets to all the processing nodes and the algorithm enters the next iteration 4.2 Performance evaluation of HPA algorithm The HPA program explained above is implemented on our PC cluster Each node of the cluster has a transaction data file on its own hard disk Transaction data is produced using data generation program developed by Agrawal designating some parameters such as the number of transaction the number of different items and so on The produced data is divided by the number of nodes and copied to each node\220s hard disk The parameters used in the evaluation is as follows The number of transaction is 5,000,000 the number of different items is 5000 and minimum support is 0.7 The size of the data is about 400MBytes in total The message block size is set to be 16KBytes according to the results of communication characteristics of PC clusters discussed in previous section The disk I/O block size is 64KBytes which seems to be most suitable value for the system Note that the number of candidate itemset in pass 2 s substantially larger than for the other passes which relatively frequently occurs in association rules mining Therefore we have been careful to parallelize the program effectively especially in pass 2 so that unnecessary itemsets to count should not be generated 14 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


The execution time of the HPA program pass 2 is shown in figure 13 as the number of PCs is changed The maximum number of PCs used in this evaluation is 100 Reasonably good speedup is achieved in this application as the number of PCs is increased 5 Conclusion In this paper we presented performance evaluation of parallel database processing on an ATM connected 100 node PC cluster system The latest PCs enabled us to obtain over 110Mbps throughput in point-to-point communication on a 155Mbps ATM network even with the so-called 217\217heavy\220\220 TCP/IP This greatly helped in developing the system in a short period since we were absorbed in fixing many other problems Massively parallel computers now tend to be used in business applications as well as the conventional scientific computation Two major business applications decision support query processing and data mining were picked up and executed on the PC cluster The query processing environment was built using the results of our previous research the super database computer SDC project Performace evaluation results with a query of the standard TPC-D benchmark showed that our system achieved superior performance especially when transposed file organization was employed As for data mining we developed a parallel algorithm for mining association rules and implemented it on the PC cluster By utilizing aggregate memory of the system efficiently the system showed good speedup characteristics as the number of nodes increased The good price/performance ratio makes PC clusters very attractive and promising for parallel database processing applications All these facts support the effectiveness of the commodity PC based massively parallel database servers Acknowledgment This project is supported by NEDO New Energy and Industrial Technology Development Organization in Japan Hitachi Ltd technically helped us extensively for ATM related issues References  R Agrawal T Imielinski and A Swami Mining association rules between sets of items in large databases In Proceedings of ACM SIGMOD International Conference on Management of Data  pages 207--216 1993  R Agrawal and R Srikant Fast algorithms for mining association rules In Proceedings of International Conference on Very Large Data Bases  1994  A C Arpaci-Dusseau R H Arpaci-Dusseau D E Culler J M Hellerstein and D A Patterson High-performance sorting on Networks of Workstations In Proceedings of International Conference on Management of Data  pages 243--254 1997  D.S Batory On searching transposed files ACM TODS  4\(4 1979  P.A Boncz W Quak and M.L Kersten Monet and its geographical extensions A novel approach to high performance GIS processing In Proceedings of International Conference on Extending Database Technology  pages 147--166 1996 15 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


 R Carter and J Laroco Commodity clusters Performance comparison between PC\220s and workstations In Proceedings of IEEE International Symposium on High Performance Distributed Computing  pages 292--304 1995  D.J DeWitt and J Gray Parallel database systems  The future of high performance database systems Communications of the ACM  35\(6 1992  J Gray editor The Benchmark Handbook for Database and Transaction Processing Systems  Morgan Kaufmann Publishers 2nd edition 1993  J Heinanen Multiprotocol encapsulation over ATM adaptation layer 5 Technical Report RFC1483 1993  M Kitsuregawa M Nakano and M Takagi Query execution for large relations on Functional Disk System In Proceedings of International Conference on Data Engineering  5th pages 159--167 IEEE 1989  M Kitsuregawa and Y Ogawa Bucket Spreading Parallel Hash:A new parallel hash join method with robustness for data skew in Super Database Computer SDC In Proceedings of International Conference on Very Large Data Bases  16th pages 210--221 1990  M Laubach Classical IP and ARP over ATM Technical Report RFC1577 1994  D.A Schneider and D.J DeWitt Tradeoffs in processing complex join queries via hashing in multiprocessor database machines In Proceedings of International Conference on Very Large Data Bases  16th pages 469--480 1990  T Shintani and M Kitsuregawa Hash based parallel algorithms for mining association rules In Proceedings of IEEE International Conference on Parallel and Distributed Information Systems  pages 19--30 1996  T Sterling D Saverese D.J Becker B Fryxell and K Olson Communication overhead for space science applications on the Beowulf parallel workstaion In Proceedings of International Symposium on High Performance Distributed Computing  pages 23--30 1995  T Tamura M Nakamura M Kitsuregawa and Y Ogawa Implementation and performance evaluation of the parallel relational database server SDC-II In Proceedings of International Conference on Parallel Processing  25th pages I--212--I--221 1996  TPC TPC Benchmark 000\001 D Decision Support Standard Specification Revision 1.1 Transaction Processing Performance Council 1995 16 Proceedings of the ACM/IEEE SC97 Conference \(SC\22297 0-89791-985-8/97 $ 17.00 \251 1997 IEEE 


In accordance with 1910.97 and 1910.209 warning signs are required in microwave areas For work involving power line carrier systems this work is to be conducted according to requirements for work on energized lines Comments s APPA objects to the absolute requirement implied by the word ensure regarding exposure to microwave radiation and recommends revision of s l iii to read when an employee works in an area where electromagnetic radiation levels could exceed the levels specified in the radiation protection guide the employer shall institute measures designed to protect employees from accidental exposure to radiation levels greater than those permitted by that guide  I1 an employee must be stationed at the remote end of the rodding operation Before moving an energized cable it must be inspected for defects which might lead to a fault To prevent accidents from working on the wrong cable would require identification of the correct cable when multiple cables are present Would prohibit an employee from working in a manhole with an energized cable with a defect that could lead to a fault However if the cable cannot be deenergized while another cable is out employees may enter the manhole but must protect against failure by some means for example using a ballistics blanket wrapped around cable Requires bonding around opening in metal sheath while working on cable Underaround EIectrical Installations t Comments t This paragraph addresses safety for underground vaults and manholes The following requirements are contained in this section Ladders must be used in manholes and vaults greater than four feet deep and climbing on cables and hangers in these vaults is prohibited Equipment used to lower materials and tools in manholes must be capable of supporting the weight and should be checked for defects before use An employee in a manhole must have an attendant in the immediate vicinity with facilities greater than 250 volts energized An employee working alone is permitted to enter briefly for inspection housekeeping taking readings or similar assuming work could be done safely Duct rods must be inserted in the direction presenting the least hazard to employees and APPA recommends that OSHA rewrite section 7\regarding working with defective cables This rewrite would include the words shall be given a thorough inspection and a determination made as to whether they represent a hazard to personnel or representative of an impending fault As in Subsection \(e EEI proposes the addition of wording to cover training of employees in emergency rescue procedures and for providing and maintaining rescue equipment Substations U This paragraph covers work performed in substations and contains the following requirements Requires that enough space be provided around electrical equipment to allow ready and safe access for operation and maintenance of equipment OSHA's position A2-16 


is that this requirement is sufficiently performance oriented to meet the requirements for old installations according to the 1987 NEW Requires draw-out circuit breakers to be inserted and removed while in the open position and that if the design permits the control circuits be rendered inoperative while breakers are being inserted and removed stated in the Rules and requests that existing installations not be required to be modified to meet NESC APPA recommends that Section u 4 i which includes requirements for enclosing electric conductors and equipment to minimize unauthorized access to such equipment be modified to refer to only those areas which are accessible to the public Requires conductive fences around substations to be grounded Power Generation v Addresses guarding of energized parts  Fences screens, partitions or walls This section provides additional requirements and related work practices for power generating plants  Entrances locked or attended Special Conditions w  Warning signs posted  Live parts greater than 150 volts to be guarded or isolated by location or be insulated  Enclosures are to be according to the 1987 NESC Sections llOA and 124A1 and in 1993 NESC  Requires guarding of live parts except during an operation and maintenance function when guards are removed barriers must be installed to prevent employees in the area from contacting exposed live parts Requires employees who do not work regularly at the substation to report their presence Requires information to be communicated to employees during job briefings in accordance with Section \(c of the Rules Comments U APPA and EEI provide comments as follows Both believe that some older substations \(and power plants would not meet NESC as This paragraph proposes special conditions that are encountered during electric power generation, transmission and distribution work including the following Capacitors  Requires individual units in a rack to be short circuited and the rack grounded  Require lines with capacitors connected to be short circuited before being considered deenergized Current transformer secondaries may not be opened while energized and must be bridged if the CT circuit is opened Series street lighting circuits with open circuit voltages greater than 600 volts must be worked in accordance with Section q\or t and the series loop may be opened only after the source transformer is deenergized and isolated or after the loop is bridged to avoid open circuit condition Sufficient artificial light must be provided where insufficient naturals illumination is present to enable employee to work safely A2-17 


US Coast Guard approved personal floatation devices must be supplied and inspected where employees are engaged in work where there is danger of drowning Required employee protection in public work areas to include the following  Warning signs or flags and other traffic control devices  Barricades for additional protection to employees  Barricades around excavated areas  Warning lights at night prominently displayed Lines or equipment which may be sub to backfeed from cogeneration or other sources are to be worked as energized in accordance with the applicable paragraphs of the Rules Comments w APPA submits the following comments regarding this Special Conditions section Recommends that the wording regarding capacitors be modified to include a waiting period for five minutes prior to short circuiting and grounding in accordance with industry standards for discharging of capacitors For series street light circuits, recommends that language be added for bridging to either install a bypass conductor or by placement of grounds so that work occurs between the grounds Recommends modification of the section regarding personal floatation devices to not apply to work sites near fountains decorative ponds swimming pools or other bodies of water on residential and commercial property Definitions x This section of the proposed Rules includes definitions of terms Definitions particularly pertinent to understanding the proposal and which have not previously been included are listed as follows Authorized Employee  an employee to whom the authority and responsibility to perform a specific assignment has been given by the employer who can demonstrate by experience or training the ability to recognize potentially hazardous energy and its potential impact on the work place conditions and who has the knowledge to implement adequate methods and means for the control and isolation of such energy CZearance for Work  Authorization to perform specified work or permission to enter a restricted area Clearance from Hazard  Separation from energized lines or equipment Comments x The following summarizes the changes in some of the definitions which APPA recommends Add to the definition for authorized employee It the authorized employee may be an employee assigned to perform the work or assigned to provide the energy control and isolation function  Recommends that OSHA modify the definition for a line clearance tree trimmer to add the word qualified resulting in the complete designation as a qualified line clearance tree trimmer Recommends that OSHA modify the definition of qualified employee" to remove the word construction from the definition since it is felt that knowledge of construction procedures is beyond the scope of the proposed rule resulting in APPA's new A2-18 I 


wording as follows more knowledgeable in operation and hazards associated with electric power generation transmission and/or distribution equipment Recommends that OSHA add a definition for the word practicable and replace the word feasible with practicable wherever it appears in the proposed regulations and that practicable be further defined as capable of being accomplished by reasonably available and economic means OTHER ISSUES Clothing OSHA requested comments on the advisability of adopting requirements regarding the clothing worn by electric utility industry employees EEI has presented comments which indicates research is underway prior to establishing a standard for clothing to be worn by electric utility employees However EEI's position is that this standard has not developed to the extent that it could be included in the OSHA Rules Both APPA and EEI state that they would support a requirement that employers train employees regarding the proper type of clothing to wear to minimize hazards when working in the vicinity of exposed energized facilities Grandfathering Due to the anticipated cost impact on the utility industry of the proposed Rules requiring that existing installations be brought to the requirements of the proposed Rules both APPA and EEI propose that the final Rules include an omnibus grandfather provision This provision would exempt those selected types of facilities from modification to meet the new rules EEI states that if the grandfathering concept is incorporated that electric utility employees will not be deprived of proper protection They propose that employers be required to provide employees with a level of protection equivalent to that which the standard would require in those instances in which the utility does not choose to modify existing facilities to comply with the final standard Rubber Sleeves OSHA requests comments from the industry on whether it would be advisable to require rubber insulating sleeves when gloves are used on lines or equipment energized at more than a given voltage EEI states its position that utilities should continue to have the option of choosing rubber gloves or gloves and sleeves to protect employees when it is necessary to work closer to energized lines than the distances specified in the clearance tables Preemuting State Laws EEI requests that the final Rules be clear in their preempting state rules applicable to the operation and maintenance work rules for electric power systems. This is especially critical since some states now have existing laws which are more stringent than the proposed OSHA Rules Examples are 1 in California and Pennsylvania where electric utility linemen are prohibited from using rubber gloves to work on lines and equipment energized at more than certain voltages and 2 in California and Connecticut where the live line bare hand method of working on high voltage transmission systems is prohibited One utility Pacific Gas  Electric has obtained a variance from the California OSHA to perform live line bare-hand transmission maintenance work on an experimental basis Coiiflicts Between the Rilles and Part 1926 Subpart V Since many of the work procedures in construction work and operation and maintenance work are similar and difficult to distinguish between EEI requests that the final order be clear in establishing which rule has jurisdiction over such similar work areas A2-19 v 


IMPACTS ON COSTS AND ASSOCIATED BENEFITS In its introduction to the proposed rules OSHA has provided an estimate of the annual cost impact on the electric utility industry for the proposed des of approximately 20.7 million OSHA estimates that compliance with this proposed standard would annually prevent between 24 and 28 fatalities and 2,175 injuries per year The utilities which have responded to this proposed standard through their respective associations have questioned the claims both of the magnitude of the cost involved and the benefit to the industry in preventing fatalities and lost-time injuries Both EEI and APPA feel that the annual cost which OSHA estimates are significantly lower than would be realized in practice Factors which APPA and EEI feel were not properly addressed include the following OSHA has not accurately accounted for cost of potential retroactive impacts including retrofitting and modifying existing installations and equipment OSHA has not consistently implemented performance based provisions in proposed rules  many portions require specific approaches which would require utilities to replace procedures already in place with new procedures Estimates were based on an average size investor-owned utility of 2,800 employees and an average rural cooperative of 56 employees, which are not applicable to many smaller systems such as municipal systems OSHA has not adequately addressed the retraining which would be necessary with modifying long-established industry practices to be in accordance with the OSHA rules EEI claims that OSHA's proposed clearance requirements would not allow the use of established maintenance techniques for maintaining high voltage transmission systems and thus would require new techniques For an example of the cost which is estimated to be experienced as a result of the new Rules one of the EEI member companies has estimated that approximately 20,000 transmission towers would need to be modified to accommodate the required step bolts in the Rules at an estimated cost of 6,200,000 Additionally this same company estimates that the annual cost of retesting live line tools for its estimated 1,000 tools would be 265,000 Additionally, both EEI and APPA question the additional benefits which OSHA claims would result from implementation of the new Rules APPA questions the estimates of preventing an additional 24 to 28 fatalities annually and 2,175 injuries per year in that it fails to account for the fact that the industry has already implemented in large part safety measures which are incorporated in the Rules EEI and APPA also point out that many preventable injuries cannot be eliminated despite work rules enforcement and safety awareness campaigns since many such accidents which result in fatalities are due to employee being trained but not following the employer's training and policies PRESENT STATUS OF RULES According to information received from the OSHA office in February 1993 the final Rules are to be published no later than July 1993 and possibly as soon as March 1993 OSHA closed their receipt of comments in March 1991 and no further changes in the rules are thought possible A2-20 


CONCLUSION The OSHA 1910.269 which proposes to cover electric utility operation and maintenance work rules affects a multitude of working procedures as are summarized in this paper It is not possible at the present time to assess the final structure of the Rules as may be proposed in 1993 or subsequent years Since the comments from the utility associations APPA and EEI were made following the initial release of the proposed OSHA Rules in 1989 a significant amount of time has elapsed where other events have occurred which may affect the form of the final Rules The 1993 NESC went into effect in August 1992 and includes some of the requirements to which the commenters objected For example a significant requirement in the Part 4 of the 1993 NESC requires that rubber gloves be utilized on exposed energized parts of facilities operating at 50 to 300 volts This requirement is in conflict with EEl\222s proposed change to the OSHA Rules which would still allow working such secondary facilities without the use of rubber gloves Electric utilities are advised to review the January 31 1989 proposed operation and maintenance Rules as summarized in this paper and to review their procedures which would be affected by application of the Rules Many of the procedures proposed in the Rules provide valuable guidance in electric utilities\222 operation and maintenance activities Where the cost impact is not significant, it is recommended that utilities consider implementing such procedures in expectation of the Rules being published in the next few months Also it would be appropriate for electric utilities to review the 1993 edition of the NESC since there are portions of the Rules which have resulted in changes in the NESC These changes mainly occur in Part 4 Rules for the Operation of Electric Supply and Communications Lines and Equipment The concerns which the commenters have addressed regarding the cost impact and the resulting benefits experienced as a result of the promulgation of the Rules are real ones and must be addressed in the final Rules As a result this paper cannot present a conclusion regarding the full impact of the Rules The development of such Rules continue to be an ongoing matter and will undoubtedly require later analysis when the final rules are published A2-21 


