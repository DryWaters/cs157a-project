Towards Real-Time Analytics in the Cloud Amr Osman Computer Science Department German University in Cairo New Cairo Egypt amr.salaheldin@guc.edu.eg Mohamed El-Refaey Middle East Mobile Innovation Center Intel Labs Cairo Egypt mohamed.elrefaey@intel.com Ayman Elnaggar Computer Science Department German University in Cairo New Cairo Egypt ayman.elnaggar@guc.edu.eg Abstract The data explosion and the tremendous growth in the volume of data generated from various IT services places an enormous demand on harnessing and smartly analyzing the generated data and enterprise contents According to recent studies it is predicted that the volume of such data will become 26 fold in the next 002ve years While there might be some existing technologies to support this industry is frantically exploring new models that lead to more ef\002cient and higher performance solutions With the aid of cloud computing and high performance analytics such as scalable-parallel machine learning big data could be the fuel to a smarter cloud-powered IT world Through our work we provide a state-of-the-art review of high-performance advanced cloud analytics in the literature in attempt to 002nd the ideal real-time platform for distributed analytic computations Index Terms Cloud Computing Real-time cloud analytics Stream Computing Map-reduce Distributed Machine learning Hadoop Big data I I NTRODUCTION Cloud computing has evolved into one of the most commonly used and wide-spread technologies nowadays The recent advancements in utility computing and resource virtualization has enabled numerous businesses to exploit the power of large-scale super computing without the need to invest into expensive operational infrastructure costs CapEx to OpEx Having commodotized distributed computing various use-cases for in-cloud operations are constantly emerging on a daily basis such as backup hosting processing and storage of\003oading medical analyt as well as v arious lar ge-scale computations Even though the cloud is often seen as an option rather than a necessity the rise of big data will certainly force the transition into cloud and distributed computing According to current numbers it is predicted that the volume of digital data will double in size every two years to reach 8 ZettaBytes by Such e xplosion in user generated and systemgenerated data demands effective analysis representation and categorization in order to extract valuable and relevant information that could potentially improve the vast majority of IT organizations While NIST classi\002es cloud computing from a serviceoriented point of vie Dif ferent clouds are not just utilitybased as in the SaaS PaaS and IaaS service models where pooled resources can be ef\002ciently provisioned elastically scaled and provided to the user on-demand Data is just the other inseparable other side of the coin By being datacentric instead of service-and-user-oriented distributed highperformance computation on large amounts of data is one of the key features that characterize such clouds The main objective is to aggregate massive amounts of data divide it into smaller chunks and then distribute processing and storage tasks across nodes within some given cluster\(s in the most ef\002cient reliable scalable and fault-tolerant way Therefore data clouds are the ideal platform for processing and analyzing big data Figure 1 Utility Clouds vs Data Even though various approaches have been suggested and introduced to address the problem of analyzing big data in the cloud achieving high performance better parallelism and realtime ef\002ciency is still a stumbling block due to the composite and unstructured nature of big data as well as the complexity of analytic algorithms Therefore throughout this paper we attempt to review and explore various proposed solutions to implement ef\002cient high performance parallel analytics inside data clouds We 002rst study the nature of big data and different types of analytics and state the main challenges Then we highlight some state-of-the-art various technologies to ef\002ciently store and perform distributed computation in search for the ideal platform We further expand to cover various implementations of cloud analytics and approaches to parallelize existing analytic algorithms Finally we summarize by providing a comparative review of our 002ndings and lessonslearned to consider during future research 
2013 IEEE Ninth World Congress on Services 978-0-7695-5024-4/13 $26.00 © 2013 IEEE DOI 10.1109/SERVICES.2013.36 428 
2013 IEEE Ninth World Congress on Services 978-0-7695-5024-4/13 $26.00 © 2013 IEEE DOI 10.1109/SERVICES.2013.36 428 
2013 IEEE Ninth World Congress on Services 978-0-7695-5024-4/13 $26.00 © 2013 IEEE DOI 10.1109/SERVICES.2013.36 428 


II O VERVIEW Before we discuss the various cloud analytics approaches we 002rst shed lights on the nature of big data the factors affecting it and why it is dif\002cult to process We also need to de\002ne and classify cloud analytics beside the 002tness of various classes of analytics  machine learning for various scenarios A Big Data In contrast to popular allegations big data isn't just about the data volume Unlike traditional structured data it is usually unstructured and varies in types sizes and ways of classi\002cation and storage A recent case in v estig ated the nature of big data in different real-world IT businesses and showed that big data depends on 3 different aspects velocity volume and variety These three dimensions are co-related and directly in\003uence the methodologies used to store and process the data Consequently affecting the different analytics techniques that could be used as well as the host data cloud technical speci\002cations such the distributed storage system the distributed database and the cluster management system Figure 2 The three Vs of Big The volume dimension does not only de\002ne the size of the data In fact it directly depends how the data and the associated meta data is stored Data could be stored as big 002les records of multiple 002le blocks or as tables As the volume grows the processing and IO load on the cluster increases which dramatically impacts the performance The variety factor also indicates the structure of the data since it could be a mixture of structured and unstructured data Therefore it could be highly multi-dimensional and sparsely distributed The more structured the data is the easier it becomes to perform analytics and infer relations between the various datasets which are fed into the analytic engine since pre-processing via categorization labeling and clustering could be skipped all together or reduced Moreover the velocity factor is two-pronged The rate at which data is generated or input to the system contributes to how often it needs to be processed in order to update the previous results Time-sensitive applications that require real-time response  processing are therefore hard to support as the three factors of big data increase B Cloud Analytics and machine learning Performance also depends on the time-space complexity and degree of parallelism of the undertaken analytic algorithms Nonetheless analytics differ according to the goals and expected outcome with respect to how advanced and detailed the resultant models should be It's a trade-off between 002negrained and coarse-grained tuning As stated in the previous section the nature of data and problem plays an important role in using/discovering the appropriate algorithm\(s Traditional analytics such as Business Intelligence Operation Research and Data Mining are no longer suf\002cient  to harv est v alue from big data and automate decision making or provide a feedback loop into systems for future improvement and self-tuning Data scientists and decision makers often have to analyze the data either visually or numerically in order to get an insight Advanced analytics however doesn't stop at this point The overall goal is to exploit and infer complex relationships between data and quantitatively measure the amounts and quality of data This usually involves machine learning and statistical analysis to make solid mathematical models and abstractions that could be used to predict future behavior or even optimize for multiple goals algorithmically Advanced analytics is brie\003y the discipline of scaling and parallelizing machine learning algorithms on big data inside the cloud in order to detect patterns classify data and infer statistical relationships for effective data mining use Learning algorithms could be divided into three main categories 017 Supervised Learning  Input training data is usually labeled and accompanied with an expected output per item Usually one or more scoring functions are used to evaluate the inferred hypothesis functions in order to obtain the 002ttest hypothesis function from the hypothesis space Examples Bayesian networks Decision trees K nearest neighbor 017 Unsupervised Learning  Unlike supervised learning the input is unlabeled and no sample expected outputs exist to evaluate the learned functions Examples k-means clustering Neural networks 017 Reinforcement Learning  Is the problem of teaching a decision agent on how to perform actions in a speci\002c environment in order to meet a set of end goals according to a reward function No input or output data is provided The agent gains knowledge by performing actions using one or more transition functions and observing the environment's response through a series of episodes Examples Genetic algorithms Simulation-based learning While machine learning is the main scienti\002c foundation of cloud analytics dif ferentiates between three main categories of advanced analytics Descriptive analytics Predictive analytics and Optimization analytics 
429 
429 
429 


Table I D ESCRIPTIVE  PREDICTIVE AND OPTIMIZATION ANALYTICS Descriptive Predictive Optimization Usually associated with data mining and segmentation Employs classi\002cation and categorization of data leading to new associations and probability analysis Yields information about past data patterns What happened how many how often and where Application of complex math machine learning and statistics to detect patterns and anomalies The outcome of descriptive analytics could be combined in order to understand causes and relations which could be used to statistically predict the future Provides information on what will happen what could happen and what actions are needed Usually concerned with optimized and ef\002cient decision making in terms of the learned patterns and predicted probabilities Describe the best possible outcome given a set of outcomes Resulting from descriptive and predictive analytics using probabilistic and stochastic methods e.g Monte Carlo Beyesian trees Likewise another di vides analytic  machine learning models into 6 main cases 017 Statistical Apply probability analysis and analyze information entropy 017 Association How could an item be related to others in the data set 017 Clustering Group several items by likelihood and similarity 017 Binary Predictions True and false decisions 017 Number-in-range predictions Use of regression to determine outliers and predict variables 017 Selections Planning and optimizing for best possible decision or solution after exploring other possibilities The transformation into a complete analytic system consisting of highly unstructured big data could therefore be visualized as applying supervised and unsupervised learning techniques throughout a pipeline consisting of descriptive predictive and optimization analytics consecutively The machine learning algorithms used are dependent on the analytic models as mentioned above With that into consideration the key challenge is to provide parallel and scalable implementations of supervised and unsupervised learning that would run ef\002ciently in a distributed environment III L ITERATURE REVIEW Through this section we research and compare current approaches towards big data parallel processing and distributed computation inside the cloud using Map-reduce We explore the current distributed implementations for machine learning and data mining and review the outcomes of each experiment with respect to key factors such as scalability fault-tolerance and performance We hope to exploit real-time computation and online implementations of analytic algorithms that are suitable for time-critical applications A Map Reduce  is a programming model that w as recently developed by Google in order to facilitate parallel programing and distributed execution on large clusters It is based on a no single point of failure architecture that is guaranteed by the underlying distributed 002le system which di vides the data into smaller chunks stores it and safely replicates it across all nodes The idea behind MR Map Reduce is to provide abstraction from the underlying hardware and eliminate the complexities of typical parallel programming models such as MPI Message Passing Interface This is achieved by introducing two key functions for processing the data A map function and a reduce function which execute back to back in the pipeline We mathematically formalize these two functions as follows the map function m is represented as h k v i m 7\000 S n i 1 fh k 0 i  v 0 i ig whereas the reduce function r which further processes the map function's output is represented as D k 0 i  000 V 00 E r 7\000 000 V 000 constrained by 8 h k 0 i v 0 i i 2 m  h k;v i  9 h k 0 i v 0 j i 002 f v 0 i  v 0 j g n V 00  036 003 1 8 h k 0 i v 0 i i  h k 0 k v 0 k i 2 m  h k;v i  k 0 i 6  k 0 k  v 0 k  2 V 00 2 j V 000 j 024 j V 00 j 3 where k v indicate an arbitrary input key along with its corresponding value from the input data respectively and V 00  V 000 are both lists of values in the form f v 1  v 2  v 3  v n g  The reduce function is preceded by a grouping function which groups the intermediate output values from m  h k v i  by key This can be shown in constraints 1 and 2 The reduce function then performs some arbitrary computation and reduces the input set of values V 00 into a smaller set V 000 as shown by constraint 3 The 003ow of execution is simple All nodes fork a local copy of the user program A master node then assigns some map tasks to a group of worker nodes and some reduce tasks to another group of workers The map function executes concurrently across all mappers and the output is written locally Only then the intermediate outputs are aggregated grouped according to the key value and then the reduce functions start executing in parallel across reducer nodes which 002nally reduce the output to a smaller set of values that are written to the global distributed 002le system It follows that MR suffers a major performance bottleneck which is that the reduce phase does not begin until the map phase is complete Moreover it is only suited for batch processing and favors reliability scalability and throughput over latency and ex 12 Consider the scenario when reduce nodes perform remote reads over the network before execution Another scenario in GFS Google File System and 
430 
430 
430 


HDFS Hadoop Distributed File System when duplication is enforced by the primary replica before providing a response Therefore real-time processing is not viable as execution stops when all reducers 002nish execution and write their output to GFS In addition the input data is immutable and stored as chunks which are passed as input to the user program at run-time Meaning of which performing computation on live dynamic data that is frequently changing is not supported This is also a limitation of the underlying 002le system besides the assumption of rare random read/write 13 B Optimizations for MR Various optimizations have been proposed in order to improve the open-source implementation of Google MR and tune it for performance and continuous processing suited for the needs of big data analytics and data-centric computation i.e mining and machine learning We classify these optimization techniques as follows 017 Job Scheduling  MR tasks distribution optimizations 017 Networking  I/O optimizations 017 Continuous cascaded MR work-\003ows 017 Optimized data-queries-oriented approach 017 Real-time optimization  suggested optimizing the o v erall response time by using a SJF-like Shortest Job First scheduling strategy by pre-calculating each task's time cost and executing the smaller tasks for each job 002rst Another follo ws a similar path by introducing a scheduler which allows dynamic resource provisioning according to user-de\002ned performance goals by setting QoS priorities to different jobs which was done by estimating the number of time slots that could be allocated in-parallel to each job Such method could optimize performance dramatically when running multiple jobs at the same time or enforcing time-constraints on different neartime analytics Similarly an algorithm for optimizing hadoop con\002guration and hideous parameters that affect different resources CPU Disk Network per MR application has been  It follo ws a signature based approach where the currently executing application's pre-computed signature is compared to a set of already-computed signatures of resourceutilization statistics for which the optimum hadoop parameters were estimated  18 smartly tackles the sam e collecti v e problems of the previous approaches and adds automated online real-time optimization capability without the need for any userinteraction It provides job-level tuning by pro\002ling the behavior of various tasks per job as well as the con\002guration parameters with respect to resource utilization using a technique similar to as discussed abo v e It also introduces w ork\003o waware scheduling where the relations between multiple jobs modeled as a DAG Directed Acyclic Graph which operate on the 002le system's data are examined to ensure maximum data-locality by moving the computation to data and reduce data-transfer overhead The computation time of various jobs with respect to the job-level con\002gurations is also speculated using a search tree to ensure best job-scheduling scenarios Finally workload-level tuning is also provided to ensure better provisioning utilization and elasticity of the cluster taking into consideration the job-level and work\003ow-level optimizations undertaken Word-count and Tera-sort yielded 1.92x and 1.47x speedup respectively versus the recommended con\002guration parameters Similar 20 follo w the same footsteps and attempt to solve the same problems of elastic provisioning and resource-aware scheduling of MR jobs and sub-jobs targeting the 002ndings of 22 which thoroughly pinpointed key factors that affect hadoop's performance However two solutions stand out by targeting different problems Facebook proposed various optimizations to the hadoop 002le system HDFS for real-time low-latency by implementing RPC Remote Procedure Call timeouts instead of waiting for longer TCP socket timeouts supporting explicit mutation-lease revocation to speedup handing locks to writers supporting reads from local replicas supporting concurrent readers and writers and eliminating the need to wait acknowledgements when performing 003ush operations Microsoft takes a different  and introduces the concept of 002lters to reduce the amount of data transfered from storage nodes to computation nodes when initiating a MR program This is done through a pre-processing stage where all the execution 003ows that lead to an intermediate output from the map function are examined via byte-code static analysis Rows or lines in the input data that do not contribute to an output are eliminated Similarly columns or data attributes per row that are not needed by the mapper are also eliminated By generating and injecting the 002lters into the original MR program no special modi\002cations are needed for hadoop Such process yielded results as high as 62 25 reduction in run-time and data-transfer respectively for some benchmark jobs Hiv and Pig[25 tak e a dif ferent route to f acilitate data mining and aggregation By exploiting the fact that data is usually aggregated and structured using a query-like manner these two projects implement a SQL-like query language to structure retrieve store and aggregate data While hive supports storing the data in the form of distributed tables on the 002le system both projects can be used in conjunction with distributed NoSQL massive databases such as The open source counter part of Google's BigT and  Hi v e supports a subset of the SQL language which can be safely parallelized and applies basic SQL optimizations such as column pruning and predicate push-down to 002lter data as early as possible before the data-size explodes up the predicate trees It also adds special optimizations to improve data-locality and reduce data-transfer overhead such as pruning unnecessary 002les from partitions on the 002le system and buffering small tables in the distributed main memory of worker nodes for faster access Pig provides a special language to easily describe the 003ow of data and how it's joined during processing Both hive and pig transform the queries into a sequential dependency-satis\002ed pipeline of ef\002cient MR 
431 
431 
431 


jobs that are scheduled for execution on the cluster Such approach while focused on query optimization could increase performance compared to native MR applications that are poorly written without considering the nature of the data Moreover being data-oriented it is more intuitive to describe the data 003ow and query the data on a high-level instead of traditionally writing MR programs We argue that the above solutions are still not suitable for real-time cloud computations and suffer multiple draw backs All the above workarounds use pre-processing for estimating resources consumption completion-time and in case of Microsoft's Rhea static code analysis Typically the jobs are run on a small subset of the input data in order to speculate such measures Even though pre-processing takes much less time than actual jobs time-sensitive applications cannot afford such cost In the same context processing on a continuous 003ow of live and dynamic data was still not addressed by any of the above solutions Even though data-locality and resourceawareness were thoroughly addressed only batch computations can be performed by design Moreover continuous iterative processing of map-reduce tasks is vital for machine learning algorithms such as k-means where the output of the current iteration depends on the previous iteration during recalculating the centroids Another example would be Genetic Algorithms GA where the ne xt population depends on individuals of the previous generation which are used for mutation and crossover C Continuous MR work-\003ow Addressing the limitations of the previous optimizations T and HaLoop[32 e xtend MR by adding iterati v e support for MR tasks within each job Map and reduce tasks within a job are executed repetitively until a speci\002ed stopping condition is met All intermediate data per MR task per iteration are cached in the distributed main memory of the worker nodes or the local disks for faster low-latency access during next iterations In addition tasks that operate on some partitions are scheduled on the respective nodes that contain such partitions in order to minimize data transfer and fully exploit data locality HaLoop used k-means clustering in order to compute the page rank of two large data sets 46GB and 54GB and reduced the execution time from 42s to 7s compared to Hadoop It was also observed that only 4 of the intermediate data was shuf\003ed during MR Twister implemented two algorithms for gene processing Pairwise distance calculation and multi-dimensional scaling Results proved that twister maintained 70 and 80 parallelef\002ciency respectively for each algorithm While the previous implementations could be set for continuous data processing they are still impacted by some of the previous limitations discussed in the previous section Operating on dynamic and frequently changing data is still not possible In addition computation is not triggered by new data Similarly the response time is still high ranged from several seconds to minutes and the model is focused on batch processing instead of a real-time data-driven approach Nonetheless there is still a pipeline stall between map tasks and reduce tasks One 34 attempts to solv e thi s problem by eliminating the stall between map and reduce tasks and providing online continuous aggregation at the expense of data accuracy This is done by pipelining tasks within each job as well as inter-jobs and returning non-\002nal outputs while in the process of computation therefore minimizing the response time dramatically Reducers can start computation and consuming the data as soon as while the mappers start producing outputs sensing for new data via polling and also commutatively aggregate the result and yield it directly in stead of waiting for the 002nal result Using different approximation techniques the non-\002nalized outputs could be good-enough compared to the long-awaited 002nal output A WordCount experiment yielded 25 shorter completion time However one major drawback impacted the response time which is the fact that mappers and reducers compete for resources since they now run together and not in two phases as with the traditional MR approach Nov attempts to support continuous incremental and non-incremental processing using Pig where new data could be processed independent of the previous data or with respect to some kept state of the old data beside the traditional batch processing approach where computation is done on all the data from scratch Some optimizations to improve data locality inspired by the previous approaches are also supported Jobs that operate on same data are pipelined and co-scheduled to minimize data-reading overhead Query optimization techniques supported by pig are also adapted since Nova is built on top of Pig Moreover garbage collection and data compression minimizes the merge step's overhead as well as the data size stored/transfered Nova also introduced various computation triggers to address one of the limitations of the other MR optimization techniques Namely data-based triggers could toggle computation on arrival of new data time-based triggers to set the computation frequency and 002nally cascade triggers that could specify inter-job dependencies or trigger one or more jobs once a different job reaches some computation state It was admitted that Nova is still limited by hadoop and Pig that are only suited for batch computations as we argued earlier It only tries provide lazy triggered processing targeting maximum throughput at the cost of latency  e xploits the concept of distrib uted shared memory computation and eliminates the signi\002cant HDD I/O latencies by introducing in-memory distributed partitions RDD Resilient Distributed Datasets Such partitions are k ept persistently in the memory of the cluster nodes and are recoverable through lineage information which is simply the sequence of steps taken to construct an RDD from the input data It targets iterative applications where the intermediate data is reused frequently across multiple computations In case the memory capacity does not 002t RDDs are 003ushed 
432 
432 
432 


to the HDDs according to their user-de\002ned persistencepriorities The developer also controls how to create and what to store in RDDs during implementation beside the main MR job Map and Reduce tasks are further scheduled nearer to where the corresponding RDDs are for maximum datalocality Two famous machine learning algorithms were tested Logistic regression and K-means clustering Both were tested for 10 iterations on a 100GB-large dataset on 100 machines Spark was proven 25.3x and 3.2x faster than hadoop for both algorithms respectively Chukw tak es a dif ferent turn and pro vides a realtime log aggregation and monitoring system by exploiting the fact that mappers can be seen as data generators log agents and reducers are data collectors The key objective was to disseminate various logs at a sustainable high rate with minimal impact on resource consumption while providing fault-tolerance and scalability Two options for data delivery were provided A reliable one which writes and replicates the data to the global 002le system and a faster non-reliable method where data is streamed through the network and stored in local hard disks to provide the shortest response time possible An experiment where the MR ratio was 200:1 showed that chukwa achieved the maximum possible throughput that was limited by the underlying 002le system and HDD rates  30 MB/s A 1:1 ratio is maintained between the data rate per agent and the data collector until the limit is reached showing that the data is delivered with minimum latency at an extreme rate Furthermore another experiment showed that the overall rate scales linearly up to 200MB/s limited by the number of nodes used as the number of collectors increases With such high-rate stream processing on the delivered data is essential to minimize latency and couple data aggregation with actual processing and live analytics D Distributed real-time stream computing To 002ll this gap many solutions have evolved to leverage real-time computation on big data streams Yahoo  and T witter Storm[41 are 3 similar solutions that tackle the streaming big data problem While there are some minor technical and naming differences their approaches can be narrowed down to one common approach Data is fed into the system as an in\002nite sequence of tuples which are transfered to processing nodes The processing nodes can then consume the data tuples and publish some results or produce more data streams to be processed and/or aggregated by other nodes In addition intermediate streams data tuples are usually buffered in the distributed memory of the worker nodes for minimum latency during data access S4 relies on event delegation to relay computation-triggering events between processing nodes When nodes receive new events they start consuming the incoming stream and emitting one or more intermediate data streams in a MR fashion  also implements a similar functionality on top of Pig and Hadoop An experiment to calculate the clickthrough rate on ads coupled with live search traf\002c was run for a period of two weeks where 250,000 users make one million searches per day The system could sustain 1600 events per second in such case The peak rate observed reached 26.7Mbps Similarly Google pro vides real-time querying on structured data However it assumes that the data is structured  stored on the underlying 002le system and does not process streams as they are entering the cloud Twitter storm on the other hand could process a million tuples of data per second per node as benchmarked on the live twitter social network traf\002c Aurora tops other approaches by adding query optimization techniques and supporting real-time continuous queries on the data QoS Quality Of Service is also speci\002ed for each application to ensure optimum scheduling while maintaining dynamic load balancing according to the incoming data stream and the resource-utilization statistics Discretized  45 is an e xtension to bring stream-processing to Spark which was analyzed in the section above The inmemory storage/computation combo with the aid of RDD could transform this approach into the ideal stream processing framework Streaming computations are performed deterministically through discrete time intervals to maintain constant sub-second latencies and mitigate stateful dependencies on the tuples order of arrival across different streams The streams are typically tuples consisting of RDD objects to be processed via one or more MR tasks D-Streams further optimized Spark for streaming by allowing Asynchronous network I/O and pipelining tasks within each discrete time window which consume or depend on the same in-memory RDDs It is argued that even though 002xed computation intervals introduce latency such technique provides faster deterministic fault tolerance and RDD recovery which is assumed to be frequent compared to record-at-a-time systems A thorough benchmark was run to compare Spark-Streaming to S4 and Twitter Storm S4 was observed to be 10x slower versus Spark and Storm which achieved similar response times However for smaller record sizes Spark beat Storm by a factor of 2x on a cluster of 30 nodes and achieved higher throughput due to the batched computation over 002xed intervals property To improve Twitter-Storm's throughput and respond to the surprising 002ndings of Spark streaming's last benchmark  introduces timely batches si milar to Spark streaming and DAG plan optimization for Storm It optimizes the batch-sizes and degree of parallelism for the nodes in order to maximize throughput and balance between computation time per batch and output rate which is mathematically related to the response time However such optimization is not adaptive such that dynamic optimization is suggested for future work Additionally no benchmark results are available yet E Distributed machine learning Having reviewed the MR computation frameworks which have matured towards real-time data-driven processing scalable and parallel machine learning algorithms to perform online analytics are the next challenge The Apache mahout 
433 
433 
433 


 pro vides a set of machine learning algorithms that were parallelized and tuned for scalability and MR execution However all algorithms as well as Mahout itself were designed under the old assumption of batch processing on traditional hadoop clusters with no support for iterative processing or online stream processing techniques as we deeply discussed through our work Nevertheless the provided parallelization techniques could be tweaked and tuned for online incremental processing  f acilitates continuous analytics on distrib uted inmemory arrays via the R statistical programming language Continuity is maintained through events and data triggers On average of various analytic algorithms presto was 37x faster than hadoop Another approach used Spark Streaming to implement an online Expectation Maximization algorithm to infer travel times in GPS F ollo wing the same context other online machine learning algorithms were developed such as 51 52 53 54 55 An intensi v e  also deeply discussed the theory behind online machine learning and its implications Other  59 60 61 aim to optimize for unbound big data and scalability during MR parallel execution where the iterative algorithm is decomposed into a series of MR jobs that are scheduled on the cluster It follows that online implementations of machine learning on big data are more complicated and harder to parallelize The literature lacks such implementations which are crucial for advanced online analytics IV C ONCLUSION AND F UTURE WORK Stream computing and low-latency map reduce clusters are key enablers to perform distributed real-time analytics on live and dynamic big data streams The transition from batch processing to stream processing inside the data cloud 002ts various time-sensitive applications and real-world use cases such as real-time search High Frequenc y T rade markets HFT Intelligent T raf 002c Systems ITS  65 Medical visual and social netw orks[66 67 The recent advancements and optimizations for map reduce clusters have dramatically reshaped the options and paved the way for distributed machine learning and advanced analytics We have walked through the time-line of various optimization techniques undertaken through the literature and provided a comparative review with respect to response-time throughput fault-tolerance scalability and performance While different optimizations have dramatically bridged the gap between the old traditional map reduce batch-computation approach and real-time stream computations on big data each approach has its own advantages and disadvantages Connecting the dots between the limitations of each approach could potentially yield to a better more-mature platform for real-time analytics We have observed that most of the benchmarks are not comprehensive and fair enough to compare different approaches Therefore in a future project we plan to benchmark and audit the performance of some of the techniques discussed through a standardized benchmark and a fair data set Similarly parallel machine learning algorithms on online streaming data are rarely present Consequently we plan to investigate some analytic algorithms and tune them for the future streaming live data Our next steps might include abstracting the computational 003ow of data streams using declarative programming techniques such as Constraint Handling Rules CHR R EFERENCES  F  W ang R Lee Q Liu A Aji X Zhang and J Saltz Hadoop-gis A high performance query system for analytical medical imaging with mapreduce tech rep Technical report Emory University 2011  Intel Bigdata101 Unstructured data analytics  Online A v ailable http://www.intel.com/content/dam/www/public/us/en/documents solution-briefs/big-data-101-brief.pdf June 2012  M Hog an F  Liu A Sok ol and J T ong Nist cloud computing standards roadmap NIST Special Publication  p 35 2011  F  Michael C Mik e E Christopher  and S Josh Massi v e data analytics and the cloud A revolution in intelligence analysis A v ailable http://www.boozallen.com/media/\002le/MassiveData.pdf 2011  P  Russom big data analyt ics  TDWI Best Practices Report 4 th Quarter 2011  2011  S Philpott and T  B CoE  Adv anced analytics  2010  N R Get analytics right from the start  Online A v ailable http www.dnm.ie/documents/whitepapers/Analyticsfromthestart.pdf February 2010  J T  Predicti v e analytics in the cloud  Online Available http://smartdatacollective.com/sites/default/docs SDC-3twn0H3IkKM5peAzhvQ5/Predictive%20Analytics%20Cloud 20Position%20Paper.pdf 2011  J Dean and S Ghema w at Mapreduce simpli\002ed data processing on large clusters Communications of the ACM  vol 51 no 1 pp 107113 2008  S Ghema w at H Gobiof f and S Leung The google 002le system  in ACM SIGOPS Operating Systems Review  vol 37 pp 2943 ACM 2003  D Borthakur  J Gray  J Sarma K Muthukkaruppan N Spie gelber g H Kuang K Ranganathan D Molkov A Menon S Rash et al  Apache hadoop goes realtime at facebook in Proceedings of the 2011 international conference on Management of data  pp 10711080 ACM 2011  A Middleton Hpcc systems Introduction to hpcc high-performance computing cluster 2011  D Borthakur  The hadoop distrib uted 002le system Architecture and design Hadoop Project Website  vol 11 p 21 2007  G ZHANG C LI Y  ZHANG C XING and J Y ANG Mapreduce Ef\002cient processing of mapreduce jobs in the cloud Journal of Computational Information Systems  vol 8 no 14 pp 57575764 2012  J Polo D Carrera Y  Becerra J  T orres E A yguad M Steinder  and I Whalley Performance management of mapreduce applications 2009  K Kambatla A P athak and H Pucha T o w ards optimizing hadoop provisioning in the cloud in Proc of the First Workshop on Hot Topics in Cloud Computing  2009  H Herodotou F  Dong and S Bab u Mapreduce programm ing and cost-based optimization crossing this chasm with star\002sh Proceedings of the VLDB Endowment  vol 4 no 12 2011  H Herodotou H Lim G Luo N Boriso v  L Dong F  Cetin and S Babu Star\002sh A self-tuning system for big data analytics in Proc of the Fifth CIDR Conf  2011  J Polo C Castillo D Carrera Y  Becerra I Whalle y  M Steinder  J Torres and E Ayguad Resource-aware adaptive scheduling for mapreduce clusters Middleware 2011  pp 187207 2011  T  Sandholm and K Lai Mapreduce optimization using re gulated dynamic prioritization in Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems  pp 299310 ACM 2009  L Phan Z Zhang Q Zheng B Loo and I Lee  An empirical analysis of scheduling techniques for real-time cloud-based data processing in IEEE International Conference on Service-Oriented Computing and Applications SOCA 2011  pp 18 IEEE 2011 
434 
434 
434 


 D Jiang B Ooi L Shi and S W u The performance of mapreduce An in-depth study Proceedings of the VLDB Endowment  vol 3 no 12 pp 472483 2010  C Gkantsidis D Vytiniotis O Hodson D Narayanan and A Ro wstron Automatic io 002ltering for optimizing cloud analytics microsoft technical report msr-tr-2012-3  A Thusoo J Sarma N Jain Z Shao P  Chakka N Zhang S Anton y  H Liu and R Murthy Hive-a petabyte scale data warehouse using hadoop in IEEE 26th International Conference on Data Engineering ICDE 2010  pp 9961005 IEEE 2010  C Olston B Reed U Sri v asta v a R K umar  and A T omkins Pig latin a not-so-foreign language for data processing in Proceedings of the 2008 ACM SIGMOD international conference on Management of data  pp 10991110 ACM 2008  A Khetrapal and V  Ganesh Hbase and h ypertable for lar ge scale distributed storage systems Dept of Computer Science Purdue University  2006  F  Chang J Dean S Ghema w at W  Hsieh D W allach M Burro ws T Chandra A Fikes and R Gruber Bigtable A distributed storage system for structured data ACM Transactions on Computer Systems TOCS  vol 26 no 2 p 4 2008  A Lakshman and P  Malik Cassandra A structured storage system on a p2p network in Proceedings of the twenty-\002rst annual symposium on Parallelism in algorithms and architectures  pp 4747 ACM 2009  J A Hartig an and M A W ong  Algorithm as 136 A k-means clustering algorithm Applied statistics  pp 100108 1979  D E Goldber g Genetic algorithms in search optimization and machine learning 1989  J Ekanayak e H Li B Zhang T  Gunarathne S Bae J Qiu and G Fox Twister a runtime for iterative mapreduce in Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing  pp 810818 ACM 2010  Y  Bu B Ho we M Balazinska and M Ernst Haloop Ef 002cient iterative data processing on large clusters Proceedings of the VLDB Endowment  vol 3 no 1-2 pp 285296 2010  T  Condie N Conw ay  P  Alv aro J M Hellerstein J Gerth J T albot K Elmeleegy and R Sears Online aggregation and continuous query support in mapreduce in ACM SIGMOD  pp 11151118 2010  T  Condie N Conw ay  P  Alv aro J M Hellerstein K Elmelee gy  and R Sears Mapreduce online in Proceedings of the 7th USENIX conference on Networked systems design and implementation  pp 21 21 2010  C Olston G Chiou L Chitnis F  Liu Y  Han M Larsson A Neumann V B Rao V Sankarasubramanian S Seth et al  Nova continuous pig/hadoop work\003ows in Proceedings of the 2011 international conference on Management of data  pp 10811090 ACM 2011  M Zaharia M Cho wdhury  M Franklin S Shenk er  and I Stoica Spark cluster computing with working sets in Proceedings of the 2nd USENIX conference on Hot topics in cloud computing  pp 1010 USENIX Association 2010  M Zaharia M Cho wdhury  T  Das A Da v e J Ma M McCaule y  M Franklin S Shenker and I Stoica Resilient distributed datasets A fault-tolerant abstraction for in-memory cluster computing in Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation  2011  J Boulon A K onwinski R Qi A Rabkin E Y ang and M Y ang Chukwa a large-scale monitoring system in Proceedings of CCA  vol 8 2008  L Neume yer  B Robbins A Na ir  and A K esari S4 Distrib uted stream computing platform in IEEE International Conference on Data Mining Workshops ICDMW 2010  pp 170177 IEEE 2010  M Cherniack H Balakrishnan M Balazins ka D Carne y  U Cetintemel Y Xing and S Zdonik Scalable distributed stream processing in Proc Conf on Innovative Data Syst Res  2003  M Nathan X James and J Jason Storm Distrib uted real-time computation system A v ailable http://stormproject.net 2012  Hstreaming  Online A v ailable http://www hstreaming.com 2011  S Melnik A Gubare v  J J Long G Romer  S Shi v akumar  M T olton and T Vassilakis Dremel interactive analysis of web-scale datasets Proceedings of the VLDB Endowment  vol 3 no 1-2 pp 330339 2010  M Zaharia T  Das H Li S Shenk er  and I Stoica Discretized streams an ef\002cient and fault-tolerant model for stream processing on large clusters in Proceedings of the 4th USENIX conference on Hot Topics in Cloud Ccomputing  pp 1010 USENIX Association 2012  M Zaharia T  Das H Li T  Hunter  S Shenk er  and I Stoica Discretized streams A fault-tolerant model for scalable stream processing tech rep UC Berkeley Technical Report UCB/EECS-2012-259 2012  M Sax M Cas tellanos Q Chen and M Hsu  Aeolus An optimizer for distributed intra-node-parallel streaming systems in Demo Will be presented at Int Conf on Data Engineering  2013  C Chu S K Kim Y A Lin Y  Y u G Bradski A Y  Ng and K Olukotun Map-reduce for machine learning on multicore Advances in neural information processing systems  vol 19 p 281 2007  S V enkataraman I Ro y  R S Schreiber  and A AuY oung Presto Complex and continuous analytics with distributed arrays 2011  T  H T  D M Zaharia A Bayen and P  Abbeel Lar ge-scale online expectation maximization with spark streaming  K.-C Lee and D Krie gman Online learning of probabilistic appear ance manifolds for video-based recognition and tracking in IEEE Computer Society Conference on Computer Vision and Pattern Recognition CVPR 2005  vol 1 pp 852859 IEEE 2005  M D Hof fman D M Blei and F  Bach Online learning for latent dirichlet allocation Advances in Neural Information Processing Systems  vol 23 pp 856864 2010  R S Sutton S D Whitehead et al  Online learning with random representations in Proceedings of the Tenth International Conference on Machine Learning  pp 314321 Citeseer 1993  S Shale v-Shw artz Y  Singer  and A Y  Ng Online and batch learning of pseudo-metrics in Proceedings of the twenty-\002rst international conference on Machine learning  p 94 ACM 2004  K Crammer  O Dek el J K eshet S Shale v-Shw artz and Y  Singer  Online passive-aggressive algorithms The Journal of Machine Learning Research  vol 7 pp 551585 2006  J Ki vinen A J Smola and R C W illiamson Online learning with kernels Signal Processing IEEE Transactions on  vol 52 no 8 pp 21652176 2004  S Shale v-Shw artz Online learning Theory  algorithms and applications 2007  A Basak I Brinster  and O J Mengs hoel Mapreduce for bayesian network parameter learning using the em algorithm Proc of Big Learning Algorithms Systems and Tools  2012  E B Reed and O J Mengshoel Scaling bayesian netw ork parameter learning with expectation maximization using mapreduce Proc of Big Learning Algorithms Systems and Tools  2012  A G Schwing T  Hazan M Pollefe ys and R Urtasun Distrib uted structured prediction for big data  J Y e J.-H Cho w  J Chen and Z Zheng Stochastic gradient boosted distributed decision trees in Proceedings of the 18th ACM conference on Information and knowledge management  pp 20612064 ACM 2009  S Daruru N M Marin M W alk er  and J Ghosh Perv asi v e parallelism in data mining data\003ow solution to co-clustering large and sparse net\003ix data in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining  pp 11151124 ACM 2009  D Peng and F  Dabek Lar ge-scale incremental processing using distributed transactions and noti\002cations in Proceedings of the 9th USENIX Symposium on Operating Systems Design and Implementation  2010  S Arnuk and J Saluzzi Latenc y arbi trage The real po wer behind predatory high frequency trading Themis Trading white paper  2009  L D Baskar  B De Schutter  and H Hellendoorn Model-based predictive traf\002c control for intelligent vehicles Dynamic speed limits and dynamic lane allocation in Intelligent Vehicles Symposium 2008 IEEE  pp 174179 IEEE 2008  B L Smith and M J Demetsk y  Short-term traf 002c 003o w prediction models-a comparison of neural network and nonparametric regression approaches in IEEE International Conference on Systems Man and Cybernetics 1994.'Humans Information and Technology 1994  vol 2 pp 17061709 IEEE 1994  M Stonebrak er  U etintemel and S Zdonik The 8 requi rements of real-time stream processing ACM SIGMOD Record  vol 34 no 4 pp 4247 2005  B Aleman-Meza M Nag arajan L Ding A Sheth I B Arpinar  A Joshi and T Finin Scalable semantic analytics on social networks for addressing the problem of con\003ict of interest detection ACM Transactions on the Web TWEB  vol 2 no 1 p 7 2008 
435 
435 
435 


        


  


Bottom Top A B 
Figure 15 Figure 16 
messages seen for all workers in a superstep \(Figures 10 and 13\. When looking at the messages sent by workers in a superstep for METIS, we see that there are message load imbalances within work ers in a superstep, caused due to concentration of vertices being traversed in that superstep in certain partitions This variability is much more pronounced in CP as compared to WG \(Figures 11 and 14\ E.g. in superstep 9 for CP, twice as many messages \(4M\ are generated by a worker compared to another \(2M\.  For Pregel BSP, the time taken in a superstep is determined by the slowest worker in that superstep. Hence increase d variability in CP causes even good partitioning strategies to cause an increase in total execution time wh en using the Pregel/BSP model VIII A NALYSIS OF E LASTIC C LOUD S CALING  Cloud environments offer elasticity  the ability to scale-out or scale-in VMs on-demand and only pay for what one uses [28   On th e f l i p s i de  on e en ds u p  paying for VMs that are acquired even if they are underutilized. We have already shown the high variation in compute/memory resources used by algorithms like BC and APSP across different supersteps. While our earlier swath initiation heuristics attempt to flatten these out by overlapping swath executions, one can consider leveraging the clouds elasticity to, instead, scale up and down the concurrent workers \(and graph partitions\ allocated in each superstep The peak and trough nature of resource utilization combined with Pregel/BSPs synchronous barrier between supersteps offers a window for dynamic scaleout and in at superstep boundaries. Peak supersteps can greatly benefit from additional workers, while those same workers will contribute to added synchronization overhead for trough supersteps We offer an analysis of the potential benefits of elastic scaling by extrapolating from observed results for running BC on WG and CP graphs, using four and eight workers.  To provide a fair and focused comparison, we turned off swath heuristics in favor of fixed swath sizes and initiation intervals Figure 15 \(Bottom\ plots the speedup of BC running on eight workers when normalized to BC running on four workers, at corresponding supersteps.  The number of workers does not impact the number of supersteps We also plot the number of active vertices \(i.e. vertices still computing for a given swath\these supersteps which is a measure of how much work is required \(Fig 15 \(Top\. We find that we occasionally get superlinear speedup spikes \(i.e. >2x\ that shows a strong correlation with the peaks of active messages, for both WG and CP graphs. At other times, the sp eedup is sublinear or even a speed-down \(i.e. <1\responding to inactive vertices.  The superlinear speedup is attributable to the lower contention and reduced memory pressure for 8 workers when the active vertices peak \(similar to what we observed for the swath initiation heuristics Similarly, the below par speedup during periods of low activity is contributed by the increased overhead of barrier synchronization across 8 workers. Intuitively, by dynamically scaling up the number of workers for supersteps with peaking active vertices and scaling them down otherwise, we can leverage the superlinear speedup and get more value per worker Using a threshold of 50% active vertices as the threshold condition for between 4 and 8 workers in a superstep, we extrapolate the time per superstep and compared this to the fixed 4 and 8 worker runtimes. We also compute the best-case run time using an oracle approach to i.e. for each superstep, we pick the minimum of the 4 or 8 workers time.  Note that these projections do not yet consider the overheads of scaling, but are rather used to estimate the potential upside if we had an ideal or an automated heuristic for scaling. The total time estimates for running BC on WG and CP graphs, normalized to  
 plot shows speedup of 8 workers relative to 4 workers, for each superstep, when running BC on WG and CP graphs plot shows the number of vertices active in that superstep Estimated time for BC using elastic scaling, normalized to time taken for 4 workers. Normalized cost is shown on secondary Y axis WG graph shown on left CP graph shown on right. Smaller is better 
022\011 022\010 022\007 022\002 006 002 007 006 002 007 010 011 012 013 014 015 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 027\031\030\037\020#@\020"\031\030\027\020\035 0201!2#\024$#\015#5\024",\020"#\017\003"\003\031\003#\011#5\024",\020"\035 024"'\033\026\0309\0201#\\031\020 2 035#\032\020"#+!\034 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 017\020\021\022\023\024\024\025\026\020#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 027\030\031\022\032\033\031\020\034\031\035#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 036\030\034\020\033"#\\0201!2 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 017\020\021\022\023\024\024\025\026\020#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035\031 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 002\003\011 002\003\013 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 033\026\030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 027\030\031\022\032\033\031\020\034\031\035#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035 031 
 
dynamically scaling ideal scaling 
Our hypothesis is that an intelligent adaptive scaling of workers can achieve a similar performance as a large, fixed number of workers, but with reduced cost 
213 


Nature Nature Ecological Applications Nature ACM International Conference on Management of Data \(SIGMOD In Parallel Object-Oriented Scientic Computing \(POOSC Science Communications of the ACM ACM Workshop on Mining and Learning with Graphs Communications of the ACM HotCloud Proceedings of the 19th ACM International Symposium on High PErformance Distributed Computing HPDC Knowledge and Information Systems KAIS International Conference on Computational Science IEEE International Conference on Cloud Computing Technology and Science ACM/IEEE Conference on Advances in Social Network Analysis and Mining \(ASONAM IEEE International Parallel and Distributed Processing Symposium \(IPDPS International Conference on Distributed Computing and Networking Journal of Mathematical Sociology International Conference on Parallel Processing Communications of the ACM 
 
observed time taken using 4 workers, are plotted in Figures 16\(A\ and 16\(B We see that our dynamic scaling heuristic using the percentage of active vertices achieves nearly the same CP\ or better \(WG\ performance as a fixed 8 worker approach. Clearly there is benefit of using fewer workers for low utilization su persteps to eliminate the barrier synchronization overhead. Also, the dynamic scaling heuristic performs almost as well as the ideal scaling. Finally, when we consider the monetary cost of the proposed approaches, assuming a pro-rata normalized cost per VM-second plotted on the secondary Y axis, we see that dynamic scaling is comparable \(CP\ or cheaper \(WG\ than a 4 worker scenario while offering the performance of an 8 worker deployment IX C ONCLUSION  In conclusion, we introduce optimization and heuristics for controlling memory utilization and show they are critical to performance.  By breaking computation into swaths of vertices and using our sizing heuristics we achieve up to 3.5x speedup over the maximum swath size that does not cause the a failure.  In addition overlapping swath executions can provide a 24% gain with automated heuristics and even greater speedup when a priori knowledge of the network characteristics is applied This evaluation offers help to eScience users to make framework selection and cost-performancescalability trade-offs. Our he uristics are generalizable and can be leveraged by other BSP and distributed graph frameworks, and for graph applications beyond BC. Our work uncovered an unexpected impact of partitioning and it would be worthwhile, in future, to examine the ability to pred ict, given certain graph properties, a suitable partitioning model for Pregel/BSP It may also be useful to perform such evaluations on larger graphs and more numbers of VMs. At the same time, it is also worth considering if non-linear graph algorithms are tractable in pr actice for large graphs in a distributed environment B IBLIOGRAPHY  1  F  L i lj er os C   Ed l i n g L  A m a r a l H  S t an ley   and Y    berg The web of human sexual contacts 
vol. 411, pp. 907908, 2001   H Je o n g  S   Ma so n A  L   B a ra b s i  a nd Z   Oltva i  L e t ha l i t y  and centrality in protein networks vol. 411, pp. 41-42 2001   O. B o din and E   E s t r ada    U s i n g n e t w ork c e nt r a l i t y  m e a s ures t o  manage landscape connectivity vol 18, no. 7, pp. 1810-1825, October 2008   D. W a ts s  and S  S t r ogat z  C olle c t i v e  d y nam i cs of  s m a ll-w orl d   networks vol. 393, no. 6684, pp. 440442, June 1998   G  Ma lew i c z   M A u s t er n A   Bik  J   Dehn er t I  Hor n   N. L e i s er and G. Czajkowski, "Pregel: A system for large-scale graph processing," in 2010   D. G r egor  and A  L u m s dain e  T h e  pa r a llel  B G L  A gen e r i c  library for distributed graph computations," in 2005   B. S h a o  H. W a n g  and Y  L i T he T r init y G r aph E n g i n e    Microsoft Research, Technical Report MSR-TR-2012-30, 2012   A  F ox  C lo ud c o m putin g w h at  s  in it for m e  as  a  s c i e n tis t     vol. 331, pp. 406-407, 2011   S. G h e m a w a t  and J  De an   Map re duc e s i m p lifi e d data  processing on large clusters vol 51, no. 3, pp. 107-113, 2008   J  L i n and M. S c hat z   Des i g n  patt er n s  for eff i ci ent gr aph algorithms in MapReduce," in 2010   L   Va l i ant   A b r id g i n g m o d e l f or pa r a llel com putati o n  vol. 33, no. 8, pp. 103-111, 1990 12 a c h e  Ha ma    O n l i n e    http://hama.apache.org   13 Ap a c h e  Ha d o op    O n l i n e    http://hadoop.apache.org     M Z a h a r i a, M. Ch ow dhu ry M F r ank l in S  S h e n k e r, and I   Stoica, "Spark: Cluster Computing with Working Sets," in 2010   J  Ekana y ak e e t a l     T w i st er A  r untim e f o r it er ati v e  MapReduce," in Chicago, 2010, pp. 810-818   U. K a n g  C  T s o u rakakis   and C. F a l outs o s  Peg a s us   Minin g  Peta-scale Graphs," in 2010   M. P a c e  B S P vs  MapR e duc e    in vol. 103.2081, 2012   S. Seo  E  Yoo n, J  K i m  S  J i n  J-S. K i m   and S   Ma e n g HAMA: An Efficient matrix computation with the MapReduce framework," in 2010, pp. 721-726   S. S a l i h ogl u  and J  W i d o m  G PS A G r a ph P r oc e s s i n g Sy s t em    Stanford University, Technical Report 2011   R L i cht e n w a l t e r and N   Cha w la D is Ne t  A fr am ew ork for  distributed graph computation," in  2011   K  Maddu r i  D. E d i g er K   J i an g  D. Bad e r  and D  Cha v a r riaMiranda, "A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets," in 2009   E  K r e p s k a, T  K i el m a nn, W  F o kkink, H   Ba l, "A  hi g h level framework for distributed processing of large-scale graphs," in 2011, pp. 155-166   L   Pa ge  S  B r in R. M o t w ani and T  W i nogr ad  T h e P a geRank citation ranking: Bringing order to the web," Stanford InfoLab Technical Report 1999-66, 1999   U  Brand  s  A f a s t er  a l gor ith m for  b e t w eenn e s s c e nt r a l i t y    vol. 25, no. 2, pp. 163-177 2001   Stan fo r d  Net w or k A na l y s is Pro j e c t  O n l in e    http://snap.stanford.edu    I  S t ant o n and G  K l i o t, "S t r e a m i n g G r aph P a rtiti o n in g  for L a rge Distributed Graphs," Microsoft Corp., Technical Report MSRTR-2011-121, 2011   G   K a ry pis and V   K um a r A fas t and hi g h qua l i t y m u l t i l evel scheme for partitioning irregular graphs," in 1995, pp. 113-122   M. A r m b r u s t e t  a l   A v i ew of  c l o u d  c o m putin g    vol. 53, no. 0001-0782, pp. 50-58 April 2010  
214 


  13  or gani c  c he m i s t r y  i n our  Sol ar  Sy s t e m       Xi a n g  L i r e c e i v e d h i s B  S   m is tr y  fr o m  th e  P e k in g  U n iv e r s ity  C h in a  in  2 0 0 3  and P h D   i n P hy s i c al  C he m i s t r y  f r om  t he  J ohns  H opk i ns  Un i v e r s i t y  i n  2 0 0 9   He  h a s  b e e n  a  R e s e a r c h  A s s o c i a t e  wi t h  a  j o i n t  a p p o i n t m e n t  a t  t h e  U n i v e r s i t y  o f  M a r y l a n d   Ba l t i m o r e  C o u n t y  a n d  N AS A G o d d a r d  S p a c e  Fl i  Ce n t e r  s i n c e  2 0 1 1   H i s  r e s e a r c h  f o c u s e s  o n  t h e  d e t e c t i o n  of  t r ac e  e l e m e nt  and as t r obi ol ogi c al l y  r e l e v ant  or gani c  mo l e c u l e s  i n  p l a n e t a r y  s y s t e ms   l i k e  M a r s   He  i s  es p eci a l l y i n t er es t ed  i n  t h e d evel o p m en t  o f  T i m e of  and I on T r ap m as s  s pe c t r om e t e r s w i t h v a r i o u s i o n i z a t i o n  ng te c h n iq u e s   Wi l l  B r i n c k e r h o f f  sp a c e  sc i e n t i st  i n  t h e  Pl a n e t a r y  En v i r o n m e n t s  La b  a t  N A S A  s  G o d d a r d  Spac e  F l i ght  C e nt e r  i n Gr e e n b e l t   M D w i t h  pr i m ar y  r e s pons i bi l i t y  f or  th e  d e v e lo p m e n t o f th e  L D TO F  m a s s  s p e c t r o  th is  p r o je c t H e  h a s  fo c u s e d  re c e n t l y  o n  t h e  d e v e l o p m e n t  o f  m i n i a t u re  l a se r d ma s s  s p e c t r o me t e r s  f o r  f u t u r e  p l a n e t a r y  mi s s i o n s  a l o n g  wi t h  b a s i c  e x p e r i m e n t a l  r e s e a r c h  i n  a s t r o b i o l o g y  a n d  p r e bi ot i c  s y nt he s i s   D r   B r i nc k e r hof f  i s  i nv ol v e d i n t he  de v e l opm e nt  of  m as s  s pe c t r om e t e r  f or  bot h t he  2011 Ma r s  S c i e n c e  L a b o r a t o r y  a n d  t h e  2 0 1 8  E x o Ma r s  mi s s i o n s   


  14   


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a key, value list using an XSTL  Queries made against this list of key, value pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


