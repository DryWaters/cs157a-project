A Data Approach Alternative at System Identification and Modeling Using the Self-organizing Associative Memory SAM\System Wei Kang Tsai Wei-min Chiu and Hon-Mun Ixe Electrical and Computer Engineering University of Cul@omiu Imine wtsai  ece uci edu  Abstract We introduce a data-based approach alternative to the rule-based piiw-ameter approach toward system identification Motivated by the design-intensive problem of the parameter approach the Selj organizing Associative Mernory SAM system 
seeks to represent the system using a subset of stored training data We surmise that knowledge is association between memorized objects not memorized rules We posmlate that only novel and distinct data should be organized into memory while familiar data my be reproduced to an acceptable degree of accuracy by association beiween memorized data The concept is materialized in several computational jFormuts and tested on four different test cases Results indicate that this data approach has high accuracy relatively design-free and requires only one pass of the training 
data to train 1 Introduction System identification the discipline of reconstructing static and dynamic behaviors of unknown systems has increasingly been done using adaptive and soft computing methais such as modern control theory neural networks and adaptive fuzzy logic systems The design of these intelligent systems may be classified as the parameter izpproach which involves the definition of a set of parameters to represent the system identifier and the optimization of that set of parameters to produce a 223learned\224 systeim model The neural network and other basis-function methods 
have performed as robust and highly adaptive system models Indeed the popular Multi-Layer Perwptron h4LP has been shown by Kolmogorov to be an universal approximator  given the right network architecture SI However the neural network as a parameter approach, is also highly design-intensive In defining a neural network many design parameters come into consideration initialization activation function weighting function bias learning step size and the training method Perhaps the most tasking aspect of neural network design is its architecture 
often designed kom experience sometimes by node 223pruning\224 or 223growing\224 such as the cascade correlation method 4 Furthermore the trainling of the MLP by way of descent methods such as the popular back error propagation or extended Kalman filter 7 requires numerous passes of the training data through the system model And when the MLP eventually identifies a system it is exceptioimally difficult for the designer to extract the knowledge 223learned\224 from the sys%em for the knowledge acquired in the MLP is really 
a global behavior encoded in its individual basis functions \(neuron nodes which serve as the system\222s memory units In cases where there is insufficient information about the system to be miodeled we cannot really make assumptions on the co~mplexity and size of the system by arbitrarily specifying and optimizing some set of system parameters to adequately model the system The design intensive nature of adaptive parameter-based systems motivates a departure an alternative data approach In this paper we propose a novel Self-organizing Associative Memory SAM 
model for identification of nonlinear dynamical systems SAM takes a fresh1 approach of buying fast learning speed with larger memory use As the semiconductor industry makes contirmous progress to produce cheaper and faster memory llogic chips it becomes extremely attractive to use larger sets of memory in exchange for faster training The concept of SAM is based on two premises The first is the assumption that rules are not memorized rather they are induced from association between memorized objects Secondly only 
novel1 and distinct objects are memorized objects which yield knowledge that cannot otherwise be obtained througlh any association between already-memorized objects 2 Self-organizing Associative Memory The SAM has a simple procedure 1 present the system with a new input point 2 the new input evokes some association between its closest neighboring points 3 should the association produce a corresponding output that is reasonably close to the desired output then 0-7803-4859-1/98 1O.W1998 EEE 2447 


the new input-output data pair is familiar and deemed redundant 4 if the association of the stored neighboring points yields an output insufficiently accurate then the new data pair is novel and is memorized So the novelty of the training data  not the frequency of its Occurrence  determines whether or not it is memorized The SAM model may be as simple or complex as the system it identifies When SAM encounters a complex system it stores numerous data and become large and complex Conversely for a system as simple as a line in two dimensions SAM is simple storing only two relevant data points From the procedure of SAM we see that in the very first pass of the training data SAM will memorize each and every important data and ignore the redundant ones making subsequent passes through the same set unnecessary In a review of related research u-e have found Memory-Based Reasoning method detailed MBR in ll and 9 to be similar to SAM in philosophy The MBR model is also based on a set of stored data and rules some form of matching of a new input stimulus with 223similar\224 stored data and interpolation between them MBR has also been employed successfully in pattern classification problems 2 and control lo often drawing from a database of rules SAM on the other hand takes the idea further structurally and computationally conforming the idea to an adaptive system identification model with different variations of local interpolative functions Looking at its weaknesses we see that SAM system takes more time to produce an output than would for example the MLP For the SAM searching for the proper neighborhood for interpolation is part of the storage and retrieval processes In every neural network architecture some form of searching is involved For example ART1 ART2 and LVQ2 3 all include explicit searching as a part of the network operation Even for the MLP searching is implicitly done via the corporate computation performed at all the nodes with the activation function serving as discriminators There are however many strengths of SAM Because of the local interpolation method of SAM a local value of a function is only dependent on a small stored subset of training samples Hence the actual recall time outside of the search for the neighborhood locale is minimal The time it takes for the SAM to 223learn\224 the training data is also short given that all SAM does for learning is to selectively memorize and index certain training data Secondly, due to the interpolation approach the SAM system directly stores the training data \(prototypes enabling SAM to do knowledge level processing very simply Since the SAM system contains actual physical data in its memory it encodes its knowledge 223transparently,\224 making rule extraction more realizable 3 Computation models of SAM For the application of system identification the nonlinear dynamical system may typically be described in the NARX Nonlinear Auto-Regression with exogenous inputs model  11 where y\(k E  u\(k E 91r k is a discrete time index and f a general vector-valued nonlinear function of multiple variables or the typical state-state model where the state x\(k E 93\223 and h is again a general vector-valued nonlinear function of multiple variables We adopt the NARX model with some controllability and observability conditions with the assumption that the state of the system in 2 x\(k may be reconstructed from p past outputs y\(k-l k-p and q past inputs u\(k k  q  1  an assumption adapted from Narendra and Parthasarathy 6 Though we have developed several models of computation for the SAM Figure 1 illustrates the general procedure of SAM Given an input SAM will search in the input-space of the stored data for the closest neighbors and generalize these neighboring data into a neighborhood or local transfer function which maps the input x onto output y\222 Should the percentage error E between the desired output y and the predicted output y\222 of the given input exceed some error threshold cl the given input and its corresponding desired output are stored Y\(k  f\(Y\(k-l k P k k-q+l 1 x\(k  1  f\(x\(k k Y\(k  h\(x\(k k 2 IIV  v4 3    ______.._.______ Figure 1 General procedure of SAM 3.1 Linear SAM using overlapping local linear approximation 2448 


The approximation method for the input-output function of the NARX model in the linear rendition of SAM is a set of locally-linear approximations that are overlapped in the global view The safest local approximation relating a set of points given a d dimensional input space is a d+l hyperplane Gliven the inputx E YV the linear SAM gathers d+l closest stored neighbors in the input-space qT and their corresponding outputs y into a matrix augmented by a 1 as reference for the hyperplane w'=A'f=\(ATA and the hyperplane being the weight matrix w in d+l Given that sometimes the matrix may be singuLar or near-singular and to allow the freedom for over determined systems we use the least squares solution pseudoinverse I:S with singular-value decompositioin for singular cases in equation 5 The hyperplane w then maps the augmented input l x onto the predicted y'=[l x x w 5 output y y  l X'IW 3.2 Nonlinear SAM using weighted pseudoinverse Working oni the same principle of generalizing a set of stored data in a neighborhood into local functions SAM may use nonlinear interpolation methods for the local functions as well Having attempted various methods such as natural splines and local fuzzy inferencing functions we have decided on a simple and effective way of introducing nonlinearity  the weighted pseudoinverse a modification to the least square cost function of ai positive-definite symmetric weighting matrix W which may be decomposed into its square roots Y and figured into the least squares cost function 8 w  YTY 6 J\(w Aw AW 7  W A diagonal matrix for which the diagonal elements indicate a confidence for pseudoinverse exhibits some smoothing properties of local multi-dimensional splines and may be a first step their corresponding stored data The weighted toward extracting rules from the data Section 4.2 lists some examples of ncinlinear weighting functions and their test results 3.3 logic SAM using weighted linear combination In cases where SAM is to learn a logical system where the system outputs are confined to the set O,l the pseudoinverse method may predict values outside af the 0 11 range because it also extrapolates A way to confine the predicted output y in the range O 13 is to take the collection of the d+l nearest stored data in equation 4 weight f with some weighting vector v and normalize the weights 9 y VJf where vT v V v cvt Since all elements off are in the set 0 l and the weighting is normalized y is bounded by 0 11 Section 4.3. will demonstrate the results of this method 4 Problems and results 4.1 Example 1 MIMO nonlinear system identitication A Multi-Input and Multi-Output MIMO nonllinear system is described by the following set of state-space equations x,\(k k l k-1 k 10 x*\(k 5Jx,o k l k 1 k x3\(k 5JqKT k l k 1 k Y,\(k 5tx,\(k k x,\(k 7 Y,\(k  2[Xl\(k 11 A Linear SAM as in section 3.1 is trained with the following data sets: all 25 possible combinations of steps starting from 0 and ending at 0 0.125 0.25 0.375 0.5 Also 25 positive and 25 negative ramps with the same starting and ending points are included in the training data set Each training set contains 100 points The total number of training data is 7500 The Linear SAM is then tested with two input sequences one containinig two pulses of magnitude 0.3 offset by 10 time steps and the other is two steps of magnitude 0.3 and 0.2 with Gaussian noise of 0.1 standard deviation superimposed on them SAM's prediction results for the pulse and the Gaussian noise sequences are shown in Figures 2a and 2b respectively The prediction errors for testing of the system with the pulsed input and the Gaussian noise inputs are 1.7073 and 3.186470 respectively 2449 


training set data appears to perform better than the MLP and the RBF 10 20 30 40 50 Figure 2 MlMO system testing outputs a with pulse inputs b with Gaussian noise inputs 4.2 Example 2 tenth-order boiler model Using the Linear SAM described in section 3.1 we attempt to identify a 10 order boiler model using a 2nd order NARX model as in equation 1 That is given only a ramp-step signal u\(k driving the boiler model and its output signal y\(k we seek to build a system identifier taking the set u\(k y\(k-1 y\(k-2 as inputs and y'\(k as the predicted output y'\(k f\(u\(k k-l k-2 Two sets of 90 data from the boiler model were used to train a Linear SAM system an adjusted 12-5-1 MLP neural network, and a radial basis function RBF neural network A third set of 90 data is used to test the three system identifiers Both SAM and the RBF were able to imitate the system dynamics well on-line given the desired output feedbacks y\(k-1 and y\(k-2 of the test set Figure 3a But suppose the boiler model is off-line e.g the desired feedback y\(k are not available and all we have are our past predictions Then u\(k y'\(k-l y'\(k-2 would be the input for the test set and i.e the entire system model dynamics would require highly accurate predictions in order to stay on track Figure 3b compares the three systems While all three systems strayed from the correct response after the 20 time step the linear SAM having stored 37 of the 180 Y'\(k f\(u\(k k  l k  q On-Line Testing 1 I 0 80 Off-Line Testing 0 0 80 Figure 3 Testing of different system identifiers We take the problem further by implementing a Nonlinear SAM as discussed in section 3.2 Five different weighting functions of the Euclidean distance between the given new input and inputs from the recalled stored data set are put on the diagonal of the weighting matrix y The test is done off-line to illustrate the high accuracy of the nonlinear SAM and the five results are similar in accuracy Note that these off-line testing cases using the nonlinear SAM with 5 different weighted pseudoinverses \(Figure 4 are significantly more accurate than those using the linear SAM Figure 3b Table 1 lists the results of off-line testing using Nonlinear SAM with different determinants of the matrix Y for the weighted pseudoinverse The Nonlinear SAM is able to store significantly more data without over-training Figure 4 Off-line testing of nonlinear SAM's 2450 


Table 1 Results of off-line testing for SAM with 0.9601 0.03 67 0.9791 0.04 64 y  IIX   11,\222 w  IIX  q 11;\222 _ 1 I644 0.05 60 1.8845 0.05 53  Y  IIX  2 IT IIX  u  exp IIx  Y II2o-\221 1.0473 0.01  91 wz I IIX  21 11\221  correctly identified 3 out of the 8 ones in the testing data set with just one pass tlhrough the training set MLP output trained 20 cycles 3 2000 00 222 1003 MLP output trained 30 cycles 2227-1 I II 4 1wO 2000 Figure 6 Outputs of the testing set for the logic system 4.3 Example 3 logic problem 4.4 Example 4 next-day stock trend prediction I Figure 5 Eight logic gates in series: \(a flow chart b the 8-1 MLP Given a sample logic system in Figure 5a of eight logic gates in series, where an input x E 3\222 has to pass through all 8 gates in order to output a 221yes\222 or 221I\222 or else it will be 221110\222 or 221O\222 a set of 2000 testing data and another set of 2000 testing data are generrted such that it is equally probable to pass or fail at each of the 8 gates This implies that of the 2000 training data there would be approximately 2000*2-8 or about 8 data out of the set of 2000 with am output of 221l\222 the rest are 2210.\222 With 99.6 of the training set being O\222s the challenge here is to 223learn\224 the 0.4 of the data the 2211\222s This is a common situation where the distinctness of the data is a more important property in identifying the system than is the frequency of occurrence of the data SAM\222s performance in the 2000-data test set is compared with a 8-1 neural network with sigmoid activation functions in the input layer Figure 5b designed specifically for a logic system 8 discriminators at input and one at the output The results are telling the Logic SAM has identified and stored all 8 of the 2211\222 data out of 2000 in the training set to be novel The well tuned 8-1 MLP treated the 2211\222 data like the rest of the 2210\221 data and did nlot classify any point in the testing set as 2231\224 until its 30 cycle The logic SAM however SAM is employed in the difficult task of predicting the next-day stock trends A way to do that would be to first predict the percentage change of the price between today and tomorrow and use its sign to determine the next-day market trend The desired training output may be formulated as in equation \(12\where p\(n is the price on day n and p\(n+I is the percentage change in the price between day n+l and n 13 P n  PI 41  P a  P,IOSt 41 Ph8b 4  PI 4 X\(n  From the wide array of combinations of stock market technical indicators we chose as input a relevant 223X indicator\224 in 13 which indicates the position of the close price relative to tlhe high and low prices of the day An experiment is set up to model next-day market trend predictions of the New York Stock Exchange closing price over 3,0131 days \(April 1980 through June 1995 A linear SAM system is set up to begin learning the real NYSE data from day 1 and predict the next-day trend each day until day 3,081 making one pass through the data while predicting at each day the stock treind of the next day To regulate the randomness the input and the desired output training sets are quantized into different levels in the domain l,+l Since the market trend prediction may be either 223up\224 or 223down,\224 the profit generated since day 1 may be calculated by having the investor buy on 223up\224 predictions and sell on 223down\224 predictions pr\(k I ud\(k k pr\(k  1  14 where pr\(k is the profit at day k d\(k is the market trend upldown prediction made on day k-1 and p\(k is 245 1 


the percentage price change from equation 12 The results of the SAM is compared with that of using the day-before X-indicator and Buy and Hold B&H the case where the investor invests everyday profit  7 5 4 3 2 2230 500 1000 1500 2000 2500 3000 Days Figure 7 Plot of profits over 3,081 days starting at 1 OO on day one Table 2 Next-day market trend prediction Prediction method  Correct  223Up\224 Buy and Hold 54.30 100 SAM training on 52.65 56.52 the X-indicator X-indicator 52.00 56.06 predictions prediction Figure 7 illustrates SAM\222s improvement over the X indicator prediction in terms of profit and percentage correct stock market trend prediction Even though B&H has a higher percentage correct prediction the accumulated profit from B&H is significantly lower than those of X-ind and SAM indicating that X-ind and SAM are more responsive and more accurate in days of large stock price changes Although SAM has not excelled the X-ind by a wide margin, it is consistent over 3,081 days 15 years of stock data and is a viable tool to improve upon good stock market indicators in the difficult task of stock market prediction 5 Conclusion In this paper we have presented the Self-organizing Associative Memory SAM system a departure from the parametric systems for identification of unknown systems that assumes that objects  not rules  constitute the knowledge-forming memory by association, and that only novel and distinct objects need to be organized into memory The Linear SAM with locally linear approximation and especially with the nonlinear weighted pseudoinverse SAM achieves high accuracy in the case of off-line test for system identification Furthermore the logic problem demonstrates that SAM is apt in identifymg unknown systems given sparse or unevenly-distributed data In the abundant-data and high noise identification case of 3,081 days or 15 years of real stock market data SAM serves to improve upon a stock trend indicator In all, the data approach allows SAM to trade memory for high accuracy and fast one-pass learning of the training data Further studies may result in more versatile nonlinear local functions and possible fuzzy rule or knowledge extraction from the stored data 6 References l Chen S and S.A Billings, \223Representations of non-linear systems: the NARMAX model,\224 Int\222l Journal of Control vol 49 no 3 \(1989\pp 1013-1032 2 Creecy R.H B.M Masand S.J Smith D.L Waltz 223Trading Mips and Memory for Knowledge Engineering,\224 Communications of the ACM vol. 35 no 8 Aug 1992\48-64 3 Hertz J A Krogh and R.G Palmer Introduction to the Theory of Neural Computation Addison-Wesley, 1991 4 Hoehfeld M and S.E Fahlman 223Learning with limited numerical precision using the cascade-correlation algorithm,\224 IEEE Trans. Neural Networks vol. 3 no 4 \(July, 1992 5 Lippmann P.R 223An introduction to computing with neural nets,\224 ASSP Magazine vol 4 no 2 pp 4-18 Apr 1987 6 Narendra K.S and K Parthasarathy 223Identification and Control of Dynamical Systems Using Neural Networks,\224 IEEE Trans Neural Networks vol 1 no 1 Mar 1990 7 Ruck D.W S K Rogers M Kabrisky P S Maybeck and M E Oxley 223Comparative analysis of backpropagation and the extended Kalman filter for training multilayer perceptrons,\224 IEEE Trans on Pattern Analysis and Machine Intelligence vol 14 no 6 pp. 686-691, June 1992 8 Santina M A R Stubberud and G H Hostetter Digital Control Systems 2nd ed Fort Worth: Saunders 1994 9 Salzberg S and A.L Delcher 223Best-Case Results for Nearest-Neighbor Learning.\224 IEEE Trans Pattern Analysis and Machine Intelligence vol 17 no 6 June 1995\599-608 lo Schaal S and C.G Atkeson 223Robot Juggling Implementation of Memory-Based Learning,\224 IEEE Control Systems vol 14 no 1 \(Feb. 1994 57-71 l I Waltz, D 223On Reasoning From Data,\224 ACM Computing Surveys vol 27 no 3 Sep 1995\356-9 2452 


Definition 4.1 Projected database Given a transaction database 7 an itemset a and an order R 1 Itemset p is called the max-prefix projection of trans action tid It E 7 w.r.t R if and only if 1 a C It and p C It 2 a is a prefix of j3 w.r.t R and 3 there exists no proper superset y of p such that y 5 It and y also has a as a prefix w.r.t R 2 The a-projected database is the collection of max prefix projections of transactions containing a w.r.t R Remark4.2 Given a transaction database 7 a support threshold  and a convertible anti-monotone constraint C Let a be a frequent itemset satisfying C The complete set of frequent itemsets satisfying C and having a as a prefix can be mined from the a-projected database The mining process can be further improved by the fol lowing lemma Definition 4.2 \(Ascending and descending orders An order R over a set of items I is called an ascending order for function h  2  R if and only if 1\for items a and b h\(a  h\(b implies a R 6 and 2 for itemsets a U a and a U b such that both of them have a as a prefix and a R b f\(a U a 5 f\(a U b R-l is called a descending order for function h For example, it can be verified that the value ascending order is an ascending order for function aug\(S and a de scending order for function maz\(S S Lemma 4.2 Given a convertible anti-monotone constraint C E f\(S 6 v 6 E I w.bt ascending/descending order R over a set of items I where f is a prefix function Let a be a frequent itemset satisfying C and al a2    a be the set of frequent items in a-projected database listed in the order of R 1 Ifitemset a U ai 1 5 i  m violates C forj such that i  j 5 m itemset Q U  aj also violates C 2 If itemset a U aj 1 5 j  m satisfies C but a U  aj  aj+l violates C no frequent itemset having a U  aj as a properprefi satisfies C Based on the above reasoning we have the algo rithm FICA as follows for mining Frequent ltemsets with  Convertible Anti-monotone constraints Algorithm 1 FICA Given a transaction database 7 a support threshold  and a convertible anti-monotone con straint C w.r.t an order R over a set of items I the algo rithm computes the complete set of frequent itemsets satis fying the constraint C Method Call fiea 0,T function fieQ a a is the itemset as prefix and 71 is the projected database 1 2 3 4 5 Scan 71 once find frequent items in 71 Let I be the set of frequent items within 71 such that Vu E I C\(a U a  true If I  0 return, else Vu E I output a U a as a frequent itemset satisfying the constraint If C is in form off S 8 where f is a prefix function and 0 E 5  using Lemma 4.2 to optimize the mining by removing items b from I such that there exists no frequent itemset satisfying C and having a U  b as a proper prefix Scan 71 once more Vu E I generate a U a projected database 71,u For each item a in II call fiea\(a U a 71au{a Rationale The correctness and completeness of the algo rithm has been reasoned step-by-step in this section The efficiency of the algorithm is that it pushes the constraint deep into the mining process so that we do not need to gen erate the complete set of frequent itemsets in most of cases Only related frequent itemsets are identified and tested As shown in Example 6 and in the experimental results the search space is decreased dramatically when the constraint is sharp 4.3 3X Mining frequent itemsets with mono tone constraints In the last two subsections an efficient algorithm for mining frequent itemsets with convertible anti-monotone constraints is developed. Under similar spirit an algorithm for mining frequent itemsets with convertible monotone constraints can also be developed Due to lack of space instead of giving details of formal reasoning, we illustrate the ideas using an example and then present the algorithm Example 7 Let us mine frequent itemsets in transaction database 7 in Table 1 with constraint C G avg\(S 5 20 Suppose the support threshold   2 In this example, we use the value descending order R exactly as is used in Ex ample 6 Constraint C is convertible monotone w.r.t order R After one scan of transaction database 7 the set of fre quent 1-itemsets is found Among the 7 frequent 1-itemsets g d 6 c and e satisfy the constraint C According to the definition of convertible monotone constraints, frequent itemset having one of these 5 itemsets as a prefix must also satisfy the constraint That is the g d b e and e projected database can be mined without testing constraint C because adding smaller items will only decrease the value of avg But a and f-projected databases should be mined with constraint C testing However as soon as its fre quent k-itemsets for any k satisfy the constraint, constraint checking will not be needed for further mining of their pro jected databases We present the algorithm TZCM for mining frequent itemsets with convertible monotone constraint as follows 439 


Algorithm 2 FIC Given a transaction database 7 a support threshold and a convertible monotone constraint C w.r.t an order R over a set of items I the algorithm com putes the complete set of frequent itemsets satisfying the constraint C Method Call ficm 0,7,1 function ticm TI check-flag 1 Scan 71 once find frequent items in TIa If check-flag is 1 let f be the set of frequent items within 71 such that Vu E I C\(a U a  true and 1 be the set of frequent items within 71 such that Vb E I C\(a U b  false If check-flag is 0 let I be the set of frequent items within 71 and I be 0 2 Vu E I output Y U a as a frequent itemset satisfy ing the constraint 3 Scan 71 once more Vu E fliUIl generate au{a projected database 71,u 4 Foreach itema inI~~,callfic,\(aU{a},7~au~a 0 Foreach itemainfI;,call fic,\(aU{a},71 1 Rationale The correctness and completeness of the algo rithm can be shown based on the similar reasoning in Sec tion 4.2 Here, we analyze the difference between 31CM with an Apriori-like algorithm using constraint-checking as post-processing Both F1CM and Apriori-like algorithms have to gener ate the complete set of frequent itemsets no matter whether the frequent itemsets satisfy the convertible monotone con straint The frequent itemsets not satisfying the constraint cannot be pruned. That is the inherent difficulty of convert ible monotone constraint The advantage of TICM a ainst Apriorix-like algo rithms lies in the fact that FIG only tests some of fre quent itemsets against the constraint. Once a frequent item set satisfies the constraint, it guarantees all of frequent item sets having it as a prefix also satisfy the constraint. There fore, all that testing can be saved An Apriori-like algorithm has to check every frequent itemset against the constraint In the situation such that constraint testing is costly such as spatial constraints, the saving over constraint testing could be non-trivial. Exploration of spatial constraints is beyond the scope of this paper 4.4 Mining frequent itemsets with strongly convert ible constraints The main value of strong convertibility is that the con straint can be treated either as convertible anti-monotone or monotone by choosing an appropriate order The main point to note in practice is when the constraint has a high selec tivity fewer itemsets satisfy it converting it into an anti monotone constraint will yield maximum benefits by search a is the itemset as prefix 71 is the a-projected database and check-flag is the flag for constraint checking space pruning When the constraint selectivity is low \(and checking it is reasonably expensive\then converting it into a monotone constraint will save considerable effort in con straint checking The constraint awg\(S  w is a classic example 5 Experimental Results To evaluate the effectiveness and efficiency of the algo rithms, we performed an extensive experimental evaluation In this section we report the results on a synthetic trans action database with IOOK transactions and 10K items The dataset is generated by the standard procedure described in l In this dataset the average transaction size and aver age maximal potentially frequent itemset size are set to 25 and 20 respectively The dataset contains a lot of frequent itemsets with various length This dataset is chosen since it is typical in data mining performance study The algorithms are implemented in C All the exper iments are performed on a 233MHz Pentium PC with 128MB main memory running Microsoft WindowsNT To evaluate the effect of a constraint on mining frequent itemsets we make use of constraint selectivity where the selectiviy S of a constraint C on mining frequent itemsets over transaction database 7 with support threshold is de fined as  of frequent itemsets NOT satisfying C  of frequent itemsets 6 Therefore a constraint with 0 selectivity means every fre quent itemset satisfies the constraint, while a constraint with 100 selectivity is the one cannot be satisfied by any fre quent itemset The selectivity measure defined here is con sistent with those used in 7,61 To facilitate the mining using projected databases we employ a data structure called FP-tree in the implementa tions of FICA and FIC FP-tree is first proposed in SI and also be adopted by 8,9 It is a prefix tree structure to record complete and compact information for frequent item set mining A transaction database/projected database can be compressed into an FP-tree while all the consequent projected databases can be derived from it efficiently We refer readers to 5 for details about FP-tree and methods for FP-tree-based frequent itemset mining Since FP-growth 5 is the FP-tree-based algorithm mining frequent itemsets and is much faster than Apriori we include it in our experiment. Comparison among FICA 3ZCM and FP-growth makes more sense than using pure Apriori as the only reference method 5.1 Evaluation of FZCA To test the efficiency of FZCd w.r.t constraint selec tivity in mining frequent itemsets with convertible anti monotone constraints, we run a test over the dataset with 440 


160 g 140 f 120 s 100 1 Figure 3 Scalability with constraint selectivity            A     FP-gmwth h FIC\(A 20 D FIC\(A 80  04 i 00 02 04 06 08 10 Support threshold Figure 4 Scalability with support threshold support threshold   0.1 The result is shown in Fig ure 3 Various settings are used in the constraint for various selectivities As can be seen from the figure 31CA achieves an al most linear scalability with the constraint selectivity As the selectivity goes up i.e fewer itemsets satisfy the con straint 31CA cuts more search space since one frequent itemset not satisfying the constraint means all frequent itemsets having it as a prefix can be pruned We compare the runtime of both Apriori and FP-growth in the same figure All these two methods first compute the complete set of frequent itemsets and then use the constraint as a filter So their runtime is constant w.r.t constraint selectivity However only when the constraint selectivity is 0 i.e every frequent itemset satisfies the constraint does FICA need the same runtime as FP-growth In all other situations FICA always requires less time We also tested the scalability of FZC\224 with support threshold and the number of transactions respectively The corresponding results are shown in Figure 4 and Figure 5 From these figures we can see that 3ZCA is scalable in both cases Furthermore the higher the constraint selectiv ity the more scalable FZCA is That can be explained by the fact that 3ZCd always cuts more search space using constraints with higher selectivity 5.2 Evaluation of FZCM As analyzed before convertible monotone constraint can be used to save the cost of constraint checking but it cannot cut the search space of frequent itemsets In our experi ments since we use relatively simple constraints, such as those involving avg and sum the cost of constraint check ing is CPU-bound However the cost of the whole frequent itemset mining process is I/O-bound This makes the effect of pushing convertible monotone constraint into the mining process hard to be observed from runtime reduction In our experiments 31CM achieves less than 3 runtime benefit 0 200 400 600 800 1000 Number of transactions K Figure 5 Scalability with number of transactions in most cases However if we look at the number of constraint tests performed, the advantage of FICM can be evaluated objec tively FZC can save a lot of effort on constraint testing Therefore in the experiments about 31C\222 the number of constraint tests is used as the performance measure We test the scalability of 3ZCM with constraint selec tivity in mining frequent itemsets with convertible mono tone constraint The result is shown in Figure 6 The fig ure shows that FZCM has a linear scalability When the constraint selectivity is low, i.e most frequent itemsets can pass the constraint checking most of constraint tests can be saved This is because once a frequent itemset satisfies a convertible monotone constraint every subsequent frequent itemset derived from corresponding projected database has that frequent itemset as a prefix and thus satisfies the con straint, too We also tested the scalability of 31CM with support threshold The result is shown in Figure 7 The figure shows that FZCM is scalable Furthermore the lower the con straint selectivity the better the scalability FZCM is In summary our experimental results show that the method proposed in this paper is scalable for mining fre quent itemsets with convertible constraints in large transac tion databases The experimental results strongly support our theoretical analysis 6 Discussions: Mining Frequent Itemsets with Multiple Convertible Constraints We have studied the push of single convertible con straints into frequent itemset mining 223Can we push mul tiple constraints deep into the frequent pattern mining pro cess?\222 Multiple constraints in a mining query may belong to the same category e.g all are anti-monotone or to different categories Moreover different constraints may be on dif ferent properties of items e.g some could be on item price 441 


1  0 20 40 60 80 100 Selectivity Figure 6 Scalability with constraint selectivity others on sales profits the number of items etc As shown in our previous analysis unlike anti monotone, monotone and succinct constraints convertible constraints can be mined only by ordering items properly However different constraints may require different and even conflicting item ordering Our general philosophy is to conduct a cost analysis to determine how to combine mul tiple order-consistent convertible constraints and how to se lect a sharper constraint among order-conflicting ones The details will not be presented here for lack of space 7. Conclusions Constraints involving holistic functions such as median algebraic functions such as avg or even those involving dis tributive functions like sum over sets with positive and neg ative item values are difficult to incorporate in an optimiza tion process in frequent itemset mining The reason is such constraints do not exhibit nice properties like monotonicity etc. A main contribution of this paper is showing that by im posing an appropriate order on items, such tough constraints can be converted into ones that possess monotone behavior To this end we made.a detailed analysis and classification of the so-called convertible constraints We characterized them using prefix monotone functions and established their arithmetical closure properties As a byproduct we shed light on the overall picture of various classes of constraints that can be optimized in frequent set mining While con vertible constraints cannot be literally incorporated into an Apriori-style algorithm they can be readily incorporated into the FP-growth algorithm Our experiments show the effectiveness of the algorithms developed We have been working on a systematic implementation of constraint-based frequent pattern mining in a data min ing system More experiments are needed to understand how best to handle multiple constraints An open issue is given an arbitrary constraint, how can we quickly check if it is strongly convertible We are also exploring the use of constraints in clustering 160000   t 221FP-growth tFIC\(M 20 tFIC\(M 80 0.0 0.2 0.4 0.6 0.8 1.0 Support threshold Figure 7 Scalability with support threshold References I R Agrawal and R Srikant. Fast algorithms for mining asso ciation rules In Proc 1994 Int Con Very Large Data Bases VLDB\22294 pages 487-499 Santiago, Chile, Sept 1994 2 R J Bayardo R Agrawal and D Gunopulos Constraint based rule mining on large dense data sets In Proc 1999 Int Conj Data Engineering ICDE\22299 Sydney, Australia Apr 1999 3 S Brin R Motwani and C. Silverstein Beyond market bas ket Generalizing association rules to correlations In Proc 1997 ACM-SIGMOD Int Con Management of Data SIG MOD\22297 pages 265-276 Tucson Arizona May 1997 4 G Grahne L Lakshmanan and X Wang Efficient min ing of constrained correlated sets In Proc 2000 Int Con Data Engineering \(ICDE\222OO pages 5 12-521 San Diego CA Feb 2000 5 J Han J Pei and Y Yin Mining frequent patterns with out candidate generation In Proc 2000 ACM-SIGMOD Int Con Managementof Data \(SIGMOD\222OO pages 1-12 Dal las, TX May 2000 6 L V S Lakshmanan R Ng J Han and A Pang Opti mization of constrained frequent set queries with 2-variable constraints In Proc 1999 ACM-SIGMOD Int Con Man agement of Data SIGMOD\22299 pages 157-168 Philadel phia PA June 1999 7 R Ng L V S Lakshmanan J Han and A Pang Ex ploratory mining and pruning optimizations of constrained associations rules In Proc 1998 ACM-SIGMOD Int Con Management of Data SlGMOD\22298 pages 13-24 Seattle WA June 1998 8 1 Pei and J Han Can we push more constraints into fre quent pattem mining In Proc 2000 Int Con Knowl edge Discovery and Data Mining KDD\222OO pages 350 354, Boston MA Aug. 2000 9 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets In Proc 2000 ACM SIGMOD Int Workshop Data Mining and Knowledge Dis covery DMKD\222OO pages 1 1-20 Dallas TX May 2000 lo R Srikant Q Vu and R Agrawal. Mining association rules with item constraints In Proc 1997 Int Con Knowledge Discovery and Data Mining KDD\22297 pages 67-73 New port Beach CA Aug 1997 442 


expect this optimization to be of greatest bene\336t when the transaction sizes are large r example if our transaction is T 000 f A\000 B 000 C\000 D\000 E g  k 000 3 fan-out 000 2 then all the 3-subsets of T are f ABC,ABD,ABE,ACD,ACE,ADE,BCD,BCE,BDE,CDE g  Figure 2 shows the candidate hash tree C 3  We ave to increment the support of every subset of T contained in C 3  We egin with the subset AB C  and hash to node 11 and process all the itemsets In this downward path from the root we mark nodes 1 4 and 11 as visited We then process subset AD B  and mark node 10 Now consider the subset CDE  We see in this case that node 1 has already been marked and we can preempt the processing at this very stage This approach can r consume a lot of memory r a n fan-out F  for iteration k  e need additional memory of size F k to store the 337ags In the parallel implementation we have to keep a VISITED 336eld for each processor bringing the memory requirement to P\000F k  This can still get very large especially with increasing number of processors In we sho w a mechanism by which further reduces the memory requirement to only k 000F  The approach in the parallel setting yields a total requirement of k 000F 000P  5 Experimental Evaluation Database T I D Total Size T5.I2.D100K 5 2 100,000 2.6MB T10.I4.D100K 10 4 100,000 4.3MB T15.I4.D100K 15 4 100,000 6.2MB T20.I6.D100K 20 6 100,000 7.9MB T10.I6.D400K 10 6 400,000 17.1MB T10.I6.D800K 10 6 800,000 34.6MB T10.I6.D1600K 10 6 1,600,000 69.8MB Table 2 Database properties 5.1 Experimental Setup All the experiments were performed on a 12-node SGI Power Challenge shared-memory multiprocessor Each node is a MIPS processor running at 100MHz There\325s a total of 256MB of main memory The primary cache size is 16 KB 64 bytes cache line size with different instruction and data caches while the secondary cache is 1 B 128 bytes cache line size The databases are stored on an attached 2GB disk All processors run IRIX 5.3 and data is obtained from the disk via an NFS 336le server We used different synthetic databases with size ranging form 3MB to 70MB 2  and are generated using the procedure described in These databases mimic the transactions in a retailing en vironment Each transaction has a unique ID followed by a list of items bought in that transaction The 2 While results in this section are only shown for memory resident databases the concepts and optimization are equally applicable for non memory resident databases In non memory resident programs I/O becomes an important problem Solutions to the I/O problem can be applied in combination with the schemes presented in this paper These solutions are part of future research 11 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


  0 500 1000 1500 2000 2500 0 2 4 6 8 10 12 Number of Large Itemsets Iterations Large Itemset at Support = 0.5 222T5.I2.D100K\222  222T10.I4.D100K\222   222T15.I4.D100K\222   222T20.I6.D100K\222   222T10.I6.D400K\222   222T10.I6.D800K\222   222T10.I6.D1600K\222  Figure 3 Large Itemsets per Iteration data-mining provides information about the set of items generally bought together Table 2 shows the databases used and their properties The number of transactions is denoted as jD j  average transaction size as j T j  and the average maximal potentially large itemset size as j I j  The number of maximal potentially large itemsets j L j 000 2000 and the number of items N 000 1000 We refer the reader to for more detail on the database generation All the e xperiments were performed with a minimum support value of 0.5 and a leaf threshold of 2 i.e max of 2 itemsets per leaf We note that the  improvements shown in all the experiments except where indicated do not take into account initial database reading time since we speci\336cally wanted to measure the effects of the optimizations on the computation Figure 3 shows the number of iterations and the number of large itemsets found for different databases In the following sections all the results are reported for the CCPD parallelization We do not present any results for the PCCD approach since it performs very poorly and results in a speed-down on more than one processor 3  5.2 Aggregate Parallel Performance Table 3 s actual running times for the unoptimized sequential and a naive parallelization of the base algorithm Apriori for 2,4 and 8 processors without any f the techniques descibed in sections 3 and 4 In this section all the graphs showing  improvements are with respect to the data for one processor in table 3 Figure 4 presents the speedups obtained on different databases and different processors for the CCPD parallelization The results presented on CCPD use all the optimization discussed 3 Recall that in the PCCD approach every processor has to read the entire database during each iteration The resulting I/O costs on our system were too prohibitive for this method to be  12 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


