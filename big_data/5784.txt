Scaling Deep Social Feeds at Pinterest Varun Sharma Pinterest Inc Email varun@pinterest.com Jeremy Carroll Pinterest Inc Email jeremy@pinterest.com Abhi Khune Pinterest Inc Email abhi@pinterest.com Abstract With the advent of Twitter the follow model has become pervasive across social networks The follow model enables users to follow other users i.e subscribe to content created by other users thereby establishing the concept of a following feed for a user At Pinterest we continually store update and serve feeds for millions of users and fan out millions of newly created pins/repins to thousands of followers leading to billions of operations everyday We describe the current feed storage solution backed by Apache HBase at Pinterest We describe how we handle data management challenges unique to our scale in the wake of strict performance and availability requirements We also present a qualitative comparison to our previous following feed architecture backed by Redis I I NTRODUCTION Pinterest is an online pinboard where people pin images and organize content into self created virtual boards Curation and discovery of inspirational content are the two primary categories of user activity on Pinterest The follow model is a construct to aid content discovery The Pinterest follow model allows users to follow other users or follow boards created by other users For example a running enthusiast could choose to either follow the nike brand on Pinterest or follow the shoes board created by the nike brand As shown in Figure 1 the follow relationship gives rise to a Follow graph relating different entities on Pinterest  Follower The user who follows/subscribes to other boards/users  Followee The user/board being followed Hence each user/board has a list of followers who subscribe to pins created on that board or by that user Similarly each user has a list of followees which are boards/users the user is following The Following Feed for a user consists of content aggregated from all the followees of the user Each time a user accesses pinterest.com we render the following feed to the user A large number of social networks have embraced the follow model including Tumblr and Instagram In fact the following feed is a core piece of user experience for the majority of social networks today It is also referred to as the News Feed or the Friend Feed The operations on the following feed implementation at Pinterest are described below  Updates The following feed needs to be updated as users follow/unfollow other users and boards The corresponding pins need to be added to or removed from the following feed of the user performing the action New content in the form of pins/repins needs to be fanned out to the following feeds of the pin creatorês followers as shown in Figure 2  Serving/Retrieval the following feed needs to be retrieved whenever an authenticated user accesses pinterest.com II P USH VS P ULL One signiìcant challenge is the delivery of content along the edges of the Follow Graph from followees to followers There are two approaches to achieve such delivery  Pull Each time the following feed needs to be served we retrieve the set of followees for that user and their pins These pins are then aggregated and ordered together to build the following feed in real time The primary advantage of this approach is that it offers exibility on how the pins are ordered in the following feed However it is harder to get a tab on tail latency and performance of serving the following feed for this model Since users could be following thousands of boards/users a pull implementation would require assembling pins from thousands of followees Such a model while being feasible requires non trivial effort to deliver consistent performance since it pushes content delivery to serving time  Push A dedicated storage layer materializes the entire following feed for the user The layer is responsible for maintaining the feed ordered according to a well deìned ranking function As new pins are added and new follow relationships are formed appropriate updates are issued to this storage layer Each time a new pin is added it is written out to the following feeds of all the pin creatorês followers One disadvantage of this approach is lack of ordering exibility However since the feed is materialized and can be quickly retrieved there is little variance in the tail latency of serving the following feed The bulk of content delivery happens at content creation time One may also conceive a hybrid Push/Pull model At Pinterest the following feed is reverse chronologically ordered 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 777 


Figure 1 Follow relationships and the follow graph by pin creation time Hence we have adopted the push model for our implementation A dedicated Feed storage layer is responsible for storing feeds at Pinterest In this paper we describe in detail the architecture of this storage layer and how it ts into the overall Pinterest infrastructure III A RCHITECTURE Pinterest uses MySQL for persistent storage of core entities pins boards and users For example all metadata corresponding to a pin such as the pinês image URL the pinês title and the pinês description are stored in the MySQL database The MySQL database is also responsible for unique identiìer generation The dataset is sharded into multiple MySQL databases across the identiìer space for scalability However feeds are not stored in MySQL for reasons we mention in the next section Figure 2 shows the write path for the following feed Follow/unfollow actions by users and creation of new content triggers writes into the following feed storage The frontend receives these actions and asynchronously enqueues a task into our message queue system The frontend does not perform the heavy lifting associated with expensive operations such as fanout of newly created pins to followers The message queue is responsible for persisting the task and retrying it until it succeeds If a new pin is created the follow store is consulted to retrieve the list of followers for the user and the pin is written to the respective following feed\(s If a follow/unfollow action is performed the recent pins of the user being followed are retrieved and are added to/deleted from the following feed of the user performing the action The Feed Storage is responsible for storing only the identiìers associated with the pins in the following feed As shown in Figure 3 the frontend consults the Feed Storage for the reverse chronologically sorted list of pin ids in a userês following feed After obtaining the list of pin ids the frontend consults the MySQL backed pin metadata store to obtain the metadata necessary to render the page to the user IV T HE S TORAGE In the push model we expect the write volume to be signiìcantly higher due to the fanout incurred during content creation Our following feed operations comprise of over 99 writes This is a notable characteristic and is a fundamental guiding factor in choosing the right storage technology B-Tree backed stores such as MySQL are not amenable to a high volume of random writes Pin creations would require updating feeds of a large number of users during the fanout operation In a B-Tree this would require updates in different parts of the B-Tree which in turn could lead to rearrangements within the tree due to node splitting etc Moreover the high write load due to fanout on the B-Tree would incur a substantial number of read operations This write ampliìcation factor due to fanout poses a requirement for high write throughput which B-Tree backed systems like MySQL canêt handle very well On the other hand a number of column oriented stores provide disproportionately higher write throughput and are better suited for our application This has been well  and it is clear that on similar hardw are Log Structured Merge LSM tree databases such as HBase/Cassandra offer substantially higher write throughput than B-Tree backed databases such as MySQL As a result we are left with the following options for the storage layer  In memory databases is an in-memory database which enjoys widespread use and is well known to be extremely performant It achieves low latency and high throughput by keeping the entire dataset in RAM It provides weak durability gaurantees and supports Master Slave replication  Log Structured Merge Trees/Bigtable like stores A number of lik e stores ha v e gained popular 778 


Figure 2 Following feed writes Figure 3 Following feed reads ity over the past few years Notable examples include  Cassandra[6 The LSM approach[7 allo ws the system to buffer writes in RAM until the buffer is exhausted When the buffer lls up a le is output as a sorted map of  key value  pairs Over time many such les accumulate A read operation requires seeking and lazily merging the accumulated sorted les to retrieve the result Compaction operations run in the background to merge the les together so that the number of les remains low This reduces the number of disk seeks required during read operations Another notable feature with these solutions HBase/Cassandra is the ability to manage sharding and machine failures automatically rather than relying on manual intervention or the application tier to do so The current feed infrastructure at Pinterest uses HBase as the backend storage We describe the system in more detail in the following sections and nally compare it with a setup backed by Redis Amongst the wide variety of LSM stores available HBase Cassandra LevelDB we chose HBase because of its maturity adoption in the community and its seamless integration with Hadoop and Hive Also Facebook already uses HBase for an online user f acing application We also had considerable experience operating Hadoop clusters and HBase uses HDFS as its underlying le system V H B ASE Apache HBase is a NoSQL distributed versioned database system modeled after Google Bigtable Similarly Apache ZooK is a distrib uted lock service modeled after Google HBase uses a ZooK eeper quorum for master election maintaining cluster state and performing cluster coordination Though HBase can use any lesystem underneath the hood a typical HBase deployment uses  the Hadoop Distrib uted File Sytem HDFS is modeled after and uses a master sla v e model Files are divided into large sized blocks which are replicated 3 times across the HDFS slaves or datanodes HDFS master or the namenode is responsible for cluster coordination block placement replication and recovery from machine failures The HDFS master also stores the mapping from les to data blocks and the machines holding the block replicas By block replicas we mean the 3 copies of each data block HBase achieves fault tolerance through HDFS replication Failure of a single machine or disk drive does not render the data unavailable since it can be restored from a replica In HBase tables are divided into regions and regions are distributed evenly across region servers The HBase region servers and HDFS datanodes run on the same set of servers 779 


Like Bigtable HBase stores data as a three dimensional sorted map from  row column timestamp    value  The data is sorted along the row dimension and regions split the row space into contiguous ranges The HBase master is responsible for distributing regions across region servers and moving regions when servers fail Each table consists of column families akin to locality groups in Bigtable VI F OLLOWING F EED ON HB ASE In the following subsections we motivate the schema design and our customizations to extract performance improve Mean Time To Recovery MTTR and availability for our HBase deployment A Schema HBase keeps rows in lexicographic order and columns within rows are also lexicographically sorted A column based schema also called wide schema would look like feed:\(rCreationTs:pinId userId empty Note that the value is empty as we have absorbed everything into the row and the column The rCreationTs is the reverse ordered creation timestamp of a pin thus enforcing reverse chronological ordering The columns are stored in a column family feed as depicted above A tall schema on the other hand would absorb all the elds into the row feed userId:rCreationTs:pinId empty Underneath the hood both schemas result in very similar storage patterns However we choose the wide schema because of the following desirable properties  Atomicity HBase provides transaction support at the row level thus providing the ability to make atomic updates to a userês feed  Locality Since a table is divided into regions based on rows a userês data lies within the same region and hence on the same region server Correctness of the following feed is of paramount importance in our application Hence it is desirable to be able to atomically update a userês feed Otherwise a user could accidently view a partially modiìed feed This requirement made it imperative to choose the wide schema over the tall schema VII P ERFORMANCE AND A VAILABILITY A Performance We have different requirements for reads and writes The read volume is comparatively lower but keeping latencies low is very important On the other hand writes can be latent since they are processed by the message queue workers However the sheer volume of writes requires the system to be high throughput The predominant component of writes is the fanout of new pins to batches of followers We realized that since writes accounted for majority of the system load an important component of optimizing writes was to minimize their interference with reads 1 Reads Intuitively we expect only a subset of the users to be active at a particular instant and we expect temporal locality for their following feed accesses We hypothesized that an LRU based caching scheme would be very effective On our HBase region servers we increased the percentage of heap memory occupied by the HBase Block cache from 25 to 40 percent In production we were able to validate our hypothesis by observing cache hit rates upwards of 85 percent with a modest amount of cache memory Another important observation was that all the data was part of the key  row column timestamp  in the underlying HFiles written by HBase Hence a preìx encoding scheme would effectively compress data Apart from saving disk space it would allow us to cache larger amounts of recently accessed data Fortunately HBase came equipped with preìx compression Enabling preìx compression lead to a 4X reduction in data size hence more data being cached and an improvement in read latency We enabled other standard read optimizations for HBase We reduced the block size for HFiles from 64K to 16K since back of the envelope calculations suggested that a userês feed would have t within roughly 16 kilobytes The HFile block cache operates at the HFile block level and the block size controls the granularity of caching We wanted blocks to be close to the size of the following feed for a particular user so that we populated and evicted an entire userês data We also enabled short circuit reads so that the HBase region server does not read data from HDFS datanode over a socket when data is local Since the node that runs a region server also runs the datanode data is available locally unless there have been machine failures causing HBase regions to get redistributed 2 Writes HBase provided very high write throughput out of the box However as writes kept coming and data was ushed to create HFiles we found that compaction activity became more and more intense The I/O generated by compactions started impacting read latencies By default whenever a regionês in memory write buffer also called the memstore reached 128M a ush was initiated We observed that with preìx compression a 128M memstore was resulting in a tiny HFile of size 8M While there is a 128M limit on the per region memstore size there is a global limit on the total memstore usage for all regions The global limit is governed by the amount of RAM available to the system Though we wanted to increase the per region memstore limit signiìcantly we could only provision upto 512M given the number of regions 12 and the global memstore limit 6G The increase resulted in less frequent HFile ushes hence fewer HFiles and less compaction 780 


activity The pressure on the I/O subsystem was immediately relieved We use the Concurrent Mark Sweep to minimize GC pauses for HBase The heavy writes generate a lot of garbage along with surviving objects which need to be promoted We realized that our tail latency for reads was as good as our new generation GC pauses Decreasing the size of the new generation resulted in more frequent smaller pauses and an immediate improvement in tail latency B MTTR Mean Time To Recovery from region server failures was a major concern for us since a failure would cause an availability loss for a subset of users The HBase Master discovers region server failures through ZooKeeper Each region server registers with ZooKeeper and negotiates a conìgurable session timeout We followed the standard practice of lowering this timeout to 30 seconds We found that this was insufìcient to guarantee a low MTTR Instead we found recovery times upwards of 10 minutes from region server failures The issue was that even though HBase discovered that a region server was dead the HDFS namenode would only mark that node as dead after 10 minutes We thought that reducing this timeout would solve our MTTR problems However this practice was strongly discouraged since upon reaching this timeout the HDFS namenode would actively start replicating the now underreplicated data blocks which belonged to the failed node This would place additional load on an already wounded cluster After looking more closely at the recovery process and more recent work in HDFS we did a few customizations to optimize our HDFS setup for a low MTTR Each region server writes edits to a Write Ahead Log WAL which is triplicated on HDFS During recovery the WAL needs to be read and then split to recover edits for each region on the dead region server The edits are replayed to recover the in memory state of the regions on the dead region server We examine our customizations in context of the recovery steps  WAL read The HDFS namenode marks a datanode as stale if it has not heartbeated for a conìgurable time period This came from recent work done by the open source community on HDFS Earlier the WAL read would rst hit the datanode which was already dead and then there would be a socket timeout which would take 60 seconds or a connect timeout which would take 45 retries of 20 seconds each We enabled the stale node setting with a timeout of 20 seconds and reduced the socket and connect timeouts to 3 seconds This means that the WAL read step would avoid hitting the dead node and even if it did timeout quickly and move on to the next functional replica  Lease recovery This was one of the least understood features of HDFS A le that was being written to in HDFS such as the WAL needed to be closed before reading it A close operation would initiate a lease recovery and a block recovery For block recovery the namenode would choose one of the replica datanodes as the primary datanode for the last block of the le i.e the block being written to The primary datanode would then reconcile the size of the block against other replica datanodes and nalize the state of the block with the namenode We found two issues with the process During recovery the namenode would choose the dead datanode as the primary datanode Secondly when instructing the primary datanode to reconcile block replicas it would include the dead datanode as a candidate replica for reconciliation We contributed HDFS 4721 to integrate lease recovery with stale node detection In our setup stale nodes are completely ignored during the lease recovery process  WAL splitting The WAL needs to be split to recover the edits for each region which went ofîine during the region server failure This is because there is 1 WAL per region server Each split gets written out as a separate le Before stale node detection the namenode could suggest the dead datanode as one of the replicas for the split This would then timeout with a default socket timeout of 480 seconds thus signiìcantly delaying the recovery However with stale node detection the dead node is avoided in this step as well Also we reduced the socket write timeout to 5 seconds in case we do hit a dead node With these customizations we were able to consistently achieve an MTTR of less than 2 minutes We thoroughly tested this by suspending the region server and datanode processes and also by blackholing a host using iptables This was critical to achieving a highly available system C Single Points of Failure We run our systems on EC2 and it is advised to not keep all of oneês data and machines/instances within the same EC2 availability zone We could have split our HBase cluster across multiple availability zones and used HDFS rack awareness feature to replicate data across availability zones However our load tests showed signiìcant performance difference due to having to replicate data across availability zones Hence we decided to house our HBase cluster within the same availability zone To be able to survive availability zone outages we setup a replica cluster in another availability zone This was expensive however it is fairly standard practice for highly available HBase setups to have 2 clusters Since we setup two clusters we went with a simple setup for the namenode The namenode is a single point of failure for HDFS The namenode is conìgured to write to ephemeral instance storage and over the network to Elastic Block Store EBS volumes This ensures persistence of 781 


the namenode fsimage even if we lose the namenode EC2 instance and allows us to restore the cluster To ensure consistency across the 2 clusters we considered HBase replication However it was not heavily used/tested at our scale So we decided to implement dual writes from our message queue The message queue would be responsible to persist the writes and retry until a write is successful to both clusters D Challenges at scale Certain challenges unique to the Following Feed problem emerge as the scale increases to millions of users and hundreds of millions of follow relationships 1 Data consistency The write path depicted by Figure 2 shows that user actions are enqueued to the message queue Workers are responsible for dequeuing and issuing the writes However our message queue may not preserve time ordering of user actions It is possible for writes to be issued out of order and result in a following feed inconsistent with user actions Such a situation occurs when user A follows user B and unfollows user B quickly afterwards Here AB f ollow represents the user action and M  AB f ollow  represents the writes being issued by the message queue AB f ollow   t 1 AB unf ollow   t 2 t 1 t 2 M  AB f ollow    t  1 M  AB unf ollow    t  2 t  1 t  2 This situation becomes more prevalent when the difference between t 1 and t 2 is of the order of milliseconds or when there are unexpected message queue delays Note that within HBase the unfollow action results in a delete operation which inserts delete markers to mask values To resolve these inconsistencies we use t 1 and t 2 as cell timestamps for the follow/unfollow actions In the above scenario the follow action will insert pins at t 1 which would still be masked by delete markers inserted by the unfollow action at t 2  Hence we are able to enforce a mutation ordering by making use of cell based timestamps in HBase 2 Unbounded data growth As new pins are created and pushed to followerês feeds the dataset grows exponentially The dataset growth is a function of the rate of incoming content and the cardinality of follow relationships in the follow graph We cap the number of pins in a userês following feed to a certain threshold One approach to achieve this would be to trim the feed in real time during writes Since the majority of writes stem from the fanout operation we would need to read the excess pins on every write and issue deletes to remove these pins This would result in a high random read volume and would completely defeat the utility of an LSM tree Another possibility would be to run a mapreduce job to trim the feeds The mapreduce job would likely impact the performance of the online serving cluster Also both these approaches would simply insert delete markers into the system rather than truly deleting data We came up with a based approach to this problem We implemented a coprocessor which would hook into the major and minor compactions and would only retain the required number of columns for each row Since the compaction sees columns within each row in sorted order we would end up retaining the most recent pins in the following feed This solved the problem without introducing any additional overhead into the system VIII C OMPARISON WITH R EDIS Previously Pinterest deployed a Redis backed storage for the Following Feed Since Redis is an in-memory store it provided high throughput and low latencies We used Redis sorted sets as the underlying data structure with pin creation timestamp as the sorting key and pin id as the object We used Redis master slave replication for redundancy and fault tolerance Moving to HBase came with a number of beneìts  Scalability The Redis deployment was manually sharded and required application level support for failing over to slaves in the event of machine failures As of today there is no well tested generic Redis clustering solution deployed at scale Adding capacity and machines was more cumbersome and required manual steps On the other hand HBase came with built-in support for fault tolerance automatic sharding and load balancing  Deeper Feeds at Lower Costs With HBase we were able to keep the hot/recently accessed feeds in memory and exploit the temporal nature of retrieval queries The rest of the data could reside on disk On the other hand we were reluctant to keep all the data in memory for Redis Hence HBase enabled us to signiìcantly increase the length of the Following Feed for our users In fact we were able to cut down our machine footprint and save costs  Data consistency As we discussed in the previous section the mutation ordering support provided by HBase allows us to resolve data inconsistencies  Durability Redis buffers edits in memory and fsyncês a secondês worth of edits to disk for durability In the case of a single machine crash some data loss is inevitable This could be rectiìed by fsyncêing every edit or fsyncêing group edits to disk for better durability However this would likely have implications on performance of Redis On the other hand HBase requires a ush to at 3 replicas before acking that a write is complete Even though durability was not a strict requirement it became a nice to have feature for the new system 782 


IX C ONCLUSION The follow graph and the following feed are an integral component of social networks today Each poses its own data management challenges In this work we have focused on the following feed problem We have described the current following feed architecture at Pinterest and shared our experiences with building a system at scale which exploits the write heavy nature of the problem Having researched various popular storage technologies we chose HBase because of its superior write throughput wide spread usage and excellent read performance Additional advantages of HBase included automatic cluster management improved data consistency and durability We have also described a user facing and mission critical application on top of Apache HBase and HDFS HBase is widely used for powering analytics machine learning and other ofîine big data applications However there are only a few examples using  for po wering user facing highly available applications Our HBase deployment of the following feed is one such example We feel that this work would be useful to social networks as an example for scaling up their feed storage and serving systems We have also included a systemic description of how we achieved a low MTTR with HBas e a topic not particularly well understood in industry but indispensable for building highly available applications on top of Hadoop and HBase Such knowledge should be generally useful for people looking at Hadoop for real time serving R EFERENCES  D Comer  Ubiquitous b-tree  ACM Computing Surveys  vol 11 no 2 pp 121Ö137 June 1979  B F  Cooper  A  Silberstein E T am R Ramakrishnan and R Sears Benchmarking cloud serving systems with ycsb in 1st ACM symposium on Cloud computing  2010  Redis  http://redis.io  F  Chang J Dean S Ghema w at W  C Hsieh D A Wallach M Burrows T Chandra A Fikes and R E Gruber Bigtable A distributed storage system for structured data ACM Trans Computer Systems  vol 26 no 2 November 2008   Apache hbase  http://hbase.apache.or g  A Lakshman and P  Malik Cassandra a decentralized structured storage system Operating Systems Review  vol 44 no 2 pp 35Ö40 April 2010  P  OêNeil E Cheng D Ga wlick and E OêNeil The logstructured merge-tree lsm-tree vol 33 no 4 pp 351Ö385 June 1996  A Aiyer  M  Bautin G J Chen P  Damania P  Khemani K Muthukkaruppan K Ranganathan N Spiegelberg L Tang and M Vaidya Storage infrastructure behind facebook messages using hbase at scale Bulletin of the IEEE Computer Society Technical Committee on Data Engineering  2012  P  Hunt M K onar  F  P  Junqueira and B Reed Zook eeper Wait-free coordination for internet-scale systems in 7th Symposium on Operating Systems Design and Implementation  2010  M Burro ws The chubby lock service for loosely-coupled distributed systems in 7th Symposium on Operating Systems Design and Implementation  2006 pp 335Ö350   Apache hdfs  http://hadoop.apache.or g/hdfs  S Ghema w at H Gobiof f and S.-T  Leung The google le system in Symposium on Operating Systems Principles  vol 37 no 5 December 2003 pp 29Ö43  Ja v a se 6 hotspot[tm virtual machine garbage collection tuning http://www.oracle.com/technetwork/java/javase/gctuning-6-140523.html  J Dean Designs lessons and advice from b uilding lar ge distributed systems in 3rd ACM SIGOPS International Workshop on Large Scale Distributed Systems and Middleware  2009  D Borthakur  K  Muthukkaruppan K Ranganathan S Rash J S Sarma N Spiegelberg D Molkov R Schmidt J Gray H Kuang A Menon and A Aiyer Apache hadoop goes realtime at facebook in SIGMOD  June 2011 pp 1071 1080 783 


particularly given that some of our workload will be portalbased user-services for whom different metrics of service and performance will be necessary There is a tension here between the constraints of working with a shared massively parallel le system and massively scalable compute We believe the scale of our upgrades will both allow us to serve the user communities and to explore both sides of this debate Whatever the workîow users settle on whether batch or cloud orientated most will have to make signiìcant improvements to their workîow to exploit massive parallelisation We have presented an example of how such workîows can be developed within a constrained virtual environment before exploiting massive batch computing but thus far these sort of interventions are very resource intensive We do not have the resources to intervene on such a scale for most users and we do not yet have suitable documentation in place to help users develop their own solutions To that end we are also procuring improvements in documentation and training materials V S UMMARY We have presented the JASMIN architecture rst year of usage and near term plans The physical architecture consists of 600 cores and 5 PB of fast disk connected by low latency networking The compute environment supports a range of virtualisation options from batch to cloud computing A diverse and growing user community is exploiting JASMIN examples include high resolution climate modelling and whole satellite mission analysis for cloud and land surface retrievals The use of Panasas for storage has been very successful with exibility reliability and low management overheads being key to that success However the existing JASMIN environment is underpowered in compute the storage is lling and difìculties exporting the high performance disk into the local VMware cloud computing environment remain JASMIN users are becoming accustomed to a new analysis environment and early adopters are getting signiìcant improvements in their workîow completely changing the nature of the science they can undertake However thus far the JASMIN support team has not yet been able to invest in comprehensive user documentation or training so not all the community has seen the beneìts of these investments To fully exploit JASMIN changes in software and workîow will be necessary for most users and these will take time to engender Recognising the limitations with the existing physical infrastructure and with new user communities anticipated hardware software and documentation will all be upgraded over the next two years R EFERENCES  B N La wrence V  Bennett J Churchill M Juck es P  K ersha w  P Oliver M Pritchard and A Stephens The JASMIN super-datacluster ArXiv e-prints  Apr 2012  K E T aylor  R  J  Stouf fer  and G A Meehl  A n o v ervie w o f CMIP5 and the experiment design Bulletin of the American Meteorological Society  vol 93 pp 485Ñ498 Oct 2011 A v ailable http://journals.ametsoc.org/doi/abs/10.1175/BAMS-D-11-00094.1  D N W illiams B N La wrence M Lautenschlager  D  Middleton and V Balaji The earth system grid federation Delivering globally accessible petascale data for CMIP5 in Proceedings of the 32nd Asia-Paciìc Advanced Network Meeting  New Delhi Dec 2011 pp 121Ö130 A v ailable http://usymposia.upm.my/inde x.php APAN  Proceedings/32nd APAN/paper/view/155  M S Mizielinski M J Roberts P  L V idale R Schiemann M E Demory J Strachan T Edwards A Stephens M Pritchard P Chiu A Iwi J Churchill C D C Novales J Kettleborough W Roseblade P Selwood M Foster M Glover and A Malcolm High resolution climate modelling the UPSCALE project a large simulation campaign Geoscientiìc Model Development  vol In preparation 2013  J.-P  Muller  P  L e wis J Fischer  P  North and U Framer  The ESA GlobAlbedo project for mapping the earths land surface albedo for 15 years from european sensors in Geophysical Research Abstracts  vol 13 Vienna 2011 p 10969 A v ailable http://www.globalbedo.org/docs/Muller-GlobAlbedo-abstractV4.pdf  D Ghent and J Remedios De v eloping rst time-series of land surface temperature from AATSR with uncertainty estimates in Geophysical Research Abstracts  vol 15 2013 p 5016 Available http://adsabs.harvard.edu/abs/2013EGUGA..15.5016G  C A Poulsen R Siddans G E Thomas A M Sayer  R  G  Grainger  E Campmany S M Dean C Arnold and P D Watts Cloud retrievals from satellite data using optimal estimation evaluation and application to ATSR Atmospheric Measurement Techniques  vol 5 no 8 pp 1889Ñ1910 Aug 2012 A v ailable http://www.atmos-meas-tech.net/5/1889/2012  J Cohen B Dolan M Dunlap J M Hellerstein and C W elton MAD skills new analysis practices for big data Proceedings of the VLDB Endowment  vol 2 no 2 pp 1481Ñ1492 2009 Available http://dl.acm.org/citation.cfm?id=1687576  K Shv achk o H K uang S Radia and R Chansler  The hadoop distributed le system in Mass Storage Systems and Technologies MSST 2010 IEEE 26th Symposium on  2010 pp 1Ñ10 A v ailable http://ieee xplore.ieee.or g/xpls/abs all.jsp arnumber=5496972  H Herodotou H Lim G Luo N Boriso v  L Dong F  B Cetin and S Babu Starìsh A self-tuning system for big data analytics in Proc of the Fifth CIDR Conf  2011 A v ailable http://x86.cs.duke.edu  gang/documents/CIDR11 Paper36.pdf  J Buck N W atkins J Lefe vre K Ioannidou C Maltzahn N Polyzotis and S Brandt Scihadoop Array-based query processing in hadoop Technical Report UCSC-SOE-11-04 UCSC Tech Rep 2011  G Sak ellari and G Loukas  A surv e y of mathematical models simulation approaches and testbeds used for research in cloud computing Simulation Modelling Practice and Theory  A v ailable http://www.sciencedirect.com/science/article/pii/S1569190X13000658 75 


Copyright © 2009 Boeing. All rights reserved  Architecture Server-1 Server-2 DB2 SURVDB XML Shredder WebSphere Message Broker Ext.4 H Ext.3 G Ext.2 F Ext.1 E C WebSphere MQ TCP/IP Live ASDI Stream IBM Cognos Server-3 IBM SPSS Modeler SPSS Collaboration Deployment Services 


Copyright © 2009 Boeing. All rights reserved  Database Modeling Schemas for correlated ASDI messages translated into equivalent relational schemas  Database tables generated based on classes created from schema definitions  Nine main, eleven supporting tables  Each main table contains FLIGHT_KEY 


Copyright © 2009 Boeing. All rights reserved  Database Modeling 


Copyright © 2009 Boeing. All rights reserved  Correlation Process To archive received ASDI data  Track messages must be correlated with flight plan messages FLIGHT_KEY assigned Uncorrelated data tagged Approx 30 minutes to correlate one day of data 


Copyright © 2009 Boeing. All rights reserved  Historical Data Processing To load correlated data  Uncompress, unmarshall  Create a list of files containing the correlated data  Write data to warehouse 


Copyright © 2009 Boeing. All rights reserved  Live Data Processing Processed using IBM MQ IBM Message Broker and a technique called XML Shredding Message Broker Compute Nodes  Uncompress Node  Extract correlated messages  Shred Node adds to DB Stored Procedure ìshreds XML docs and adds to tables 


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a ìkey, valueî list using an XSTL  Queries made against this list of ìkey, valueî pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


