Extended Real-time Learning Behavior Mining Yen-Hung Kuo 1 Yueh-Min Huang 2 Juei-Nan Chen 3 Yu-Lin Jeng 4 Department of Engineering Science, National Cheng Kung University, Taiwan No. 1, Ta-Hsueh Road, Tainan 701, Taiwan, R.O.C keh 1 nan 3 jack 4 www.mmn.es.ncku.edu.tw, huang@mail.ncku.edu.tw 2 Abstract Based on our previous work [3  l e ar ni ng pat t e r n s  can be discovered and recommend to the learners. This paper extends the proposed problem to handle the 
questionable mining results. According to the learning patterns are discovered by using learning histories. It may be happened whenever the learners have ineffective learning behaviors, and we define them as questionable mining results. These ineffective behaviors may induce the bias suggestions. Therefore we propose a candidate sequence set generation process to take care the stumble learning behavior Keywords: data mining, stumble learning pattern 1 Introduction According to previous research w h ich prov ide s an approach to discover frequent learning patterns and divides sequence database into three sets to perform real-time learning behavior mining. This paper extends 
the proposed problem to handle the questionable mining results. For the extended of this article, we expand the learning sequence generation process. The generation process evaluates the learning effects, and produces the candidate learning sequences to sequence database. Besides, the generation process can prevent the stumble learning sequences to become a rule which may effect further learning activities. After appending sequence generation, learning behavior and learning histories construct a life-cycle relationship; they influence and improve each other 
Recently, web mining becomes an important issue which uses sequential mining techniques to discover the browsing behavior patterns over Internet Mining for learning behavior is a form of web mining which analyzes the navigating sequential patterns to infer what is the interested things of us  According to web mining technique, this paper proposes the algorithm for real-time learning behavior mining, and we modify some characteristics of web mining to fit in with learning activities. Besides, the 
patterns that we recognize are different with traditional sequential patterns, and our patterns are emphasizing the strong orderly relationship between learning sequences. For each recognized association rules, the present form shows a relationship between two learning sequences, ex. < A, B C, D > [sup 10%, conf m eans lea s t 10% of a ll s e q u en ce s   which shows < A, B > and < C, D > occur continuous and together. Furthermore, it also indicates when < A B > appears then in 50% cases < C, D > also appears successively The rest of this article is organized as follows 
Section 2 defines some notations and definitions use in this paper. Following the definitions, Section 3 shows the candidate sequence set generation approach Finally, section 4 draws our conclusions Table 1. Notations Symbols Semantics S A learning sequence S ab A learning sequence combined with S a and S b where S a 
 S b and S ab  S  Zero or more unknown elements in a sequence i A candidate elements set of the i th * in a sequence E Li The element in front of the i th * in a sequence E Ri The element behind the 
i th * in a sequence S c A candidate subsequence which is between E Li and E Ri in S xy  Proceedings of the Fifth IEEE International Conference on Advanced Learning Technologies \(ICALTê05 0-7695-2338-2/05 $20.00 © 2005 IEEE 


2. Definitions Based on previous research w e ref er t h e definition 1 to 4 in this paper, and append new definition to handle the sequence generation process We append wild card in this article to present the stumble learning behavior. Generally, after learner completing a learning activity, he/she should give some feedback to be his/her learning result. Suppose the learning feedback does not satisfy the threshold, the learning activity should be seen as a stumble learning behavior. Therefore, the stumble learning activity should not be recorded in the learning sequence, and the learning sequence should denote the activity as wild card. Definition 1 indicates the wild card definition, and then Section 1 introduces how to transfer stumble sequence to candidate sequence set Definition 1 Wild card \(*\ns zero or more unknown learning elements in a learning sequence Furthermore, wild card has some features as following in a sequence can be replaced by variable number of elements. Moreover, each element that replaced by * must be different and can not appear within the sequence. Ex. < A, *, B > can be replaced as A, B >, < A, C, B>, < A, C, D, B > or any other sequences satisfy this condition If * appears at beginning or end of a sequence, the has to drop from the sequence. Ex. < *, A, *, B has to become < A, *, B If more than one * appears successively, these continuous * can be seen as one. Ex. < A B, *, C > has to become < A, *, B, *, C 3. Generating candidate learning sequences Consider figure 1, it shows the pseudo code for GenerateCandidateSequences algorithm, which converts the learning sequences contain wild card characters to candidate learning sequences. In step 5-12, it creates candidate elements into candidate set  i hich belong to i th wild card \(*\aracter in input learning sequence. Step 6 put an empty element into candidate set i says the wild card \(*\ be replaced by empty element. In step 7-11, it creates candidate subsequence S c using learning sequences in believable set all conditions in step 8 are satisfied, each candidate subsequence S c as to put into candidate set n tep 13, after generating all candidate elements, then use these candidate elements to produce candidate sequences  Procedure GenerateCandidateSequences  S  begin 1.      let S a learning sequence which contains unknown learning elements 2.      let S xy valid learning sequences 3.      let n 1 4.      let N the count of wild cards in S 5 repeat  6.            put < > into n 7 for each do  8 if and \(                \ and E Ln is front of E Rn and then 9.                        put S c into n 10 endif 11 12 until  n  N  13 repeat S c in n to generate valid candidate learning sequence set end   xy S xy Ln S E  xy Rn S E  S S c  Procedure GenerateCandidateSequences  S  begin 1.      let S a learning sequence which contains unknown learning elements 2.      let S xy valid learning sequences 3.      let n 1 4.      let N the count of wild cards in S 5 repeat  6.            put < > into n 7 for each do  8 if and \(                \ and E Ln is front of E Rn and then 9.                        put S c into n 10 endif 11 12 until  n  N  13 repeat S c in n to generate valid candidate learning sequence set end   xy S xy Ln S E  xy Rn S E  S S c  Figure 1. Algorithm for generate candidate sequence set 4. Conclusions We have defined a wild card notation to present stumble learning patterns and proposed a candidate sequence set generation algorithm to improve our previous work. Besides, The candidate learning sequence generation uses wild card to replace stumble learning behaviors, and then produces candidate learning sequence set. The process diminishes the bias of mining results and gives a reasonable method to handle unsupervised learning behaviors  Acknowledgment The authors would like to thank the National Science Council of the Republic of China for financially supporting this research under Contract No NSC 93-2524-S-006-001 References 1 R. Coole y  P  N  T a n, a nd J  Sriv a s ta v a   D is c o v e r y o f  Interesting Usage Patterns from Web Data Proc. of the Intêl Workshop on Web Usage Analysis and User Profiling 1999 pp. 163 Ö 182 2 S   P a r t h a sarath y  M. J  Zak i, M.Og ih ara, a n d S  Dwarkadas, çIncremental and Interactive Sequence Mining Proc. of the eighth intêl conference on Information and knowledge management Kansas City, Missouri, USA, 1999 pp. 251 Ö 258  Y H Kuo  J N Ch en  Y M  Hu an g Real-tim e  Learning Behavior Mining Algorithm IEEE Learning Technology newsletter Vol. 6 Issue 4, October, 2004, pp 89-92 Proceedings of the Fifth IEEE International Conference on Advanced Learning Technologies \(ICALTê05 0-7695-2338-2/05 $20.00 © 2005 IEEE 


sharp boundaries because of classical set theory [1]. We can improve the problem by introducing fuzzy set theory. In fuzzy set theory, an element can belong to a set with a membership degree in [0, 1]. This value is assigned by the membership function associated with each fuzzy set. Fig. 1 is an example of fuzzy membership functions for age. The fuzzy set provides a smooth transition to tackle the sharp boundary problem. An example of stock price variation is given to show this transformation Example 1. Table 3 contains company A  s stock closing price variation data. If we consider the raise condition only, we can transform the data in Table 3 by partitioning the quantitative attribute  closing price variation  into 3 fuzzy intervals raise little, if price goes up from 0 to 4%}, {raise medium, if price goes up from 1.7 to 5.2%}, {raise high, if price goes up from 4% to 7%}. Fig. 2 shows the fuzzy set definition and the associated membership functions and Table 4 shows the results after transformation In Table 3, we can find that the company A  s closing price variation in day 1 is 3.5% and it has membership degree of 0.17 in  raise little  0.9 in  raise medium  and 0 otherwise That is, this record will contribute 0.17 to the support of the fuzzy set  raise little  and 0.9 to  raise medium  The sum of the item  closing price variation  in this record  s contributions to all fuzzy intervals is greater than 1. This means that the record will be counted 1.07 times for the item  closing price variation  This also means that the item in the record is more important than the same item in the other records. This is not reasonable since any item should have equal contribution in any transaction. To solve this problem, a further normalization process for the fuzzy attributes should be taken Let t = {t.v1, t.v2  t.vm} be a transaction with m attributes, where t.vj, 1d j d m, is the value of a certain fuzzy attribute j. Suppose l represents the maximum number of fuzzy sets for attribute j and Pi\(t.vj of t.vj in the ith fuzzy set. Then t.vj can be mapped to {\(Pi\(t.vj for all i, 1didf\(j follows  c  l i ji ji ji vt vt vt 1       P P P The above example can be further transformed from raise little, 0.17 raise medium, 0.9 raise little 0.143 raise medium, 0.857 2.2. Inter-transaction Association Rules Tung et al. proposed a framework for mining inter-transaction association rules that is based on Apriori algorithm [2]. In this section, we are going to introduce some important concepts and give some definitions before dealing with the problem of inter-transaction association rules mining Definition 1. Let I = {i1, i2  ik} be a set of items. Let D be a dimensional attribute and Dom\(D transaction database is a database containing records in the form \(d, Ij D 


form \(d, Ij D of database a 1-dimensional database The dimensional attribute is used to describe the properties associated with the items, such as time and location. It is assumed that the domain of the dimensional attribute is ordinal and can be divided into equal length intervals. For example, time can be divided into day, week month, etc. These intervals can be represented by integers 0 1, 2, etc., without loss of generality An inter-transaction association rule that spans across p intervals is found if an association exists between items that are p intervals apart. Many resources may be required to The 2005 IEEE International Conference on Fuzzy Systems792 discover all possible inter-transaction association rules that may span across different intervals. Besides, users may not be interested in the rules that span longer than a certain number of intervals. In order to avoid spending unnecessary resources to mine the rules which users are not interested in a sliding window denoted by w is introduced. When mining inter-transaction association rules, only the rules spanning shorter than or equal to w intervals will be discovered. Users can thus use sliding window to avoid mining the rules that span across lengthy intervals. The following is an example to illustrate this concept Example 2. Fig. 3 shows an 1-dimensional transaction database T with its dimensional attribute, trading day. In database, five transactions locate at intervals 1, 3, 6, 9, and 11 Assuming that the length of sliding window is 4, we will have five sliding windows W1, W2, W3, W4, and W5, with addresses of 1, 3, 6, 9, and 11, respectively. From Fig. 3, it can be seen that the sub-window W1[0] contains items a, b, e and g while the sub-window W1[3] contains c, f, and i Each sliding window forms a mega-transaction. A mega-transaction M that is contained within W will be denoted as follows M = {ik\(j   1}, where W is a sliding window with w intervals and u is the number of items in I = {i1, i2  iu In example 1, the mega-transaction in W1 will be {a\(0 b\(0 0 0 2 2 2 mega-transaction from the items in a traditional transaction the items in a mega-transaction are called extended items. We denote the set of all possible extended items as Ic. Given I and w, we will have Ic = {i1\(0  i1\(w-1 0  i2\(w-1  iu\(0   iu\(w-1 Now, we can define the concept of inter-transaction association rule Definition 2. An inter-transaction itemset is a set of extended i t e m s  B    I c  s u c h  t h a t   i k  0     B   1  d  k  d  u  Definition 3. An inter-transaction association rule has the form X ? Y, where 1. X ? Ic, Y ? Ic 2    i k  0     X   1  d  k  d  u    i k  j     Y   1  d  k  d  u   j  z  0  4. X ? Y Definition 4. Let MTxy be the set of mega-transactions that contains a set of extended items X ? Y and MTx be the set of mega-transactions that contains X. Let S be the number of transactions in the transaction database. Then, the support and confidence of an inter-transaction association rule X ? Y are defined as sup x xyxy T T confidence S T port 


port Like the mining algorithm for intra-association rule, a minimum support, minsup, and a minimum confidence minconf, will be given and our task is to discover the inter-transaction association rules from the transaction database with support and confidence greater than or equal to the minimum requirements III. THE PROPOSED INTER-TRANSACTION FUZZY ASSOCIATIONS Although the association rules mining research has fascinating progress in many directions, there is no algorithm that deals with quantitative inter-transaction association rules Two examples of stock price variations are given to compare the difference between the proposed inter-transaction fuzzy association rules and intra-transaction ones Example 3. Intra-transaction association rule: When the price of company A goes up, the price of company B also goes up the same day The above example reflects some relationships among the prices and it expresses the associations among the price variation in the same transaction \(day think the prediction has limited reference value and hope to know the associations among different transactions. The following example gives this idea Example 4. Inter-transaction fuzzy association rule: When the price of company A goes up 2% to 3%, the price of company B goes up 3% to 5% the next day Current mining algorithms cannot discover the rule like example 4. Tung et al. proposed a framework that can only discover inter-transaction association rules, whereas Srikant et al. proposed an approach to mine quantitative intra-transaction association rules. In order to discover quantitative inter-transaction association rules, a new method is developed to extract rules from 1-dimensional transaction database Our mining process of inter-transaction fuzzy association rules from 1-dimensional database can be divided into four steps: data preparation, quantitative attribute transformation the discovery of frequent inter-transaction itemsets, and association rule generation 3.1. Data Preparation The first step is to organize the transactions based on intervals of the dimensional attribute. For example, to find the long-term variation trend of stock prices across different weeks or months, we need to convert daily prices variation into weekly group or monthly group 3.2. Quantitative Attribute Transformation The second step is to map each quantitative attribute into its fuzzy intervals. Note that only the quantitative attributes The 2005 IEEE International Conference on Fuzzy Systems793 need to be transformed to its fuzzy intervals. After quantitative attributes are mapped to their fuzzy intervals, the number of items will increase. Let I = {i1, i2  ik} be the set of all items that belongs to the original database. In order to simplify the explanation here, we assume the database has only one quantitative attribute, ij, where 1 d j d k. If ij is mapped to l fuzzy intervals, then the new set of all items If will become as {i\(1, B 2, B  i\(j, 1 j, 2  i\(j, l j+1, B   i\(k, B r, B j,s is a fuzzy attribute For a quantitative attribute value x and its domain domx, if the membership function is f for a certain fuzzy interval A then we have fA\(x x x membership degree for that quantitative attribute in a certain fuzzy interval A While mapping a certain quantitative attribute into its fuzzy intervals, the summation of membership degrees for all fuzzy intervals may be greater than 1; therefore, an extra normalization process needs to be taken. Otherwise, the record will be counted more than once for that attribute 3.3. The Discovery of Frequent Inter-Transaction Itemsets 


3.3. The Discovery of Frequent Inter-Transaction Itemsets Let If be the new set of all items that is defined from the previous phase and W be a sliding window with w intervals along the dimensional attribute. We now redefine a mega-transaction M contained within W to be M = {i\(j, x t j, x  a binary attribute, else 1 d x d l if it is a fuzzy attribute, 0 d t d w  1 We also need to redefine the set of all possible extended items as fI c  = {i\(1, B 0  i\(1, B w - 1 2, B 0  i\(2, B w   1  i\(j, 1 0  i\(j, 1 w - 1  i\(j, l 0  i\(j l w  1 j+1, B 0  i\(j+1, B w  1  i\(k B 0  i\(k, B w  1 Now, we could proceed to find frequent itemsets. An inter-transaction k-itemset is the set B = {i\(1, x t1 2, x t2   i  k   x   t k     w h e r e  B     s u c h  t h a t   i f I  c   j  x   0     B   1  d  j  d  k   T o find the frequent itemsets, we can follow the Apriori algorithm to perform level-wise mining work, i.e., using inter-transaction frequent k-itemsets \(for k t 2 candidate \(k+1 Apriori algorithm. The processing cost of the first two iterations \(i.e., obtaining L1 and L2 mining cost [5]. The reason is that, for a given minimum support, we usually have a very large L1, which in turn results in a huge number of itemsets in C2 to process. In the inter-transaction association rules, this situation becomes much more serious as a lot of additional 2-itemsets like i\(j,x 0 j,x 1 amount of C2. In order to improve the performance EH-Apriori adopts a similar technique of hashing as [5] to filter out unnecessary candidate 2-itemsets. When the support of candidate C1 is counted by scanning the database EH-Apriori accumulates information about candidate 2-itemsets in advance in such a way that all possible 2-itemsets are hashed to a hash table. Each bucket in the hash table consists of a number to represent how many itemsets have been hashed to this bucket thus far. Such a resulting hash table can greatly reduce the number of 2-itemsets in C2 The following is a more detail description of how frequent itemsets are generated Step 1. Discover the frequent 1-itemset L1. In fact, the candidate set C1 of 1-itemsets is , the set of all possible extended items. We can scan the database from mega-transaction T fI c 1 to determine whether a special item i\(j,x t j,x t a binary attribute, else calculate its membership degree, if it is a fuzzy attribute. Through one scan of the database, L1 can be found Step 2. Generate the 2-item candidate set. The 2-item candidate set is of the form C2 = {{a\(0 t 0 t L1, \(t = 0 ? a &lt; b t z 0 b cannot be fuzzy attributes that come from the same quantitative attribute at the same time. For example, in Table 2, the fuzzy attribute young cannot join with fuzzy attribute middle age to form a 2-item candidate set Step 3. Reduce the size of C2. In order to reduce the number of 2-itemsets in C2, hash-based technique is used in this phase. We can hash all 2-itemsets like {a\(0 t contained in the current series of transactions into the corresponding buckets of a hash table and prune the unnecessary 2-itemsets from C2, whose corresponding bucket count in the hash table is less than the minsup. A single 2-itemset in its hash table bucket is not always counted as 1, that is, its support value is not always 1. A valid k-itemset B  s support value sup can be calculated as follows  Bi xj 


xj xj iB      P , where P\(i\(j,x j,x Boolean attribute, else P\(i\(j,x degree, if it is a fuzzy attribute Step 4. Discover the frequent 2-itemset L2. Now the size of C2 is smaller and then simply apply the Apriori algorithm to get L2 Step 5. Generate the k-item candidate set Ck and discover the frequent k-itemset Lk, where k &gt; 2. Given Lk-1, we can get Ck by join Lk-1 with Lk-1. Then all itemsets B?Ck that have k-1 in the pruning phase. Fig. 4 gives the detail algorithm of the join phase Step 6. Repeat step 5 until no more frequent itemsets can be found Step 7. Generate the association rules. The generation of inter-transaction association rules is similar to the generation of the classical association rules, except the The 2005 IEEE International Conference on Fuzzy Systems794 calculation of rules  confidence should be     x xy , where sup\(xy of items X ? Y and sup\(x that contains items X IV. EXPERIMENTAL RESULTS AND DISCUSSIONS In this section an example is given to illustrate how the proposed framework is applied to mining the inter-transaction fuzzy association rules Table 5 shows the original 3 stocks  price variation data To simplify our work, we only consider the raise condition for each stock. The quantitative attribute  closing price variation  is mapped to three fuzzy intervals, {raise little, if price goes up from 0% to 4%}, {raise medium, if price goes up from 1.7% to 5.2%} and {raise high, if price goes up from 4% to 7%}. Table 6 shows the result after transformation Next, we rename the fuzzy attribute {raise little} for company A to a1, {raise medium} to a2, and {raise high} to a3. The same rule applies to company B and C. Fig. 5 shows the result after this transformation. The sliding window length is set to 4 days, and we assume the minsup and minconf being 30% and 45%, respectively. Tables 6 and 7 indicate that the data increases and this will result in more itemsets added to candidate itemsets, especially to C2. In order to construct a smaller C2, we hash all the possible 2-itemsets first, and prune all the unnecessary 2-itemsets so that the corresponding bucket value in the hash table will become less than the minimum support. Then we generate 2-item candidate itemsets from L1, and we can see that there should be 6 candidates but now only three 2-itemsets left in Table 8.Table 7 shows the 1-item candidate set C1 and frequent 1-item set L1. Table 8 shows the 2-item candidate set C2 and frequent 2-item set L2. From Table 9, we can find an inter-transaction fuzzy association rule a1\(0 0 3  if company A  s stock price raise little and company B  s stock price raise medium the same day, then company C  s stock price will raise little three days later  The inter-association fuzzy rules we generate not only can predict the stock price variation trend but also tell us how big the variation and when this variation will happen in the future If we try to discover the classical intra-transaction association rules, we can get L1 = {\(stock A raise little stock B raise medium stock C raise little stock A raise little, stock B raise medium 


stock A raise little, stock B raise medium same minimum support and confidence values. The rule we get is  if company A  s stock price raise little, then company B  s stock price raise medium the same day  Comparing to inter-transaction fuzzy association rule, the prediction function from the classical association rules has limited use and investors may dislike them V. CONCLUSION In this paper we proposed a model to find inter-transaction fuzzy association rules that can predict the variations of events. The proposed algorithm first mapped a quantitative attribute into several fuzzy attributes. A normalization process was taken to prevent the total contribution of fuzzy attributes from being larger than 1. In order to mine inter-transaction fuzzy association rules, both the dimensional attribute and sliding window concepts were introduced into our framework. The method of finding frequent itemsets was based on Apriori algorithm. Since items may increase dramatically, a hash-based technique was implemented to help reduce the complexity. In fact, there still has space to discuss the strategy of candidate itemsets generation. The future work will include finding n-dimensional inter-transaction association rules. For example, by adding spatial attribute, one can mine spatial-temporal inter-transaction association rules in the remote sensing data ACKNOWLEDGMENT This work is supported by National Science Council, Taiwan R.O.C. under Grants NSC93-2213-E-036-024 NSC92-2516-S-036-001, and by Tatung University under Grant B93-I01-032 REFERENCES 1] A. Gyenesei  A fuzzy approach for mining quantitative association rules  Technical Report of Turku Centre for Computer Science, no. 336 March 2000 2] A.K.H. Tung, H. Lu, J. Han and L. Feng  Efficient mining of intertransaction association rules  IEEE Transactions on Knowledge and Data Engineering, vol. 15, no. 1, pp.43-56, Jan./Feb. 2003 3] A. Savasere, E. Omiecinski, and S. Navathe  An efficient algorithm for mining association rules in large databases  Proc. Int  l Conf. Very Large Data Bases, pp.432-443, Sept. 1995 4] H. Lu, J. Han, and L. Feng  Stock movement and n-dimensional intertransaction association rules  Proc. SIGMOD Workshop Research Issues on Data Mining and Knowledge Discovery, vol. 12, pp.1-7, June 1998 5] J.S. Park, M.S. Chen, and P.S. Yu  An effective hash-based algorithm for mining association rules  Proc. ACM SIGMOD Int  l Conf. Management of Data, pp.175-186, May 1995 6] R. Agrawal, T. Imielinski, and A. Swami  Mining association rules between sets of items in large database  Proc. of ACM SIGMOD Int  l Conf Management of Data, pp.207-216, 1993 7] R. Srikant and R. Agrawal  Mining quantitative association rules in large relation tables  Proc. of ACM SIGMOD Int  l Conf. Management of Data pp.1-12, 1996 Table 1. A database with a quantitative attribute  age  and a categorical attribute  gender   ID Name Gender Age 1 Peter Male 29 2 Mary Female 30 3 John Male 51 Table 2. The data transformed from Table 1. Gender is mapped to male and female. Age is mapped to [20, 29] , [30, 45], and [46, 99 ID Name Male Female Young 20, 29 Middle age 30, 45 Senior 46, 99 1 Peter 1 0 1 0 0 2 Mary 0 1 0 1 0 3 John 1 0 0 0 1 The 2005 IEEE International Conference on Fuzzy Systems795 


The 2005 IEEE International Conference on Fuzzy Systems795 Table 3. The original stock price variation data Day Company A  s stock closing price variation 1 0.035 2 0.015 3  Fig 1. An example of the fuzzy sets associated with membership functions.Table 4. The data  transformed from Table 3. The  closing price variation  is mapped to 3 fuzzy intervals, {raise little}, {raise medium}, {raise high Day Company A  s stock closing price variation Raise little Raise medium Raise high 1 0.17 0.9 0 2 1 0 0 3  Fig 2. The fuzzy sets and associated membership functions for Example 1 Table 5. The original stock price variation data Day T 1 a, b, e, g 2 3 c, f, i 4 5 6 a, e, d, h 7 8 9 a, e, d 10 11 b, c, f 12 13 14 Day Company A stock closing price variation Company B stock closing price variation Company C stock closing price variation W1 W2 1 0.019 0.036 0.056 4 0.018 6 0.0176 0.041 9 0.0177 0.042 0.01 10 0.009 0.018 W3 W4 W5 Table 7. The 1-item candidate itemsets and the corresponding support value 1-item candidate itemset Support value Frequent itemset minsup = 1.5 a1\(0 a1\(2 a1\(3 a2\(0 a2\(2 b1\(0 b1\(1 b2\(0 b2\(2 b2\(3 b3\(0 b3\(2 b3\(3 c1\(0 c1\(1 c1\(3 Fig. 3. An 1-dimensional database with a sliding window of 4 intervals Lk: frequent inter-transaction k-itemset, Ck: candidate inter-transaction k-itemset 


Lk: frequent inter-transaction k-itemset, Ck: candidate inter-transaction k-itemset Definition: itemi\(titemi titemj 1 2 D e f i n i t i o n   i t e m i  t i t e m i     i t e m j  t i t e m j    i f  a n d  o n l y  i f  e i t h e r  o f  t h e  c o n d i t i o n s  h o l d   1    t i t e m i    t i t e m j      i t e m i    i t e m j     2   t i t e m i   j Insert into Ck Select p.item1\(titem1 titem2   p.itemk-1\(titemk-1 titemk-1 From Lk-1 p, Lk-1 q Where p.item1\(titem1 titem1   p.itemk-2\(titemk-2 titemk-2 p   i t e m k 2  t i t e m k 1     q  i t e m k 1  t i t e m k 1  F i g   4   T h e  d e t a i l  a l g o r i t h m  f o r  j o i n i n g   w i t h   1  1  L Day T 1 a1, a2, b1, b2, c3 2 3 4 c1, c2 5 6 a1, a2, b2, b3 7 8 9 a1, b2, b3, c1 10 b1, c1, c2 11 12 13 W1c2\(0 c2\(1 c2\(3 c3\(0 W2 W3 Table 8. The 2-item candidate itemsets and the corresponding support value 2-item candidate itemset Support value Frequent itemset a1\(0 0 a1\(0 3 b2\(0 3 W4 Table 9. The 3-item candidate itemsets and the corresponding support value 3-item candidate itemset Support value Frequent itemset a1\(0 0 3 Fig. 5. The 1-dimensional data set which is transformed from Table 6. The sliding window length is 4 intervals W5 Table 6. The data transformed from Table 5 Company A stock closing price variation Company B stock closing price variation Company C stock  closing price variation Day Raise little Raise medium Raise high Raise little Raise medium Raise high Raise little Raise  medium Raise high 1 0.90 0.1 0.15 0.85 1 4 0.95 0.05 6 0.95 0.05 0.90 0.10 9 1 0.90 0.10 1 10 1 0.95 0.05 The 2005 IEEE International Conference on Fuzzy Systems796 pre></body></html 


0-7695-2263-7/05 $20.00  2005 IEEE pre></body></html 


n M L N n t n t n t n t L M L t L t L tt L t kkkk kkkk kkk kkkk kkkkkkkkP VK VK VK VK PP       kkP t 31 where L  s the error covariance associated with the state estimate t i    kkLX  tt kkk P1  00 0  0                     s s sss s s sss s s sssss N n t n t n 


n t n N n t n t n t n N n t n t n t n t n t n c t L kkkkkk kkkkk kP VKVK VKVK  32 4. Simulations One has run simulations comparing the sequential implementations of MSJPDA algorithm and the new algorithm here. A typical multisensor multitarget tracking environment is assumed in the simulations. According to article [1,3], One known that the performance of sequential MSJPDA is better than the performance of parallel MSJPDA. Therefore, the performance of parallel MSJPDA algorithm will not be compared here There are three sensors, which are fixed in three platforms. Regarding the 2nd sensor as fusion centre situation of the other sensors are: =?-500m?-500m 0m??N =?-500m? 500m?0m??The distance error of each sensor is: =300m, =200m, =100m?The bear error of each sensor is 0.03rad, =0.02rad, =0.01rad?The of sample is T=1s?The nonparametric model of clutter is used in the simulations, and expected number of false measurement is m=1.8 1 sN 3 s 1r 2 2r 3 3r 1 Simulations have been run for racking two targets. The true initialization state of the targets is X1?[-29500m,400m/s,34500m,-400m/s X ?[-26250m,296m/s,34500m,-400m/s]'? 2 The two targets will cross above 31seconds later. To evaluate tracking performance, 50 Monte Carlo runs were performed for three case of the target detection probability Pd=0.97 ? Pd=0.76 ? Pd=0.58. In every run, the total simulation time is 140 steps 


simulation time is 140 steps            Figure 1  RMS position error in case of Pd=0.97          Figure 2  RMS velocity error in case of Pd=0.97       Figure 3  RMS position error in case of Pd=0.76 567 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005         Figure 4  RMS velocity error in case of Pd=0.76         Figure 5  RMS position error in case of Pd=0.58          Figure 6  RMS velocity error in case of Pd=0.58  Table 1 The emanative times comparison for sequential MSJPDA and SD-CMSJPDA algorithm  Pd N A  0.97 0. 76 0.58 Sequential MSJPDA 2 11 17 SD-CMSJPDA 0 3 5 Pd denotes detection probability, N denotes emanative 


Pd denotes detection probability, N denotes emanative times, A denotes the kind of algorithm Table 1 shows the summation of emanative times for sequential MSJPDA and SD-CMSJPDA algorithm in 50 Monte Carlo simulations. From table 1 , it is shown that the stability of SD-CMSJPDA is better than that of sequential MSJPDA as the detection probability varied Figure 1,2 show the RMS errors for position and velocity in case of Pd=0. 97, respectively; Figure 3,4 show the RMS errors for position and velocity in case of Pd=0.76 respectively; Figure 3,4 show the RMS errors for position and velocity in case of Pd=0.58, respectively. From the figures we can see that the average RMS position error is lower for the SD-CMS JPDA algorithm. We also see that the state estimation precision of sequential MSJPDA get worse as the detection probability decreases The reasons for these simulation results lies:1 state estimation precision will get worse when the detection probability decrease;2 algorithm is to process measurement from each sensor using single sensor JPDA algorithm sequentially. Therefore the estimation error from each sensor will be accumulated Moreover, the sequential MSJPDA algorithm can  t improve the joint detection probability of the multisensor system The estimation error of the SD- CMSJPDA  algorithm will not be accumulated for it processes the measurement from each sensor directly in the mean time .What  s more the new method can greatly improve the joint detection probability of the multisensor system. Therefore, the tracking performance of SD-CMSJPDA algorithm is better than that of sequential MSJPDA. Algorithm All of the simulations are run in the personal computer with a 2.0G CPU and a 256M memory. The average cost time per step is 0.0251 in the sequential implementations of MSJPDA algorithm. And the average cost time per step is 0.0282 in the sequential implementations of MSJPDA algorithm. According to the results we can see that there is few difference in real time between the new method and the sequential   MSJPDA when there is not so many sensors and targets 568 Proceedings of  the Fourth International Conference on Machine Learning and Cybernetics  Guangzhou, 18-21 August 2005  5. Conclusion In order to solve the problem of multisensor multi target tracking, a new centralized multisensor  joint probabilistic data association  algorithm is proposed in this paper. The simulation results shows that the tracking performance of the new algorithm is better than that of the sequential MSJPDA algorithm The computational complexity of the new method will increase as the number of sensors and targets grow Therefore, how to improve the real time of SD- CMSJPDA algorithm will be pay attention References 1] He You, Wang Guohong, Lu Dajin, Peng Yingning Multisensor Information Fusion With Application[M Publishion House of Electronics Industry. 2000, Beijing.  [11] B..Zhou and N.K.Bose Multitarget  Tracking in Clutter:Faste Algorithms for Data Association .IEEE Transaction on Aerospace and Electronic Systems 1993,29\(2 2] Bar-shalom,Y\(Ed Applications and Advances,2: Norwood,MA Artech  House, 1992 3] L.Y. Pao, C.W.Frei. A Comparison of Parallel and Sequential Implementation of a Multisensor Multitarget Tracking Algorithm. Proc. 1995 American Control Conf. Seattie, Washington,June 1995 1683~1687 


4] K. Chang, C. Chong, Bar-Shalom, Joint Probabilistic Data Association in Distributed Sensor Networks IEEE Transactions on Automatic Control, AC-31\(10 octobre 1986 5] Bar-shalom, Multitarget Multisensor Tracking Advanced Application ,:YBS Publishing, 1990 6] Bar-shalom, Multitarget Multisensor Tracking Principles and Techenices ,:YBS Publishing, 1995 7] Hu Wenlong, Mao shiyi, Multisensor Data Association Based on Combinatorial Optimization[J]. Journal of Systems Engineering and Electronics . 1997 NO.1,1~9 8] Pattipati K R, Passive Multisensor Data Association Using a New Relaxation Algorithm.In Multitarget-Multisensor Tracking: Advanced and Applications,Y.Barshalom,Norwood,MA:Aretech,199 0 9] Deb S,et al, An S-Dimentional Assignment Algorithm for Track Initiation ,Proc. Of the IEEE Int. Conf Systems Engineering, Kobe, Japan,Sept 1992 527~530 10] Deb S,et al, A Multisensor-Multitarget Data Association Algorithm for Heterogeneous Sensors[J].IEEE Trans. on AES 1993 ,29 \(2 560~568 12] Bar-shalom,Y.,and Fortmann,T.E  Tracking and Data Association  New York:Academic press,1988 13] J,A,Roecher and G.L.Phillis,Suboptimal Joint Probabilistic Data Association .IEEE Transaction on Aerospace Electronic Systems.1993,29\(2 14] J,A,Roecker,A Class of Near Optimal JPDA Algorithm IEEE Transaction on Aerospace and Electronic Systems,1994,30\(2 15] Han Yanfei, Analysis and Improvement of Multisensor Multitarget Probabbilistic Data Association Filting Algorithm[J].  Journal of Systems Engineering and Electronics, 2002, Vol.24, 36~38  569 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207ñ216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intíl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intíl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





