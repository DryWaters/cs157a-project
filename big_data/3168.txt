A Fast Association Rule Algorithm Based On Bitmap and Granular Computing T.Y.Lin Xiaohua Hu Eric Louie Dent of Comnuter Science Colleae of Information Science IBM Almaden Research Center I San Jose State University Drexel University San Jose California 95192 Philadelphia PA 19104 tylin@cs.sjsu.edu thuOcis.drexel.edu Abstracf-Mining association rules from databases is a time consuming process Finding the large item set fast is the crucial step in the association rule algorithm. In this paper we present a fast association rule algorithm \(Bit-AssoRule\based on granular computing. Our Bit-AssocRule doesn\222t follow the generation-and-test strategy of Apriori algorithm and adopts the divide-and-conquer strategy thus avoids the time 
consuming table scan to find and prune the itemsets all the operations of finding large itemsets from the datasets are the fast bit operations based on its corresponding granular. The experimental result of our Bit-AssocRule algorithm with Apriori, AprioriTid and AprioirHybrid algorithms shows Bit AssocRule is  to 3 orders of magnitudes faster. Our research indicates that bitmap and granular computing can greatly improve the performance of association rule algorithm and are very promising for data mining applications I INTRODUCTION An association rule in a transaction database is an expression X Y where X and Y are sets of items XnY  4 1,2 
Given a set of transaction D the problem of mining association rules is to generate all association rules that meet certain user-specific minimum support and confidence The problem can be decomposed into two subproblems 1 finding all combinations of items that have transaction support above the minimum support 2 use the large itemsets to generate the desired rules A lot of association rule algorithms have been developed in the last decades 1,2,3,4,6,9,11,12,13,14 which can be classified into two categories 1 candidate-generation-and-test approach such as Apriori 2 2 pattern-growth approach 6,12,13 The challenging issues of association rule algorithms are multiple scan 
of transaction databases and huge number of candidates In this paper we present a novel association rule algorithm Bit-Assoc based on bitmap and granular computing approach Traditional Apriori algorithms require full table scan and multiple passes of the itemsets in order to finding association rules from large database Our Bit-AssocRule avoids these time-consuming operations and relies on the fast bit operations of its granular to find the large itemsets With bitmap techniques we can greatly improve the performance of the association rule algorithms The rest of the paper is organized as follows: we discuss granular computing and bitmap techniques in Section 2 In Section 
3 we present the granular-based association algorithm Bit-AssocRule and the comparison results of Bit AssocRule with various Apriori algorithms Apriori 650 Hany Road San Jose CA 95120 ewlouie@almaden.ibm.com AprioriTid and AprioriHybrid Section 4 concludes the paper with some discussions 11 GRANULAR COMPUTING AND BITMAP TECHNIQUES Granular computing was first proposed by TY Lin 7 and bas become a very important tool in data mining since then A granule is a clump of objects drown together by indistinguishability similarity proximity or functionality The equivalent relations are the granules of the relation Each unique attribute value in the relational table is a 
granule and each granule is a list of tuples that have the same attribute value Only the tuple name, or the reference to the tuple, is stored in the granules The bitmap technique was proposed in the 1960\222s SI and bas been used by a variety of products since then. Bitmap technique delivers far superior query performance on unselective low cardinality data than traditional B-Tree indexing techniques. Bitmap represents each distinct value as arrays of bits where a 1 or 0 in each relative position in the array represents True or False for that value for the corresponding relative record within the database relational table. This approach is sometimes referred as inverted-list But the real benefit of the bitmap index is the processing speed Combining 
the process of performing a logical operation AND OR or NOT on a serious of bitmaps is very efficient particularly compared with performing similar processes on lists of tuple-Ids The granular concept and bitmap index are very closely related to each other Other than a list the granular can be represented in bitmap Each tuple in the relation has one unique offset position in the bitmap The bits are set to 1 for those tuples having the attribute value of the granule and the bits are set to 0 for those tuples not having the attribute value of the granule. This is the bitmap representation of 
the granules of the lists 111 AN ASSOCIATION RULE ALGORITHM BIT-ASSOCRULE The most influential algorithm Apriori \(as described below developed by Rakesh etc 1,2 generates the k-candidate by combining two k-I that have the first k-2 attribute values in the two k-I the same the last pair does not. A new k-candidate becomes a k large itemset if every fi-I of the k-candidate is a large itemset otherwise it is removed. This algorithm need to do table scan of the 0-7803-781 0-5/03/$17.00 a003 IEEE 678 The IEEE International Conference on Fuzzy Systems 


whole data set and examine the item sets multiple times, the process is very time consuming Algorithm Apriori Ll  large I-itemset For k=2 Lk.l  kt do begin Ck  apriori-gen\(Lk  new candidate Forall transactions t E D do begin C subset\(Ck 1  Candidates contained in t Forall candidates c E C,do end Lk  c E Ck I c.count  minsup c.count  End Answer  ut Lk Lk Set of large k-itemsets those with minimum support Each member of this set has two fields 1 itemset and 2 support count ck Set of candidate k-itemsets potential large itemsets Each member of this set has to fields I itemset and 2 support count The aprior-gen function takes as an argument Lk the set of all large k-1 It returns a superset of the set of all large k-itemsets First in the join step Lk join with Lk., to obtain a superset of the final set o candidates Ck The union p U q of itemset p q E  is inserted in Ck if they share k 2 first items Recently many attempts have been given to applying bitmap techniques in the association rule algorithms in 7,8,10,15,l8,19 The use of bitmaps improves the performance to find association rules The bit representation of bitmaps offers efficient storage while the intersection of bitmaps offers fast computation in finding association rules The AND SHIFT and COUNT operations among bitmaps are extremely fast Unlike the traditional Apriori algorithm which generates k-candidate by combining two k-I large itemsets Our Bit-AssocRule algorithm generates k candidate by intersecting the hitmap of 1 attribute value with bitmaps of other k-1 attribute values Below we describe the algorithm in details A The Generation of Combinations Creating combinations or pattems is a computational process of forming sets of attribute values. The number of combinations for one selected attribute is the number of distinct values of the selected attribute Each candidate contains one attribute value from that selected attributes is the Cartesian product q1 x q2 where q and q2 are the number of distinct values the first and second selected attributes respectively Each combination contains two attribute values one attribute value form the first and one attribute value form the second For the general case the number of k-candidates is q1 x q2 x q3 x  xqk where qi is the number of distinct values of the i attribute Each k candidate contains one attribute value from each selected attribute The more general case is to create k-candidate among the n attributes in the relation Since no columns are selected beforehand k unique attributes are chosen from the n attributes in the relation and from those chosen k attributes all combinations of attribute values form the domains of those attributes are formed There are C\(n.k possible ways to choose k attributes form n attributes and each possible way has its own set of k-candidates among the k attributes In short the number of k-candidate is the total sum of the each subtotal combination from each possible way to select k attributes The general equation for the total number of k-candidate on n attributes in the relation is the following Number of candidate of Length k 221k-l qg  Zh=g+l\224*-\222 qh i=h+ln\221k-3 qi lo qz  so far the equation deals with various number of distinct values on the columns in the relation For the special case suppose all attributes in the relation have the same number of distinct values F the equation simplifies to C\(n,k Generally, the number of distinct values of each attribute in the relation are not the same Nonetheless taking the average number of distinct values of all the attributes may he useful to estimate the number of combinations of length k c\(n,k  p=l\222qs For a relation with many attributes or attribute values, the number of combinations can he very large Each combination requires a count of the number of tuples in the relation that the combination contains The combinations with the count greater than the minimal support are association rules The next subsection demonstrate methods to reduce the number of combinations In doing so the number of comparisons between the combinations and the tuples in the relations are saved The number of combinations is an important factor to consider in the process Each combination has the bitmaps in the combinations intersected and the result counted If a combination does not have the potential of becoming a large itemset the Combination should not be undergoing this process So only potential combinations are generated in the process The algorithm starts with a list L which contains all the 1 itemset all the counts of the hitmaps of these 1-itemset are greater than the minimal counter number When making k candidates all the elements in the list L are verified if they exist as elements in any k-I If an element does not exist in any k-1 it is removed from the list Next new k-candidates are created from the k-l and the list L by joining a  with element in L that has an attribute index greater than all attribute indexes of elements in that k-1 Only the new k candidates that have every k-1 a large itemset are kept Algorithm Bit-AssocRule LI  bitmaps of large I-itemset For  k=2 Lk   k do begin 679 The IEEE International Conference on Fuzzy Systems 


Remove those elements in Ll which are not included in Ck  Bit-apriori-gen\(L L  new candidate Lk  c E ck 1 bitmap count of c  minsup any itemset of Lk.1 prune the LI End Answer  uk Lk The algorithm starts with a list L which contains attribute values also called I-itemset all the counts of the hitmaps of these 1-itemset are greater than the minimal counter number The k-candidates consist of k attribute values X _ Xk.,,,k I from k attributes Using bitmap techniques the candidate is a large itemset if the hit count on the intersection of all the bitmaps B,d2n  nBr suppose 9 is the hitmap of the Xk,cl is equal or greater than the minimal count The bit count is the numher of 1's in the bitmap indexes from the result of the intersection of the hitmaps Below is an example of creating 4-combinations from 3 itemsets Suppose the following are the current 3-itemsets x6.0 xll.0 x12.0 x6.0 xll.0 xl4,2 x 0 XI 0 XI5 2 X6.OX11.1 x12.3 x6.Oxll,l x14,2 a xl1.l x15,2 x6.0 x14.2x1S.2 I xll.0 XlZ,O x6.l xll.2 XI xII.Ox14.2 Xl5,21 XII,IXI4,2X15.2 And suppose the following are the 1-itemset in the list First since o lo are not in any 3-itemsets thus and are removed from the list L Any 4-combinations that include these attribute values would not he an large itemset Below are the remaining attribute values in the list Next each 3-itemset is combined with attribute value in L to create 4-candidates if possible This 3-itemset X6.0 Xll,o X12,o is combined with these two attribute value in L X14.2 and this 3-itemset is combined with one attribute value in L XI~.2 and so on with each 3 itemset All the 4-combinations are shown below Ll 0.x4,10.~6.0.~6.1,~ll.0,~II.I.~12.0.~12,3 x14,2.xI5.2 6.0.~6.l,~ll,0,~ll,l.~I2.0.~12,3 x14,2,XI5,2 X6.0 x11.0 xIZ.Ox14.2 x6.0 xll,O Xl2.0XI5,Z x6.0 x11.0 x14.2 x15.2 x6.0 xI1.1 x12.3 X14,Z fX6.0 xll.1 x12.3 xI5.2 x6.0 xll,l x14.2 x152 x6.l xll.1 xl2,3 xI5.2 O xII.1 x14.2 x15.2 I Xll.0 XI20 x14,2 x6.1 Xll.0 x12.0 XI5,2 x11.1 X12,3X14.2 I XII.1 XI2,3Xl5.2 All the 4-combinations are generated and their bitmaps are counted. If the bitmap counter of the 4-candidates is greater or equal to the minimum support, then it becomes a 4 large itemset, otherwise it is deleted B The Storage and Management of Bitmaps The bitmaps are stored on disk in data pages The data pages offer flexibility in accessing and processing the bitmaps Portions of the hitmaps called slices are stored in the data pages and the data pages are connected by links Below shows an example of several data pages holding three bitmaps Each bitmap has its initial page on a different page,, The initial pages for each hitmap are not necessarily consecutive Su pose there are 3 different values xj Xlkl from the I fl  J 4h and 1 attributes in the data set, their hitmaps are Bj,u B1.u  Bitma Each bitmap requires the same number of data pages to store the binary representation of the list and for this example, let's assume that each hitmap has 5 data pages For the combination X~.L X,.U Xlkl the bitmaps Bisl Bj,u Bl,k3 are intersected and the result is counted. Since the bitmaps are stored in data pages the data pages 1,2,and 5 are intersected first, and the bits in the result are counted Next pages 3 4 and 6 are intersected, and the bits in this intersection are counted This subtotal is added to the previous subtotal. This continues on for the third forth and fifth pages of the bitmap Bicl B,,u and Bl,u After the last page the combination Xi.kI Xi,u,Xlk3 is a large itemset if the total counter is greater than the minimal support Through the intersection and bit counting many data pages are read among the k-combinations in each cycle Even though the bitmaps in each k-combination do not repeat within the k-combination the bitmaps in one k-combination may appear among other k-combinations. This means that data pages read for one combination may be needed for other combinations So to reduce the physical data pages reads for bitmaps all the nL data pages on each hitmap from each k-combination are processed at the same time In other words all the nL data pages on each hitmap are intersected the results of the intersections are counted and the counts are added to each combination's subtotal before continuing to the next data pages on each combination As a result the data pages for each bitmap are read once from disk to memory for k-combinations in determining which are large item sets The total storage cost for bitmaps is based on the number of bitmaps in the relation and the number of data pages per bitmap The number of pages per bitmap is dependent on the number of tuples in the relation and the number of tuples that can he stored per pages The equation for the total storage cost is the following Storage-cost  Number of bitmaps  Number of tuples  Max bits per pages 1  Datagage-size For example the relation has 1000 bitmaps and 1000,000 tuples Each data page is 4096 bytes, and 4080 out of 4096 680 The IEEE International Conference on Fuzzy Systems 


bytes are available to store data for bitmaps the last 16 byte is used as a pointer to point the next page in the bihnap Then the following is the total storage costs Storage cost  1000\222  rioooooo I 4080\2228 1  4096  125Mbyte Data set DSI DSZ DS3 C Comparison Between Apriori Its Variations and Bit AssocRule Algorithm The traditional Apriori algorithm its variations and bitmap-based Bit-AssocRule are compared based on their key operations For the Bit-AssocRule the number of AND operations between the bitmaps determines the cost For the Apriori algorithm, the number of comparisons between the attribute values in the candidates and the tuples determines the cost A ration is used to compare the two algorithms Apriori and Bit-AssocRule X Number of Comparison operations in Apriori I Number of AND operations in Bit-AssocRule For the Aprioir algorithm if there are p candidates and q tuples in the relation p*q comparisons are needed to determine the count for the p candidates A hash-tree table is used to reduce the comparisons The node of the bash tree table is either a leaf node containing some candidates or an interior node containing a hash table 2 The hasb table consists of references to the interiors nodes or leaf node The bash-tree table reduces the candidates to compare on each tuple by didiving the p candidates among the leaf modes All the p candidates have the same length k Each tuple in tbe relation has C\(n,k subsets and each subset is hashed on the hash-tree table For subsets that hash onto a non-visited leaf node each candidate in those leaf node is compared to the tuple The subsets that hash to a visited leaf nodes or interior node are skipped This is the subset function that is applied each tuple in the relation The cost of the subset function is determined by two factors the number of subsets per tuple and the number of candidates for all visited-once nodes per tuple The equation is m\(c\(n.k  Z;.,\224\(v,*t The first term is the costs of calling the bash function for m tuples in the relation The second term is the sum cost of each tuple\222s comparisons The vi is the number of visited-once leaf nodes per tuple and t is the average number of candidates per leaf node n is the number of attributes in the relation and k is the current length of the candidates in the hash-tree table Apriori is improved further by short-circuiting the comparisons on each candidates to each tuple and by comparing the k-candidates to a  bar table consisting of k-I ids The first improvement bas the comparisons stop once the first element in the k candidate is not contained in the tuple The second improvement uses the k-J bar table to compare only the k-I ids of the tuples Basically when a k-candidate is compared to a tuple in the k-I bar table the two E-I ids that generated the k candidate is compared to the k ids in the Ro Colu of Table Bitm Mini ws mn ite size ap mal ms size Supp ort 400 16 199 25.6MB 10.6 20K K MB 800 20 247 64MB 25.0 40K K MB 1.0 30 709 120MB 90M 50K tuple. If both k-1 ids exist for that tuple the k candidate id for this k-candidate is inserted into the k candidate bar table for that tuple For more detail, refer to 2,9 The Apriori and ApriorTid algorithms generate the candidate itemsets to be counted in a pass by using only the itemsets found large in the previous pass without considering the transaction in the databases The AprioriTid algorithm has the additional property that the database is not used at all for counting the support of candidate itemsets after the first pass AprioriHybrid uses Apriori in the initial passes and switches to AprioriTid when it expects that the set of candidate itemsets at the end of the pass will fit in the memory 1,2 The worst case for the Apriori algorithm is m*\(k*q comparison operations where m k defined above and q is the average number of attribute values per candidates. Eacb candidate compares with each tuple to determine if it is contained in the tuple The worse case for Bit-AssocRule algorithm is m/32 k-l AND operations So the ratio is the following X m  r*q  m/32*\(r-l  32  k I k-1 The Bit-AssocRulemethod can execute 32 times faster or more than Apriori in theory. The subset function in Apriori lowers the costs if the subset function reduces the number of r-candidates to compare to each tuples in the relation However the cost varies per tuple in the relation and is affected by the number of attribute in the relation D Experimental Runs Apriori, AprioriTid, AprioriHybrid and Bit-AssocRule Three different data are generated to compare the run time on these algorithms to find association rules The data varies in the number of tuples the attributes per relation and the minimal support The program for Apriori AprioriTid and AprioriHybrid are our honest implementations of the algorithms in 2 In the implementation we use some buffer scheme to speedup readwrite for all algorithms In the implementation we use some buffer scheme to speedup readwrite for all algorithms The tests were conducted using an IBM PC with 933Mhz CPU 512MB memory under Window 2000 The program is coded in C lM I IB e L  BitAsso Aprio Apriori Apri Cand item Rule ri Tid ori 681 The IEEE International Conference on Fuzzy Systems 


n set g t 2 16333 103 18.426s 1402 1669.18 1403 Hybr id Total time I I  _I Table 2 Experimental Run of 3 Data Sets Here are some observations and explanations on the results The total time of our comparison includes the time to write the association rules to a file; Bit-AssocRule is 2 to 3 orders of magnitude faster than the various Apriori algorithms 64 221 times faster\The big the test data set the big the time difference between the Bit-AssocRule and the various Apriori algorithms We haven\222t compared our algorithm with some of the other association rule algorithms such as VIPER 15 CHARM 19 CLOSE 6 CHARM and CLOSE are based on the closed frequent itemsets concept but based on their published comparison results with Apriori our Bit-AssocRule is very competitive compared to them and a direct comparison will be conducted and reported in the near future Bit-AssocRule takes the same or litter longer time than the various Apriori algorithms in constructing the I-itemsets because of the extra cost of building the bitmaps for the 1 itemsets But after the 1-itsemtset is done Bit-AssocRule is significant faster than the Apriori algorithms in constructing large frequent itemsets because it only uses the fast bit S os 673.288 68286 68267.5 5144 S s34s 34s 2.56 IQ operations \(AND COUNT and SHIFT\and doesn\222t need to test the subsets of the newly candidate Bit-AssocRule only stores the hitmaps of the frequent items, and the bitmap storage \(uncompressed\is less than the original data set U2 to 1/4 of the original data size The main reasons that Bit-AssocRule algorithm is significant faster than Apriori and its variations are Bit-AssocRule adopts the divide-and-conquer strategy the transaction is decompose into vertical bitmap format and leads to focused search of smaller domain There is no repeated scan of entire database in Bit-AssocRule Bit-AssocRule doesn\222t follow the traditional candidate-generate-and test approach thus saves significant amount of time to test the candidates In Bit-AssocRule the basic operations are hit Count and hit And operations which are extremely faster than the pattern search and matching operations used in Apriori and its variations IV CONCLUSION We present a bitmap based association rule algorithm using granular computing technique and introduce the bitmap technique to the data mining procedure and develop a bitmap based algorithm \(Bit-AssocRule\to find association rules Our Bit-AssocRule avoids the time-consuming table scan to find and prune the itemsets all the operations of finding large itemsets from the datasets are the fast bit operations The experimental result of our Bit-AssocRule algorithm with Apriori AprioriTid and AprioirHybrid algorithms shows Bit-AssocRule is 2 to 3 orders of magnitude faster This research indicates that bitmap and granular computing techniques can greatly enhance the performance for finding association rule and bitmap techniques are very promising for the decision support query optimization and data mining applications Bitmap technique is only one way to improve the performance data mining algorithm. Parallelism is another crucial aspect of DSS and data mining performance We are currently working on paralleling the hitmap-based algorithms and hope to report our fmdings in the near future REFERENCES l Agrawal R Srikant R 223Fast Algorithm for Mining Association Rules\224 Prod of the 20th VLDB conf. 1994  Agrawal R Mannila H Srikant R., Toivonen H Verkamo A 223Fast Discovery of Association Rules\224 in Advances in Knowledge Discovery and Data Mining MIT 1996 3 Aganval R Agganval C., Prasad V 223A Tree Projection Algorithm for Generation of Frequent Itemsets\224 Journal of Parallel and Distributed Computing 2002 141 Bayardo R.J.Jr Agrawal R Gunopulos D 223Constraint-Based Rule Mining in Large Dense 682 The IEEE International Conference on Fuzzy Systems 


Databases\224 Proc of\222 the ISth Int\221l Con on Data Engineering ICDEI999  Bertino E Ooi B.C Sacks-Davis R etc 223Indexing Techniques for Advanced Database Systems\224, Kluwer Publisher 6 Han J Pei J Yin Y 223Mining Frequent Patterns without Candidate Generation\224 Prod of the SIGMOD-2002 7 Lin T.Y 223Data Mining and Machine Oriented Modeling A Granular Computing Approach\224 Journal of Applied Intelligence, Oct. 2000 8 Louie E Lin T.Y 223Finding Association Rules using Fast hit  Computation Machine-Oriented Modeling\224 ISMIS-2000 9 Mannila H., Toivonen H Verkamo A Efficient Algorithms for Discovering Association Rules\224 in KDD94 lo Morzy T Zakrzewicz M 223Group Bitmap Index A Structure for Association Rules Retrieval\224 Prod of the 4\222 Int\222l Conf on Knowledge Discovery and Data Mining KDD-98 ll Pasquier N Bastide Y Taouil R Lakhal L 223Discovering Frequent Closed Itemsets for Association Rules\224, 1CDT2000 I21 Pei J Han H Lu S Nisbio S Tang, and D Yang 223H-Mine: Hyper-structure Mining of Frequent Patterns in Large Databases\224 Proc The 2001 IEEE Int\222l Conference on Data Mining I31 Pei J Han J Lakshmanan, \223Mining Frequent Itemsets with Convertible Constraints in ICDE2001 I41 Savasere  A Omiecinski E Navathe S 223An Efficient Algorithm for Mining Association Rules in Large Databases\224 in Prod. of the 21\224 VLDB conf 15 Shenoy P et al, Turbo-charging vertical mining of large databases in SIGMOD 00 16 Wu M Bucbmann A 223Encoding Bitmap Indexing for Data Warehouse\224, Proc of the 14th Int\222l Conference on Data Engineering, 220-23 1 1998 I71 Zaki M 223Generating Non-Redundant Association Rules\224 in KDD-2002 1181 Zaki M.,et al New Algorithms For Fast Discovery of Association Rules In KDD97 19 Zaki M Hsian C.J 223CHARM An Efficient Algorithm for Closed Association Rule Mining\224, Tech Report CS dept., RPI, USA 683 The IEEE International Conference on Fuzzy Systems 


not share an y itemsets with the b oundary of an y itemset X k 2X whic hisac hild of X  In other w ords for eac h c hild X k 2X of X w e remo v e from F  X c  all mem ber itemsets in F  X k c   Then these pruned b oundaries ma ybe used in order to generate the rules The resulting algorithm is illustrated in Figure 6 This algorithm uses as input the itemsets X whic h are generated in the 014rst phase of the algorithm at the appropriate lev el of minsupp ort  The algorithm FindBoundary of Figure 5 ma y b e used as a subroutine in order to generate all the b oundary itemsets These b oundary itemsets are then pruned and the rules are generated b y using eac h of the itemsets corresp onding to the b oundary in the an teceden t 4.1 Rules with constrain ts in the an teceden t and consequen t It is easy enough to adapt the ab o v e rule generation metho d so that particular items o ccur in the an teceden t and/or consequen t Consider for example the case when w e are generating rules from a large itemset X  Supp ose that w e desire the an teceden t to con tain the set of items P and the consequen t to con tain the set of items Q W e assume that P  Q 022 X  W e shall refer to P as the ante c e dent inclusion set  and Q as the c onse quen t inclusion set  In this case w e need to rede\014ne the notion of maximalit y and b oundary itemsets A v ertex v  Y  is de\014ned to b e a maximal ancestor of v  X  at con\014dence lev el c an teceden t inclusion set P  and consequen t inclusion set Q if and only if P 022 Y  Q 022 X 000 Y  S  Y  S  X  024 1 c  and no strict ancestor of Y satis\014es all of these constrain ts Equiv alen tl y  the b oundary set con tains all the itemsets corresp onding to maximal ancestors of X  It is easy to mo dify the algorithm discussed in Figure 5 so that it tak es the an teceden t and consequen t constrain ts in to accoun t The only di\013erence is that w e add an un visited v ertex v  T  to LIST if and only if S  T  024 S  X  c  and T 023 P  Also a v ertex v  R  is added to BoundaryList  only if it satis\014es the mo di\014ed de\014nition of maximalit y  5 Generation of the adjacency lattice In this section w e discuss the construction of the adjacency lattice The pro cess of constructing the adjacency lattice requires us to 014rst 014nd the primary itemsets There are t w o main constrain ts in v olv ed in c ho osing the n um ber of itemsets to prestore 1 Memory Limits In order to a v oid I/O one ma y wish to store the primary itemsets and corresp onding adjacency lattice in main memory  1 Recall that Theorem 2.1 c haracterizes the size required b y the adjacency lattice for this purp ose Assume that w e desire to 014nd N itemsets Note that b ecause of ties in the supp ort v alues of the primary itemsets supp ort v alues ma y not exist for whic h there are exactly N itemsets Th us w e assume that for some slac kv alue N s w e wish to 1 Storing the adjacency lattice on disk is not suc h a bad option after all The total I/O is still prop ortional to the size of the output rather than the n um b er of itemsets prestored Recall that the graph searc h algorithms used in order to 014nd the large itemsets and asso ciation rules visit only a small fraction of the v ertices in the adjacency lattice F unction NaiveFindThr eshold\(Numb er ofIt emset s N Slack N s  b egin High  max i f Supp ort of item i g Low 0 Gener ated 0 while  Gener ated 62  N 000 N s N  b egin Mid  High  Low   2 Gener ated  DH P  Mid  end return Mid  end Algorithm ConstructL attic e\(Numb er ofItem sets N Slack N s  b egin p  NaiveFindThr eshold\(N N s  F or eac h itemset X  f i 1 i r g with S  X  025 p do Add the v ertex v  X  to the adjacency lattice with lab el S  X  Add the edge E  X 000f i k g X  for eac h k 2f 1 r g end Figure 7 Constructing the adjacency lattice 014nd a primary threshold v alue for whic h the n um ber of itemsets is b et w een N 000 N s and N  2 Prepro cessing Time There ma y b e some practical limits as to ho wm uc h time one is willing to sp end in prepro cessing Consequen tly ev en if it is not p ossible to 014nd N itemsets within the prepro cessing time it ough t to b e able to terminate the algorithm with some v alue of the primary threshold for whic h all itemsets with supp ort ab o v e that v alue ha v e b een found A simple w a y of 014nding the primary itemsets is b y using a binary searc h algorithm on the v alue of the primary threshold using the DHP metho d discussed in Chen et al as a subroutine This metho d is somewhat naiv e and simplistic and is not necessarily e\016cien t since it requires m ultiple executions of the DHP metho d This metho d of 014nding the primary threshold is discussed in the algorithm NaiveFindThr eshold of Figure 7 The time complexit y of the pro cedure can b e impro v ed considerably b y utilizing a few simple ideas 1 It is not necessary to execute the DHP subroutine to completion in eac h and ev ery iteration F or estimates whic h are lo w er b ounds on the correct v alue\(s of the primary threshold it is su\016cien t to terminate the procedure as so on as N or more large itemsets ha v e b een generated at the lev el of supp ort b eing considered 2 It is not necessary to start the DHP pro cedure from scratc h in eac h iteration of the binary searc h pro cedure It is p ossible to reuse information b et w een iterations Let I  s  denote the itemsets whic hha v e supp ort at least s  It is p ossible to sp eed up the preprocessing algorithm b y reusing the information a v ailable in I  Low  Generating k itemsets in I  Mid  is only a matter of pic king those k itemsets in I  Low  whic h ha v e supp ort at least Low  This do es not mean that ev ery itemset in I  Mid  can b e immediately generated using this metho d Recall from 1 ab o v e that the DHP algorithm is often terminated b efore completion if more than N itemsets ha v e b een generated in that iteration Consequen tly  not all itemsets in I  Low  ma ybea v ailable but only those k itemsets for whic h k 024 k 0  for some k 0 are a v ailable Th us w eha v e all 


 0 1 2 3 4 5 6 7 8 9 x 10 4 0 0.002 0.004 0.006 0.008 0.01 0.012 0.014 0.016    Primary threshold Number of itemsets prestored T10.I4.D100K  T10.I6.D100K  T20.I6.D100K  Figure 8 Threshold v aration with itemsets prestored DataSet Conf Sup DHP Online T10.I4.D100K 90 0  3 100 sec instan taneous T10.I6.D100K 90 0  3 130 sec instan taneous T10.I6.D100K 90 0  2 240 sec 2 seconds T20.I6.D100K 90 0  5 100 sec instan taneous T able 3 Sample illustration s of the order of magnitude adv an tage of online pro cessing those k itemsets in I  Mid  v ailable for whic h k 024 k 0  These itemsets need not b e generated again 6 Empirical Results W e ran the sim ulation on an IBM RS/6000 530H w orkstation with a CPU clo c k rate of 33MHz 64 MB of main memory and running AIX 4.1.4 W e tested the algorithm empirically for the follo wing ob jectiv es 1 Prepro cessing sensitivit y The prepro cessing tec hnique is sensitiv e to the a v ailable storage space The larger the a v ailable space the lo w er the v alue of the primary threshold W e tested ho w the primary threshold v alue v aried with the storage space a v ailabili t y W e also tested ho w the running time of the prepro cessing algorithm scaled with the storage space 2 Online pro cessing time W e tested ho w the online pro cessing times scaled with the size of the output W e also made an order of magnitude comparison b et w een using an online approac h and a more direct approac h 3 Lev el of redundancy W e tested ho w the lev el of redundancy in the generated output set v aried with user sp eci\014ed lev els of supp ort and con\014dence W e sho w ed that the lev el of redundancy in the rules is quite high Th us redundancy elimination is an imp ortan t issue for an online user lo oking for compactness in represen tation of the rules 6.1 Generating the syn thetic data sets The syn thetic data sets w ere generated using a metho d similar to that discussed in Agra w al et al Generating the data sets w as a t w o stage pro cess 0 1 2 3 4 5 6 7 8 9 x 10 4 0 2 4 6 8 10 12 14 16 18 x 10 4    Number of itemsets prestored Relative Computational Effort for preprocessing T10.I4.D100K  T10.I6.D100K  T20.I6.D100K  Figure 9 Computation v ariation with itemsets prestored 0 5000 10000 15000 0 10 20 30 40 50 60    Number of rules generated Response Time in seconds T10.I4.D100K  T10.I6.D100K  T20.I6.D100K  Figure 10 Online resp onse time v ariation with rules generated 20 30 40 50 60 70 80 90 100 0 2 4 6 8 10 12   Support fixed at 0.15 Confidence Total Rules Generated Essential Rules  T10.I4.D100K  T10.I6.D100K   Figure 11 Redundancy lev el v ariation with con\014dence 


 0.1 0.15 0.2 0.25 0 10 20 30 40 50 60 70 80 90   Confidence fixed at 90 Support Total Rules Generated Essential Rules  T10.I4.D100K  T10.I6.D100K   Figure 12 Redundancy lev el v ariation with supp ort 1 Generating maximal p oten tially large itemsets The 014rst step w as to generate L  2000 maximal p oten tially large itemsets These p oten tially large itemsets capture the consumer tendencies of buying certain items together W e 014rst pic k ed the size of a maximal p oten tially large itemset as a random v ariable from a p oisson distribution with mean 026 L  Eac h successiv e itemset w as generated b y pic king half of its items from the curren t itemset and generating the other half randomly  This metho d ensures that large itemsets often ha v e common items Eac h itemset I hasaw eigh t w I asso ciated with it whic hisc hosen from an exp onen tial distribution with unit mean 2 Generating the transaction data The large itemsets w ere then used in order to generate the transaction data First the size S T of a transaction w as c hosen as a p oisson random v ariable with mean 026 T  Eac h transaction w as generated b y assigning maximal p oten tially large itemsets to it in succession The itemset to b e assigned to a transaction w as c hosen b y rolling an L sided w eigh ted die dep ending up on the w eigh t w I assigned to the corresp onding itemset I  If an itemset did not 014t exactly itw as assigned to the curren t transaction half the time and mo v ed to the next transaction the rest of the time In order to capture the fact that customers ma y not often buy all the items in a p oten tially large itemset together w e added some noise to the pro cess b y corrupting some of the added itemsets F or eac h itemset I w e decide a noise lev el n I 2 0  1 W e generated a geometric random v ariable G with parameter n I  While adding a p oten tially large itemset to a transaction w e dropp ed min f G j I jg random items from the transaction The noise lev el n I for eac h itemset I w as c hosen from a normal distribution with mean 0.5 and v ariance 0.1 W e shall also brie\015y describ e the sym b ols that w eha v e used in order to annotate the data The three primary factors whic hv ary are the a v erage transaction size 026 T  the size of an a v erage maximal p oten tially large itemset 026 L  and the n um b er of transactions b eing considered A data set ha ving 026 T  10 026 L  4 and 100 K transactions is denoted b y T10.I4.D100K W e tested ho w the primary threshold v aried with the n um b er of itemsets prestored This result is illustrated in Figure 8 The 014gure sho ws that the primary threshold initially drops considerably as the n um b er of primary itemsets increases but it b ottoms out after a while W e also illustrate the v ariation of the computational e\013ort required with the a v ailable storage space in Figure 9 W e note that for the itemset T10.I4.D100K the computational e\013ort required in order to 014nd additional large itemsets after 014nding 20000 itemsets increases considerably with the n um b er of itemsets prestored This is b ecause for this particular data set the a v erage size of a maximal p oten tially large itemset or bask et is only 4 Consequen tly  the total n um b er of p ossible large itemsets is relativ ely limited On the other hand the computational e\013ort for prepro cessing required b y the data sets T20.I6.D100K and T10.I6.D100K is relativ ely similar This sho ws that the computational e\013ort required to 014nd a sp eci\014c n um b er of primary itemsets is more sensitiv e to the size of a t ypical bask et in the data rather than to the size of a transaction W e also tested the v ariation in the online running time of the algorithm with the n um b er of rules generated W e ran the online queries for v arying lev els of input parameters in order to test the correlation b et w een the running time and the n um b er of rules generated This is illustrated in Figure 10 This result is signi\014can t in that it sho ws that the running time of the algorithm increases linearly with the n um b er of rules generated for all the data sets used The absolute magnitude of time required in order to generate the rules w as an order of magnitude smaller than the time required using a direct itemset generation approac h lik e DHP  A brief summary of some sample relativ e 014ndings is illustrated in T able 3 W e also discuss the lev el of redundancy presen t in the rule generation pro cedure Figures 11 and 12 illustrate that the n um b er of redundan t rules is often m uc h larger than the n um b er of essen tial rules The b enc hmark for measuring the lev el of redundancy is referred to as the redundancy ratio and is de\014ned as follo ws Redundancy Ratio  T otal Rules Generated Essen tial Rules 1 Th us when the redundancy ratio is K  then the n um ber of redundan t rules is K 000 1 times the n um b er of essen tial rules The redundancy ratio has b een plotted on the Y-axis in Figures 11 and 12 W e see that in most cases the n um ber of redundan t rules is signi\014can tl y larger than the n um ber of essen tial rules This illustrates the lev el to whic h useful rules often get buried in large n um b ers of redundan t rules Also the redundancy lev el is m uc h more sensitiv e to the supp ort rather than the con\014dence The lo w er the lev el of supp ort the higher the redundancy lev el 7 Conclusions and Summary In this pap er w ein v estigated the issue of online mining of asso ciation rules The t w o primary issues in v olv ed in online pro cessing are the running time and compactness in represen tation of the rules W e discussed an OLAP-lik e approac h for online mining asso ciation rules whic ha v oids redundancy  


Ac kno wledgemen ts W ew ould lik e to thank V S Ja yc handran and Jo el W olf for their extensiv e commen ts and suggestions References  Aggarw al C C and Y uP  S Online Generation of Asso ciation Rules IBM R ese ar ch R ep ort R C 20899  Agra w al R Imielinski T and Sw ami A Mining association rules b et w een sets of items in v ery large databases Pr o c e e dings of the A CM SIGMOD Confer enc e on Management of data pages 207-216 W ashington D C Ma y 1993  Agra w al R and Srik an tR.F ast Algorithms for Mining Asso ciation Rules in Large Databases Pr o c e e dings of the 20th International Confer enc eon V ery L ar ge Data Bases pages 478-499 Septem b er 1994  Agra w al R and Srik an t R Mining Sequen tial P atterns Pr o c e e dings of the 11th Internation al Confer enc e on Data Engine ering pages 3-14 Marc h 1995  Agra w al S Agra w al R Deshpande P  M Gupta A Naugh ton J F Ramakrishnan R and Sara w agi S On the Computation of Multidimensi on al Aggregates Pr o c e e dings of the 22nd International Confer enc eon V ery L ar ge Datab ases pages 506-521  Chen M S Han J and Y uP  S Data Mining An Ov erview from Database P ersp ectiv e IEEE T r ansactions on Know le dge and Data Engine ering V olume 8 Num b er 6 Decem b er 1996 pages 866-883  Dyreson C Information Retreiv al from an Incomplete Data Cub e Pr o c e e dings of the 22nd International Confer enc eon V ery L ar ge Datab ases pages 532-543 Mumbai India 1996  Gupta A Harinara y an V and Quass D Aggregatequery pro cessing in data w arehousing en vironmen ts Pr o c e e dings of the 21st Confer enc eon V ery L ar ge Datab ases Zuric h Switzerland Septem b er 1995  Han J and F u Y Disco v ery of Multiple-Lev el Assocaition Rules from Large Databases Pr o c e e dings of the 21st Internation al Confer enc eon V ery L ar ge Data Bases Zuric h Switzerland 1995 pages 420-431  Harinara y an V Ra jaraman A and Ullman J Implemen ting Data Cub es E\016cien tly  Pr o c e e dings of the 1996 A CM SIGMOD c onfer enc e on Management of Data Mon treal Canada June 1996 pages 205-227  Houtsma M and Sw ami A Set-orien ted Mining for Asso ciation Rules in Relational Databases Pr o c e e dings of the 11th Internation al Confer enc e on Data Engine ering Marc h 1995 pages 25-33  Kaufman L and Rousseeu wP J Finding Gr oups in Data A n Intr o duction to Cluster A nalysis Wiley Series in Probabilit y and Mathematical Statistics 1990  Klemen ttinen M Mannila H Ronk ainen P  T oiv onen H and V erk amo A I Finding in teresting rules from large sets of disco v ered asso ciation rules Pr o c e e dings of the Confer enc e on Information and Know le dge Managements Gaithersburg MD USA 28 No v 2 Dec 1994  Len t B Sw ami A and Widom J Clustering Asso ciation Rules Pr o c e e dings of the Thirte enth International Confer enc e on Data Engine ering pages 220-231 Birmingham UK April 1997  Mannila H T oiv onen H and V erk amo A I Ef\014cien t algorithms for disco v ering asso ciation rules AAAI Workshop on Know le dge Disc overy in Datab ases pages 181-192 Seattle W ashington July 1994  Ng R T and Han J E\016cien t and E\013ectiv e Clustering Metho ds for Spatial Data Mining Pr o c e e dings of the 20th Internation al Confer enc eon V ery L ar ge Data Bases San tiago Chile 1994 pages 144-155  P ark J S Chen M S and Y uP  S An E\013ectiv e Hash Based Algorithm for Mining Asso ciation Rules Pr o c e e dings of the 1995 A CM SIGMOD International Confer enc e on Management of Data pages 175-186 Ma y 1995  Piatetsky-Shapiro G Disco v ery  Analysis and Presentation of Strong Rules Know le dge Disc overy in Datab ases 1991  Sa v asere A Omiecinski E and Na v athe S An E\016cien t Algorithm for Mining Asso ciation Rules in Large Data Bases Pr o c e e dings of the 21st Internation al Confer enc eon V ery L ar ge Data Bases Zuric h Switzerland 1995 pages 432-444  Sh ukla A Deshpande P  M Naugh ton J F and Ramasam y K Storage Estimation for Multidimensi on al Aggregates in the Presence of Hierarc hies Pr o c e e dings of the 22nd Internationa l Confer enc eon V ery L ar ge Datab ases pages 522-531 Mum bai India 1996  Srik an t R and Agra w al R Mining Generalized Asso ciation Rules Pr o c e e dings of the 21st International Confer enc eon V ery L ar ge Data Bases  pages 407-419 Septem b er 1995  Srik an t R and Agra w al R Mining quan titativ e association rules in large relational tables Pr o c e e dings of the 1996 A CM SIGMOD Confer enc e on Management of Data Mon treal Canada June 1996  T oiv onen H Sampling Large Databases for Asso ciation Rules Pr o c e e dings of the 22nd International Conferenc eonV ery L ar ge Datab ases pages 134-145 Mumbai India 1996  Ziark o W The Disco v ery  Analysis and Represen tation of Data Dep endencies in Databases Know le dge Disc overy in Datab ases 1991 


CMP A Fast Decision Tree Classifier Using Multivariate Predictions  449 H Wang and C Zaniolo Mining Recurrent Items in Multimedia with Progressive Resolution Refinement  461 0 Zai'ane J Hun and H Zhu Panel Session 22 Is E-Commerce a New Wave for Database Research Moderator Anant Jhingran IBM T.J Watson Research Center USA Panelists Sesh Murthy IBM T.J Watson Research Center USA Sham Navathe, Georgia Institute of Technology USA Hamid Pirahesh IBM Almaden Research Center USA Krithi Ramamrithan University of Massachusetts-Amherst USA Industrial Session 23 Java and Databases Pure Java Databases for Deployed Applications  477 N Wyatt Database Technology for Internet Applications  700 A Nori Session 24 Association Rules and Correlations Finding Interesting Associations without Support Pruning  489 E Cohen M Datar S Fujiwara A Gionis P Indyk R Motwani J Ullman and C. Yang Dynamic Miss-Counting Algorithms Finding Implication and Similarity Rules with Confidence Pruning  501 S Fujiwara J Ullman and R Motwani Efficient Mining of Constrained Correlated Sets  512 G Grahne L Lakshmanan and X Wang Session 25 Spatial and Temporal Data Analyzing Range Queries on Spatial Data  525 J Jin N An and A Sivasubramaniam Data Redundancy and Duplicate Detection in Spatial Join Processing  535 J.-P Dittrich and B Seeger Query Plans for Conventional and Temporal Queries Involving Duplicates and Ordering  547 G Slivinskas C Jensen, and R Snodgrass xi 


Industrial Session 26 XML and Databases Oracle  The XML Enabled Data Management System  561 S Banerjee V Krishnamurthy M Krishnaprasad, and R Murthy XML and DB2  569 J Cheng and J Xu Session 27 High-Dimensional Data Independent Quantization An Index Compression Technique for High-Dimensional Data Spaces  577 S Berchtold, C Bohm H Jagadish H.-P. Kriegel and J Sander Deflating the Dimensionality Curse Using Multiple Fractal Dimensions  589 B.-U Pagel F Korn and C. Faloutsos Similarity Search for Multidimensional Data Sequences  599 S.-L Lee S.-J Chun D.-H Kim, J.-H Lee and C.-W Chung Session 28 Web-Based Systems WRAP An XML-Enabled Wrapper Construction System for Web Information Sources  611 L Liu C Pu and W. Hun Self-Adaptive User Profiles for Large-scale Data Delivery  622 U Cetintemel M Franklin and C. Giles Industrial Session 29 Main Memory and Small Footprint Databases In-Memory Data Management in the Application Tier  637 The TimesTen Team SQLServer for Windows CE -A Database Engine for Mobile and Embedded Platforms  642 P Seshadri and P. Garrett Join Enumeration in a Memory-Constrained Environment  645 I Bowman and G Paulley xii 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


