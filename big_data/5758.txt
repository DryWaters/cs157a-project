CORE Cross-Object Redundancy for Efìcient Data Repair in Storage Systems Kyumars Sheykh Esmaili Nanyang Technological University Singapore 639798 kyumarss@ntu.edu.sg Lluis Pamies-Juarez Nanyang Technological University Singapore 639798 lpjuarez@ntu.edu.sg Anwitaman Datta Nanyang Technological University Singapore 639798 anwitaman@ntu.edu.sg Abstract Erasure codes are an integral part of many distributed storage systems aimed at Big Data since they provide high fault-tolerance for low overheads However traditional erasure codes are inefìcient on replenishing lost data vital for long term resilience and on reading stored data in degraded environments when nodes might be unavailable Consequently novel codes optimized to cope with distributed storage system nuances are vigorously being researched In this paper we take an engineering alternative exploring the use of simple and mature techniques  juxtaposing a standard erasure code with RAID-4 like parity to realize cross object redundancy CORE and integrate it with HDFS We benchmark the implementation in a proprietary cluster and in EC2 Our experiments show that for an extra 20 storage overhead compared to traditional erasure codes CORE yields up to 58 saving in bandwidth and is up to 76 faster while recovering a single failed node The gains are respectively 16 and 64 for double node failures I I NTRODUCTION In order to meet the conîicting needs of high faulttolerance and low storage overhead erasure codes are increasingly being embraced for distributed storage systems aimed to store high volumes of data Traditional erasure codes have mostly been designed to optimize the performance of communication-centric applications and are not necessarily amenable to the needs of storage systems Some such desirable properties include efìcient replenishment of lost redundancy repair following the failure of some system components and efìcient access of data while the system is yet to complete remedial actions following such failures degraded reads/access To that end there has been tremendous interest in both coding theory and storage systems research communities to build new erasure codes with good repairability properties as well as building robust storage systems leveraging on the novel codes for instance Windows Azure Storage using Local Reconstruction Codes In this paper we explore an alternate design looking at an instance of product codes A traditional erasure code is rst applied on individual data objects followed by the creation of RAID-4 like parity over erasure encoded pieces of different objects creating c  ross-o bject re dundancy This results in high fault tolerance provided by the traditional code and This work is supported by Singaporeês A*Star Grant 102 158 0038  National Research Foundation Grant NRF-CRP2-2007-03 cheap repairs provided by the parity code The approach is simple and based on mature techniques that have long been used as stand-alone approaches these are desirable for practical and implementation considerations yet it achieves very good less communication and computation repairability and degraded data access under many fault-conditions We accordingly build the CORE storage primitive as a general purpose block level fault-tolerant data storage layer that can be readily integrated into distributed le systems relying on an underlying block level storage providing signiìcant performance boost We integrate CORE into Hadoop Distributed File System HDFS and benchmark it over a wide range of system conìgurations comparing it with state-ofthe-art alternatives 3 to demonstrate its ef cac y  CORE builds upon our recent theoretical study where we made a simple observation by introducing a RAID-4 like parity over a small set of erasure encoded pieces it is possible to achieve signiìcant reduction in the expected cost to repair lost redundancy Moreover since these extra parities are relatively-small the fault tolerance of the resulting system is only marginally lower than what is achieved with optimal maximum distance separable or MDS codes i.e ReedSolomon codes This suboptimal fault-tolerance is expected from any code aiming to reduce the costs of repairs Gopalan et al studies the involved trade-offs  In Figure 1 we show an example to elaborate the basic idea on which CORE is built Consider three objects  a  b  c  each comprising of 6 blocks Each of these objects are rst individually encoded using a 9,6 Reed-Solomon code Note that each row represents an object along with its three parity blocks depicted in gray Additionally a simple parity check i.e an XOR is computed over each columnês blocks and thereby a new row is added at the bottom of the matrix In this example this extra row increases the storage overhead by 33 In the general case the overhead is 1 t more than MDS codes where t is the number of objects cross-coded together As shown in our technical report COREês parameters can be adjusted to operate at reasonable overheads even while achieving very good fault-tolerance as well as repairability In particular for equivalent storage overhead COREês performance beneìt is signiìcantly better than the state-of-the-art Locally Reconstruction Codes used in Azure while CORE achie v e s f ault-tolerance ignoring 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 246 


a 6 a 7 a 8 b 6 b 7 b 8 c 6 c 7 c 8 p 6 p 7 p 8 c 3 c 4 c 5 c 0 c 1 c 2 a 3 a 4 a 5 b 3 b 4 b 5 a 0 a 1 a 2 b 0 b 1 b 2 p 3 p 4 p 5 p 0 p 1 p 2 Three Objects in a RS\(9,6 Scheme Vertical XOR Parities Figure 1 An example illustrating the basic idea of CORE repairs comparable to optimal MDS erasure codes for an acceptable 20 more storage overhead The main advantage that the vertical parities introduce in CORE is the increased efìciency of repairs Fewer blocks are needed to carry out a repair Furthermore repair related computation is cheap a simple XOR operation compared to the expensive RS decoding procedure In the example of Figure 1 repairing any single failure would require XORing 3 blocks Apart from the repairability beneìts COREês vertical parities also improve fault tolerance For instance in Figure 1 an object i.e a row with more than three failures can be still recovered with the help of vertical parities This improvement is however not optimal in terms of the additional storage overhead as the primary purpose of the vertical parities is to enhance the repairability aspect The low repair cost in CORE also naturally translates into better degraded reads The main contributions of this work are as follows  Design and implementation of a general purpose efìciently repairable block level storage primitive CORE and its integration with a popular distributed le system HDFS our implementation is available at  In the process we identify a few ways to optimize the existing HDFS-RAID implementation on which we build CORE  We design novel algorithms to understand the failure patterns and adaptively exploit the better repair exibility afforded by COREês code design in order to achieve fast and cheap repairs To repair single failures CORE consumes 50 less bandwidth and is between 43 to 76 faster compared the classic erasure code In case of double failures and in the worst case scenario when both failed blocks belong to the same le it consumes 16 less bandwidth and is 13 to 59 faster We thus hope that COREês design and analysis is not only academically interesting but the performance boost it achieves despite a very simple design makes it a serious candidate for wide-scale adoption We have also carried out analytical study of many more complex failure patterns but we exclude the theoretical results due to space constraint and they can be found in an extended version II R ELATED W ORK Erasure codes have long been explored as a storage efìcient alternative to replication for achieving fault-tolerance  in the peer to-peer P2P systems literature and ha v e led to numerous prototypes e.g OceanStore and T otalRecall  to name a f e w  I n recent years erasure codes ha v e gained traction e v en in main-stream storage technologies such as RAID The ideas from RAID systems are in turn permeating to Cloud settings 14 and erasure codes have become an integral part of many proprietary le systems used in data-centers 16 as well as open-source variants With the proliferation of erasure codes in storage-centric applications there has been a corresponding rise in the exploration of novel erasure codes which cater to the nuances of distributed storage systems Speciìc aspects that have been investigated in designing such new coding techniques include i efìcient degraded data access  17 ii good repairability  19 by either combining standard codes  20 21 applying netw ork coding techniques 19  23 or designing completely ne w codes with lo wer repair fan-in  and iii fast creation of erasure coded redundancy 29 Despite the plethora of works investigating novel erasure codes most existing distributed le systems using erasure codes do so by adapting traditional erasure codes Microsoftês Windows Azure Storage is a prominent exception which uses an optimized version of Pyramid codes called Local Reconstruction Code LRC 3 Some recent academic prototypes NCFS and 31 1 likewise explore the feasibility of applying network coding techniques for repairing lost data The latter systems do not address the issue of degraded reads In contrast to these systems based on proprietary and novel erasure coding techniques with signiìcant system design complexity CORE combines two mature techniques standard erasure codes and RAID-4 like parity while achieving very good repairability and degraded read performance This makes CORE suitable for ready integration with many block based storage/ìle systems and its simple design makes it amenable to third party reimplementations III B ACKGROUND We next provide some background on what erasure codes are and how they are used in distributed storage systems followed by a discussion on local repairability which has come to the fore in the design of novel storage-centric erasure codes Finally we discuss the code used in Azure system which to the best of our knowledge was the rst deployment of repairable codes in a large-scale commercial cloud storage system 1 Coincidentally uses the same name CORE for co  llaborative re generation 247 


A Classic Erasure Codes Traditionally large data objects have been stored by splitting them into blocks of say size q bits which are then replicated across multiple storage nodes In contrast an  n k  erasure code takes k different data blocks of size q  and computes m  n  k parity blocks of the same size each to be stored in a different storage node Then in the event of disk failures the k original blocks can be reconstructed by collecting and decoding a subset of k   k blocks out of the total n stored blocks Consider that the vector o  o 1 o k  denotes a data object composed of k blocks of q bits each That is each block o i is a string of q bits The encoding operations are performed using nite eld arithmetic where the two bits  0  1  form a nite eld F 2 of two elements while o i likewise belongs to the binary extension eld F 2 q containing 2 q elements Then the encoding of the object o is a linear transformation deìned by a k  n generator matrix G such that we can obtain an n dimensional codeword c  c 1 c n  of size n  q bits by applying the linear transformation c  o  G  A code with such a generator matrix G is usually referred to as an  n k  When the generator matrix G has the form G  I k G   where I k is the identity matrix and G  is a k  m matrix  m  n  k  the codeword c becomes c  o  p  where o is the original object and p is a parity vector containing m  q parity bits The code is then said to be systematic  in which case the k parts of the original object remain unaltered after the coding process We want to note that the main advantage of systematic codes is that the original data o can be accessed without requiring a decoding process by just reading the systematic blocks of c  The above encoding process stretches the original data by a factor of n/k ratio known as the stretch factor  occupying n/k times more storage space than the size of the original object By choosing a suitable code with a stretch factor satisfying n/k  r  signiìcant storage space savings can be achieved in comparison to a system using r replicas Finally an optimal erasure code in terms of the trade-off between storage overhead and fault tolerance is called a maximum distance separable MDS code and has the property that the original object o can be reconstructed from any k out of the total n  k  m stored blocks i.e k   k  tolerating the loss of any arbitrary m  n  k blocks The fault-tolerance of MDS erasure codes has been previously analyzed and compared with replication 10 pro viding guidelines to choose suitable code parameters n and k for a desired level of resilience under an expected level of failures of individual storage nodes B Locally Repairable Codes A critical drawback of MDS codes is their high reconstruction cost Repairing/reading a single failed block requires to download an amount of information equivalent to the size of the whole data object o  which is k times larger than the amount of data being repaired/read Since repairs and degraded reads are frequent in storage systems several recent works 17 24]Ö[26 ha v e looked at reducing the number of blocks needed to carry out the repair/reconstruction of an inaccessible block which is needed for both repair and access Such a property is achieved by introducing local dependencies among encoded blocks and can be called repair locality Local repairability is achieved when a block c i can be expressed as a linear combination of d  d<k  other blocks c i   1 c  1   2 c  2     d c  d  where c  j  c s.t c  j   c i  and  j  F 2 q for all j 1 d  This local repairability property allows to reduce the number of blocks accessed and transferred during degraded reads or repairs from k to d  where d can be as small as d 2  25 Unfortunately  achieving such code locality leads to poorer fault-tolerance for a given storage overhead in comparison to MDS codes  Hence the design of such codes poses a trade-of f between three important desirable system properties i high fault-tolerance ii low storage overhead and iii efìcient repairs and degraded reads For example Pyramid codes the code behind Azure were not originally conceived for efìcient repairs per se but to provide degraded read capabilities In this case the code cannot obtain efìcient repairs for all encoded blocks IV C ROSS O BJECT R EDUNDANCY We next explore how product codes  can achie v e good repairability without compromising either the degraded read performance or the fault-tolerance of the code Speciìcally by combining a long and a short linear erasure code we realize a product code with high fault tolerance mainly provided by the long code and high repair locality provided by the short code This is achieved by encoding multiple already-encoded objects together or cross-object encoding thus reusing existing encoding/decoding/repair mechanisms already deployed in a distributed storage system facilitating an organic integration of the approach Example 1 Suppose that we have two different data objects o 1  o 11 o 12 o 13  and o 2  o 21 o 22 o 23  to be encoded with a 5,3 systematic MDS erasure code with a generator matrix G o  Then we obtain the codewords c 1  o 1  G o  o 11 o 12 o 13 p 11 p 12   c 2  o 2  G o  o 21 o 22 o 23 p 21 p 22   By grouping symbols from c 1 and c 2 in a per-column basis we obtain the set of vectors P    o 11 o 21    o 12 o 22    o 13 o 23    p 11 p 21    p 12 p 22    We encode then each vector x i P cross-object encoding with a 3,2 systematic code a simple parity check code or SPC with generator matrix G g  I 2  1 2   where I 2 is the identity matrix and 1 2 is a vector with two ones For 248 


   Figure 2 Example of a simple product code The blue parity blocks are generated using a horizontal 5,3 MDS code whereas the red block is a simple parity check of the column or a 3,2 code each x i P we obtain p g,i  x i  G g  x i  p g,i   where p g,i   x i   The vector with all the cross-object parity blocks p g  p g 1  p g 5   contains p g  o 11  o 21 o 12  o 22 o 13  o 23 p 11  p 21 p 12  p 22   In Fig 2 we depict this two-phase encoding process Note that p g can be viewed as the Reed Solomon encoding of the respective parities of the systematic symbols We refer to a code with a generator matrix G that takes a composed data object o  o 1  o 2  and encodes it to a codeword c  o  G  c 1  c 2  p g   as the product code of G g and G o It is easy to see how this example product code repairs any single missing block by using the outer erasure code G g  e.g we can repair o 1  1 using o 1  1  o 2  1  p g 1  In addition in case of more than one failure per column the code still has the opportunity to repair up to two failures per codeword c i  and up to two failures within the extra parity vector p g  Deìnition of COREês Product Code Let G c and G o respectively be the generator matrices of an  n c k c  and an  n o k o  code Then the product code of G c and G o isa n c n o k c k o  linear code with generator matrix G  G c  G o  where the operator  represents the Kronecker product In the case of the product code used in CORE we will consider that the single parity check SPC with generator matrix G c  I t  1 t   i.e a  t 1 t  MDS erasure code over F 2 q is the vertical code For an input o  o 1 o t   o i  F q  this code generates a systematic codeword c  c 1 c t 1   o 1 o t c t 1   where c t 1   t i 1 o i  Since F 2 q is a binary extension eld the last symbol in the codeword corresponds to the exclusive-or XOR of the t original symbols It can repair any single erasure in the codeword by xoring the remaining t symbols The inner  horizontal  code G o used in CORE is a MDS  n k  erasure code For the sake of simplicity we will consider that it is a  n k  Reed-Solomon code with generator matrix G o  I k H   where H is a k  m Vandermonde matrix recall that m  n  k  H      0 1   m  1 1           0 k   m  1 k     for any  i  F 2 q  Then the COREês product code is a linear code that cross-encodes t different data objects using a generator matrix G  G c  G o  We will refer to such a code as a  n k t  CORE product code Lastly it is worth noting that by varying the value of t  CORE allows for tuning the trade-off between good repairability small t  and good storage overhead large t  In our analytical study as well as in our e xperiments we have chosen t  k 2  in order to strike a balance between the two criteria V CORE S A LGORITHMIC A SPECTS One of the new aspects of CORE is its higher level of granularity i.e instead of working with individual independent objects it works with a matrix of t objects This higher granularity provides new opportunities e.g in a CORE scheme of  n k t  it is possible to repair an object that has more than n  k failed blocks and also poses new challenges e.g given a pattern of failures is it possible to recover  i.e repair all the failed blocks  the CORE matrix or what is the best schedule for repairing a set of failures In this section we look at these algorithmic problems and provide solutions for them We adopt a divide-and-conquer approach to tackle these issues Speciìcally given a matrix representing the available and failed nodes subsequently called the CORE matrix we rst split the failures into independent clusters deìned below Other algorithms i.e recoverability-checking and repair scheduling can be performed within each cluster We discuss these next A Identifying Independent Clusters We deìne disjoint subsets of failed nodes that can be handled without interference as independent clusters 2 Essentially two different clusters must not share any common row or column containing failed nodes Two important beneìts of such clusters are i they allow parallel repairs ii they may allow partial recovery when the full CORE matrix in not recoverable A naive way to create the clusters is as follows Initially each single failure is considered a cluster Two clusters are then merged if there exists at least one common row or column on which both clusters have a failure The process is continued until there are no mergeable clusters left The number of clusters in a CORE matrix is between 0 and t number of rows To investigate the distribution of the number of clusters based on the number of failures we ran our clustering algorithm on 10M randomly-generated CORE matrices for code parameters 14,12,5 and varied the number of random failures from 1 to 20 The results depicted in Figure 3 show that after an initial increase the 2 In other parts of this paper we also use the term computer/node cluster in the common sense of the word which should not be confused with the failure clusters in the CORE matrix 249 


0 1 2 3 1234567891011121314151617181920 Number of Failures Avg Number of Clusters Figure 3 The average number of clusters versus the number of failures for COREês code parameters 14,12,5 number of clusters begins to drop for failure numbers greater than 6 B Recoverability-Checking Algorithm For coding schemes that work at the level of single objects given the number of failures one can directly infer whether an object is recoverable or not In the case of CORE however this is more subtle For instance objects may still be recoverable even if there are more than n  k failed blocks within a single CORE row We rst identify two bounds and then introduce an algorithm to determine an objectês recoverability The Ir\verability Bounds  For a  n k t  code  the lower bound of irrecoverability  L  is 2   n  k 1 It occurs if two 3 rows are minimally irrecoverable each has n  k 1 failures and the column indexes of their failures are identical i.e no vertical repair possible  the upper bound of recoverability  U  is t   n  k 2 k  n   1 This occurs when all rows are maximally recoverable each has n  k failures and have identical failure column indexes i.e the remaining k   n  k  k  n columns can each tolerate a single failure These two bounds deìne an interval For any failure number outside of this interval the ir/recoverability can be immediately decided More precisely if the number of failures is smaller than L then the pattern is recoverable  although as we will see later this is a very pessimistic bound  likewise if the number is greater than U  then it is certainly not recoverable For all the values within the above interval inclusive the outcome depends on the distribution of the failures We propose a recursive algorithm which is able to decide whether a given CORE matrix with a speciìc failure pattern is recoverable or not At each step of the algorithm all the repaired and repairable rows/columns are removed and the algorithm restarts with the reduced matrix as the new 3 Any single-row failure pattern is always recoverable 99.996 99.969 99.877 99.634 99.102 98.021 96.066 92.699 87.253 78.900 66.928 51.237 33.061 15.784 4.125 0 20 40 60 80 100 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of Failures Likelihood  Figure 4 The recoverability likelihood of the scheme 14,12,5 based on the number of failures input If it results in an empty matrix then the patterns is recoverable otherwise it is not We implemented this algorithm and used it to carry out an analysis on the recoverability likelihood of different patterns Figure 4 obtained from 10M random runs shows the recoverability likelihood of the CORE matrix of size 14,12,5 for failure numbers between the lower bound of irrecoverability  L 6  and the upper bound of recoverability  U 20  It clearly illustrates the fact that COREês lower bound of irrecoverability is too strict For a detailed and more rigorous study of fault-tolerance and recoverability we refer the reader to our technical report C Repair Scheduling Algorithms Many different repair schedules may exist for a given fault pattern Here we rst investigate two straw man approaches namely column-ìrst and row-ìrst  then propose an algorithm called Recursively Generated Schedule RGS Analytical and experimental studies show that RGS outperforms the baseline approaches The column-ìrst algorithm always gives higher priority to vertical repairs and applies horizontal repair when no further vertical repairs are possible The row-ìrst analogously prefers horizontal repairs In both algorithms while doing horizontal repairs always the best candidate the one with maximum number of failures but still repairable is prioritized over the other ones Recursively Generated Schedule RGS algorithm  This algorithm rst identiìes the critical set of failures failures that decrease the minimum number of required vertical or horizontal repairs and repairs them rst along the call chain of a recursive cost function c  All other repairs non-critical ones are then scheduled using c   a simple non-recursive cost function In order to identify the critical failures we deìne two variables v and h  as follows v  t  i 1 minV  Row i  h  k  j 1 minH  Col j  in which minV  Row i  returns the minimum number of vertical repairs required by row Row i  and minH  Col j  250 


returns the minimum number of horizontal repairs required by column Col j  more precisely minV  Row i   0 if  X   n  k   X   n  k  otherwise minH  Col j   0 if  X  1  X  1 otherwise The most important element of RGS is the recursive cost function c  h v  deìned as c  h v   012 012  012 012  c  h dec  v   t if v 0 c  dec  h  v  k if v 0 or dec  v  is not applicable in which dec  v  and dec  h  reîect the decreases in the values of v and h after a single repair is performed The cost function c decreases the values of rst v and then h by at least one unit at each recursion step until we reach c 0  0  which is the base case 4  The notable property of the base case is that any remaining repair can be done either vertically or horizontally In other words there is at most one failure per column and at most n  k failures per row Therefore all remaining repair decisions can be safely made using the static cost function c  deìned below c   r  015 k if repaired horizontally r  t if repaired vertically in which r denotes the number of remaining repairs for a given row To demonstrate the differences between the repair schedules generated by the above three algorithms we use two failure pattern examples in the CORE matrix of size 14,12,5 a 3-failure step shaped pattern and a 5-failure plus shaped one These examples are shown in Table I         00   00   X 0   XX   00                 000   0 X 0   XXX   0 X 0   000         Table I The step-shaped and the plus-shaped failure pattern examples representing two classes of failure patterns It should be noted that since swapping any two rows or any two columns in the CORE matrix results in an equivalent failure matrix each of these patterns represents a class of failure patterns and not singular instances Table II presents the schedules generated by each algorithm for each failure pattern along with its calculated cost in terms of repair trafìc The corresponding experimental results are reported in Section VII Finally we generalized our analytical study of the above three algorithms to include failure patterns of size 1 to 20 The results for 10,000 randomly-generated recoverable failure patterns are depicted in Figure 5 Four conclusions 4 If the failure pattern is recoverable the base case will always be reached  Row-First Column-First RGS Step Schedule R 3 R 2 C 1 R 2 C 0 c 1  0 R 3  c 0  0  C 1 Cost 2 k 24 2 t  k 22 k  t 17 Plus Schedule R 1 R 3 C 0 R 2 C 0 C 2 R 1 R 2 C 1 c 2  1 C 0  c 2  0 R 2  c 1  0 R 1  c 0  0  C 1 Cost 3 k  t 41 3 t 2 k 39 2 t 2 k 34 Table II The analytical cost number of blocks read  of repairing the Step and Plus failure patterns using Row-First Column-First and RGS algorithms where k 12 and t 5  0 20 40 60 80 100 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 RGS Column First Row First Repair Cost blks Number of Failures Figure 5 Comparing the Column-First Row-First and RGS algorithms w.r.t number of blocks required to carry out the repair on the scheme 14,12,5 can be drawn from this gure i RGS and column-ìrst perform better than row-ìrst and this is especially noticeable when the number of failures is very small which is in essence the MDS code vs CORE comparison ii as the number of failures and consequently the number of choices to make increases the beneìts of RGS over column-ìrst become more pronounced iii for the large failure numbers distinct schedule possibilities are limited and all the algorithms perform similarly and nally iv a more general conclusion is that if one wishes to avoid the relatively complex scheduling algorithms then the naive column-ìrst approach nevertheless delivers signiìcant beneìts w.r.to the row-ìrst which is roughly like for MDS codes highlighting the immediate beneìts of COREês product code VI I MPLEMENTATION To implement the CORE primitive we used HDFSRAID an open-source module inspired by DiskReduce and de v eloped at F acebook It wraps around Apache Hadoopês distributed le system HDFS and provides HDFS with basic erasure coding capabilities encoding and decoding Below we rst introduce HDFS-RAID then explain two optimizations that we did on HDFS-RAID to improve its performance and nally give an overview of our implementation of CORE A HDFS-RAID HDFS-RAID embeds the Apache HDFS inside an erasure code-supporting wrapper le system named Distributed Raid File System DRFS DRFS supports both Reed-Solomon coding as well as simple XOR parity les These two coding 251 


alternatives are orthogonal and used separately based on user preference Furthermore both provide two basic features encoding a.k.a RAIDing data blocks and repairing the corrupt/missing blocks The two main components of HDFS-RAID are RaidNode and BlockFixer RaidNode is a daemon responsible for the creation only once following the initial le write and maintenance re-creating periodically or on demand the corrupt/missing parities and purging orphan ones of parity les for all data les Since the default block policy of HDFS is not aware of the dependency relation between the data and parity blocks of a given le HDFS-RAID manages the placement of parity blocks to avoid co-location of data blocks and parity blocks The BlockFixer component reconstructs missing or corrupt blocks by retrieving the necessary blocks encoding/decoding them and sending the reconstructed blocks to new hosts B HDFS-RAID Optimizations In our experiments with HDFS-RAID we noticed two common performance inefìciencies and optimized them Opt1 The HDFS-RAID implementation uses the generator polynomial and not the more well-known generator matrix  representation of Reed-Solomon codes In this representation typically and as is in the HDFS-RAID implementation always all the remaining blocks of a given row which can be more than k  are fetched and used to repair the missing ones Generally this use of extra blocks results in faster decoding since there will be fewer equations to solve However for cases in which network is a bottleneck this trade-off fetching extra blocks versus faster decoding does not pay off Our optimized version retrieves exactly k blocks and pretends that all other n  k blocks are missing As conìrmed by our experimental results the bandwidth-scarce clusters can greatly beneìt from this optimization Opt2 The HDFS-RAID implementation implicitly assumes that there is only a single failure per row stripe In case there are more failures they are discovered only when the read access attempts fail These newly-detected failed blocks are then added to the list of failed blocks and the repair process starts again Our optimized implementation checks for multiple failures beforehand and repairs them simultaneously amortizing the repair costs C CORE Implementation The CORE storage primitive has been organically integrated with HDFS-RAID by extending its two main functionalities as described below RAIDing The CORE implementation allows vertical coding across les in a given directory The cross-object size parameter  t  can be conìgured similar to the row stripe size parameter of HDFS-RAID The vertical encoding is reused in the full matrix RAIDing rst row-by-row then column-by-column for both data and parity blocks Repair An additional vertical repair option is introduced The 2-dimensional repair feature implements all the algorithms discussed in Section V i failure detection and failure matrix population ii failure clustering iii recoverability-checking and iv repair scheduling The correctness of our implementation was veriìed through multiple test cases in which the MD5 hash values of the repaired les were compared against those of the original les Moreover since all changes have been made within the RAID subdirectory of the HDFSês code replacing the corresponding Java library is sufìcient to upgrade HDFSRAID to CORE The source codes binary distribution and documentations of our implementation are available at http://sands.sce.ntu.edu.sg/StorageCORE  VII E XPERIMENTS We benchmarked the implementation with experiments run on two different HDFS clusters of 20 nodes each  Network-Critical cluster A university cluster which has one powerful PC 4  3.2GHz Xeon Processors with 4GB of RAM hosting the NameNode/RaidNode and 19 HP t5745 ThinClients acting as DataNodes The average bandwidth of this cluster is 12MB/s  Computation-Critical cluster An Amazon EC2 cluster of 20 homogeneous nodes of type m1.small approximately 1.2 GHz 2007 Xeon Processor with 1.8GB of RAM In this cluster one node is hosting the NameNode/RaidNode and the rest are used as DataNodes The maximum bandwidth between EC2 m1.small instances is 250MB/s The block size  q  used was 64MB Files were added to HDFS and encoded horizontally rst and then the vertical parity was computed We ran two sets of experiments one set to compare the performance of CORE with that of HDFS-RAID and another set to study the repair scheduling algorithms In both sets we primarily use the completion time of the repair process as the main comparison measure However we also measured the amount of transferred data in each experiment as repair trafìc The data transfer numbers serve two purposes i to verify the correctness of our implementation they must match the analytical numbers and ii to use as a reference point in analyzing the completion time numbers  since the amount of transferred data is independent of the type of cluster used Finally in all experiments the reported numbers are the average of 10 runs Since the variations were small up to few percents they are omitted from the graphs A CORE vs HDFS-RAID In these experiments we compared three methods namely HDFS-RAID HDFS-RAID-Optimized and CORE 252 


0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 n=9 k=6 t=3 n=14 k=12 t=5 X CORE X HDFS RAID Optimized X HDFS RAID XX CORE XX HDFS RAID Optimized XX HDFS RAID Bytes Read GB a Transferred data 0 20 40 60 80 100 120 140 160 180 200 n=9 k=6 t=3 n=14 k=12 t=5 X CORE X HDFS RAID Optimized X HDFS RAID XX CORE XX HDFS RAID Optimized XX HDFS RAID Repair Time seconds b Time network-critical cluster 0 20 40 60 80 100 120 140 160 180 200 n=9 k=6 t=3 n=14 k=12 t=5 X CORE X HDFS RAID Optimized X HDFS RAID XX CORE XX HDFS RAID Optimized XX HDFS RAID Repair Time seconds c Time computation-critical cluster Figure 6 Comparing the repair performance of HDFS-RAID HDFS-RAID-Optimized and CORE using two different sets of coding parameters 9,6,3 and 14,12,5 inspired respectively by the code length and storage overheads of Googleês GFS and Microsoft Azure In these schemes the overhead of COREês extra parities are 1  3=33 and 1  5  20 accordingly In each case two different failure patterns were enforced a one-failure pattern represented by X and a two-failures pattern represented by XX  For the two-failures pattern both are set to happen in the same object i.e on the row The reason for this setting is two-fold i it favors the HDFSRAID since at almost the same cost it can repair two failures instead of one ii if two failures happen on different rows the experiment will be in effect a variation of the onefailure pattern From the results shown in Figure 6 we can draw several conclusions  For single failure the overhead of CORE is less than 50 of HDFS-RAID This is quite signiìcant since in real-world clusters e.g in the Facebook cluster single f ailures per stripe are by far the most common type of failures This improvement results from two inherent advantages of CORE i single failure can be repaired vertically using far fewer blocks and ii it uses a much cheaper XOR operation instead of expensive decoding/re-encoding this is particularly signiìcant in the computation-critical cluster  The impact of our rst HDFS-RAID optimization Opt1 in Section VI-B can be seen in the results the difference between the 2nd and the 3rd chart bars As explained before this optimization is targeted speciìcally for the clusters in which the network is a scarce resource part b in Figure 6 The improvements are particularly pronounced in cases where the number of avoided block retrievals are higher e.g one failure in the scheme 9,6,3  The gains from our second HDFS-RAID optimization Op2 in Section VI-B are also noticeable the 5th and the 6th chart bars in all setups  Growth in the CORE matrix size from 9,6,3 to 14,12,5 results in even higher gains especially in clusters where computation power is scarce B Repair Scheduling Algorithms In this set of experiments the three repair scheduling algorithms of Section V-C were compared using the Step and Plus failure patterns HDFS-RAID has neither a notion of repair scheduling  it treats objects independently  nor can it fully recover from the Plus failure pattern so it was not considered in the following experiments These experiments were run for CORE matrix of size 14,12,5 The results are shown in Figure 7 and as expected the data part of this gure part a  mirrors the analytical results presented in Table II Moreover the completion time numbers parts b and c  are also to large extent in-line with the data results The only two discrepancies are explained below  Completion time of the Column-First algorithm on the Plus pattern in the network-critical cluster part b  is longer than expected This is caused by the last repair which uses two other freshly-repaired blocks Accessing those blocks is delayed until NameNodeês heartbeat-driven mapping tables are updated  Completion time of the RGS algorithm in the computationcritical cluster part c  is only slightly better than that of Column-First despite applying one vertical repair less see Table II for the schedules This is due to the fact that for these patterns the RGS and Column-First apply the same number of horizontal repairs and these are the main driving factor of the cost in the computation-critical cluster VIII C ONCLUSIONS AND F UTURE W ORK In this paper we demonstrated that some simple and standard techniques and thus easy to implement and organically 253 


0 0.5 1 1.5 2 2.5 3 Step Plus Row First Column First RGS B ytes R ea d GB a Transferred data 0 50 100 150 200 250 300 Step Plus Row First Column First RGS Repair Time seconds b Time network-critical cluster 0 50 100 150 200 250 300 Ste p Plus Row First Column First RGS Repair Time seconds c Time computation-critical cluster Figure 7 Performances of the repair scheduling algorithms on two different failure patterns integrate can provide signiìcant data repair and access boost in erasure coded distributed storage systems Specifically we studied our approach of introducing cross-object coding on top of normal erasure coding The ideas were implemented and integrated with HDFS-RAID available at  and benchmark ed o v er a proprietary cluster and EC2 Experiments with the implementation as well as accompanying analytical studies comparing the approach with not only MDS codes but also with the very recently proposed Local Reconstruction Codes used in Azure demonstrate the superior performance of CORE over state-of-the-art techniques for data reads and repairs While naive solutions can be readily used in future we will like to explore the CORE code properties to achieve better performance also during data insertion/updates The current evaluations are static based on snapshots of the system state We speculate that COREês better repair properties will yield a system in a better state over time We will thus carry out trace driven experiments to study the systemês dynamics better R EFERENCES  P  Elias Error Free Coding  Transactions on Information Theory  vol 4 no 14 1954  HDFS-RAID http://wiki.apache.or g/hadoop/HDFSRAID  C Huang et al Erasure Coding in W indo ws Azure Storage  in USENIX ATC  2012  A Datta et al Redundantly Grouped Cross-object Coding for Repairable Storage in Proc APSys  2012  P  Gopalan et al On the locality of code w ord symbols  Information Theory IEEE Transactions on  vol 58 no 11 pp 6925Ö6934 2012  K S Esmaili et al The CORE Storage Primiti v e  CrossObject Redundancy for Efìcient Data Repair and Access in Erasure Coded Storage CoRR  vol abs/1302.5192 2013  CORE http://sands.sce.ntu.edu.sg/StorageCORE  H W eatherspoon et al Erasure Coding vs Replication A Auantitative Comparison in Proc IPTPS  2002  J K ubiato wicz et al OceanStore An Architecture for Global-Scale Persistent Storage in Proc ASPLOS  2000  R Bhagw an et al T otal Recall System Support for Automated Availability Management in NSDI  2004  S Plank The RAID-6 Liber8T ion Code  Intl Journal of High Performance Computing Applications  vol 23 no 3 2009  A P atterson et al A Case for Redundant Arrays of Ine xpensive Disks RAID SIGMOD Records  vol 17 no 3 1988  O Khan et al Rethinking Erasure Codes for Cloud File Systems Minimizing I/O for Recovery and Degraded Reads in USENIX FAST  2012  B F a n e t al DiskReduce Replication as a Prelude to Erasure Coding in Data-Intensive Scalable Computing CMU Tech Rep CMU-PDL-11-112 2011  B Calder et al W indo ws Azure Storage A Highly A v ailable Cloud Storage Service with Strong Consistency in ACM SOSP  2011  A Thusoo et al Data W arehousing and Analytics Infrastructure at Facebook in ACM SIGMOD  2010  C Huang et al Pyramid Codes Fle xible Schemes to T rade Space for Access Efìciency in Reliable Data Storage Systems in IEEE NCA  2007  F  Oggier et al Coding T echniques for Repairability in Networked Distributed Storage Systems FnT in Communications and Information Theory  vol 9 no 4 2013  A Dimakis et al A Surv e y on Netw ork Codes for Distributed Storage The Proc of IEEE  vol 99 2011  A Duminuco et al Hierarchical Codes Ho w t o Mak e Erasure Codes Attractive for Peer-to-Peer Storage Systems in Proc P2P  2008  M Li et al GRID Codes Strip-Based Erasure Codes with High Fault Tolerance for Storage Systems ACM Trans on Storage  vol 4 2009  A K ermarrec et al Repairing Multiple F ailures with Coor dinated and Adaptive Regenerating Codes in Proc NetCod  2011  K W  Shum Cooperati v e Re generating Codes for Distributed Storage Systems in Proc ICC  2011  F  Oggier et al Self-Repairing Homomorphic Codes for Distributed Storage Systems in Proc INFOCOM  2011  F  Oggier et al Self-Repairing Codes for Distrib uted Storage A Projective Geometric Construction in Proc ITW  2011 26 D Papailiopoulos et al Locally Repairable Codes in Proc ISIT  2012  M S et al Xoring elephants No v e l erasure codes for big data Proceedings of the VLDBê13 To appear  2013  L P amies-Juarez et al Data Insertion  Archi ving in Erasure-coding Based Large-scale Storage Systems in Proc ICDCIT  2013  L P amies-Juarez et al RapidRAID Pipelined Erasure Codes for Fast Data Archival in Distributed Storage Systems in Proc INFOCOM  2013  Y  Hu NCFS On the Practicality and Extensibility of a Network-Coding-Based Distributed File System in Proc NetCod  2011  R Li et al CORE Augmenting Re generating-coding-based Recovery for Single and Concurrent Failures in Distributed Storage Systems in Proceedings of IEEE MSSTê13  2013  J I Hall Notes on coding theory  Citeseer 2003  K V  Rashmi and others A Solution to the Netw ork Challenges of Data Recovery in Erasure-coded Distributed Storage Systems A Study on the Facebook Warehouse Cluster in Proceedings of USENIX HotStorageê13  2013 254 


 L Kaufman and P  Rousseeuw  Clustering by means of medoids Technische Hogeschool Delft Netherlands Department of Mathematics and Informatics Tech Rep 1987  S Deerwester  S Dumais G Furnas T  Landauer  and R Harshman Indexing by latent semantic analysis  vol 41 no 6 pp 391Ö407 1990  C Boutsidis J Sun and N Anerousis Clustered subset selection and its applications on it service metrics in  2008 pp 599Ö608  C Boutsidis M W  Mahone y  and P  Drineas  An impro v ed approximation algorithm for the column subset selection problem in  2009 pp 968Ö977  C Boutsidis P  Drineas and M Magdon-Ismail Near optimal column-based matrix reconstruction in  2011 pp 305 314  J Dean and S Ghema w at MapReduce Simpliìed data processing on large clusters  vol 51 no 1 pp 107Ö113 2008  T  White  1st ed OêReilly Media Inc 2009  A Frieze R Kannan and S V empala F ast Monte-Carlo algorithms for nding low-rank approximations in  1998 pp 370 378  P  Drineas A Frieze R Kannan S V empala and V  V inay  Clustering large graphs via the singular value decomposition  vol 56 no 1-3 pp 9Ö33 2004  P  Drineas R Kannan and M Mahone y  F ast Monte Carlo algorithms for matrices II Computing a low-rank approximation to a matrix  vol 36 no 1 pp 158Ö183 2007  P  Drineas M Mahone y  and S Muthukrishnan Subspace sampling and relative-error matrix approximation Column-based methods in  Springer Berlin  Heidelberg 2006 pp 316Ö326  A Deshpande L Rademacher  S V empala and G W ang Matrix approximation and projective clustering via volume sampling  vol 2 no 1 pp 225Ö247 2006  A C  i vril and M Magdon-Ismail Column subset selection via sparse approximation of SVD  vol 421 no 0 pp 1  14 2012  A K F arahat A Ghodsi and M S Kamel  An ef cient greedy method for unsupervised feature selection in  2011 pp 161 170   Ef cient greedy feature selection for unsupervised learning  vol 35 no 2 pp 285Ö310 2013  T  Elsayed J Lin and D W  Oard P airwise document similarity in large collections with MapReduce in  2008 pp 265Ö268  A Ene S Im and B Mosele y  F ast clustering using MapReduce in  2011 pp 681Ö689  H Karlof f S Suri and S V assilvitskii A model of computation for MapReduce in  2010 pp 938Ö948  S Dasgupta and A Gupta An elementary proof of a theorem of Johnson and Lindenstrauss  vol 22 no 1 pp 60Ö65 2003  D Achlioptas Database-friendly random projections Johnson-Lindenstrauss with binary coins  vol 66 no 4 pp 671Ö687 2003  P  Li T  J Hastie and K W  Church V ery sparse random projections in  2006 pp 287Ö296  G Golub and C V an Loan  3rd ed Johns Hopkins Univ Pr 1996  A Deshpande and L Rademacher  Ef cient v olume sampling for row/column subset selection in  2010 pp 329 338  V  Gurusw ami and A K Sinop Optimal column-based lo wrank matrix reconstruction in  2012 pp 1207Ö1214  D D Le wis Y  Y ang T  G Rose and F  Li Rcv1 A ne w benchmark collection for text categorization research  vol 5 pp 361Ö397 2004  W Y  Chen Y  Song H Bai C.-J Lin and E Chang Parallel spectral clustering in distributed systems  vol 33 no 3 pp 568 586 2011  A T orralba R Fer gus and W  Freeman 80 million tin y images A large data set for nonparametric object and scene recognition  vol 30 no 11 pp 1958Ö1970 2008  N Halk o P G Martinsson Y  Shk olnisk y  and M T ygert An algorithm for the principal component analysis of large data sets  vol 33 no 5 pp 2580Ö2594 2011 
Journal of the American Society for Information Science and Technology Proceedings of the Seventeenth ACM Conference on Information and Knowledge Management CIKMê08 Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms SODAê09 Proceedings of the 52nd Annual IEEE Symposium on Foundations of Computer Science FOCSê11 Communications of the ACM Hadoop The Deìnitive Guide Proceedings of the 39th Annual IEEE Symposium on Foundations of Computer Science FOCSê98 Machine Learning SIAM Journal on Computing Approximation Randomization and Combinatorial Optimization Algorithms and Techniques Theory of Computing Theoretical Computer Science Proceedings of the Eleventh IEEE International Conference on Data Mining ICDMê11 Knowledge and Information Systems Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies Short Papers HLTê08 Proceedings of the Seventeenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDDê11 Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms SODAê10 Random Structures and Algorithms Journal of computer and System Sciences Proceedings of the Twelfth ACM SIGKDD international conference on Knowledge Discovery and Data Mining KDDê06 Matrix Computations Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science FOCSê10 Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms SODAê12 The Journal of Machine Learning Research Pattern Analysis and Machine Intelligence IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE Transactions on SIAM Journal on Scientiìc Computing 
180 


Bottom Top A B 
Figure 15 Figure 16 
messages seen for all workers in a superstep \(Figures 10 and 13\. When looking at the messages sent by workers in a superstep for METIS, we see that there are message load imbalances within work ers in a superstep, caused due to concentration of vertices being traversed in that superstep in certain partitions This variability is much more pronounced in CP as compared to WG \(Figures 11 and 14\ E.g. in superstep 9 for CP, twice as many messages \(4M\ are generated by a worker compared to another \(2M\.  For Pregel BSP, the time taken in a superstep is determined by the slowest worker in that superstep. Hence increase d variability in CP causes even çgoodé partitioning strategies to cause an increase in total execution time wh en using the Pregel/BSP model VIII A NALYSIS OF E LASTIC C LOUD S CALING  Cloud environments offer elasticity Ö the ability to scale-out or scale-in VMs on-demand and only pay for what one uses [28   On th e f l i p s i de  on e en ds u p  paying for VMs that are acquired even if they are underutilized. We have already shown the high variation in compute/memory resources used by algorithms like BC and APSP across different supersteps. While our earlier swath initiation heuristics attempt to flatten these out by overlapping swath executions, one can consider leveraging the cloudês elasticity to, instead, scale up and down the concurrent workers \(and graph partitions\ allocated in each superstep The peak and trough nature of resource utilization combined with Pregel/BSPês synchronous barrier between supersteps offers a window for dynamic scaleout and Öin at superstep boundaries. Peak supersteps can greatly benefit from additional workers, while those same workers will contribute to added synchronization overhead for trough supersteps We offer an analysis of the potential benefits of elastic scaling by extrapolating from observed results for running BC on WG and CP graphs, using four and eight workers.  To provide a fair and focused comparison, we turned off swath heuristics in favor of fixed swath sizes and initiation intervals Figure 15 \(Bottom\ plots the speedup of BC running on eight workers when normalized to BC running on four workers, at corresponding supersteps.  The number of workers does not impact the number of supersteps We also plot the number of active vertices \(i.e. vertices still computing for a given swath\these supersteps which is a measure of how much work is required \(Fig 15 \(Top\. We find that we occasionally get superlinear speedup spikes \(i.e. >2x\ that shows a strong correlation with the peaks of active messages, for both WG and CP graphs. At other times, the sp eedup is sublinear or even a speed-down \(i.e. <1\responding to inactive vertices.  The superlinear speedup is attributable to the lower contention and reduced memory pressure for 8 workers when the active vertices peak \(similar to what we observed for the swath initiation heuristics Similarly, the below par speedup during periods of low activity is contributed by the increased overhead of barrier synchronization across 8 workers. Intuitively, by dynamically scaling up the number of workers for supersteps with peaking active vertices and scaling them down otherwise, we can leverage the superlinear speedup and get more value per worker Using a threshold of 50% active vertices as the threshold condition for between 4 and 8 workers in a superstep, we extrapolate the time per superstep and compared this to the fixed 4 and 8 worker runtimes. We also compute the best-case run time using an çoracleé approach to i.e. for each superstep, we pick the minimum of the 4 or 8 workerês time.  Note that these projections do not yet consider the overheads of scaling, but are rather used to estimate the potential upside if we had an ideal or an automated heuristic for scaling. The total time estimates for running BC on WG and CP graphs, normalized to  
 plot shows speedup of 8 workers relative to 4 workers, for each superstep, when running BC on WG and CP graphs plot shows the number of vertices active in that superstep Estimated time for BC using elastic scaling, normalized to time taken for 4 workers. Normalized cost is shown on secondary Y axis WG graph shown on left CP graph shown on right. Smaller is better 
022\011 022\010 022\007 022\002 006 002 007 006 002 007 010 011 012 013 014 015 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 027\031\030\037\020#@\020"\031\030\027\020\035 0201!2#\024$#\015#5\024",\020"#\017\003"\003\031\003#\011#5\024",\020"\035 024"'\033\026\0309\0201#\\031\020 2 035#\032\020"#+!\034 017\020\021\022\023\024\024\025\026\020 027\030\031\022\032\033\031\020\034\031\035 017\020\021\022\023\024\024\025\026\020#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 027\030\031\022\032\033\031\020\034\031\035#?#/\027\031\030\037\020#@\020"\031\030\027\020\035 036\030\034\020\033"#\\0201!2 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 017\020\021\022\023\024\024\025\026\020#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035\031 006 006\003\007 006\003\011 006\003\013 006\003\015 002 002\003\007 002\003\011 002\003\013 006 006\003\002 006\003\007 006\003\010 006\003\011 006\003\012 006\003\013 006\003\014 006\003\015 006\003\016 002 011#5\024",\020 B\034\0267 015#5\024",\020 B\034\0267 1\0332\031\030\037\020 033\026\030\034\025 1\0332\031\030\037\020 036\024\017\020 024!\0341 024\035\031#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#.\024\035\031 027\030\031\022\032\033\031\020\034\031\035#+!\034\022&\030'\020#>\024"'\033\026\0309\0201#\031\024#\011#5\024",\020"#&\030'\020 011#5\024",\020"#&\030'\020 015#5\024",\020"#&\030'\020 024\035 031 
 
dynamically scaling ideal scaling 
Our hypothesis is that an intelligent adaptive scaling of workers can achieve a similar performance as a large, fixed number of workers, but with reduced cost 
213 


Nature Nature Ecological Applications Nature ACM International Conference on Management of Data \(SIGMOD In Parallel Object-Oriented Scientic Computing \(POOSC Science Communications of the ACM ACM Workshop on Mining and Learning with Graphs Communications of the ACM HotCloud Proceedings of the 19th ACM International Symposium on High PErformance Distributed Computing HPDC Knowledge and Information Systems KAIS International Conference on Computational Science IEEE International Conference on Cloud Computing Technology and Science ACM/IEEE Conference on Advances in Social Network Analysis and Mining \(ASONAM IEEE International Parallel and Distributed Processing Symposium \(IPDPS International Conference on Distributed Computing and Networking Journal of Mathematical Sociology International Conference on Parallel Processing Communications of the ACM 
 
observed time taken using 4 workers, are plotted in Figures 16\(A\ and 16\(B We see that our dynamic scaling heuristic using the percentage of active vertices achieves nearly the same CP\ or better \(WG\ performance as a fixed 8 worker approach. Clearly there is benefit of using fewer workers for low utilization su persteps to eliminate the barrier synchronization overhead. Also, the dynamic scaling heuristic performs almost as well as the ideal scaling. Finally, when we consider the monetary cost of the proposed approaches, assuming a pro-rata normalized cost per VM-second plotted on the secondary Y axis, we see that dynamic scaling is comparable \(CP\ or cheaper \(WG\ than a 4 worker scenario while offering the performance of an 8 worker deployment IX C ONCLUSION  In conclusion, we introduce optimization and heuristics for controlling memory utilization and show they are critical to performance.  By breaking computation into swaths of vertices and using our sizing heuristics we achieve up to 3.5x speedup over the maximum swath size that does not cause the a failure.  In addition overlapping swath executions can provide a 24% gain with automated heuristics and even greater speedup when a priori knowledge of the network characteristics is applied This evaluation offers help to eScience users to make framework selection and cost-performancescalability trade-offs. Our he uristics are generalizable and can be leveraged by other BSP and distributed graph frameworks, and for graph applications beyond BC. Our work uncovered an unexpected impact of partitioning and it would be worthwhile, in future, to examine the ability to pred ict, given certain graph properties, a suitable partitioning model for Pregel/BSP It may also be useful to perform such evaluations on larger graphs and more numbers of VMs. At the same time, it is also worth considering if non-linear graph algorithms are tractable in pr actice for large graphs in a distributed environment B IBLIOGRAPHY  1  F  L i lj er os C   Ed l i n g L  A m a r a l H  S t an ley   and Y    berg The web of human sexual contacts 
vol. 411, pp. 907908, 2001   H Je o n g  S   Ma so n A  L   B a ra b s i  a nd Z   Oltva i  L e t ha l i t y  and centrality in protein networks vol. 411, pp. 41-42 2001   O. B o din and E   E s t r ada    U s i n g n e t w ork c e nt r a l i t y  m e a s ures t o  manage landscape connectivity vol 18, no. 7, pp. 1810-1825, October 2008   D. W a ts s  and S  S t r ogat z  C olle c t i v e  d y nam i cs of  s m a ll-w orl d   networks vol. 393, no. 6684, pp. 440Ö442, June 1998   G  Ma lew i c z   M A u s t er n A   Bik  J   Dehn er t I  Hor n   N. L e i s er and G. Czajkowski, "Pregel: A system for large-scale graph processing," in 2010   D. G r egor  and A  L u m s dain e  T h e  pa r a llel  B G L  A gen e r i c  library for distributed graph computations," in 2005   B. S h a o  H. W a n g  and Y  L i T he T r init y G r aph E n g i n e    Microsoft Research, Technical Report MSR-TR-2012-30, 2012   A  F ox  C lo ud c o m putin g w h at  s  in it for m e  as  a  s c i e n tis t     vol. 331, pp. 406-407, 2011   S. G h e m a w a t  and J  De an   Map re duc e s i m p lifi e d data  processing on large clusters vol 51, no. 3, pp. 107-113, 2008   J  L i n and M. S c hat z   Des i g n  patt er n s  for eff i ci ent gr aph algorithms in MapReduce," in 2010   L   Va l i ant   A b r id g i n g m o d e l f or pa r a llel com putati o n  vol. 33, no. 8, pp. 103-111, 1990 12 a c h e  Ha ma    O n l i n e    http://hama.apache.org   13 Ap a c h e  Ha d o op    O n l i n e    http://hadoop.apache.org     M Z a h a r i a, M. Ch ow dhu ry M F r ank l in S  S h e n k e r, and I   Stoica, "Spark: Cluster Computing with Working Sets," in 2010   J  Ekana y ak e e t a l     T w i st er A  r untim e f o r it er ati v e  MapReduce," in Chicago, 2010, pp. 810-818   U. K a n g  C  T s o u rakakis   and C. F a l outs o s  Peg a s us   Minin g  Peta-scale Graphs," in 2010   M. P a c e  B S P vs  MapR e duc e    in vol. 103.2081, 2012   S. Seo  E  Yoo n, J  K i m  S  J i n  J-S. K i m   and S   Ma e n g HAMA: An Efficient matrix computation with the MapReduce framework," in 2010, pp. 721-726   S. S a l i h ogl u  and J  W i d o m  G PS A G r a ph P r oc e s s i n g Sy s t em    Stanford University, Technical Report 2011   R L i cht e n w a l t e r and N   Cha w la D is Ne t  A fr am ew ork for  distributed graph computation," in  2011   K  Maddu r i  D. E d i g er K   J i an g  D. Bad e r  and D  Cha v a r riaMiranda, "A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets," in 2009   E  K r e p s k a, T  K i el m a nn, W  F o kkink, H   Ba l, "A  hi g h level framework for distributed processing of large-scale graphs," in 2011, pp. 155-166   L   Pa ge  S  B r in R. M o t w ani and T  W i nogr ad  T h e P a geRank citation ranking: Bringing order to the web," Stanford InfoLab Technical Report 1999-66, 1999   U  Brand  s  A f a s t er  a l gor ith m for  b e t w eenn e s s c e nt r a l i t y    vol. 25, no. 2, pp. 163-177 2001   Stan fo r d  Net w or k A na l y s is Pro j e c t  O n l in e    http://snap.stanford.edu    I  S t ant o n and G  K l i o t, "S t r e a m i n g G r aph P a rtiti o n in g  for L a rge Distributed Graphs," Microsoft Corp., Technical Report MSRTR-2011-121, 2011   G   K a ry pis and V   K um a r A fas t and hi g h qua l i t y m u l t i l evel scheme for partitioning irregular graphs," in 1995, pp. 113-122   M. A r m b r u s t e t  a l   A v i ew of  c l o u d  c o m putin g    vol. 53, no. 0001-0782, pp. 50-58 April 2010  
214 


  13  or gani c  c he m i s t r y  i n our  Sol ar  Sy s t e m       Xi a n g  L i r e c e i v e d h i s B  S   m is tr y  fr o m  th e  P e k in g  U n iv e r s ity  C h in a  in  2 0 0 3  and P h D   i n P hy s i c al  C he m i s t r y  f r om  t he  J ohns  H opk i ns  Un i v e r s i t y  i n  2 0 0 9   He  h a s  b e e n  a  R e s e a r c h  A s s o c i a t e  wi t h  a  j o i n t  a p p o i n t m e n t  a t  t h e  U n i v e r s i t y  o f  M a r y l a n d   Ba l t i m o r e  C o u n t y  a n d  N AS A G o d d a r d  S p a c e  Fl i  Ce n t e r  s i n c e  2 0 1 1   H i s  r e s e a r c h  f o c u s e s  o n  t h e  d e t e c t i o n  of  t r ac e  e l e m e nt  and as t r obi ol ogi c al l y  r e l e v ant  or gani c  mo l e c u l e s  i n  p l a n e t a r y  s y s t e ms   l i k e  M a r s   He  i s  es p eci a l l y i n t er es t ed  i n  t h e d evel o p m en t  o f  T i m e of  and I on T r ap m as s  s pe c t r om e t e r s w i t h v a r i o u s i o n i z a t i o n  ng te c h n iq u e s   Wi l l  B r i n c k e r h o f f  sp a c e  sc i e n t i st  i n  t h e  Pl a n e t a r y  En v i r o n m e n t s  La b  a t  N A S A  s  G o d d a r d  Spac e  F l i ght  C e nt e r  i n Gr e e n b e l t   M D w i t h  pr i m ar y  r e s pons i bi l i t y  f or  th e  d e v e lo p m e n t o f th e  L D TO F  m a s s  s p e c t r o  th is  p r o je c t H e  h a s  fo c u s e d  re c e n t l y  o n  t h e  d e v e l o p m e n t  o f  m i n i a t u re  l a se r d ma s s  s p e c t r o me t e r s  f o r  f u t u r e  p l a n e t a r y  mi s s i o n s  a l o n g  wi t h  b a s i c  e x p e r i m e n t a l  r e s e a r c h  i n  a s t r o b i o l o g y  a n d  p r e bi ot i c  s y nt he s i s   D r   B r i nc k e r hof f  i s  i nv ol v e d i n t he  de v e l opm e nt  of  m as s  s pe c t r om e t e r  f or  bot h t he  2011 Ma r s  S c i e n c e  L a b o r a t o r y  a n d  t h e  2 0 1 8  E x o Ma r s  mi s s i o n s   


  14   


Copyright © 2009 Boeing. All rights reserved  Issues and Observations Initial load of one day of data ~ 7 hours Optimizations  Write data in batches  Use a mutable data structure to create data strings  Deploy a higher performance machine  Use load instead of insert  Use DB2 Range-Partitioned tables  Database tunings Time reduced from 7 hours to approx 30 minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Use a mutable data structure to create data strings  Original application created the SQL statement by appending elements to a Java String  It was taking five hours \(of the seven hours Strings  Instead Java StringBuilder used  Java Strings immutable  Time savings of 71.4 


Copyright © 2009 Boeing. All rights reserved  Optimizations Deployed on a higher-performance machine  Application ported from IBM Blade Center HS21 \(4GB of RAM and 64-bit dual-core Xeon 5130 processor to Dell M4500 computer \(4GB of RAM and 64-bit of quad-core Intel Core i7 processor  Reduced the time to thirty minutes Bulk loading instead of insert  Application was modified to write CSV files for each table  Entire day worth of data bulk loaded  Reduced the time to fifteen minutes 


Copyright © 2009 Boeing. All rights reserved  Optimizations Range-Partitioned tables \(RPT  To limit the size of tables, the original code created multiple tables per table type  This puts burden on the application to query multiple tables when a range crosses several tables  With RPT, user is not required to make multiple queries when a range crosses a table boundary  Increased the time to thirty minutes  Additional fifteen minute cost per day of partitioning enabled time savings during queries 


Copyright © 2009 Boeing. All rights reserved  Optimizations Database tunings  Range periods changed from a week to a month  Automatic table space resizing changed from 32MB to 512KB  Buffer pool size decreased  Decreased the time to twenty minutes Overall, total time savings of 95.2 


Copyright © 2009 Boeing. All rights reserved  20 IBM Confidential Analytics Landscape Degree of Complexity Competitive Advantage Standard Reporting Ad hoc reporting Query/drill down Alerts Simulation Forecasting Predictive modeling Optimization What exactly is the problem What will happen next if What if these trends continue What could happen What actions are needed How many, how often, where What happened Stochastic Optimization Based on: Competing on Analytics, Davenport and Harris, 2007 Descriptive Prescriptive Predictive How can we achieve the best outcome How can we achieve the best outcome including the effects of variability Used with permission of IBM 


Copyright © 2009 Boeing. All rights reserved Initial Analysis Activities Flights departing or arriving on a date Flights departing or arriving within a date and time range Flights between city pair A,B Flights between a list of city pairs Flights passing through a volume on a date. \(sector, center, etc boundary Flights passing through a volume within a date and time range Flights passing through an airspace volume in n-minute intervals All x-type aircraft departing or arriving on a date Flights departing or arriving on a date between city pair A,B Flights departing or arriving on a date between a list of city pairs Flights passing through a named fix, airway, center, or sector Filed Flight plans for any of the above Actual departure, arrival times and actual track reports for any of the above 


Copyright © 2009 Boeing. All rights reserved  Initial SPSS Applications Show all tracks by call sign 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case For a given Airspace Volume of Interest \(AVOI compute distinct traffic volume at some point in the future  Aim to alert on congestion due to flow control areas or weather if certain thresholds are exceeded  Prescribe solution \(if certain thresholds are exceeded Propose alternate flight paths  Use pre-built predictive model  SPSS Modeler performs data processing Counts relevant records in the database \(pattern discovery Computes traffic volume using statistical models on descriptive pattern Returns prediction with likelihood 


Copyright © 2009 Boeing. All rights reserved  Predictive / Prescriptive Analytics Use-Case Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  24 Pulls in the TRACKINFO table of MAIN using SQL Limits the data to database entries which fall inside the AVOI Combines the SOURCE_DATE and SOURCE_TIME to a timestamp that can be understood by modeler Computes which time interval the database entry falls in. The time interval is 15 minutes Defines the target and input fields needed for creating the model Handles the creation of the model Produces a graph based off of the model results Final prediction 


Copyright © 2009 Boeing. All rights reserved  Initial Cognos BI Applications IBM Cognos Report Studio  Web application for creating reports  Can be tailored by date range, aircraft id, departure/arrival airport etc  Reports are available with links to visuals IBM Framework Manager  Used to create the data package  Meta-data modeling tool  Users can define data sources, and relationships among them Models can be exported to a package for use with Report Studio 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 1 of 3 Report shows the departure date, departure and arrival locations and hyperlinks to Google Map images DeparturePosition and ArrivalPosition are calculated data items formatted for use with Google Maps Map hyperlinks are also calculated based on the type of fix 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 2 of 3 DeparturePosition, Departure Map, ArrivalPosition and Arrival Map are calculated data items \(see departure items below DepartureLatitude DepartureLongitude DeparturePosition Departure Map 


Copyright © 2009 Boeing. All rights reserved  Flights Departing Las Vegas on Jan 1, 2012 3 of 3 


Copyright © 2009 Boeing. All rights reserved  Conclusion and Next Steps Current archive is 50 billion records and growing  Approximately 34 million elements per day  1GB/day Sheer volume of raw surveillance data makes analytics process very difficult The raw data runs through a series of processes before it can be used for analytics Next Steps  Continue application of predictive and prescriptive analytics  Big data visualization 


Copyright © 2009 Boeing. All rights reserved  Questions and Comments Paul Comitz Boeing Research & Technology Chantilly, VA, 20151 office Paul.Comitz@boeing.com 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  31 


Copyright © 2009 Boeing. All rights reserved Advanced Arrival Procedures with Active Abatement Potentials   9/23/10  32 Backup Slides 


Copyright © 2009 Boeing. All rights reserved  Initial Approach Initial Investigations  Apache Solr/Lucene  Data Warehouse Evaluate Hadoop in the future 


Copyright © 2009 Boeing. All rights reserved  Using SOLR Uncompress Track Information Messages To use with Solr  Transforming track messages from their  original schema to Solr required building a ìkey, valueî list using an XSTL  Queries made against this list of ìkey, valueî pairs Transformation Process  One day of data ~ 4.5 hours Once transformation complete search/query performance very good Geo spatial queries using  unique query language 


Copyright © 2009 Boeing. All rights reserved  Representation Aviation data is frequently represented in more than one form 


