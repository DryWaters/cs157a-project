 The Algorithm of Rough Set-Based Grid Fuzzy Cluste ring & its Application and Research in Tongue Diagnosis of Traditional Chinese Medicine  Yuke Wei                    Jiangping Li                       Renhuang Wang  Faculty of Computer       Faculty of Material and Energy Sources          Faculty of Automation Guangdong University              Guangdong University               Guangdong Uni versity of Technology                      of Technology                        of Technology China                           China                                China weiyuke@gdut.edu.cn              J_p_lee@163.com                    wr h@gdut.edu.cn  Abstract The tongue intelligent diagnosis and inference system of Traditional Chinese Medicine\(TCM was a complex large-scale system which data quantity was  extremely huge specially along with long-distance networking diagnosis movement the data quantity increased dramaticaly. The system requested fast da ta search the fuzzy clustering analysis could solve t hese difficult problems This paper on the research clus ter analysis basic principle and above the algorithm foundation utilized one kind of improvement rough set-based grid fuzzy clustering algorithm in data mining for tongue diagnosis system of TCM, and made  the grid division first in front of the definition degree of membership function and formed a data bunch of basic shape and provided the real parameter information and participated hereafter degree of membership function definition The degree of membership function surmounted evaluation influence  bunch of shape factor. Algorithm through grid divis ion acceleration cluster process has overcome shortcoming of big time consumption quantity in the  traditional fuzzy clustering algorithm. The applica tion experimental result indicated the algorithm that t he paper have studied enhanced the speed, the reliabil ity and the rate of accuracy in tongue diagnosis system of TCM and realized the higher intellectualization t he digitized request 1   Introduction Tongue Diagnosis\(TD is the serious part of diagnostics of TCM it is considered as a comprehensive conclusion of pathogen nature position and tendency of disease. It is enhanced st udy's position of TD, so we should say that enhancing TD is significant on enhancing the diagnositic level and enlightening the special feature of TCM  TD of TCM system is a complex and large-scale system in whic h the data types are various, and the original data c an be structured, such as text, graphics, image data, and even in the distribution network of heterogeneous data The volume of data is extremely large especially with the long-distance interconnection diagnosis running it  is multiplied growth only the relevant patient information and data on the symptoms is a very alarming. The system is required to search data qui ckly using existing database technology is difficult to meet the requirement in this case only relying on the traditional method of treatment has too many proble ms to deal with. Faced with a deluge of data, how quic kly and  accurately search object which is one of the research hotspots in information processing technol ogy How to analyze and extract information that lies th e deep of data, which is also one of the focus studie s in information processing technology today This syste m is to deal with the deluge of data collected by TD of TCM survey  it requires to study an intelligent da ta search and analysis methods in order to  search an d extract quickly the rules and information in TD of TCM reasoning process This paper studied a method of grid-based clustering analysis by the requireme nts of the users or the actual operating system, realiz ing in mass data extracting useful information high-speedi ly and accurately used computer simulation the search  process in the human brain realized intelligence f or TD 0f TCM, and explored and engaged in the forefron t issues of the information processing technology To aim at TD of TCM data that have the characteristics of self-described and dynamic variability, and the large volume of data, we adopt the grid-based cluster analysis algorithm, dividing the data space of TD of TCM to limited grid structure. Durin g the constructing of data model and the diagnosis inference processing, their object is grid, and pro vides a solutions to found the profoundly information of data distribution at a heterogeneous database environmen t 000\003 2   Cluster analysis 2.1 The definition of cluster analysis 
Fifth International Conference on Fuzzy Systems and Knowledge Discovery 978-0-7695-3305-6/08 $25.00 © 2008 IEEE DOI 10.1109/FSKD.2008.541 77 


Definition 1 clustering clustering makes a group of relevant sets in accordance with some similarity  function or similarity criterion divide into some categories, in order to make the individual differe nces in the same type of category achieve minimum and that in the different category achieve maximum. A s et U, U contains n data objects {x 1   x 2  002 x n each data object x i  has m attributes namely data object can be expressed as x i  x i1  x i2  002 x im  Clustering is that making a data set U according to certain rules according to the similarity between data objects di vide into several \(for example: K\clustering C 1  002 C K 002 and satisfy as following conditions 002  1. C i 001 0 002 i=1,…k 002  2. C i 002 C j 0, i=1,…k, j=1,…k 002 i 001 j 002  3 001 k i k C 1  S Definition 2:A cluster is a set of data objects which are “similar” between them and are “dissimilar” to the objects belonging to other clusters Definition 3: Clustering analysis: It is that divid ed a given set of data objects into different clusters namely in the space X  given a limited sampling point set or from the database acquired a limited sample set x i  n i 1  The goal of clustering is that gathering data into categories, in order to make the similarity be tween individual in the same type of category achieve minimum and that in the different category achieve  maximize 2.2  A lgorithm of grid-based fuzzy clustering The method of grid-based measured the target space a limited number of units and formed a girde d structure in which all clustering operations are processed The main advantage of this method is its  speed quickly the processing time is independent o f the number of data objects and only relate to the number of units in each dimension of measure space  The method of grid-based clustering separates the data space into basic girded units which formed the  basis of a data cluster and through the judgment o f threshold value decide whether to merge the neighboring grid and further polymerize data cluste r By analyzing  integrate the thinking of rough set and fuzzy clustering on the operation of dividing g rid and first divide the data sets into grids, form the basic cluster formation Integrating the parameter information provided by the process of division  gi ve the simplified expression of the membership functio n to accurately reflect the distribution characterist ics The algorithm has a ability to learn again when receiving new data at the end of training, at first cluster according as the  membership function and then update the characteristics parameters of the releva nt clusters  in order to describe the environment on t he distribution of new data , and not like other clust ering methods as re-implement the original algorithm once  again The major steps on the processes of algorit hm are shown in Figure 1      Figure 1  The flowchart on the algorithm of grid-based fuzzy clustering 3. Data mining system in TD of TCM  3.1. The principle of tongue diagnosis system  The main symptoms of every illness are various There are many factors led to these include the men tal emotional environments social factors such as differences in physical condition and generally th ey are latent Therefore in order to accurately diagn osis illness state a variety of possible symptoms must be considered comprehensively and dialectically Each clinical symptom\(including physical characteristics\has a dialectical sense Each of th e symptoms for the diagnosis sense of the syndrome is  not a simple one-to-one relationship but a symptom  has different diagnostic value to a variety of synd romes the diagnosis of each syndrome often requires a var iety of clinical manifestations symptoms can make a diagnosis Therefore we should understand the dialectical meaning of various common symptoms Solve two problems 0017 What are the symptoms for some syndromes performance  0018 Each symptom of the syndrome how much is the contribution or confidence number And give a quantitative description Differentiation is mainly to identify the location of disease and pathogenesis at this stage, and its spe cific content is called as dialectic elements. The locati on of disease mainly includes heart, brain, lung, spleen liver kidney stomach gall small intestine large intes tine bladder uterus sperm room and so on The pathogenesis mainly includes wind, cold, wet, dry fire thermal   s p u t u m   b r e a t h  s t a g n a t i o n   b l o o d  s t a s i s   deficiency of vital energy blood deficiency Yin deficiency Yang deficiency and body fluid deficien cy and so on The more common and more normative clinical disease name generally is composed of the Finishing  mesh parameters  G iven all the data cluster membership function  relearning after clustering  Data preparation  Four sub-grid  Merger adjacent dense  grid  Improve shape of cluster s  
78 


different elements of disease location and pathogenesis , add certain pathology and conjunctio ns What all elements of the diagnosis in system was confirmed is 100 as a universal value that means it can be diagnosed as the dialectical elements, when the sum contribution that symptoms for elements of dialectical  reaches or exceeds 100 However the state of symptoms may be a little or more a light or heavy, so it should be followed by diagnostic thres hold for lifting conditioning. The threshold can be lowe r for light conditions and as state-symptom illness when  heavy or complex increased diagnostic threshold What each symptoms of condition is light or heavy the results of diagnosis are differences In genera lly the moderate is view as standard If the symptom is  serious, its quantitative value can be multiplied b y 1.5 while the symptoms is light and it can only multip ly 0.7 In clinical application at first the symptoms of patients are accumulative sum respectively includi ng minus the negative value\according to the tip of dialectical factors then take the item that their threshold value are more than 100,\(or higher projec t as dialectical diagnosis. Finally, linking and comb ining them what reach the diagnosis threshold of projects  and completing the diagnosis of the disease In our  system we are programming debugging and implementing in the environment of VC++ 6.0   3.2. The implementation process of diagnosis module  1\ Start 2\ Extract the weighted sum of the problem, obtain  primary and secondary symptoms  3\ characterized the vector of symptoms 4\ neural network learning reasoning  5\ gain the most likely syndrome  6 whether credible If credible show the diagnostic results, and the algorithm end  7\ If it's not, call the algorithm of grid-based f uzzy clustering or inquire doctor  8\ get the hidden symptoms or similar symptoms  9\ go to \(4  3.3. The algorithm describe  Questionnaire of tongue diagnosis clinical cases roots in the hospital outpatient data in the case including the Age Nationality Gerder Marriage Occupation Weight Height Fei Pi Gan Shen Tou  Dan, chang, Wei, ferver, cough, and many other fiel ds but some of the fields do not have great impact on the diagnosis, such as Name, Occupation, Weight, Height  and so on therefore in clustering analysis get ri d of them and see other fields as the dimension of clustering analysis When data processing the parameters of the disease are if it gets this disease the item is o ne no such disease it is zero If the patient gets heada che faint no stomach pain the data of headache is o ne and that of stomach is 0  For the dimension of age use the original data in the standardization of minimum to maximum to do linear transformation Referring to the existing da ta the min value is 0, the max value is 120, standardi ze all ages in the span [0.0,1.0   F o r  e x a m p l e   t h e  a g e  o f min is 0 the max value is 120 will be standardized 60  to 0.0,1.0   0 0.0 0.1 0 120 0 60    v 0.5 Actually using this formula can be directly used the age to divide 120 that is the data after conversion Afte r finishing the patient record form as Table 1 Table 1  results characterized the vector of symptoms Divide the data grid into four part by the followin g algorithm Parameters the value of the grid center’s  X  Coordinate the value of the grid center’s  Y  Coordinate the depth of grid the size of grid\( it can be calculated by Depth Divide the data grid into four part recursive  quartering\(nXCenter-nSize/2,nZCenter-nSize/2,nSize/2,d epth+1 quartering\(nXCenter+nSize/2,nZCenter-nSize/2,nSize/2 depth+1 quartering nXCenter+nSize/2,nZCenter+nSize/2,nSize/2, depth +1 quartering nXCenter-nSize/2,nZCenter+nSize/2,nSize/2 depth +1    This process divide the data space into four part grid and the process end when the grid is blank a nd the counting in the grid is change to 0. \(do the co unt by the count\(\ function\. If the  division  go to the end computing the value of the grid centre  A k    k G X i G d X k i 001                    Formula 3-1 002  X i means data vector, A k means the   abstract center of the grid what the expressions computing is the average of the attribute of the dimension  The data center” that have bean receive may be is  virtual data element group Then computing the radius of the data distribution in the grid name  gend age  ferver  cough  tou  wei   N1 0 0.32  0 1 1 0 N2 1 0.25  1 1 1 0 N3 0 0.1  0 0 0 1            
79 


R k max X i A k 2   X i G 001\031 k    002 Formula 3-2 002  And then  establish adjacent table for each thickness grid, records  all its adjacent grid, and used markers  to  mark the  dense grid  for use  after Computing  the distance of the two grid center d ij 2 001  p k jk ik x x 1 2          002 Formula 3-3 002  Judge  whether  the two grid  are the  dense grids is base in take  A i A j 2 003\004 cen   and d\(G i d\(G j   003\004 den 002 in here 004 cen R i R j 002 0.75 002  004 den min{ d\(G i  002 d\(G j 0.2: If the conditions are met dense grid then merge  the  two densely populated block After the merger, computing the center A k   and the radius R k of the new densely populated block       A k          j i j j i i G d G d G d A G d A       002 Formula 3-4 002  R k max{R i 002 R j  2   2 j i A A      002 Formula 3-5 002  Record the new value of the A k  and R k  Its algorithm processes are as follows 0017 Start 0018 Read the value of A k R k D\(G k  0019 If A i A j 2 003\004 cen and d\(G i d\(G j   003\004 den    then Merger them, and then computing the value of the ne w A k R k D \(G k  001 Record the value of new A k R k D\(G k  002  001 End Determine whether it is necessary to improve the form clusters by use of computing the distance between some  data point  X i and the center A k of densely populated blocks X i A k 2 003\004 dis  002 004 dis    8.0 k k G d R 002 If the requirements are met absorbing this point and every point X i  absorption by adding a dense grid currently adjust   the statistical information of the grid in time Th e formula for computing the new centre and the radius  are as follows A k  1       k i k k G d X A G d           002 Formula 3-6 002  R k max{R k 002 X i A k 2      002 Formula 3-7 002  Its algorithm processes are as follows 0017 Start 0018 Read in x i A k R k D\(G k  002  0019  If X i A k 2 003  004 dis   then  Merger them  and then computing the value of the new A k R k D \(G k  001 Record the value of new A k R k D\(G k  002  001 End Considering the close distance and the location of point,also the shape and size of cluster data and o ther factors Calculations, if D j R j 002  set the value by  005 1 0.5 003 1\0131 002 R j D j 003°\003 1 002 1 002 R j D j 003 0131  if R j D j 2R 002 set the value by  005 2 0.5 002 1 002 D j 013R j  006 2R j  002  if D j 2R j 002 set the value by  005 3 0.5 002 1 002 exp\(D j 013R j  001    Each receives a new data it is necessary to calculate it in the membership function mapping the  value of membership and in accordance with its 004  attribution determine the cluster’s ownership If membership values exceeds a given threshold 004 is that the data belongs to the corresponding cluster at t he same time to amend the statistical information of t he cluster When the point of all membership values have not achieved 004 record it into the grid information table, as isolated existence Timing scanning the grid information table found a spot on the grid cluster  concentration more than the minimum concentration requirements we generate a new cluster the cluste r record in the table registration relevant statisti cal information. Its algorithm processes are as follows   0017 Read in x j A k R k D\(G k   0018 Computing the distance D j  between  x j  and Clustering centre  0019 If  D j R j 002 then computing the value of m 1 002 goto  001   001 If D j R j   and D j 2R j 002 then computing the value of m 2   goto  001   001 Computing the value of m 3   001 If 005  004 002 then computing the value of  A k R k D\(G k   001 Record them into  grid information table in isolation, or as a new cluster  001 The end 4  Test results and analysis  According to the Guangzhou University of Traditional Chinese Medicine Research Group to provide the clinical cases the paper of the lung card category of simulation experiments using Visual C    6.0 as a development platform, and to establish the network diagnosis model Choose six of a total of 180 cases of card sample data for clinical samples Each sample consisted of  chest pain, chest tightness, cough, Tan Zhi, sputum less Jiubuyushi long have asthma or shortness of breath  and other symptoms of information 33 Sample data 
80 


permit classification as follows: 26 cases of lung cold beam, the cold drink in 30 cases, table 34 cases of cold heat heat Obstructing evil pulmonary 30 cases 28 cases of lung phlegm Yun, 32 cases of lung Tanreqin g Yu For six cards each for 10 cases of data as a traini ng sample of 120 patients with final data for testing  First data on 180 cases of normalized data processing the value of all the data transformatio n between [0,1   60 cases of training samples were divided into 28 groups produce  28 cluster centers including the threshold 007   0.3 It means that the hidden nodes desirable 28 Requirements for the experimental results were divided into six categories, set six output nodes Output 1,0,0,0,0,0 0,1,0,0,0,0 0,0,1,0,0,0 0,0,0  1,0,0 0,0,0,0,1,0 0,0,0,0,0,1 were identified that s ection 1,2,3,4,5, Category 6, so  the construction of netw ork structure is 33-28-6 Procedures use the follow rule for the output of th e network the output node with 6 dimension choose it s maximum value and then set 1 to it the other five  output set value to 0. Error use 2 1 1  2 N k k E e   001 N is the number of samples the program settings  when error  or the number of training are  more than 10,000 ,en d  the training, that is the end of the proceedings Experimental operating results  as shown in table 2. When the training times reach to 3935, errors E 0 procedural convergence verified  120 samples  and  the recognition rate was 92 and operating time for  two seconds From the results  after the clustering algorithm embed into the model, it can effectively search for the importation sample data  The clustering algorithm  can quickly cluster data for the high-dimension and big date Therefore the clustering algorithm  has a fa st convergence and a stronger ability to identify al so a stronger generalization ability, and other characte ristics It has  achieved the desired objectives Table 2   Asthma" dialectical test results Annotations: TN is the number of training  samples 002 TT is training times 002 HNN is the number of hidden nodes 002 Cer is convergence error 002 TSN is the number of testing samples 002 RR is recognition rate ; RT is running time\(second  5  Conclusion This paper mainly aimed at the characteristics that  the data type of tongue diagnosis system of TCM is diverse and the data volume is extremely huge and applied the algorithm integration with  the process of grid clustering and the thought of rough set An improvement of grid-based on fuzzy clustering algorithm was used in the data mining of the tongue  diagnosis of TCM This fuzzy clustering algorithm through acceleration the process of clustering tole rated the noise data so it  overcame shortcomings of expensive time-consuming in the traditional fuzzy clustering algorithm.The clustering mechanisms of combination of software and hardware were to overcome their own insufficient, making the process of enhancing the clustering accuracy and flexibility The algorithm was used in the tongue diagnosis system o f TCM so that the reasoning of the learning process could quickly and accurately obtain useful informat ion timely search the database of experience diagnosis  The algorithm was easy to adapt to different issues and environmental requirements and improved the diagnostic accuracy rate and speed  R eferences  1  Shao Feng-jing,Yu zhong-qing. Data Mining and Algorithm. China Water Conservancy and Hydropower P ress a ~ 271 ~ 30,197 2  J i a n g  W e i   c l u s t e r i n g  a l g o r i t h m  b a s e d  o n  t h e  r e search and application of. Master's degree thesis, Liaoning En gineering Technology University. 2004.2: 8 3  Zhexue Huang  Extensions To the K-Means Algorithm for Clustering Large Data Sets with Categorical Val ues. Data Mining and Knowledge Discovery 2 002 1998 002 283 003\032 304 002  4 J i a n n  X i a o y i 002  Karin Abennlen 002 Horst Bunke 002 Dynamic computation of generalized median strings 002 Pattern Anal 000\003 Application 002 2003,\(6 002 185 002  5  P a n  Y u q i   a n d  o t h e r s   F u z z y  c l u s t e r i n g  b a s e d  o n the analysis of the data retrieval application  Microel ectronics and Computer. 2005,22 \(6\: 167 ~ 172 6  G u  J u n h u a   e t c   F u z z y  c l u s t e r i n g  m e t h o d s  i n  t h e analysis of DNA sequence classification of. Computer simulat ion 2005,10:108 to 111 TN  TT HNN  Cer  TSN  RR RT  60  3935  28 0 120 92  2 
81 


 6 In Case 1.2, the modulation automatically adapts to the current SNR which significantly reduced the packet losses. In fact, 1.05% of packet losses all occurred during the mode change. If the packet losses that occurred during the mode change are eliminated \(requiring a minor software fix 0 In Case 1.3, DM was disabled and ARQ was enabled instead. Despite ARQ\222s retransmission attempts \(up to 7 7% of packets were lost. This demonstrates that during long fades, even 7 retransmissions were not enough to save those 7% of packets. The average latency of 951 msec indicates each packet is retransmitted about 1.3 times on average Note that the propagation delay is 250 msec and each retransmission adds 550 msec to the latency since the retransmission interval is 550 msec. Therefore, the average number of retransmissions is roughly estimated as \(951250\550 002 1.3. The maximum latency reflects the maximum number of retransmissions plus processing time and queuing delays. Compared to Case 1.1, the ARQ scheme reduced the PLR by about 50% at the cost of increased latency In Case 1.4, both DM and ARQ were enabled. Most of the packets made it through the fading channel, and the small number of packets dropped during the mode change was recovered by ARQ, which resulted in zero PLR. The increases in the average and maximum latencies compared to those from Cases 1.1 and 1.2 were due to retransmission of lost packets during the mode change In Cases 2.1 through 2.4, the channel fluctuated much faster than in the previous cases. Case 2.1 lost about 11% of packets, which is comparable to case 1.1 In Case 2.2, despite the DM scheme, about 9% of packets were lost. This was because it took some time for the DM scheme to measure the current SNR and determine a new modulation until the new modula tion was in effect, which took at least one round-trip time \(500 msec\Therefore, DM was not able to keep up with the channel In both cases 2.3 and 2.4, the ARQ scheme effectively recovered all of the lost packets caused by channel fades Case 2.4 achieved smaller latencies with the help of DM RxRate in Case 2.4 \(0.94 Mbps\urned out to be less than expected and thus needs further investigation Performance Evaluation with Real-Time Video Streaming In this scenario, cross-laye r performance of DM, ARQ, and ACA techniques is evaluated using a real-time streaming video application. The configuration presented in Figure 2 was used in this emulation. A VLC application running at VIDEO SERVER transmits an MPEG4-encoded video stream at the rate of 875 kbps to another VLC application running at VIDEO CLIENT via SALEM2, DELAY STRETCH, and SALEM1 nodes in sequence. A VLC application at VIDEO CLIENT displays the incoming video together with MPEG frame statistics. Note that in this emulation, the SNR was manually lowered to a certain level to emulate a channel fade Table 3 shows the test results from various cases. Each case was defined by a combination of DM, ARQ, and ACA options and an SNR value. The data rate in the table indicates the measured data rate by VIDEO CLIENT. VQI ranges between 0 and 100. The higher the VQI value, the better the quality of the video. In the following discussion, a frame refers to an MPEG frame, not a link-layer frame Initially, SNR was set to 36 dB and modulation was set to 16 QAM. Case 3.1 shows the result with the initial setup Since no frames were lost in this case, the result indicates the best-case scenario, as captured in Figure 11 \(a In Case 3.2, the noise level was increased to make SNR=18.1 dB and DM was not yet enabled. Since DM was not enabled, the modulation remained at 16 QAM, which caused more than 50% of the pack ets to be lost, as the data rate of 344 kbps and the frame rate of 10 fps indicate. The Table 2. Test Results with N2X Case ID Channel Fade DM ARQ PLR  RxRate Mbps Avg. Lat msec Max. Lat msec 1.1 Long OFF OFF 14.07 0.83 252 253 1.2 Long ON OFF 1.05 0.99 253 255 1.3 Long OFF ON 7.05 0.91 951 5568 1.4 Long ON ON 0 1.0 328 1317 2.1 Short OFF OFF 11.27 0.84 252 253 2.2 Short ON OFF 8.83 0.82 253 255 2.3 Short OFF ON 0 0.98 1528 3506 2.4 Short ON ON 0 0.94 1188 3144 Table 3. Test Results with Real-Time Streaming Video Case ID DM ARQ ACA SNR Modulation Data Rate Kbps Frame Rate FPS Avg. Frame Size \(Byte VQI Case 3.1 OFF OFF OFF 36 dB 16 QAM 873 31.2 2661 100 Case 3.2 OFF OFF OFF 18.1 dB 16 QAM 344 10.0 2126 45.4 Case 3.3 ON OFF OFF 18.1 dB QPSK 580 19.8 2144 51.9 Case 3.4 ON OFF ON 18.1 dB QPSK 579 29.9 1349 89.4 Case 3.5 ON ON ON 18.1 dB QPSK 628 33.6 1448 99.8 


 7 VQI dropped down to 45.4 and th e video was unintelligible as captured in Figure 11 \(b Case 3.3 enabled DM, and thus the modulation was switched to QPSK. In this case, since QPSK is more robust even with an SNR level of 18 1 dB, more packets survived as indicated by the data rate of 580 kbps and the frame rate of 19.8 fps. However, now the bandwidth of the channel was reduced by 50% and thus a lot of packets were still dropped, resulting in video quality that was still very poor This is indicated by a VQI of 51.9 and the screen capture in Figure 11 \(c Therefore, in Case 3.4, the application codec was commanded to reduce the data rate by half, which is reflected in the average frame size shortened by half. The VQI then significantly improved to 89.4 as well as the quality of the video, as shown in Figure 11 \(d\me packets were still lost, however, due to imperfect channel conditions and thus we turned on ARQ in Case 3.5 ARQ\222s retransmission mechanis m adds more packets onto the channel upon packet losses and thus we doubled the channel bandwidth without changing the modulation in Case 3.5. According to Table 3, the data rate and the frame rate in Case 3.5 are about 10% larger than those from Case     b c   d e Figure 11 226 Screen Captures from VLC CLIENT a  


 8 3.4, which reflects ARQ\222s recove ry of lost packets. Average frame size indicates that the a pplication codec generates the same streaming rate. The VQI improved to almost 100 and the quality of the video was also almost perfect, as shown in Figure 11 \(e\e that Case 3.1 captured better video quality than did Case 3.5 even though their VQI values are almost the same. This is because in Case 3.5 the video was generated in a lower resolution mode to achieve a lower data rate, which is reflected in the smaller average frame size in Case 3.5  In the emulation scenario with real-time video streaming, a combination of all three mitigation techniques significantly improved the quality of a streaming video over a certain noise level 4  C ONCLUSION  In this paper, we introduced a real-time emulation test bed by integrating STRETCH and SALEM, both of which are unique capabilities developed in-house at The Aerospace Corporation. Using this test bed, various combinations of three mitigation techniques DM, ACA, and ARQ were applied to a stream of raw UDP packets and a real-time streaming video. The test results demonstrate that a combination of DM and ACA works well with long durations of channel fades However, for a fading channel of short duration, the DM and ACA requiring SNR measurements do not adapt fast enough to mitigate the impairments. ARQ, however, works well in this scenario when lost frames can be retransmitted. In the real-time streaming video test, the combination of all three techniques achieved the best performance R EFERENCES   E Grayver and P Dafesh, \223Multi-Modulation Programmable Transceiver System with Turbo Coding,\224 IEEE Aerospace, Big Sky, MT, 2005   E M c Donal d R Speel m a n, E. Gray ver,  and N. W a gner 223Real-Time Hardware/Softw are Approach to Phase Noise Emulation,\224 IEEE Aerospace, Big Sky, MT, 2006  J. Kim  J. Hant, and V. Swam inathan 223Real-Tim e Emulation of Internet Applications over Satellite Links Using the SAtellite Link EMulator \(SALEM\AIAA San Diego, CA, 2008   VideoLAN- VLC m e dia play er, http://www.videolan.org   E Gray ver, J. C h en, and A. Ut t e r, \223Appl i cat i on-l a y e r Codec Adaptation for Dynamic Bandwidth Resource Allocation,\224 IEEE Aerospace, Big Sky, MT, 2008 B IOGRAPHY  Dr. Eugene Grayver received a BS degree in electrical engineering from Caltech and a PhD degree from UCLA in 2000. He was one of the founding team of a fabless semiconductor company working on low-power ASICs for multi-antenna 3G mobile receivers.  In 2003 he joined The Aerospace Corporation, wher e he is currently working on flexible communications pl atforms.  His research interests include reconfigurable implementations of digital signal processing algorithms, adaptive computing, lowpower VLSI circuits for communications, and system design of wireless data communication systems. Lately, he has been concentrating on the interaction between nonlinear amplifiers and modern error correction codes.  He is also participating in the software-defined radio community trying to define a common configuration standard and determine optimal partitioning between software and hardware  Dr. Joseph Kim received a BS degree in computer science from Seoul National University, Korea, and a PhD degree in electrical & computer engineering from University of California, Irvine in 1988 and 1998, respectively. He joined The Aerospace Corporation in July 2005 His current responsibilities incl ude architecture, design development, and integration of SAtellite Link EMulator SALEM\advanced space communication and networking pr otocols and applications under various satellite channels with interference. He has been supporting the Transformational Satellite Communications System \(TSAT\he Global Positioning System III programs. Prior to joining The Aerospace Corporation, he wo rked for several companies including several start-ups. The areas of work included HomePNA, HomePlug, WiFi, WiBro, ATM, CDMA2000 Satellite Digital Multimedia Broadcasting \(DMB\eoon-Demand \(VOD Service \(QoS\DVD/VCD Encoder/Decoder, MPEG, real-time scheduling and resource allocation, etc  Dr. Jiayu Chen received his BS degree in electrical engin eering from National Taiwan University in 1981. He received his MS and PhD degrees, both in electrical engineering from the University of Southern California USC\in 1983 and 1986, respectively He joined The Aerospace Cor poration in July 2005. His responsibilities include various satellite communication networking areas such as network protocols, QoS resource management, routing, etc. Prior to joining The Aerospace  


 9 Corporation, he worked for AT&T Labs \(formerly AT&T Bell Labs\for 19 years as a Senior Technical Specialist. He played an important role in the transformation of the AT&T telephony network including dynamic routing evolution and circuit switching to packet switching transition. His work included voice over IP \(VoIP\network architecture dynamic routing, numbering planning, network integration and testing. He received nine US patents as a result of his work at AT&T Labs. At USC, his research area included optical and radar target detection and medical applications of robotics  Dr. Eric McDonald received his BS degree in electrical engineering from the University of Pittsburgh in 1998, where he studied VLSI design. He continued his education at Cornell University and received his PhD in electrical and computer engineering in 2004. He focused on methods for synthesizing low-power analog circuits using floating-gate transistors while also gaining a breadth of knowledge in networking as well as asynchronous and synchronous digital design. He joined The Aerospace Corporation\222 s Digital Communication Implementation Department in November 2005 Alex Utter received a BSE degree from Harvey Mudd College in 2005, and an MS degree in electrical engineering from Stanford University in 2007. He joined The Aerospace Corporation in 2007.  His areas of expertise include software and hard ware design for digital electronic systems, especially design of FPGA- and microprocessor-based systems Dr. James Hant received a BS degree in biomedical engineering from the University of California, at San Diego \(UCSD\in 1993 and MS and PhD degrees in electrical engineering from the University of California, Los Angeles UCLA\in 1996 and 2000 respectively. From June 2000 to June 2001, he was a Systems Software Engineer at Conexant Systems in Newport B each, CA, designing and implementing speech recogniti on software for internet applications. He joined the Aerospace Corporation in June 2001, where he has worked on the simulation, modeling and design of satellite communication systems. Dr. Hant\222s research interests include networking, satellite payload design, and speech signal processing David Kun received his B.S. and M.S degrees in electrical engineering from the University of Minnesota-Twin Cities and the University of Illinois at UrbanaChampaign, respectively. Prior to joining The Aerospace Corporation in 2006, he worked as a system engineer and later as a design/test engineer for Northrop Grumman Space Technology. His research interests include signal detection and estimation theory, FPGA hardware prototyping of digital signal processing algorithms, and software defined radios    All trademarks, service mark s, and trade names are the property of their respective owners  


SPARQL query discussed in more detail in Section 6 has retrieved the SEAS Plane model information for a particular F-15 instance gure  Mvlititary Asset utnology A number of other military ontologies also exist The MilOrg ondology describes military and related political organizations as well as particular Roles they play in military affairs Tasks they are required to perform and specialized military Actions that perform them MilGeo defines a military perspective on geography with emphasis on terrain polpulation and cultural features of interest to military operations MilGeo contains classes that describe military facilities including Forts Airbases NavalPorts and Forward Operating Locations The MilSit ontology describes Conflicts Campaigns Missions Plans and similar concepts It also defines exactly what constitutes a situation and classifies various types of situations Specialized ontologies relating to military information and communications exist and may be imported in some cases to provide increased detail for those concerns Finally to support military Modeling and Simulation applications the MilSim ontology provides a platform for importing relevant ontologies for a particular study or exercise and a location to create concepts peculiar to individual simulations 10 


impodt Top Leve M d Ile Level Dom1ain Lave i 1po Scenwd1o Level Figure 8 Ilium Ontology Suite typical It is possible and in fact common to employ only a few of these ontologies in particular studies For example many studies are only concerned with physical platforms and therefore only need the MilAsset ontology which imports IlumAsset IliumFramework and DUL Figure 8 illustrates the existing Ilium Ontology Suite and a typical import pattern for a complex Modeling and Simulation application scenario Currently the illustrated suite less Millnfo MilComm and UAV defines 1240 OWL Classes 274 OWL Object Properties and 188 OWL Datatype Properties 6 AN EXAMPLE APPLICATION We are currently using the described approach to conduct systems requirements and other engineering analyses for military aerospace systems In these investigations it is useful to consider detailed aspects of the system design or software prototype behavior in the context of large scale network-centric military operations In the past year in particular we have assembled a collection of well known and widely accepted military simulations to prepare for an extensive investigation of requirements for autonomy in unmanned military platforms It is believed that useful insights in the study will be sensitive to important details of system behavior supporting sensor platform and communications technologies as well as the combined effects of these technologies on an advanced highly networked military force Thus the study environment must provide a means to reliably model and evaluate significant technical detail and to measure the effects as well as propagation of those effects throughout a broad operational context 11 


I r Z  Figure 9 A Military Modeling and Simulation Configuration of the Ilium Framework To do that we have assembled a collection of trusted military simulations that span the range of such applications from fine-grained simulations that focus on detailed interactions between two entities to coarse-grained simulations of military campaigns Figure 9 illustrates the Ilium Framework configured to accommodate those simulations as well as representative prototype systems In this case the legacy applications are simulations analysis systems and design tools Prototypes are notional UAV autopilots route planners decision support systems and similar applications The Framework augments these applications with control objects and agents that coordinate computations in the composed system and specialize agents that typically represent the characteristics of a new system or technology that cannot be easily or adequately simulated in any of the component legacy systems In our current study the following systems have Ilium Framework plugins and are used as components in the study environment 12 Al f<cOmmi.ain FIa 1A  


THUNDER is a campaign simulation that models national military forces and military operations that extend over months 30  Characteristics of individual systems are evaluated statistically and their effects on the overall campaign are not explicitly known Political social and cultural objects and concepts are not included in the model In this configuration THUNDER is used to generate force level tasking mission orders and to evaluate and adjust for the results of missions with respect to campaign plans SEAS is a force level simulation that simulates battles between major forces in combat operations that typically last for hours and as much as a day SEAS features a flexible rules based decision logic that can influence behavior at both the commander and individual combatant level SEAS executes the missions requested by THUNDER and provides a manageable dynamic context for examining the behavior of prototypes in a range of typical operational situations  31 A dynamic plug-in supports interaction with the Framework and other pugged-in components EADSIM is a trusted model of air defense systems that is in particular sensitive to many important design attributes of individual systems In certain modified forms it can reference advanced sensor and engagement decision models In our study configuration EADSIM simulates enemy air defenses in selected e.g significant to our investigation portions of the virtual battle A dynamic plug-in supports system control as well as interaction with the Framework and other pugged-in components A prototype aircraft mission planning system plug-in supports virtual real time mission plan creation and updates In particular the system provides automatic route planning for selected platforms that have been assigned missions by THUNDER and that are subsequently executed in simulation in SEAS and EADSIM The Ilium Framework itself provides software agents that are used to model notional or experimental UAV characteristics and behaviors Depending on the objectives of a particular study the Framework may also provide agents that address advanced Command and Control concepts to coordinate the interaction of the various component systems We maintain a semantic consistency among the plugged-in component applications by developing a single operational scenario as initial input for a study and deriving the necessary application configuration data from that source We create an RDF model of the scenario based on the Ilium suite of ontologies that includes political context issues objectives sensitivities etc military context centers of gravity campaign objectives etc geophysical environment military units order of battle unit equipment lists etc command and control assets platforms weapons ISR systems etc Figure 11 below is an excerpt from an operational scenario set in the Southwest U.S depicting a description of an Air Force Wing assigned to a notional Joint Task Force The Wing is based at Bishop Air Base and has six Fighter Squadrons assigned to it Additional detail about each of those squadrons as well as the base is found in the model in this case an rdf:resource  associated with it morg:AirForceWing rdf:ID="USAF_366 Air_Exp Wg dc:creator>Doug Holmes</dc:creator mgeo:basedAt rdf:resource="#BishopAB geo:positionedAt rdf:resource="#BishopAB_pos morg:assignedOrg rdf:resource="#USAF 390_Ftr_Sq morg:assignedOrg rdf:resource="#USAF_494_Ftr_Sq dc date I 0/20/07</dc date morg:assignedOrg rdf:resource="#USAF_496 Ftr_Sq rdfs:comment>366 AEW F-22 F-15 KC-135 130]</rdfs:comment morg:assignedOrg rdf:resource="#USAF 389_Ftr_Sq morg:assignedOrg rdf:resource="#USAF 391 Ftr_Sq morg:assignedOrg rdf:resource="#USAF 495_Ftr_Sq rdfs:label>366 Air Expeditionary Wing</rdfs:label morg:AirForceWing ab9 7P.b ii uI1 L L CFigure 11 An RDF model of a notional USAF Wing Figure 12 is another excerpt from the same scenario illustrating a model of a particular Fighter Squadron and one of the F-15E aircraft it operates Figure 10 Legacy Components in the Ilium Framework 13 suka 


morg:AirliorceSquadron rd:lD U SAF _8 htr Sq rdfs:label>8th Fighter Squadron</rdfs:label geo:positionedAt rdf:resource="#GeorgeAB_pos msim:hasSeasModel rdf:resource="#BAFGA mast:hasModel rdf:resource="#BAFGA rdfs:comment>An F15E Squadron</rdfs:comment dc:date>9/26/07</dc:date dc:creator>Doug Holmes</dc:creator mgeo:basedAt rdf:resource="#GeorgeAB morg:AirForceSquadron F1 5E rdf:ID="F1 5E 05 dul:isReferenceOfRealization rdf:resource="#AirObj ect_2007 mast:equipment-of rdf:resource="#USAF 7 Ftr_Sq dc:creator>Doug Hollmes</dc:creator dc:date>9/26/07</dc:date msim:hasSeasPlane rdf:resource="#F15E mgeo:basedAt rdf:resource="#GeorgeAB geo:positionedAt rdf:resource="#GeorgeAB-pos rdfs:label>F-15E 005</rdfs:label F-15E Figure 12 An RDF model of a Fighter Squadron and one of its aircraft Note that these models also have associated SEAS models that contain information peculiar to the SEAS simulation about these entities This information is used to configure SEAS to properly represent these particular entities We are also able to insert objects and information that may be of interest in our study that is not represented in any of the component simulations or other applications Where those objects are related to an object that is simulated that relationship permits inferences about the effects on them as a result of actions that are computed in a simulation For example it is possible to indicate that GeorgeAB defends the capital city and lend special significance to the actions of aircraft that are based there even though none of the simulations in the composite system have any notion of capital city Finally the operational scenario provides an explicit record of the assumptions that underly the study and can also include and explicit representation of system and study goals This practice improves analysis and may enable future knowledge based analytical tools Throughout the process of constructing the operational scenario and when it is complete we use one or more Description Logic Reasoners to ensure the logical consistency of the the model A number of these automatic theorem provers are freely available including Pellet 32 33 FaCT  34 and OWLIM 35 are used in the Framework to compute logical entailments and to complete RDF models as well as to ensure the consistency of models In the later capacity frequent checks will identify errors in the construction some e.g Pellet also indicate the source of the error and aid in repairing it As a result we are confident that the operational scenario is a sound model The primary use of the Reasoners allows us to significantly extend the explicit RDF model that is created For example a squadron is explicitly specified as assignedTo a wing and that relationship is the inverse of assignedOrg then the system can infer that the wing has the squadron as an assigned organization even though that fact has never been asserted Similarly if the same relationship is defined as a transitive relationship it is possible to infer that a flight that is assigned to the squadron is also assigned to the wing Once we have an operational scenario that has been classified by a Reasoner we then use the completed model to configure the composite system Ilium and the pluggedin systems to execute the simulation We use SPARQL a W3C standard query language designed to access RDFL to extract data from the scenario to create the input files needed to configure the various applications.[36 SELECT name pos long lat alt vis W1HLERE name rdf:type mast SeasLocation name geo:positionedAt pos pos pos:long long pos pos:lat lat pos pos:alt alt name msim:Visible vis  Figure 13 A typical SPARQL query In a similar fashion SPARQL is used to extract data from the scenario to create the necessary Ilium java surrogate control and agents SPARQL can also be used to review the scenario and answer questions that have arisen since the inception of the study At this point we are able execute the scenario with confidence that it will produce reliable results 7 CONCLUSIONS We have developed a methodology and supporting tools for creating an operational scenario that supports the semantic interoperability of an ad hoc collection of legacy applications and extends their capabilities The method depends on and ensures the logical integrity of the composite system Therefore when we assemble as a collection of legacy component systems that were not originally intended to interoperate with other systems we can be confidant that the composite system will produce consistent results Logical consistency implies that to the degree that we trust the interpretation underlying the model the results of operations on the model are trustworthy If for example the model is based on a Newtonian interpretation of physics the model ought to provide reliable answers to questions about automotive and even aeronautical engineering but probably not to all questions in astronomy or cosmology This methodology extends the utility of trusted simulations allowing integration of finegrained simulations that are sensitive to design requirements with high level coarse-grained simulations that are sensitive to acquisition issues and policy The potential benefits of 14 


this sort of interoperation may range from obvious production efficiencies to clearer insights into system requirements Finally an important side-effect of the approach is the OWL/RDF knowledge base formed by the combination of the operational scenario and the results of the operations of the legacy systems That knowledge base and the use of SPARQL queries and SWRL rules effectively expands the functionality of the system and greatly improves the analysis of the output of the system of cooperating simulations and tools We have prepared a foundation for the application of Semantic Web and other knowledge based tools in the analysis and design of unmanned systems We anticipate the development and application of these tools and the addition of autonomy directed Multiple Agent Systems MAS that use RDF/XML inter-agent communications in the coming year REFERENCES 1 NAFCAM 2001 Exploiting EManufacturing:Ineroperability of Software Systems Used by U.S Manufacturers available at 12 Protege Ontology Editor documentation available at http proteae.stanf6rd.edu 13 Top Braid Composer Datasheet available at 14 W and Nicola Guarino 2001 Support for Ontological Analysis of Taxonomic Relationships J Data and Knowled 39\(1 October 2001 15 Natalya F Noy and Deborah L McGuinness Ontology Development 10 1 A Guide to Creating Your First Ontology  Stanford University Stanford CA 94305 16 Alan Rector Modularisation of Domain Ontologies Implemented in Description Logics and related formalisms including OWL K-CAP'03 October 23-25 2003 Sanibel Island Florida USA pp 121-8 2003 17 Cyc Homepage available at htc.c 18 Open Cyc home page available at 19 SUMO Description and Home Page available at 19 SENSUS Description and Home Page available at 2 Bemers-Lee Tim Hendler Jim Lasilla Ora The Semantic Web Scientific American available at 20 DOLCE A Descriptive Ontology for Linguistic and Cognitive Engineering ontology and documents available at 3 Bemers-Lee Tim Blog on Design Issues available at 4 RDF Primer available at s chema/#ref-rdf-primer 5 Lacey Lee OWL Representing Information Using the Web Ontology Language Trafford Publishing 2005 6 OWL Web Ontology Language Guide available at Jten pHomewPage availal adt 7 Jena Home Page available at 8 Protege Home Page available at h1ttp  protegest nftanford.edu,X 1 9 Baader Calvanese McGuiness Nardi and PatelSchnieder Description Logic Handbook 10 Oberle Daniel Semantic Management of Middleware Springer 2006 OWL Web Ontology Language Guide available at 21 Nicola Gauarino Claudio Masolo Stefano Borgo Aldo Gangemi and Alessandro Oltramari Ontology Infrastructure for the Semantic Web Wonder World Deliverable DI 8 Laboratory for Applied Ontology Trento Italy 2001 available on line at ht X1t1 22 DUL.owl ontology available at www.1oa23t og DLl 23 Amy Knutilla Steven Polyak Craig Schlenoff Austin Tate Shu Chiun Cheah Steven Ray and Richard Anderson Process Specification Language An Analysis of Existing Representations NIST report available at http llwww.mel.nlist.gov/msid.librarZ/d.oc/psl-1 _.df 24 Process Specification Language Ontology available at http-//www,55,me l.nit gov/psl 25 Ontology for Geography Markup Language GML3.0 owl ontology available at ok i.cae.drexel.edu./-wbs/onltology/2004/09/ogc-gmI1 26 Ontology for Geography Markup Language GML3.0 of Open GIS Consortium OGC Home Page available at 15 


 Peter Maguire Using THUNDER for Campaign Studies DSTO-TN-0303 DSTO Melbourne August 2000 28 User Manual SEAS Version 3.7 U.S Air Force SMC/XR February 2007 29 Bijan Parsia and Evren Sirin Pellet and OWL DL Reasoner MINDSWAP Research Group University of Maryland College Park available at 30 Pellet Home Page available at 31 FaCT Home Page available at 32 OWLIM Home Page available at 3 A SPARQL Tutorial available at BIOGRAPHY Douglas Holmes is co-founder and Senior Partner of Java Professionals Inc In the past twenty-two years he has managed and participated in numerous artificial intelligence and knowledge-based programs for DARPA and other research agencies as well as commercial applications in the petroleum and other sectors He is currently developing ontologies and applying Semantic Web technology to support research and development of military unmanned systems He also has over twenty years experience as an Air Force Fighter Pilot and Fighter Weapons School Instructor Mr Holmes has a B.S in Mathematics and Basic Sciences from the U.S Air Force Academy and a M.S in Management Information Systems from Golden Gate University Richard Stocking is the lead Program Investigator/PM for Net Centric Operations Warfare Analysis efforts for Lockheed Martin Aeronautics Advanced Development Programs The Skunk WorksTM He is currently leading efforts researching autonomous UAV operations Current efforts include the integration of Multiple Agent Systems and other autonomy systems within the Ilium Framework He has over thirty years experience and over 11,000 flight hours with multiple C4ISR systems in the US Army and US Navy Mr Stocking has a M.S in Systems Technology from the Naval Postgraduate School 16 


 17 Chris Burton received an Associate degree in electronic systems technology from the Community College of the Air force in 1984 and a BS in Electrical Engineering Technology from Northeastern University in 1983.  Prior to coming to the Georgia Institute of Technology \(GTRI\ in 2003, Chris was a BMEWS Radar hardware manager for the US Air Force and at MITRE and Xontech he was responsible for radar performance analysis of PAVE PAWS, BMEWS and PARCS UHF radar systems Chris is an accomplished radar-systems analyst familiar with all hardware and software aspects of missile-tracking radar systems with special expertise related to radar cueing/acquisition/tracking for ballistic missile defense ionospheric effects on UHF radar calibration and track accuracy, radar-to-radar handover, and the effects of enhanced PRF on radar tracking accuracy.  At GTRI, Chris is responsible for detailed analysis of ground-test and flight-test data and can be credited with improving radar calibration, energy management, track management, and atmospheric-effects compensation of Ballistic Missile Defense System radars   Paul D. Burns received his Bachelor of Science and Masters of Science in Electrical Engineering at Auburn University in 1992 and 1995 respectively. His Master\222s thesis research explored the utilization of cyclostationary statistics for performing phased array blind adaptive beamforming From 1995 to 2000 he was employed at Dynetics, Inc where he performed research and analysis in a wide variety of military radar applications, from air-to-air and air-toground pulse Doppler radar to large-scale, high power aperture ground based phased array radar, including in electronic attack and protection measures. Subsequently, he spent 3 years at MagnaCom, Inc, where he engaged in ballistic missile defense system simulation development and system-level studies for the Ground-based Midcourse defense \(GMD\ system. He joined GTRI in 2003, where he has performed target tracking algorithm research for BMD radar and supplied expertise in radar signal and data processing for the Missile Defense Agency and the Navy Integrated Warfare Systems 2.0 office.  Mr. Burns has written a number of papers in spatio-temporal signal processing, sensor registration and target tracking, and is currently pursuing a Ph.D. at the Georgia Institute of Technology  


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


