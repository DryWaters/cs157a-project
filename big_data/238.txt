Incremental Mining of Association Patterns on Compressed Data Ng Vincent To-Yee Wong, Jacky Man-Lee Paul Bao Department of Computing The Hong Kong Polytechnics University Hung Hom Kowloon Hong Kong ABSTRAm Introducing data compression concept to large databases has been proposed for many years In this project we propose a new algorithm for the compression of large databases Our goal is to optimize the YO effort for finding association rules The algorithm partitions the databases into two parts and all transactions will 
be compressed with the help of a reference transaction found in the small partition We also compared the proposed compression algorithms with a normal compression algorithm  the binary compression Empirical evaluation shows that the proposed algorithm performs well both in reducing the storage space and the YO process required to find the large itemsets for association NleS 1 INTRODUCTION In the operations of a large database it is impossible to upload the whole database into the main memory This leads to the frequently read/write operations to the disk 
or tape As data volume increases the YO readwrite frequency pays an important role for the performance of database mining By reducing the YO operations during the process it is possible to improve the overall efficiency A possible method is to use compression to improve the efficiency of database mining In data compression, most of the algorithms are based on the sequential data compressors of Lempel and Ziv 1,2 and adopted a model of compression by Shannon\222s model of communication 3 
Some methods such as the statistical modeling technique 4 and the move to-front coding 5 are also popular with high compression rate However there are many differences between the data compression and database compression Many algorithms used in data compression are not suitable for database compression The difference between them is discussed in section 3 In section 2 we have the problem definition first Section 4 presents the idea of Tuple Differential Coding 
6 In section 5 we will introduce the Apriori-algorithm used to find the large itemsets for association rules IZ It is followed by the description of binary compression in section 6 and the proposed compression algorithm Referenced-Binary compression in section 7 Section 8 discusses the preliminary experimental results and we concludes in section 9 2 PROBLEM DEFINITION There are different kinds of databases Statistical database is one kind of databases that is 
commonly seen in daily life Statistical databases refer to the database holding information for statistical analysis They generally have the following characteristics 9  Usually large in size and the retention is indeJnite The data is normally in raw form More stable than ordinary databases Similar to relational databases that each data sei is made up by tuples and have a fued number of attributes The records are usually clustered Because of 
their nature, the statistical database requires a lot of U0 operations The different characteristics of a database may affect the performance of a compression algorithm In this project we focus on the statistical databases that have the following characteristics Large item range  Items per transaction is small Largedatavolume Here we focus on the optimization 
of YO operations in finding large itemsets for the association rules The algorithm we adopted to find the association rules is the Apriori Algorithm 12 Our idea of compression is based on the characteristics of the statistical The work of the authors are supported in part by the Central Grant of The Hong Kong Polytechnics University research project code GS979 0-7803-7@78-3/0~10 C IEEE Page 441 


databases that many tuples in these kinds of databases have similar pattems In binary compression many zeros are introduced after the compression However we observe that tuples in the statistical database are having similar pattems If we find a reference tuple we can represent the whole database with less space by storing only the difference between the tuples and the reference In our work we also consider we consider how to discover association rules incrementally when given 2 or more partitions of a database We also apply the compression algorithm on different partition ratios to review the final compressed file size The algorithm we proposed is divided into two parts The first part is pre-processing and the second part was the incremental mining The details of these two parts will be shown in section 7 3 DIFFERENCE BETWEEN DATA COMPRESSION AND DATABASE COMPRESSION The requirements of database compression are very different from data compression First of all in database compression data are inherently lossless We can only apply the lossless techniques so as to fully recover the data from the compressed format Existing lossless data compression techniques can be classified into two classes statistical techniques and textual substitution techniques The first one focuses on how to capture the frequency of occurrence of a source word in the data stream The second one belongs to the class of Lempel-Ziv techniques that compress data by replacing a string with pointers to previous occurrences of the same strings The requirements for database compression conflict with the ideas of data compression in many ways For example database compression techniques should not affect standard database operations such as tuple access insertioddeletion and any kinds of modification in the database We can say that database compression algorithms should support random and non-FIFO access to the compressed database This may lead to the data-model consistency problem of the data model proposed in 31 There are lot of database compression techniques such as the bit compression adaptive text substitution SI and the tuple differential coding 6,7l In this paper we focus on how to reduce the redundant information in the transactions of a statistical database with the ideas similar to the Tuple Differential Coding 4 THE TUPLE DIFFERENIAL CODING Wee Keong Ng and Chinya V Ravishankar introduced the TDC in 6 9 10 111 The idea of TDC is to capture and store the differences among the tuples If the differences require smaller space to store than the original format compression is achieved The detail description of the algorithm is in 7 The algorithm can be simplified as the following steps 1 Define the whole database as a R-space 2 Use a p  function to convert each tuple into a unique integer 3 Partition the whole database into several blocks 4 For each blocks use thefirst tuple as the reference 5 Record and store the diflence between each tuple and the reference tuple in each block In this project we are interested on how to find a good reference transaction and how to store the differences The details will be given in section 7 5 THE APRIORI ALGORITHM Rakesh Agrawal and Ramakrishnan Srikant proposed the Apriori algorithm 1121 This algorithm is used for finding the large itemsets in the process of finding association rules The algorithm and the notations are shown in Fig 1 and Fig 2 I Large I-itemset For m  2;1k f 0;mtt begh CDk  apriori-gen\(Ik.1 For all transactions T E D begin C subset of CD,,T For all candidates CD E CD begin CD.counter+t end Ik CDE CD I CD.counterbin-support end Final answer  q It Fig 1 The Apriori Algorithm Page 442 


CDK lk k  itemset The apriori-gen function in Fig 1 uses the set of all large k-I itemsets i.e Ik as the input and returns a superset of the Ik The followings show how this function works The set of candidates of K-itemset These candidates itemset were potentially the large itemset and each element in this set contains two fields 1 ltemset itself 2 support counter The set of large k-itemset which already satisfj the minimum support each of the element in this set contains two fields 1 Itemset itself 2 support counter The itemset having k items The insertion to CO Select the a.iteml,a.item2,.. .,a.itemk.l,b.itemk.I from Ik I,a Ik.,,b where the a.iteml=b.iteml  a.itemk  b.itemk.z a.itemk.1 b.itemk.l The Prune Step Delete the itemset CD E CQ such that the subset of k-1 itemset was not in it 6 THE BINARY COMPRESSION In database compression we need to consider the integrity of the database The data cannot be lost after the compression The binary compression algorithm is a lossless compression algorithm that transforms a transaction with ASCII numbers to a binary representation For each item in the transaction we use one bit to represent it 2231\224 for exist and 2230\224 for not exist For example if the item range is only 0 to 10 each transaction needs 11 bits to store i.e ASCII representation Binary representation Trans I 4589 0001100110 Trans 123 0111000000   When we use the ASCII format to store the transactions say transaction 1 we need 8 X 4 bits  32 bits 1 byte for one character to store the transaction but for binary representation we only need 1 1 bits to store the transaction Apparently the binary representation compression performs quite well in reducing the data volume Actually the performance of the binary compression greatly depends on the data distribution or the item range In the previous example the item range is only 0 to IO so each transaction only needs 11 bits to store If the item range changes to 0  500 we will then have ASCII representation Binary representation Grans 1 1 455 499 0 1 0 0   0 0 I 0 Trans 2 2 500 00 1 o  000 1 Now for transaction 1 in the previous example the ASCII format only needs 13 X 8  I04 bits including the space but after the binary compression it requires 501 bits The main problem of binary compression is when the item range gets bigger while the itemdtransaction is small the bit-vector contains a lot of zeros In the example, over 99 of the bits are zero 7 REFERENCED-BINARY COMPRESSION In performing association mining when we use the binary compression as a pre-processing step there many unused zeros in the bit-vector and it will slow down the mining performance Here we like to propose another compression algorithms that maintains the data integrity at the same time As before we divide the mining process into two parts the pre-processing and the incremental mining Here is the outline of the Referenced-Binary Compression 1 Partition the data file according to a threshold ratio say I:n processes the small partition and find the large item-set in this partition During this process we\222 also record which transaction contains the largest number of I-itemsets. This transaction will be used as the reference transaction in step 3 2 Output the large item-set found in step 1 to a file 3 Compress the whole data file according to the reference transaction in the following way U Convert the ASCII transaction format into the binary one 0-7803-7@78-WOl/$10.00 C IEEE Page 443 


Check byte-by-byte the difference between the reference transaction and the transaction to be compressed If the byte pattem is different record the position of this byte Repeat c until the whole transaction has been scan once For bytes that are different from the reference transaction build up a new bit stream, like the following Byte Byte mition content  Byte Byte wsition content Check if the size of the bit-stream formed in step e is greater than the originals bit stream If so write out the original bit stream otherwise write out the bit stream formed in e Repeat a to f until the whole data file was processed Fig 3 shows the block diagram of the algorithm After the pre-processing two files are generated The first one contains the large itemsets of the small partition of the data file The second file is the compressed data file The files generated by referenced-binary compression can be further processed After we have the two large item-set files the small partition and the large whole database according to the rules below Let S be the large itemsets for the small partition, and L be the large itemsets for the large partition partition we can find the overall large itemsets of the E Snxe L scan for x in L to check if the occurrence of x  support percentage If so then x is a large item-set for the whole data file 3y LnysS scan for y in S to check if the occurrence of x  support percentage If so then y is a large item-set for the whole data file 3x E s nx E L  x must be the large itemset for the whole data file The threshold ratio is chosen so that the workload of the pre-processing will not be so heavy If the ratio is too big the volume of the data processed in the pre processing will be large and it is not guarantee that we can find a better reference The main target of the pre-processing is to find out a reference transaction This transaction is used as the reference for the compression of the whole database When the differences between transactions are small we will only need to pre-process a small partition of the database file to get a representative transaction Database w Partitioning AA Large Partition Apriori Algorithm l-l Large iteinsets Fig 3 Block diagram of the Reference-binary algorithm 0-7803-7078-3/OU$lO.lM C IEEE Page 444 


8 EXPERIME~TAL RESULTS Initially we like to determine what is the appropriate partition-ratios to find the reference transaction in a database We create a data file with 10,000 transactions randomly and each transaction has an average of 30 items. In Fig 5 the results show the compressed sizes are optimal near the 25 partition ratio We carried out two other experiments One is to test for the improvement of the storage space and the other is to test for the improvement of the process time The parameters used in these experiments are Experiment 1  1 Total 10000 transactions in the data tile 2 10 data files with different item ranges were tested 3 Average number of items per transaction is 30 4 The support percentage is 5 5 Partition ratio is 1:4 1 10 data files with different number of transactions were tested 2 Average number of items per transaction is 30 3 The support percentage is 5 4 Partition ratio is 1 4 Experiment 2 In experiment 1 we try to compress different data files with different item ranges and recorded their file sizes shown in Fig 4 Oab Ske V S La Ram0 lor Dlnaal Data Fomu 12"Q I al  I Fig 4 Data Size V.S Item Range for Different Data Formats In the experiment 2 we process the compressed files to find the overall large itemsets and recorded the CPU time We repeat the same experiment with the original binary file without compression The result in Fig 6 shows that our algorithm out-performs the original one Fig.5 Different partition ratios V.S Compressed file sizes In Fig 6 we can see that the referenced-binary compression decreased the time for finding large itemsets Also as the item range increases only a small increase in the compressed file size is resulted If the data volume is large this difference in file size original and compressed can be significant Minii lime V.S MRmnd Fils Sue J Fig 6 Process time V.S Different file sizes with different data formats On the other hand, the referenced-binary compression algorithm has another advantage When new data was appended we can consider the new data as the large partition of the database describe in section 7 We can used the large itemset file of the original database as the reference for the new data and perform this algorithm again 9 CONCLUSION Many databases having the characteristics described previously One example is the selling records for a shop For each transaction the number of itemdtransactions was small and new data is added to the database everyday i.e fiequently update The 0-7803-7U78-3/0U$lO.W 02001 IEEE Page 445 


algorithm we proposed is suitable for this kind of databases For databases that need to be updated frequently and satisfy the characteristics listed in section 2 the referenced-binary compression gives an efficiency way to compress the databases When a database is updated, we do not need to scan the old database every time With the help of the itemsets file that stored the large itemsets of an old database we can decrease the workload of finding large itemsets for association rules With the referenced-binary compression the storage space the efficiency of analyzing or finding the association rules from the statistical database will be improved REFERENCE J Ziv and A.Lempe1 A universal algorithm for sequential data compression IEEE Transactions on Information Theory Vol IT 23 No 3 May 1977 pp.337-343 J.Ziv and A Lempel Compression of individual sequences via variable rate coding IEEE Transactions on Information Theory Vol IT-24 No 5 September 1978 pp 530 535 C.E Shannon 223A Mathematical Theory of Communication,\224 Bell System Technical J Vol 27 No 3 pp379-423 1948 T.Bell I.H Witten, and J.G Cleary Modeling for test compression ACM Computing Surveys Vo1.32  No.4 December 1989 pp 557-589 J.L Bentley D.D Sleator R.E Tarjan and V.K Wei A locally adaptive data compression algorithm Communications of the ACM Vol 29 No 4 April 1986 pp 320 330 W.K Ng and C.V Ravishankar 223Data Compression System and Methods Representing Records as Differences between Sorted Domain Ordinals Representing Fields Values,\223 U.S Patent No 5,603,022 Feb 1997 Wee Keong Ng Chinya V Ravishankar 223Block-Oriented Compression Techniques for Large Statistical Databases\224 IEEE Trransactions on Knowledge and Data Engineering Vol 9 No 2 pp314-328 March  April 1997 T.A Welch 223A Technique for High Performance Data Compression\224 Computer Vol 17 no 6 pp 8-19 June 1984 W.K Ng and C.V Ravishankar, \223Attribute Enumerative Coding A Compression Technique for Tuple Data Structures,\224 Proc Fourth Data Compression Conf p.461  Snowbird Utah Mar 29-3 I 1994 W.K Ng and C.V Ravishankar 223A Physical Storage Model for Efficiency Statistical Query Proceedings,\224 Proc Seventh IEEE Int\222l Working Conf Statistical and Scientific databases, pp 97-106 Charlottesville Va Sep 28-80 1994 W.K Ng and C.V Ravishankar 223Relational Database Compression Using Augmented Vector Quantization,\224 Proc 1 Ith IEEE Int\222l conf. Data Eng pp 540-549 Taipei, Taiwan March 6-10 1995 Rakesh Agrawal and Ramakrishnan Srikant 223Fast Algorithms for Mining Association Rules\224 Proceedings of the 20 VLDB Conference Santiago pp 487-499 Chile 1994 0-7803-7078-3/Ol/$lO.@l C IEEE Page 446 


Table 4 Running times in seconds for synthetic data sets We omitted parameter combinations where 11  12'1 because transaction size is too small for potential frequent subgraphs A dash in the table means we had to abort the computation for the set of parameters because of either memory exhaustion or taking too long time N 111 12'1 335 10 20 40 355 10 20 40 3 7 10 20 3 10 10 20 143 434 RunningTime[sec u=2 u=1 12 22 30 40 112 390 18 32 51 102 189 736 66 4512 5817  6110  1953  40  8290    20 40 255 10 20 2 7 10 20 2 10 10 20 U  2 10 16 20 35     27 52 25 1 2246   40  557 6203   40      20 20 I88 10 10 190 20 40 ime[secl U  1 17 25 40 98 18 51 119 246 816 I506 3199   53 71 196 279 N 111 IT1 u=2 u=l 10 16 28 20 34 38 RunningTime[sec u=2 I u=1 10 20 40 4055 10 20 40 40 7 10 20 40 40 10 10 20 27 44 44 47 84 89 20 28  29 60 55 131 177 234 197 1236 861 5273 2456 9183 9687    3271 10520 20 5 5 10 single bonding type. Essentially with U  lo this dataset becomes similar to the synthetic datasets where N  2 3.3 Summary of Discussions We summarize the characteristics of FSG performance First FSG works better on graph datasets with more edge and vertex labels During both candidate generation and frequency counting what FSG essentially does is to solve graph or subgraph isomorphism Without labels assigned determining isomorphism of graphs is more difficult to solve because we can not use labeling information as con straints to narrow down the search space of vertex mapping We can confirm it by comparing the results in Table 4 with various values of the number of edge and vertex labels N Second, the running time depends heavily on the size of frequent subgraphs to be discovered If input transactions contain many large frequent patterns such as more than 10 edges the situation corresponds to the parameter setting of 111  10 where FSG will not be likely to finish its compu tation in a reasonable amount of time The same thing hap pened with the chemical dataset with a support threshold less than 10 If we compare Figure 3\(a and Figure 3\(b we notice the running time increases at a higher rate than the number of discovered subgraphs does as we decrease the minimum support With a lower support criteria we start getting larger frequent subgraphs and both candidate generation and frequency counting become much more ex pensive On the other hand as for the cases of 111  5 in 10 19 20 51 u=2 u=1 10 20 25 20 40 20 7 10 48 117 182 233 193 804 Table 4 FSG runs fast The result of the chemical dataset is consistent with it For example if we use U  10 for the chemical dataset FSG spends 28 seconds to get 882 fre quent subgraphs in total The largest frequent graphs among them have 11 edges, and there are only 10 such frequent 11 subgraphs discovered Another important factor is the size of a transaction If the average size of transactions becomes large, frequency counting by subgraph isomorphism becomes expensive re gardless of the size of candidate subgraphs. Traditional fre quent itemset finding algorithms are free from this problem They can perform frequency counting simply by taking the intersection of itemsets and transactions As of the number of transactions FSG requires running time proportional to the size of inputs under the same set of parameters This is the same as frequent itemset discovery algorithms 4 Conclusion In this paper we presented an algorithm FSG for finding frequently occurring subgraphs in large graph databases that can be used to discover recurrent patterns in scientific, spatial and relational datasets Our experimen tal evaluation shows that FSG can scale reasonably well to very large graph databases provided that graphs contain a sufficiently many different labels of edges and vertices 319 


8000 7000  6000 25000 I 000 3000 2000 1000 a i     a Minimum support o and running time 25001 b Minimum support U and the number of discovered fre quent subgraphs Figure 3 Performance with the chemical compound dataset Acknowledgment We deeply thank Professor Takashi Washio Professor Hiroshi Motoda and their research group at the Institute of Scientific and Industrial Research Osaka University and Mr Akihiro Inokuchi at Tokyo Research Laboratory IBM Japan Ltd for providing the source code of AGM and use ful comments References l R C Agarwal C C Aggarwal V V V Prasad and V Crestana A tree projection algorithm for generation of large itemsets for association rules IBM Research Report RC21341 1998 2 R Agrawal and R Srikant Fast algorithms for mining as sociation rules In Proc the 20th VLDB pages 487-499 Morgan Kaufmann 1994 3 R Agrawal and R Srikant Mining sequential patterns In Proc the Ilth ICDE pages 3-14 IEEE Press, 1995 4 R N Chittimoori, L B Holder, and D J Cook Applying the SUBDUE substructure discovery system to the chemical toxicity domain In Proc the 12th Int Florida AI Research Society ConJ pages 90-94 1999 5 L Dehaspe H Toivonen, and R D King Finding frequent substructures in chemical compounds In Proc the 4th ACM SIGKDD KDD-98 pages 30-36 AAA1 Press 1998 6 D Dupplaw and P H Lewis Content-based image retrieval with scale-spaced object trees In Proc SPIE Storage and Retrieval for Media Databases volume 3972 pages 253 261,2000 7 S Fortin The graph isomorphism problem Technical Re port TR96-20 Department of Computing Science, Univer sity of Alberta, 1996 8 M R Carey and D S Johnson Computers and Intractabil ity A Guide to the Theory of NP-Completeness W H Free man and Company New York 1979 9 J Han J Pei, and Y Yin Mining frequent patterns without candidate generation In Proc ACM SIGMOD 2000 0 lo L Holder, D Cook and S Djoko. Substructure discovery in the SUBDUE system In Proc the Workshop on Knowledge Discovery in Databases pages 169-1 80 1994 I I A Inokuchi T Washio, and H Motoda An apriori-based al gorithm for mining frequent substructures from graph data In Proc PKDD'OO pages 13-23,2000 12 H Kalviainen and E Oja Comparisons of attributed graph matching algorithms for computer vision In Proc. STEP-90 Finnish ArtiJicial Intelligence Symposium pages 354-368 1990 13 M Kuramochi and G. Karypis Frequent subgraph discov ery Technical Report 01-028 Department of Computer Sci ence University of Minnesota, 2001 I41 D A L. Piriyakumar and P Levi An efficient A based algorithm for optimal graph matching applied to computer vision In GRWSIA-98 1998  151 http://oldwww.comlab.ox.ac.uk/oucVgroups/machlearn I61 R C Read and D G Corneil The graph isomorph disease Journal of Graph Theory 1:339-363 1977  171 A Srinivasan, R D King S Muggleton and M J E Stern berg Carcinogenesis predictions using ILP In Proc the 7th Int Workshop on Inductive Logic Programming volume 1297, pages 273-287 Springer-Verlag, Berlin 1997  181 A Srinivasan, R. D. King S H Muggleton and M Stern berg The predictive toxicology evaluation challenge In Proc the 15th IJCAI pages 1-6 Morgan-Kaufmann 1997 19 J R Ullman An algorithm for subgraph isomorphism Journal ofthe ACM 23\(1 1976 20 K Yoshida and H Motoda CLIP: Concept learning from in ference patterns Artificial Intelligence 75 1 1995  M J Zaki Scalable algorithms for association mining Knowledge and Data Engineering 12\(2 22 M J Zaki and K Gouda Fast vertical mining using diffsets Technical Report 01-1 Department of Computer Science Rensselaer Polytechnic Institute 2001 23 M J Zaki and C.-J. Hsiao CHARM An efficient algorithm for closed association rule mining,. Technical Report 99-10 Department of Computer Science Rensselaer Polytechnic Institute 1999 320 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


