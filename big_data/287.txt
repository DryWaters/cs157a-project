Mining Frequent Closed Itemsets with the Frequent Pattern List Fan-Chen Tseng, Ching-Chi Hsu*, and Henry Chen Department of Computer Science and Information Engineering National Taiwan University Taipei Taiwan 106 cchsu @csie.ntu. edu tw Abstract The mining of the complete set of frequent itemsets will lead to a huge number of itemsets Fortunately this problem can be reduced to the mining of frequent closed itemsets FCls which results in a much smaller number of 
itemsets The approaches to mining frequent closed itemsets can be categorized into two groups those with candidate generation and those without In this paper we propose an approach to mining frequent closed itemsets without candidate generation with a data structure called the Frequent Pattern List FPL We designed the algorithm FPLCI-Mining to mine the frequent closed itemsets FCls Experimental result shows that our method is faster than the previously existing ones Keywords frequent closed itemset frequent pattern list Corresponding author Fax 
886-2-236281 67 Tel 886-2-2391 7406 Email cchsu@csie.ntu.edu.tw 1 Introduction and Problem Definitions The mining of the complete set of frequent patterns often leads to a huge number of results and the effectiveness of the association rules derived from them will be decreased Fortunately Pasquier et al. showed that this problem could be solved by mining only frequent closed itemsets FCIs which are a small portion of the complete set of solutions They developed an A-Close algorithm  11 which took the 
generation-and-test approach for mining FCIs Recently Pei Han and Mao designed an algorithm CLOSET 2 for mining FCIs without candidate generation by a combination of FT-tree and projected database Here we use a simpler and more efficient data structure the frequent pattern list FPL 3 for mining frequent closed itemsets without candidate generation We redefine the frequent closed itemset and then develop our approach FPLCI-Mining for mining the frequent closed itemsets Let I  il i2 
 in be a set of items An itemset X is a non-empty subset of I A transaction database DB is a set of transactions Each transaction Tx is a pair <tid X where tid is a unique transaction identifier and X is an itemset A transaction Tx  tid X is said to contains an itemset Y if Y X Every item in Y is 
said to be contained in Tx if Y is contained in Tx With these descriptions we have the following definitions Definition 1 Maximal frequent itemset A frequent itemset is called a maximal frequent itemset if there is no other frequent itemset to be its proper superset Definition 2 Frequent closed itemset A frequent closed itemset is either a maximal frequent itemset or a frequent itemset whose support is higher than the supports of all 
its proper supersets 2 Algorithm for mining frequent closed itemsets with FPL Based on Definition 2 we take the following strategy for mining frequent closed itemsets FCIs finding frequent itemsets with items as many as possible and with supports as high as possible We formally define the algorithm FPLCI-Mining of mining frequent closed itemsets with FPL Algorithm FPLCI-Mining Input FPL constructed using a transaction database DB Output The complete set of frequent closed itemsets 
Method Call FPLCI-Mining\(FPL,n, t 7    FPLCI-Mining \(FPL n min-sup parent-itemset FCIS Forj=ntol  If there is no existing FCI in FCIS that contains item j  parent-itemset and has a support equal to item node and a support threshold t  check the necessity to visit item node j 0-7695-1119-8/01 17.00 0 2001 IEEE 653 


j Then  bit countingl Examine the bit-count array of item node j and ignore the LSB, which corresponds to item j since it will always be included in the solution;2. Divide the surviving bits \(whose counts are above min-sup into two groups Group One bit counts equal to item node j Group Two bit counts less than item node j 3 Generate a FCI denoted as fci-Groupone fci-GroupOne  Group one items}U item j}U parent-iternset with count  item node j 4 Use all the signatures in node item j with the surviving bits in Group Two as filtering mask to keep their corresponding items, to form fci-Groupone\222s conditional database and construct a conditional FPL FPL~ci-~,oupOnc from this database Let FPLf,i~,~,up have m item nodes 5 Call FPLCI-Mining FPLfcj-~rorrpOnrr m min-sup fci-Groupone FCIS  I end of bit counting  conducting signature trimming and migration For each transaction Tx in item node j consider its full-length j-bit signature  1 Trim the LSB corresponding to item j and then trim all the trailing 0-bits 2 Find the least significant I-bit and find the item corresponding to this bit 3 Migrate the trimmed signature to the item node containing this item 4 For the bit-count array of the target item node increment the count values by one for the elements that correspond to the I-bits in Tx  end of signature trimming and migration  I end of for loop of index j Remove item node j from the FPL   end of procedure FPLCI-Mining 3 Experimental Results and Discussion We test our algorithms on the synthetic data set T25.120.DIOOK There are 10K items The number of transactions is set to 100K The average transaction size and average maximal potentially frequent itemset size are set to 25 and 20 respectively To compare our method with the existing ones: A-Close I CHARM 4 and CLOSET 2 we run our program on a Pentium 233-MHz PC with 128 megabytes main memory running Microsoft Windows 98 The algorithms are implemented with Microsoft Visual C 6.0 The run time is the total execution time, including the time for disk YO and the time for constructing the FPL from the original databases The result is shown in Figure 1 From the result we see that FPLCI-Mining is much faster than A-Close and CHARM and is also faster than CLOSET by 12 percent in average The efficiency of our method is due to the following factors no candidate generation and testing simple data structures and simple operations Besides the optimization techniques of CLOSET can be implemented in our method For optimization 1 we encode the database into the global FF\222L and the conditional FPL can be derived directly in step 4 of FPLCI-Mining Optimization 2 is implemented in step 2 and 3 of FPLCI-Mining to extract items appearing in every transaction of the item node; that is, the Group-One items The single-path FP-tree of optimization 3 can also be detected in our FPL data structure, and the same technique can be used to speed up the mining process Finally for optimization 4 the checking before bit counting for the necessity to visit the item node prunes the search space e  A-Close  w M.0 10.0 0.0  0.7 09 1.1 1.3 1.5 Support threshold  Figure 1 Experimental results 4 Conclusions In this paper we proposed an efficient approach FPLCI-Mining to mining frequent closed itemsets with the simple structure FPL frequent pattern list There are several issues related to FTL-based mining For example the patterns in the transaction signatures should be studied to derive more efficient algorithms References I Pasquier N Bastide Y  Taouil R Lakhal L 223Discovering frequent closed itemsets for association rules.\224 Proc 7th Int Conf Database Theory ICDT\22299 pp 398-416, Jan 1999 2 Pei J Han J Mao R CLOSET 223An efficient algorithm for mining frequent closed itemsets.\224 Proc 2000 ACMSIGMOD lnt Workshop Data Mining and Knowledge Discovery DMKDOO pp 11-20 May 2000 3 Tseng Fan-Chen Hsu Ching-Chi 223Generating Frequent Patterns with the Frequent Pattern List.\224 Proc The Fifth Pacific-Asia Conference on Knowledge Discovery and Data Mining pp 376-386 April 16-18 2001 Hong Kong 4 Zaki M et al 223An Efficient Algorithm for Closed Association Rule Mining.\224 Technical Report 99 10 Computer Science Rensselaer Polytechnic Institute 1999 654 


each candidate sequence At the end of the pass it is determined which of the candidate sequences are actu ally large These large candidates become the seed for the next pass The two algorithms generate too many candidate sequences to be counted and the database needs t.0 be scaned repeatedly In 16 the DHP algorithm is shown to provide the best performance for large itemsets generation Hence DHP is used as the base algorithm to compare with our algorithm DLG The analysis and experimental results are shown in Section 3.3 AprioriAll and Apri oriSome have the similar performance which is shown in 5 We take AprioriAll algorithm to compare with our algorithm DSG The analysis and the experimen tal results are shown in Section 4.3 3 Association Rule Discovery In this section we present the algorithm DLG for efficient large itemset generation There are three phases in the DLG algorithm The first phase is the large 1-itemset generation phase which generates large items large 1-itemsets and records related informa tion The second phase is the graph construction phase which constructs an association graph to indicate the associations between large items In this phase, large 2-it,emset can also be generated The last phase is the large itemset generation phase which generates large k-itemsets IC  2 based on the constructed associa tion graph In the previous approaches a 4 11 13 la they all need to sort the items in each transaction in their lexicographic order However our approach need not to sort the items in each transaction 3.1 Association graph construction Before performing the DLG algorithm each item is assigned an integer number Suppose item i represents the item whose item number is i In the first phase algorithm DLG scans the database once to count the support and build a bit vector for each item The length of each bit vector is the number of transactions in the database If an item appears in the ith trans action the ith bit of the bit vector associated with this item is set to 1 Otherwise the ith bit of the bit vector is set to 0 The bit vector associated with item i is denoted as BV The number of 1\222s in BK is equal to the number of transactions which support the item i that is the support for the item i For example consider the database in Table 1 Each record is a TID Itemset pair where TID is the identifier of the corresponding transaction, and Itemset records the items purchased in the transac tion Table 1 A database of transactions Assume that the minimum support is 2 transac tions In the large 1-itemset generation phase the large items found in the database shown in Table 1 are items 1 2 3 and 5 and BVl BV2 BV3 and SV are lolo Olll 1110 and Olll respectively Property 1 The support for the itemset il,iz  ik is the number of 1\222s in BV A BV A  A Bx where the notation 224A\224 is a logical AND operation After the first phase the database need not be scanned again In the graph construction phase DLG constructs an association graph to indicate the asso ciations between items For the association graph if the number of 1\222s in BV A BV i  j is no less than the minimum support a directed edge from item i to item j is constructed Also itemset i,j is a large 2-itemset The association graph for the above exam ple is shown in Figure 1 and the large 2-itemsets are 1,3 2~31 2,5 and 3,5 1010 1 I I 0111 2  5 0111 1 110 Figure 1 The association graph and the bit vector associated with each large item for Table 1 3.2 Large itemset generation The large k-itemsets k  2 are generated based on the association graph constructed in the second phase The data structure used to implement the association graph is a linked list The large 2-itemsets La is found in the graph con struction phase In the large itemset generation phase the DLG algorithm generates large k-itemsets LI k  2 For each large k-itemset in LI k 2 a the last item of the k-itemset is used to extend the itemset into k  1-itemsets Suppose iI,iZ  ik is a large k-itemset If there is a directed edge from item ik to item U then the itemset il ia  ik is extended into k  1-itemset il,i2  ik The itemset il,i i~,u is a large k 1-itemset if the number of 1\222s in BV ABV A  A BV A BV is no less than the mini mum support If no large IC-itemsets can be generated the DLG algorithm terminates For example consider the database in Table 1 In the second phase the large 2-itemsets L2  1,3 2,3  2,5}, {3,5 is generated For large 2-itemset tl 2,3  there is a directed edge from the last item 3 of the itemset 2,3 to item 5 Hence the 2-itemset 2,3 is extended into 3-itemset 2,3,5 The number of 1\222s in BV2ABV3ABV5 i.e 0110 is 2 Hence the 10 


3-itemset 2,3,5 is a large 3-itemset since the num ber of 1's in its bit vector is no less than the minimum support The DLG algorithm terminates because no large 4-itemsets can be further generated After com pleting the DLG algorithm on Table 1 the large item sets are 1,3 2,3 2,5 3,5 and 2,3,5 The DLG algorithm for each phase is shown as follows  Large l-itemset generation phase  forall items i do for\(j=l;j<N;j dobegin  N is the number of transactions in database D  set all bits of SV to 0 forall items i in the jth transaction do begin i.count set the jth bit of BK to 1 end end forall items i in database D do begin L1  4 if i.count 2 minsup then  minsup is the minimum support threshold  L1  L1 U ij end  Graph construction phase  if L1  4 then begin forall large l-itemsets 1 E L1 do Lz  4 for every two large items i j i  j do begin allocate a node for Item[l and Item[l].link=NULL if the number of 1's in SV A By 2 minsup then begin  create an directed edge from i to j in the association graph  allocate a node p p.linle  Item[l,].link p.Item  lb Item[l,].link  p  generate large 2-itemsets  L2  L2U i,j j end end end  Large itemset generation phase  le  2 while Lk  4 do begin Lk+1 7 4 forall itemsets il iz  ik E Lk do begin pointer  Item[ik].linle while pointer  NULL do begin U  pointer.Item if number of 1's in Bx A  A BKL ASV 2 minsup then Lk+l  Llc+lu il  U pointer  pointer.link end end k=k+l end 3.3 Experimental results To assess the performance of the DLG algorithm for large itemset generation we perform several ex periments on Sun SPARC/10 workstation The exper iments show that the DLG algorithm is very efficient for large itemset generation because it takes only one database scan to generate large itemsets We first de scribe how the datasets are generated for the perfor mance evaluation We then compare the performance of DLG and DHP 16 by performing experiments on the generated datasets Finally we demonstrate the scale-up properties of the DLG algorithm 3.3.1 generation of synthetic data The synthetic database of sales transactions is gener ated to evaluate the performance of the algorithms The method to generate synthetic transactions is sim ilar to the one used in 4 The parameters used in our experiments are shown in Table 2 Average size of the potentially large itemsets Number of large itemsets Average size of the transactions Maximum size of the transactions Table 2 The parameters We first generate a set L of the potentially large itemsets and then assign a large itemset picked up from L to a transaction The size of each potentially large itemset is between 1 and MI The probabil ities for sizes 1 2  and MI are obtained by a Possion distribution with mean equal to 111 These probabilities are normalized such that the sum of these probabilities is 1 For example suppose average size III of the large itemsets is 3 and maximum size MI1 of the large itemsets is 5 According to the Possion distribution with mean 111 the probabilities for sizes 1 2 3 4 and 5 are 0.17 0.26 0.26 0.19 and 0.12 respectively after the normalization process These probabilities are then accumulated such that each size falls in a range which is shown in Table 3 For each potentially large itemset we generate a random real number which is between 0 and 1 to determine the size of the potentially large itemset I f I 0.18  0.43 1 0.44  0.69 Table 3 The probabilities for the sizes of itemsets The number of the potentially large itemsets in L is set to ILI Items in the first large itemset are cho sen randomly Some fraction of items in subsequent 11 


large itemsets are chosen from the previously gener ated large itemset For each item in the previous large itemset we flip a coin to decide whether the item will be retained in the current large itemset The remain ing items in the large itemset are picked at random After generating the set L of large itemsets we then generate transactions in the database The size of each transaction is picked from a Poisson distribution with mean equal to ITI and the size is between 1 and IMTI The method to determine the size of a transaction is the same as the method to determine the size of a large itemset For a transaction we randomly choose a large itemset from L to fit in the transaction and assign it to the transaction The remaining items of the first transaction are chosen randomly The frac tion of the remaining items of the subsequent transac tion are chosen from the previously generated trans action For each item in the previous transaction we also flip a coin to decide whether the item is retained in the transaction After choosing the items from a large itemset and from the previous transaction the remaining items in the transaction are picked at ran dom The same as 4 we also use a corruption level during the transaction generation to model the phe nomenon that all the items in a large itemset are not always bought together Each transaction is stored in a file system with the form of transaction identifier the number of items items We generate datasets by setting N  1000 and IL  2000 We choose three values for ITI 5 10 and 20 and the corresponding lMTl  10 20 and 40 respectively We choose two values for 11 3 and 5 and the corresponding MI1  5 and 10 respectively The number ID1 of transac tions is set to 100,000 We use Ta.MTz.lb.MIy to mean that a  ITI z  IMTI b  111 and y  MI We generate the following datasets for the experiments T5 MT 10 I3 Ml5 T 10 MT20 I3 Ml5 T1O.MT20.15.Ml10 and T20.MT40 I3.M I5 JLlI\(ILi 1 I logical AND operations on bit vectors to construct association graph and generate large 2 itemsets DHP needs to generate candidate 2-itemsets and prune these candidate 2-itemsets using the hash table created in the first pass Besides DHP needs to scan database to count support for candidate 2 itemsets and trim the database DB to generate a re duced database These jobs needed by DHP are more costly than these logical operations performed by DLG in the second pass In the kth k  2 pass DLG extends each large k  1-itemset into k-itemsets according to the asso ciation graph and performs logical AND operations Suppose on the average each node item has Q out degrees in the association graph DLG performs k  1 x LI x q logical AND operations to find all large k-itemsets Hence as the minimum support decreases the number of logical AND operations per formed increases because the two values ILk-11 and q increase In the kth pass DHP generates candidate k-itemsets Gk from large k  1-itemsets Lk-1 Af ter generating Ck DHP scans each transaction in the database DBI to count supports for these candidate k-itemsets and trim the database DBk to generate an other reduced database DBI Hence the execution time of DHP depends on the number of generated can didate itemsets and the amount of data that has to be scanned 3.3.2 Figure 2 shows the relative execution time for DHP 16 and DLG using the four synthetic datasets de scribed in Section 3.3.1 In these experiments the hash table size lHal used in DHP is set to  x Cy which was found to have better overall performance in ls where N is the number of items Suppose there are ID1 transactions in database DB and m items in each transaction on the average In the kth pass the large k-itemsets LI is generated For the first pass DLG and DHP both need to scan each transaction in DB to count support for each item By the way DLG records the bit vectors for each item However DHP needs to take extra overhead to combine every two items to form a 2-itemset in each transaction Totally there are Dl x CF combinations needed For each combination DHP uses the hash function to locate the 2-itemset in the hash table Hence DHP takes much more time than DLG in the first pass Suppose there are lLkl large itemsets generated in the kth pass In the second pass DLG performs comparison of DLG and DHP Table 4 Comparisons of DLG and DHP We perform an exper iment on dataset TlO.MT20.15.MIlO with minimum support 0.75 The experimental results are shown in Table 4 where MI denotes the number of transac tions in DBk and mk denotes the number of items in each transaction on the average In this experiment there are 238 nodes and 687 edges in the association graph Hence, on the average the out-degrees ofeach node is 3 Table 4 shows that in each pass the number of logical AND operations per formed by DLG is much less than the size of database scanned and the number of candidate itemsets gen erated by DHP Hence DHP takes much more time than DLG for large itemset generation Figure 2 shows that the DLG algorithm outperforms the DHP algo rithm significantly and the performance gap increases as the minimum support decreases because the num ber of candidate itemsets and the number of database scans increases for DHP 12 


0.5 1 1.5 2 2.5 3 3.5 Mini Support l%l Figure 2 Relative Execution Time 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 T5.MT10.13.MI5  TlO.MT20.13.MI5  TI0 .MTZO I5 MI10 D B 100 200 300 400 500 600 700 800 900 1000 Number of Transactions in 000s Figure 3 Scale-up Number of Transactions 3.3.3 discussions for DLG The memory space needed for performing DLG is dominated by the bit vectors Since the len th of each bit vector is the number of transactions fD1 in DB there are N x ID1 bits needed where N is the number of items In our experiments N  lo3 and ID1  lo5 Hence lo3 x lo5 bits 12.5MB are needed to store all bit vectors Figure 3 shows how DLG scales up as the number of transactions is increased from 10,000 to 100,000 trans actions We use the three datasets T5.MT10.13.MI5 TlO.MT20.13.MI5 and TlO.MT20.15.MI10 and set the minimum support to 1 As shown the execu tion times of DLG increase linearly as the database sizes increase because the number of large itemsets increases Next we examine how DLG scales up as the num T5.MT10.13.MI5  TlO.MT20.13.MI5  T10.MT2O.I5.MI10 0 0.1 0.2 1000 kzl==-l 2000 3000 4000 5000 6000 7000 8000 9000 10000 Number of Itemsets Figure 4 Scale-up Number of Items ber of items increases from 100,000 to 1,000,000 for the three datasets T5.MTlO.I3.MI5 TlO.MT20.13.MI5 and TlO.MT20.15.MIlO The minimum support is set to 1 for this experiment and the results are shown in Figure 4 The execution times decrease slightly because the number of large itemsets de creases as we increase the number of items 4 Sequential Pattern Discovery In this section we present the algorithm DSG for ef ficient sequential pattern generation In 5 the prob lem of mining sequential patterns is splitted into the following phases 1 Sort phase 2 Large itemset phase 3 Transformation phase 4 Sequence phase and 5 Maximal phase The Sort phase is to con vert the original transaction database into a database of customer-sequences The customer-sequence is a list of itemsets which are ordered by increasing transaction-times The Large itemset phase is to find all large itemsets \(or large 1-sequences The Transfor mation phase is to transform each original customer sequence into a transformed customer-sequence which is an ordered list of large itemsets The Sequence phase and the Maximal phase are the main portions for mining sequential patterns In these two phases we propose an algorithm DSG to generate sequential patterns which needs only one database scan The DSG algorithm is also splitted into two phases The first phase is the graph constructzon phase which constructs an association graph to indi cate the associations between large itemsets or large 1-sequences and records related information In this phase large 2-sequences can also be generated The second phase is the sequentaal pattern generataon phase which generates large k-sequences k  2 based on the constructed association graph and finds maximal large sequences or sequential patterns 13 


A 4.1 Association graph construction After completing the Transformation phase we are given a database of transformed customer-sequences In the graph construction phase DSG algorithm scans each customer-sequence in the database to combine every two large itemsets to generate a 2-sequence and count support for the 2-sequence For each Zsequence the set of identifiers of the customer-sequences where the 2-sequence appears is recorded When the sup port for a 2-sequence achieves the minimum support threshold the DSG algorithm creates a directed edge from the first itemset to the second itemset in the 2 sequence In the following 3 denotes the set of customer identifiers of the customer-sequences where sequence s appears The cardinality of 3s is equal to the number of customer-sequences which support the sequence s that is the support for the sequence s C1L I Csequence 1 I ABCD Table 5 A database of customer-sequences For example Table 5 is a database of customer sequences after completing the Transformation phase Each record is a CID Csequence pair where CID is the customer identifier of the corresponding customer sequence and Csequence is the customer-sequence Csequence is a list of large itemsets which are ordered by increasing transaction-times A large itemset is de noted by an alphabet Assume the minimum support is 2 customer sequences After scanning the database of customer sequences in Table 5 the association graph and the recorded information are shown in Figure 5 where the set of numbers on each edge XY is the set of customer identifiers of the customer-sequences where the large 2-sequence  X,Y  appears After completing the graph construction phase the large 2-sequences are CID and C,E and the set S<A,B of cus tomer identifiers of the customer-sequences where the 2-sequence A,B appears is 1 3 and so on 4.2 Sequential pattern generation In this section we describe how to generate large k-sequence k  2 based on the association graph and the recorded  and further to find se quential patterns The large 2-sequences LS2 is found in the graph construction phase In the sequential pattern generation phase the DSG algorithm gener ates large k-sequences LSk k  2 For each large k-sequence in LSk k 2 a the last itemset of the k-sequence is used to extend the sequence into k  1 sequences i A,B>, <A,C A,D B,D B,E C,B D Figure 5 The association graph and the set of iden tifiers of the customer-sequences where each large 2 sequence appears for Table 5 Property 2 The support for the k-sequence  SI s2  Sk  is the cardinality of the set 3<sl,sz fl s<s,,s n  n s<sk--l,s Suppose  SI sa1  sk  is a large k-sequence If there is a directed edge from itemset sk to itemset U the sequence  s1 sa  sk  is extended into k  1 sequence  SI s2  sk v  The k  1-sequence  SI s2  Sk U  is a large k+l-sequence if the support for the k  1-sequence is no less than the minimum support If no large k-sequence can be generated the DSG algorithm terminates After finding all large sequences LS the large se quences which are subsequences of the other large se quences are deleted from LS The remaining large sequences are maximal large sequences that is se quential patterns For example consider the database in Table 5 and the association graph in Figure 5 For large 2-sequence C,B there is a directed edge from the last itemset B of the sequence C,B to itemset E Hence the 2-sequence C,B can be extended into 3-sequence C,B,E and the set S<C,B,E can be obtained by performing set intersection on sets S<C,B and set S<B,E is 2,5 the set S.<C,B,E is {2,5 The 3 sequence CIBIE is a large 3-sequence, since the car dinality of set c,B,E is no less than the minimum support After completing the DSG algorithm on Ta ble 5 the sequential patterns are A,B AIC A,D>, <B,D CID and C,B,E The DSG algorithm for each phase is shown as follows  Graph construction phase  LS1   large 1-sequences   result of the Large itemset phase  if LS1  4 then begin forall large 1-sequences 1 E LS1 do c c  c s<B,E Because the set S<C,B is 2,3,5 and the allocate a node for Itemset and Itemset[l link=NULL forall permutation 1,1 where 1 and E are selected from LS1 do S<I,,I  4  3<lz,ly records the set of identifiers of the customer-sequences where I I appears  14 


LS2  b for i  1 i 5 N i   do begin  N is the number of customer-sequences in database D  scan the ith customer-sequence c I  the set of all itemsets in c forall combination lakb do begin  1 and lb are selected from I  c s<la,lb  Im<ra,lb i  record related information  if  la b  count 5 minsup then if  I lb  count  minsup then begin  la,lb  count    generate large sequences  CreateEdge\(l jb  create an edge from 1 to b in the association graph  Ls2=Ls2u{<l,,bb  end end end end CreateEdge\(l lb allocate a node p p.link  Item[l,].link p.Item  lb Item[l,].link  p  Sequential pattern generation phase  k=2 while Lsk  4 do begin LSk+l  4 forall sequences  SI s2  sk E Lsk do begin pointer  Itemset[sk].link while pointer  NULL do begin v  pointer.Itemset if the cardinality of set 3<sl,sz>n pointer  pointerlink c a n  n 3<sk,su 2 minsup then LSk+l  LSk+l U  s1 sa  Sk v  end end k=k+1 end Answer  Maximal sequences in Uk LSk 4.3 Experimental results To evaluate the performance of the DSG algorithm for sequential pattern generation we also perform sev eral experiments on Sun SPARC/10 workstation The experiments show that the DSG algorithm is very ef ficient for sequential pattern generation because it takes one database scan to construct an association graph and the large sequences are generated based on the association graph directly We first generate datasets for the experiments and then compare the performance between DSG and AprioriAll by perform ing experiments on the generated datasets The scale up properties of the DSG algorithm are also demon strated 4.3.1 generation of synthetic data The method to generate synthetic datasets is sim ilar to the one used in DLG algorithm The dif ference between the two methods is described be low For the generated dataset used in DLG algo rithm the items in each transaction are generated in their lexicographic order However for the generated dataset used in DSG algorithm the itemsets in each customer-sequence are generated in an arbitrary or der Each transaction is stored in a file system with the form of customer-sequence identifier the num ber of itemsets itemsets The parameters used in the experiments are as shown in Table 2 with some modifications In Table 2 the term transactions is changed to  customer-sequences the term item sets is changed to sequences the term items is changed to itemsets and the notation  L T and MT are changed to LS C and MC respec tively The number Dl of customer-sequences is set to 100,000 We also set N lo00 and lLSl 2000 for the generated datasets and generate the four datasets C5.MC10.13.MI5 C1O.MC20.13.MI5 ClO.MC20.15.MI10 and C20.MC40.I3.MI5 in the experiments 4.3.2 Figure 6 shows the relative execution time for Aprio riAll algorithm 5 and DSG algorithm over various minimum supports ranging from 0.5 to 3.5 Suppose there are M customer-sequences in the database and m itemsets in each customer-sequence on the average In the kth pass the set of large k sequences Lsk is generated In the second pass AprioriAll uses LS1 to gen erate 2 x CFsl candidate 2-sequences CS2 More over, AprioriAll scans the database to combine every two sequences to form a 2-sequence in each customer sequence Totally there are M x CF combinations needed For each combination AprioriAll searches for the candidate 2-sequences in CS2 to determine whether the combination is in CS2 for large 2-sequence generation However, when ILSlI is large 2 x Crsl becomes an extremely large number It is very costly to determine large 2-sequences from a large number of candidate 2-sequences In this pass DSG scans each customer-sequence in the database to combine every two sequences to form a 2-sequence and count sup port for the 2-sequence to determine whether the 2 sequence is large Because AprioriAll needs to search for a large amount of candidate 2-sequences DSG out performs AprioriAll in this pass In the kth pass k  a AprioriAll generates candi date k-sequences based on large k 1-sequences LSk-1 and scans the database to count supports for the can didate k-sequences for large k-sequence generation AprioriAll needs to combine every k sequences to form a k-sequence in each customer-sequence and totally M x Cp combinations are needed For each combina tion AprioriAll searches for candidate k-sequences in CSk to determine whether the k-sequence is in CSk for comparison of AprioriAll and DSG 15 


0.5 1 15 2 2.5 3 3.5 Minimum Support 1 Figure 6 Relative Execution Times large k-sequence generation Hence as the minimum support decreases the execution time of AprioriAll in creases because the candidate sequence generated in creases and the number of database scans increases For DSG algorithm the large k-sequences k  2 can be generated by extending large k  1-sequences into k-sequences based on the association graph and performing set intersections on the related informa tion Suppose on the average the number of out degrees of each node itemset in the association raph is q In the kth i k  2 pass DSG performs A-1 x q x ILSk-1 I set intersections to find all large k sequences Hence as the minimum support decreases the number of set intersections performed increases because the values q and ILSk-11 increases However DSG need not generate candidate k-sequences k 2 1 nor scan the database for large k-sequence generation Since the number of set intersections performed for DSG is much less than the size of database scanned and the number of candidate itemsets generated for AprioriAll DSG outperforms the AprioriAll for var ious minimum supports Figure 6 shows that the performance gap increases as the minimum support decreases because the number of candidate itemsets generated by AprioriAll increases and the number of database scans also increases k 5 2 4.3.3 discussions for DSG The main memory space needed for performing DSG is to store customer identifiers on each edge in the association graph Suppose there are 1 edges in the association graph and on the average the cardinality of the set of customer identifiers on each edge is k 1 x k customer identifiers need to be stored Figure 7 shows how DSG scales up as the num ber of customer-sequences increases from 10,000 to 100,000 customer-sequences We use the three datasets C5.MC10.13.MI5, ClO.MC20.13.MI5 and I 100 200 300 400 500 600 700 eoo 900 1000 Number of Customers in OOOsi Figure 7 Scale-up Number of Customers 0.3  I 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Number of Itemsets Figure 8 Scale-up Number of Itemsets ClO.MC20.15.MI10 and set the minimum support to 1.5 As shown the execution time of DSG in creases linearly as the database size increases because the number of large sequences increases Next we investigate the scale up as we increase the number of itemsets from 1,000 to 10,000 for the three datasets C5.MCIO.I3.MI5 CIO.MC20.13.MI5 and ClO.MC20.15.M110 The minimum support is set to 1.5 for this experiment Figure 8 shows the results When the number of itemsets increases the execution time decreases slightly because the number of large sequences decreases 5 Conclusion and Future Work We study two problems mining association rules and mining sequential patterns in a large database of customer transactions The problems of mining asso ciation rules and mining sequential patterns focuses 16 


on discovering large itemsets and discovering large se quences respectively We present two algorithms DLG and DSG which need only one database scan for efficient large itemset generation and efficient sequential pattern generation respectively These two algorithms construct an as sociation graph to indicate the associations between items and then traverse the graph to generate large itemsets and large sequences respectively We compare DLG and DSG algorithms to the pre viously known algorithms DHP 16 and AprioriAll 5  respectively The experimental results show that DLG and DSG outperform DHP and AprioriAll re spectively When the minimum support decreases the performance gap increases because the number of candidate itemsets candidate sequences generated by DHP AprioriAll increases and the number of database scans also increases We demonstrate that the execution time of these two algorithms increases linearly as the database size increases and the execution time decreases slightly as the number of items itemsets increases For our graph-based approach the related informa tion may not fit in the main memory when the size of the database is very large In the future we shall de velop a mining algorithm based on our graph-based approach such that in a very large database environ ment the mining algorithm can also be run in the main memory We shall consider mining various dif ferent relationships among data in a large database of customer transactions such as is-a relationships and part-of relationships We shall also apply our graph based approach on different applications such as doc ument retrieval and resource discovery References R Agrawal and et al Database Mining A Per formance Perspective In IEEE Transactions on Knowledge and Data Enganeering pages 914-925 1993 R Agrawal and et al Mining Association Rules Between Sets of items in Large Databases In Proceedzngs of ACM SIGMOD pages 207-216 1993 R Agrawal and et al An Interval Classifier for Database Mining Applications In Proceed ings of International Conference on Very Large Data Bases pages 560-573 Vancouver British Columbia 1992 R Agrawal and R Srikant Fast Algorithm for Mining Association Rules In Proceedzngs of International Conference on Very Large Data Bases pages 487-499 1994 R Agrawal and R Srikant Mining Sequential Patterns In Proceedings of International Confer ence on Data Engineering pages 3-14 1995 Y Cai N Cercone and J Han An Attribute Oriented Approach for Learning Classification Rules from Relational Databases In Proceedzngs of International Conference on Data Engineering Los Angeles pages 281-288 Feb 1990 7 W Chu and et al Using Type inference and Induced Rules to Provide Intensional Answers. In Proceedings of International Conference on Data Engineering pages 396-403 1991 8 M Hammer and S.B Zdondik Knowledge-based query processing In Proceedings of International Conference on Very Large Data Bases pages 137-146 1980 9 J Han and et al Knowledge Discovery in Databases An Attribute-Oriented Approach In Proceedangs of International Conference on Very Large Data Bases pages 547-559 1992 lo J Han and et al Data-Driven Discovery of Quan titative Rules in Relational Databases in IEEE Transactions on Knowledge and Data Engineer ing pages 29-40 1993 ll M Houtsma and A Swami Set-Oriented Mining for Association Rules in Relational Databases In Proceedings of International Conference on Data Engineering pages 25-33 1995 la C Malley and S Zdonik A Knowledge-Based Approach to Query Optimization In Proceedings of the First Expert Database System Conference pages 243-257 1986 13 H Mannila H Toivonen and A.I Verkamo Efficient Algorithm for Discovering Association Rules In Proceedings of AAAI Workshop on Knowledge Discovery zn Databases pages 181 192, 1994 14 A Motro Using Integrity Contraints to Provide Intensional Answers to Relational Queries In Proceedzngs of International Conference on Very Large Data Bases 1989 15 G Oosthuizen Lattice-Based Knowledge Dis covery In Proceedings of AAAI Workshop on Knowledge Discovery in Databases pages 221 235 1991 16 J.S Park M.S Chen and P.S Yu An Effec tive Hash-Based Algorithm for Mining Associa tion Rules In Proceedings of ACM SIGMOD 24 2  175 186, 1995 17 M.S.E Sciore and et al A Method for Automatic Rule Derivation to Support Semantic Query Op timization In ACM Transactions on Database Systems pages 563-600 1992 18 M Siegel Automatic Rule Derivation for Se mantic Query Optimization In Proceedings of the Second International Conference on Expert Database Systems pages 371-385 1988 17 


19 U.Chaliravarthy D Fishman and J Minker Semantic Query Optimization in Expert Sys tems and Database Systems In Proceedzngs of the Fzrst Internatzonal Conference on Expert Database Systems pages 326-340 1984 20 S.J Yen and A.L.P Chen Neighbor hood/Conceptuai Query Answering with Impre cise/Incomplete Data In Proceedzngs of Inter natzonal Conference on Entzty-Relatronshap Ap proach pages 151-162 1993 21 S.J Yen and A.L.P Chen The Analysis of Re lationships in Databases for Rule Derivation In Journal of Intellzgent Informatzon Systems Vol 7 pages 1-24 1996 22 S.J Yen and A.L.P. Chen An Efficient Algorithm 200or Deriving Compact Rules from Databases In Proceedzngs of Internatzonal Conference on Database Systems for Advanced Applzcataons pages 364-371 1995 23 C Yu and W Sun. Automatic Knowledge Acqui sition and Maintenance for Semantic Query Op timization In IEEE Transactzons on Knowledge and Data Enganeerzng pages 362-375 1989 24 W Ziarko The Discovery Analysis and Rep resentation of Data Dependencies in Databases In Proceedzngs of AAAI Workshop on Knowledge Dzscovery zn Databases pages 195-209 1991 18 


addition for some data types search can be further optimized For example if the indexed categorical data have 223xed-dimensionality 000 we know that the area of each indexed signature is 223xed to 000  We can use this property to derive stricter lower bounds for the directory node entries 001  instead of the rather relaxed 002 004 006 006 t 013 r 001 020 022 004 025 027 For this example a better bound is 002 004 006 006 t 013 r 001 020 022 004 025 027 033 t 000 037    t 001 020 022 004 025 r 013 027 027  We plan to study such search optimizations using domain properties or statistics from the indexed data References  C  C  A ggarw al  J  L  W ol f and P  S  Y u A N e w Method for Similarity Indexing of Market Basket Data SIGMOD Conference  pages 407\205418 1999  R  A gra w al and R  S ri kant  F as t A l gori t h ms for M i n ing Association Rules in Large Databases VLDB Conference  pages 487\205499 1994  K  S  B e y er  J  G ol ds t e i n  R  R amakri s hnan and U Shaft When Is 215Nearest Neighbor\216 Meaningful International Conference on Database Theory  pages 217\205235 1999  T  B ri nkhof f H.-P  K ri e g el  a nd B  S e e g er  E f 223 ci ent Processing of Spatial Joins Using R-Trees SIGMOD Conference  pages 237\205246 1993  A  C orral  Y  Manol opoul os  Y  T heodori d i s  a nd M Vassilakopoulos Closest Pair Queries in Spatial Databases SIGMOD Conference  pages 189\205200 2000  A  P  d e V ries N  M amoulis N  N es a nd M K e r sten Ef\223cient k-NN Search on Vertically Decomposed Data SIGMOD Conference  pages 322\205333 2002  U  D eppisch S-T r ee A D ynamic B alanced Signature Index for Of\223ce Retrieval ACM SIGIR Conference  pages 77\20587 1986  V  G aede a nd O G 250 unther Multidimensional Access Methods ACM Computing Surveys  30\(2\170\205231 1998  V  G ant i  J  Gehrk e  a nd R  R a makri s hnan C A C T US 205 clustering categorical data using summaries ACM SIGKDD Conference on Knowledge Discovery and Data mining  pages 73\20583 1999  D Gi bs on J  M Kl ei nber g  a nd P  R a gha v a n C l us tering Categorical Data An Approach Based on Dynamical Systems VLDB Conference  pages 311\205322 1998  A Gi oni s  D Gunopul os  a nd N K oudas  Ef 223 c i e nt and Tunable Similar Set Retrieval SIGMOD Conference  2001  S  Guha R  R as t ogi  a nd K S h i m  R OC K A R obust Clustering Algorithm for Categorical Attributes International Conference on Data Engineering  pages 512\205521 1999  A Gut t m an R T rees  A Dynami c I nde x S t r uct u re for Spatial Searching SIGMOD Conference  pages 47\205 57 1984  S  Hel m er and G  M oerk ot t e  A S t udy of F our Inde x Structures for Set-Valued Attributes of Low Cardinality Technical Report University of Mannheim  number 2/99 1999  G R Hjaltason a nd H Samet Distance Bro w sing in Spatial Databases TODS  24\(2\265\205318 1999  A K J a i n and R  C  D ubes  Algorithms for Clustering Data  Prentice-Hall 1988  I Kamel a nd C  F a louts o s  Hilbert R tree An Improved R-tree using Fractals VLDB Conference  pages 500\205509 1994  F  K o rn N  S i d i r opoul os  C  F al out s o s  E S i e g el  a nd Z Protopapas Fast Nearest Neighbor Search in Medical Image Databases VLDB Conference  pages 215\205 226 1996  N K oudas a nd K C  S e vci k  H i g h D i m ens i onal S i m i larity Joins Algorithms and Performance Evaluation International Conference on Data Engineering  pages 466\205475 1998  N R ous s opoul os  S  K el l e y  and F  V i n cent  Neares t Neighbor Queries SIGMOD Conference  pages 71\205 79 1995  Y  S a kurai  M  Y os hi ka w a  S  U emura and H  K oj i m a The A-tree An Index Structure for High-Dimensional Spaces Using Relative Approximation VLDB Conference  pages 516\205526 2000  The U C I KDD Archi v e ht t p    kdd.i c s  uci  edu 23 R W e b e r  H.-J S ch ek  a n d S Blo tt A Q u a n titative Analysis and e Study for SimilaritySearch Methods in High-Dimensional Spaces VLDB Conference  pages 194\205205 1998  86  Proceedings of the 19th International Conference on Data Engineering \(ICDE\22203 1063-6382/03 $ 17.00 \251 2003 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


