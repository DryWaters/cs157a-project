Analysis on Probabilistic and Binary Datasets through Frequent Itemset Mining Robin Singh Bhadoria   Ram Kumar    Manish Dixit  Dept of CSE    Dept of CSE    Dept of CSE IITM,Gwalior\(MP\INDIA Rayat Bahra IITM,Sonepat\(Harayana\INDIA MIST,Gwalior\(MP\INDIA Email: ssr_robin@yahoo.co.in  Email: ram.amu786@gmail.com  Email: dixitmits@gmail.com Abstract-Association rule mining is the process of discovering relationships among the data items in large database. It is one of the most important problems in the field of data mining. Finding frequent itemsets is one of the most computationally expensive tasks in association rule mining. The classical frequent itemset mining approaches mine the frequent itemsets from the database where presence of an item in a transaction is certain. Frequent itemset mining under uncertain data model is a new area of research. In this case the presence of an item is given by some likelihood measure. In this paper, we have developed a hyper structure based pattern growth method for frequent itemset mining from uncertain data. We have also developed a maximal clique based candidate pruning method for uncertain data. We have implemented and analyzed the performance of the well known algorithms for frequent itemset mining for both binary and uncertain data model. Our empirical results show that in case of dense binary datasets, FP-growth outperforms all other algorithms, whereas in case of sparse data H-mine outperforms other algorithms I. INTRODUCTION Over the last two decades, with advances in storage media technology, storage devices have become larger and more economically viable. As a result, businesses, large corporations etc., have started storing and archiving various types of data in the form of large databases. The purpose of storing data is twofold: first, to access these data in the future, and second, for analysis and discovery of co-relation or relationships among data items stored in database. The task of determining association and/or co-relation among the data items is known as association rule mining  The main motivation comes from market basket analysis or MBA\ [1, 8 W h en in  a s u per m a r k et a cu s t o m er co m e s to buy some items, how likely is she to buy some other specific items or how often customers are purchasing a set of items together Recently, researchers have started finding rules for data where existence of an item is not certain th is ca se, an item is present in transaction by some likelihood measure. For example, consider the database D of research papers from various fields and a black box algorithm B. Algorithm B takes as input a research paper from D and outputs a set of subjects that this paper may belong to, along with their corresponding probabilities. The results of algorithm B may be based on different parameters such as frequencies of words, phrases, etc The output can be viewed as a transaction. For example B, may process papers and output the probabilities, as shown in Table 1.1. Analysis of this type of database may used to classify a research paper as belonging to a specific field. For example, a paper with high probabilities of subjects like algorithms and biology may belong to bioinformatics, and a paper with high probabilities of biology and chemistry may belongs to biochemistry field  Table 1.1: Example of a research papers database Association rule mining has been recognized as one of the big research areas in the field of data mining II. FREQUENT ITEMSET MINING & ALGORITHM FOR DATA MODEL  To compute the support count of an itemset we require to access the database. The database can be stored in memory in the following ways  Horizontal representation This is the basic representation of transactional database. In this, each row consists of transaction identifier followed by set of items present in the transaction  Vertical representation In this type of representation, each row of database consists of an item followed by a set of transaction identifiers in which the item is present. The main difference in this to approaches is, in case of horizontal representation the computation of support count of an itemset requires complete scan of database, whereas in vertical representation the support count of an itemset is calculated by intersection of transaction sets of items in itemset  Bit-vector representation This is the more general representation of database. In this, database is stored in the form 263 978-1-4673-0126-8/11/$26.00 c  2011 IEEE 


of 2-dimensional bit vector. Table 2.1 shows the bit vector representation of the transactional database  Table 2.1: Bit-vector representation of transactional database A.    Apriori Algorithm Apriori is a well known algorithm for frequent item mining which employs level wise search, i.e., breath first search where it uses frequent k-itemset to discover the \(k+1\itemset. In preprocessing of Apriori a scan of database is performed to find out the support count of each item. Then all those items whose support count is less the minimum support threshold a, i.e., all infrequent 1-itemset, are removed from the database Apriori property  A l l n o n e m p t y  su bs et s of a f r e q u e n t  itemset must also be frequent  This property can be described as follows. If X is an itemset and support count of X is less than minimum support threshold, a, then X is an infrequent itemset. If we add an item A to the itemset X, then the support count of resulting itemset X   A cannot be more than support count of X. Therefore, the resulting itemset cannot be frequent if X is not frequent. Whenever a kitemset is found it checks each of its size \(k-1\subsets. If all the size \(k-1\ubsets are frequent then only scans the database to check the support of that k-itemset. The Apriori follows two step process to find out the frequent itemset. A two-step process consisting of join and prune actions 1. Join step In this step, it joins L k-1 with itself to get L k which is the set of frequent               k-itemset. This join operation is a simple natural join. Resulting set after join is called as candidate set and is denoted by C k  2. Prune step Clearly, C k is the superset of L k Some candidates of set C k may not be frequent. As C k is the superset of L k it contains all possible frequent k-itemset and some infrequent k-itemset. To determine L k i.e., set of k frequent k-itemsets \ of database is required to find the support count of each candidate in C k All those candidates whose support count does not satisfy the minimum support threshold a, has been removed from the C k III.  FP-GROWTH Apriori algorithm generates candidates and tests the occurrence frequency of generated candidate set. This process required scanning of database for each candidate. Candidate set generation and test in huge database requires significant amount of time. In short, we can say that in Apriori -like algorithms the bottleneck is candidate set generation and test. FP-growth is known as one of the fastest algorithms of frequent set mining  s es a co m p act data s t ru ctu re cal led FP t ree w h ich is a n  extended prefix tree. The main advantage of FP over Apriori is that FP does not generate candidate sets, instead it uses pattern growth method to generate frequent itemsets. FP-growth approach first represents the frequent item in the form of frequent pattern tree \(FP-tree\, which is a compressed structure This compressed database is divided into set of projected databases, known as conditional database. All the frequent itemsets can be generated mining this conditional database A.     FP-tree construction First we build the header table which consists of item name and link field corresponding to each item. All link entry of header table is initially set to null. Whenever an item first time added into the tree, the corresponding entry of header table is updated. The root, labeled as çnullç, is created. Children are added by scanning the database Table 3.1: Transactional database after  preprocessing First a path that shares same prefix is require to be search. If there exist a path that is same as any prefix of current items of transaction \(in  ist order\en the count of prefix portion is incremented by one in the tree, remaining items of same transaction \(which do not share the path\rom the last node of sharing portion in  ist order and their count value is set to 1. If items of a transaction do not share any path of tree then they are added from the ro ot in  ist order. Each path of prefix tree FP-tree\ents a set of transactions. The FP-tree for transactional database of Table 3.1 is shown in Figure 3.1 Figure 3.1: FP-tree for above example 264 2011 World Congress on Information and Communication Technologies 


B.      H-mine : Another pattern Growth Method H-mine is another pattern growth method for frequent pattern mining ca s e of s p ars e data, t h e F P t ree i s  n o t able  to achieve good compression. We found that in case of sparse data H-mine empirically performs better than FP-growth. Also the space requirement of H-mine is polynomial. In order to achieve pattern growth H-mine uses a hyper structure called as H-struct. In H-struct frequent item projection of each transaction of database D is taken as an element of a queue In Table 3.1, the right column shows the frequent item projections of each transaction. H-mine represents frequent item projection of each transaction in a particular order which is called the flist order figure 3.2 : H-struct H-mine uses divide-and-conquer strategy to mine all the frequent patterns. The general consideration is that the frequent patterns can be partitioned into different sets. The partitioning can be done as follows. First partition contains all patterns that contain the first item of  flist, second partition contains all the patterns which contain second item but not the first item, third partition contain all the patterns that contain third item in flist order but not first and second item and so on C.      Probabilistic pattern Growth: P-Hmine The general idea of P-Hmine is that it represents the database in form of a new structure called P-Hstruct. P-Hstruct is similar to H-struct of [22  w i th t h e ad d itio n a l cap ab il it y o f  handling uncertain data. In P-Hstruct, we represent the database as a set of queues   Figure 3.3: P-Hstruct  IV. EXPERIMENTAL ANALYSIS & RESULT We analyze the running time of algorithms on both synthetic and real datasets. The synthetic dataset generator is taken from IBM Almaden website A.    Datasets The dataset mushroom is a description of hypothetical samples corresponding to different species of mushrooms. This dataset consists of 8124 instances of 119 attributes which are derived from 24 species. It is a dataset containing long pattern with significant repetitions and overlaps between transactions i.e., a dense dataset. The chess dataset is also a dense dataset containing of 3196 instances and 74 items. The synthetic data generator takes as input the following parameters T is the average length of a transaction N is the number of items in a dataset D is the number of transaction in the dataset To study the performance of U-Apriori,       UFP-growth and PHmine, we have used the same datasets discussed above. To introduce uncertainty we assigned random existential probability to each item of the transactions by randomly assigning value P i 0 P i 1, to them. High probability values are assigned to retain the long patterns. In our experiments, we have used the value of P is greater than 0.6 B.     Analysis on Binary Datasets On mushroom dataset FP-growth is faster than all the other algorithms as shown in Figure 4.1. The performance of Hmine is not as goo d as FP-growth on dense dataset, because in case of dense datasets a lot of traversal is required between different queues. Apriori generates huge number of candidates testing of these candidates results high execution time for Apriori  Figure 4.1: Performance of algorithms on Mushroom dataset If the support threshold is high, the performance of Apriori is similar to H-mine, because in case of high support threshold the number of candidates that needs to be examined and the number of maximal cliques is very less. As shown in figure, when the support threshold is above 10% all the algorithms execute very fast 2011 World Congress on Information and Communication Technologies 265 


Figure 4.2: Space requirements of algorithms on Mushroom datasets In Figure 4.2, when the support threshold is above 15%, the space requirement of Apriori and UFP-growth is camparable with other two algorithms Real dataset mushroom contains a lot of repeated long patterns \(very few items differ between consecutive transactions which results in very compact FP-trees. Due to repeated patterns most of the transactions get merged and a single branch of FPtree represents a large set of transactions C.   Analysis on Probabilistic Datasets When the data is very dense, the performance of FPgrowth is better than H-mine. However, the results obtained on dense probabilistic data shows that, the behavior of UFP-growth is not so closely related to            P-Hmine. This is because in case of probabilistic data, it is possible for a data item to have different probability values in different transactions, due to which the basic data structure of UFP-growth, UFP-tree, has large number of no des and large number of leaves which results in big UFP-tree Figure 4.3: Performance of algorithms on uncertain Mushroom dataset In Figure 4.3 shows that performance of all the algorithms on uncertain mushroom dataset. This can be seen from the figure, in case of low support threshold UFP- growth requires high execution time than P-Hmine The reason for this is larger UFP-trees. When the support threshold is above 6%, the execution time of FP- growth is similar to   P-Hmine. In case of low support threshold both UApriori  generate huge number of candidates; testing of this candidates required scanning of the database Figure 4.4: Space requirement of algorithms on uncertain Mushroom Figure 4.4 shows the memory requirements of all the algorithms In case of uncertain data the UFP-tree contains large number of nodes and leaves. The large UFP-tree also generates large conditional FP-trees. Due to this large structure, UFP-growth requires more memory. The bigger UFP-trees generates bigger conditional UFP-trees which is one of the time consuming tasks of UFP-growth. Mining of large UFP-tree results in high running time. P-Hstruct does not depends on different probability values of an item and do not require much space in recursive steps,requires almost constant amount of memory. When the support threshold is above6% the space requirement of UFPgrowth becoming similar to P-Hmine V. CONCLUSION In this paper, we extended H-mine for uncertain data We have also developed algorithm based candidate pruning  for uncertain data. Finally, we analyzed the performance of frequent pattern mining algorithms on few benchmark metrics, over both binary and uncertain datasets. In case of binary dense data \(data contains a lot of repeated long patterns\model, FP-growth performs better than other algorithms, because the dense data results in very compact FP-trees and this requires very less amount of memory. whereas H-mine fails in achieving good division of data and patterns in this case. In case of sparse datasets, data contains small patterns and there are not much repetition of patterns in dataset, H-mine performs better than FPgrowth. The reasons for this are, as mentioned earlier in case of sparse data the size of FP-tree will be bigger and FP-growth spends a lot of time in building and traversing the conditional FPtrees  On the other hand, in this case the H-mine will be able to achieve good division of data and patterns. Apriori like algorithms, i.e., algorithms which are based on candidates generation and test, generate a lot of candidates and thus, require high running time. Candidate pruning based approach such as Hmine and P-Hmine saves a lot of scans of the database and achieves better performance than Apriori on all the tested datasets. In case of uncertain data, empirical results show that this hyper structure based algorithm, P-Hmine, outperforms all the other algorithms in most of the cases that we have examined 266 2011 World Congress on Information and Communication Technologies 


In case of dense uncertain datasets UFP-tree does not attain good compact structure and requires large memory, and this results in high execution time for UFP-growth. We also found that PHmine is scalable for both large number of data items and large number of transactions. We found that there exists no such algorithm that outperforms all other algorithms in all cases. In practice, we can switch the algorithms according to the applications References 1  A g r a w a l   T   I m iel i ns ki, a nd A  Sw am i, Min i n g as s o cia tio n r u l e s  be tw ee n sets of items in large databases, in Proceedings of the 1993 ACM SIGMOD international conference on Management of data, ACM New York, NY,USA 1993, pp. 207Ö216 2 R. A g r a w a l and R  S r ik an t, F a s t  al g o r ithm s f o r m i n i ng as s o c i a tio n r u l e s  in  Proc. 20th Int. Conf. Very Large Data Bases, VLDB, vol. 1215, 1994, pp. 487 499 3 K  A l i, S  Ma ng an ar is an d R. S r i k a n t  P a r t ial cl assif ica tio n us i n g asso cia tio n  rules, in Proceedings of the 3rd International Conference on Knowledge Discovery in Databases and Data Mining, AAAI Press, 1997, pp. 115Ö118 4  R   B a y a rd o J r an d R  A g ra wa l Mi ni n g  t h e m o s t  i n t e res t i n g rules  i n  Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, ACM New York, NY, USA, 1999, pp. 145Ö154 5  B e yer a n d R   R a m a k r i s hna n  Bot t o m up c o m p u t at i o n of s p a r s e  a n d I c eb erg  CUBE, in Proceedings of the 1999 ACM SIGMOD international conference on Management of data, ACM New York, NY, USA, 1999, pp. 359Ö370 6  Bo do n, A tr ie b as e d A P R I O R I im pl e m e n tat io n f o r m i ni ng f r e que nt ite m  sequences, in Pro ceedings of the 1st international workshop on open source data mining: frequent pattern mining implementations, ACM New York, NY, USA 2005, pp. 56Ö65 7 V  B o u c h i t t  e a n d I  T o d i n c a L i s t in g a l l p o t e nt ia l m a xi m a l c l i q u e s of a  gra p h   Theoretical Computer Science, 276 \(2002\, pp. 17Ö32 8 R  Br ac hm an T   K h abaz a, W  K l oe sge n, G  P i ate tsky S hapir o  a nd E   Simoudis, Mining business databases, \(1996 9 C Br o n a nd J  K e r b o s ch, A l go r i th m 45 7 f i n d i ng al l cl iq ue s o f an u n d i r e c te d graph, Communications of the ACM, 16 \(1973\. 575Ö577 10 D  B u r d ic k M   Cal im l i m  J  F l anni ck J  G e hr ke an d T  Y i u  MAFIA: A maximal frequent itemset algorithm, IEEE Transactions on Knowledge and Data Engineering, 17 \(2005\, pp. 1490Ö1504 11 C Ch ui B. K a o  and E  H u ng Mi ni ng f r e que nt i t e m s e ts f r o m unce r tai n da ta   Lecture Notes in Computer Science, 4426 \(2007\ p. 47  X Da i M  Yiu  N Ma m o u l i s  Y T a o a n d M Va it i s  Proba bi li s t i c  s p at i a l  queries on existentially uncertain data, Lecture notes in computer science, 3633 2005\, p. 400 13 J  H a n, G  D o ng and Y  Y i n  E cl ie nt m i ni ng o f par ti al pe r i o d ic pa tte r n s i n  time series database, in Data Engineering, 1999. Proceedings., 15th International Conference on, 1999, pp. 106Ö115  J  Han and M K a m b er Dat a  m i ni n g c onc ep ts an d t ech ni qu es   M o rga n Kaufmann, 2006 15 J  H a n, J  P e i, Y  Y i n  a nd R. M a o  M i ni ng f r e que nt pat te r n s  w i tho ut  candidate generation: A frequent-pattern tree approach, Data Mining and Knowledge Discovery, 8 \(2004\, pp. 53Ö87 16 H  J i aw e i P  J i an Y  Y i w e n, e t al M i n i ng f r e que nt pa tte r n s w i t h o u t  candidate generation, in Proc of the ACM SIGMOD International Conference on Management of Data. Dallas, USA, 2000, pp. 1Ö12  R  K a rp  R e du ci bi li t y a m on g c o m b in a t oria l p r ob lem s  C o m p lex i t y of  computer computations, 43 \(1972\ 85Ö103 18  C. L e ung M Ma te o  an d D   Br ajcz u k A tr e e bas e d a ppr o a ch f o r f r e que nt  pattern mining from uncertain data, Lecture Notes in Computer Science, 5012 2008\, p. 653  W  L i n  S  A l va re z a n d  C  Ru iz E ci en t ad ap ti ve-s u p p o rt  a s s o c i a tion ru le mining for recommender systems, Data Mining and Knowledge Discovery, 6 2002\, pp. 83Ö105 20 B  L i u, W  H s u, an d Y  M a I n te g r ating cl as s i f i c a t i o n a n d as s o c i a tio n r u l e  mining, Knowledge Discovery and Data Mining, \(1998\, pp. 80Ö86  K  Mak in o an d T  Un o New a l gori t h m s for enu m era t i n g  a ll m a xi m a l cliques, Lecture Notes in Computer Science, \(2004\, pp. 260Ö272  J  Pei  Pa t t ern grow t h m e t h od s for fr eq u e n t pa tt ern m i nin g Ph D t h es i s  Simon Fraser University, 2004 23  R. S r i k an t a n d R  A g r a w a l  Mi n i ng g e ne r a l i z e d as s o ci at io n r u l e s  in  Proceedings of the International Conference on very large Data Bases, Institute of Electrical & Electronics Engineers \(IEEE\, 1995, pp. 407Ö419  P T a n  V K u m a r a n d J  Sri va s t a v a  Selec t i n g th e ri gh t i n t e res t i n gn es s  measure for association patterns, in Conference on Knowledge Discovery in Data Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, Association for Computing Machinery, Inc, One Astor Plaza, 1515 Broadway, New York, NY, 10036-5701, USA, 2002 25 F  T s e n g an d C. H s u G e ne r a tin g f r e que nt pa tte r n s w i t h  the f r e que nt pa tte r n  list, Lecture notes in computer science, \(2001\, pp. 376Ö386  H W a n g  a n d Y L e e Gen e N e t w ork Pred i c t i on from M i c r oa rra y D a t a  b y  Association Rule and Dynamic Bayesian Network, in Computational Science and Its Applications, ICCSA 2005: International Conference, Singapore, May 9-12 2005: Proceedings, Springer, 2005, p. 309  M Z a k i  a n d K   G o u d a  F a s t vert i c a l m i n i n g u s i n g di s e t s  in Proc eed i n gs of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, ACM New York, NY, USA, 2003, pp. 326Ö335  M Z a k i  S Pa rtha s a rat h y  M  Ogi h a ra W   L i  et a l   Ne w a l gori t h m s for fa s t  discovery of association rules, in 3rd Intl. Conf. on Knowledge Discovery and Data Mining, vol. 20, 1997, pp. 283Ö286 2011 World Congress on Information and Communication Technologies 267 


and rule interestingness. Usually, a larger number of 1-itemsets will result in a larger number of all itemsets with a higher probability, which will thus usually imply more interesting association rules. In this paper, the proposed approach uses the above two objective functions to find appropriate Pareto solutions for the genetic-fuzzy mining problems D. Fitness Assignment The fitness assignment is similar to that used in SPEA2 25]. The fitness of a chromosome Cq is calculated by using the formula as follows f\(Cq Cq Cq where R\(Cq Cq the density information of a chromosome. The raw fitness is used to exhibit the strength of each chromosome, and is defined as follows qjPPjq jSCR   where the strength value S\(j dominates of chromosome Cj, and is calculated as follows S\(j where |?| means the cardinality of a set, + represents multiset union and the symbol ;  means the Pareto dominance relation. In other words, the raw fitness of a chromosome is determined by the strength of its dominators in both the population P and the archive P . Thus, the lower the raw fitness is, the better the chromosome is. The density information of a chromosome Cq is defined as follows 2 1   k q qCD  where ?qk is the distance of Cq to its k-th nearest chromosome in both the population P and the archive P , and k is calculated by NN + . The density information is used to distinguish the difference of chromosomes which have the same raw fitness    


E. Genetic Operators Genetic operators are important to the success of specific GA applications. Two genetic operators, the max-min-arithmetical \(MMA  the one-point mutation, are used in the proposed approach The MMA crossover operator will generate four candidate chromosomes from them. The best two chromosomes of the four candidates are then chosen as the offspring. The one-point mutation operator will create a new fuzzy membership function by adding a random value ? \(between wjk to + wjk linguistic term, say Rjk. Assume that c and w represent the center and the spread of Rjk. The center or the spread of the newly derived membership function will be changed to c or w + ? by the mutation operation. Mutation at the center of a fuzzy membership function may however disrupt the order of the resulting fuzzy membership functions. These fuzzy membership functions then need rearrangement according to their center values IV. THE PROPOSED MINING ALGORITHM The proposed SPEA2-based genetic-fuzzy minng algorithm for deriving both membership functions and fuzzy association rules is described below  INPUT: A body of n quantitative transactions, a set of m items, each with a number of linguistic terms, a set of minimum support values {ms1, ms2, , msh}, a population size N, an archive size N , a crossover rate Pc, a mutation rate Pm, a number of generation G and a confidence threshold OUTPUT: A set of non-dominated solutions \(sets of membership functions rules STEP 1: Randomly generate a population P of N individuals with each one being a set of membership functions for all the m items, encode each set of membership functions into a string representation according to the schema stated in Section 3, and initialize the non-dominated \(archive STEP 2: For each chromosome Cq, calculate its two objective values, the suitability and the total number of large 1-itemsets according to the given set of minimum 


support values {ms1, ms2, , msh}, by the following substeps SUBSTEP 2.1: For each transaction datum Di, i = 1 to n and for each item Ij, j = 1 to m, transfer the quantitative value vj\(i i as      jl i jl j i j j i j R f R f R f  2   2 1   1 using the corresponding membership functions represented by the chromosome, where Rjk is the k-th fuzzy region \(term i i membership value in region Rjk, and l \(= |Ij number of linguistic terms for Ij SUBSTEP 2.2: For each item region Rjk, calculate its scalar cardinality on the transactions as follows    


n i i jkjk fcount 1   SUBSTEP 2.3: For each Rjk , 1 ? j ? m, 1? k ? |Ij|, check whether countjk is larger than or equal to the set of minimum support values {ms1, ms2, , msh} . If Rjk satisfies the above condition, set || 1 gms qL  = || 1 g ms qL 1, where || 1 gms qL  is the number of large 1-itemsets obtained by using the set of membership functions in chromosome Cq and the minimum support value msg let the second objective value of Cq be the following    h g ms qq gL CtotalNumL1 1 1 SUBSTEP 2.4: Calculate the suitability value suitability\(Cq by using the formula defined in Section 3; set it as the first objective value of Cq STEP 3: Calculate the raw fitness R\(Cq Cq by using the formula defined in Section III STEP 4: Calculate the density information D\(Cq chromosome Cq by using the formula defined in Section III STEP 5: Set the fitness value of each chromosome as follows f\(Cq Cq Cq STEP 6: Copy nondominated chromosomes to archive P . In other words, the chromosomes with their fitness 


values smaller than one will be copied to the archive STEP 7: Execute environmental selection according to the number of chromosomes in the archive. There are three cases 1 P | equals to N , go to the next step 2 is smaller than N , select the best \( N - | P dominated chromosomes with their fitness values larger than one from the previous population and archive 3 the archive exceeds N , use the truncation operator to reduce the size of the archive. At each iteration the chromosome Cq with the smallest ?qk, which is the distance of Cq to its k-th nearest chromosome in the archive, is removed until | P | = N . In case of many chromosomes have the same minimum distance, the second smallest distance is chosen for removal, and so on STEP 8: Use the selection operation to choose appropriate individuals from the archive P  to form the next generation. Here, the binary tournament selection is performed STEP 9: Execute the crossover operation on the population STEP 10: Execute the mutation operation on the population    STEP 11: If the termination criterion is not satisfied, go to Step 2; otherwise, do the next step STEP 12: Mine fuzzy association rules from the given database and based on the derived chromosomes in archive P , where each chromosome represents a set of membership functions. The fuzzy mining algorithm proposed in [13] is then adopted to achieve this purpose for each set of membership functions STEP 13: Output the archive P  and their corresponding fuzzy association rules V. EXPERIMENTAL RESULTS In this section, experiments made to show the performance 


of the proposed approach are described. A simulation dataset with 64 items and 10000 transactions were used in the experiments. The dataset followed the exponential distribution. The initial population size P is set at 50, the archive size is set at 30, the crossover rate pc is set at 0.8, and the mutation rate pm is set at 0.001. The parameter d of the crossover operator is set at 0.35 according to Herrera et al.s paper [14] and the set of minimum support values is {3 4%, , 13%}. The experiments were first made for demonstrating the evolution of the Pareto fronts by the proposed approach. The evolution of the Pareto fronts of chromosomes in the archive along with different generations by the proposed approach is shown in Fig. 1 From Fig. 1, we can observe that the solutions were distributed on the Pareto fronts and the final solutions after 500 generations were better than those in different generations. Besides, we can also found that the derived solutions on a Pareto front are trade-offs between the two objectives. It thus depends on the user preference to decide which solutions on a Pareto front are desired. The experiment was then made for comparing the final Pareto front of chromosomes in the archive of the proposed approach with the previous approach [2], and is shown in Fig. 2  250 300 350 400 450 500 550 600 60 70 80 90 100 110 120 130 140 Suitability To tal N um be r o f L 1 


Generation = 0 Generation = 100 Generation = 200 Generation = 300 Generation = 400 Generation = 500  Fig. 1. The Pareto fronts derived by the proposed approach with different generations 450 500 550 600 65 70 75 80 85 90 95 Suitability To tal N um be r o f L 1 The Proposed Approach The Previous Approach  Fig. 2. Comparison results of final Pareto fronts between the proposed approach and the previous approach  From Fig. 2, it is easily to know that the Pareto front derived by using the proposed approach is better than the previous one.  From the experimental results, we thus can conclude that the proposed approach is not only effective in finding an appropriate set of solutions, but also can provide different options to users for further analysis VI. CONCLUSIONS AND FUTURE WORKS The SPEA2 adopted a fine-grained fitness assignment strategy, a density estimation technique, and an enhanced archive truncation method to derive better Pareto solutions 25]. In this paper, we have utilized it to propose a more sophisticated multi-objective approach to find the appropriate sets of membership functions for fuzzy data mining. Two objective functions are used to find the Pareto front. They are minimizing the suitability of membership functions and maximizing the total number of large 1-itemsets respectively Experiments on a simulation dataset were also made to 


show the effectiveness of the proposed approach. The results show that the proposed approach is effective in finding an appropriate set of solutions. Further, the experiments also show that the proposed approach can derive better Pareto front than the previous one [2]. In the future, we will continuously enhance the multi-objective genetic-fuzzy approach for more complex problems REFERENCES 1] C. C. Chan and W. H. Au, Mining fuzzy association rules, The Conference on Information and Knowledge Management, Las Vegas pp. 209-215, 1997 2] C. H. Chen, T. P. Hong, Vincent S. Tseng and L. C. Chen, A multi-objective genetic-fuzzy mining algorithm, The 2008 IEEE International Conference on Granular Computing, 2008 3] C. H. Chen, T. P. Hong, Vincent S. Tseng and C. S. Lee, A genetic-fuzzy mining approach for items with multiple minimum supports, Soft Computing, Vol. 13, No. 5, pp. 521-533, 2009 4] C. H. Chen, Vincent S. Tseng and T. P. Hong, Cluster-based evaluation in fuzzy-genetic data mining, IEEE Transactions on Fuzzy Systems, Vol. 16, No. 1, pp. 249-262, 2008 5] O. Cordn, F. Herrera, and P. Villar, Generating the knowledge base of a fuzzy rule-based system by the genetic learning of the data base IEEE Transactions on Fuzzy Systems, Vol. 9, No. 4, pp. 667674, 2001 6] C. A. Coello, D. A. Van Veldhuizen and G. B. Lamont, Evolutionary Algorithms for Solving Multi-objective Problems, Kluwer Academic Publishers, 2002    7] K. Deb, Multi-objective Optimization Using Evolutionary Algorithms John Wiley & Sons, 2001 8] K. Deb, S. Agrawal, A. Pratab and T. Meyarivan, A fast and elitist multiobjective genetic algorithm: NSGA-II, IEEE Transactions on Evolutionary Computation, Vol. 6, No. 2, pp. 681-695 9] C. M. Fonseca and P. J. Fleming, "Genetic algorithms for multiobjective optimization: Formulation, discussion and generalization," The International Confidence on Genetic Algorithms pp. 416-423, 1993 10] T. P. Hong, C. H. Chen, Y. L. Wu and Y. C. Lee, " Genetic-Fuzzy Data Mining with Divide-and-Conquer Strategy", IEEE Transactions on Evolutionary Computation, Vol. 12, No. 2, pp. 252-265, 2008 11] T. P. Hong, C. H. Chen, Y. L. Wu and Y. C. Lee, "A GA-based fuzzy 


mining approach to achieve a trade-off between number of rules and suitability of membership functions", Soft Computing, Vol. 10, No. 11 pp. 1091-1101. 2006 12] T. P. Hong, C. S. Kuo and S. C. Chi, "Mining association rules from quantitative data," Intelligent Data Analysis, Vol. 3, No. 5, pp 363-376, 1999 13] T. P. Hong, C. S. Kuo and S. C. Chi, "Trade-off between time complexity and number of rules for fuzzy mining from quantitative data," International Journal of Uncertainty, Fuzziness and Knowledge-based Systems, Vol. 9, No. 5, pp. 587-604, 2001 14] F. Herrera, M. Lozano and J. L. Verdegay, Fuzzy connectives based crossover operators to model genetic algorithms population diversity Fuzzy Sets and Systems, Vol. 92, No. 1, pp. 2130, 1997 15] M. Kaya and R. Alhajj, A clustering algorithm with genetically optimized membership functions for fuzzy association rules mining The IEEE International Conference on Fuzzy Systems, pp. 881-886 2003 16] M. Kaya and R. Alhaji, Utilizing genetic algorithms to optimize membership functions for fuzzy weighted association rules mining Applied Intelligence, Vol. 24 ,  No 1, pp. 7-15, 2006 17] M. Kaya and R. Alhajj, Integrating multi-objective genetic algorithms into clustering for fuzzy association rules mining, The IEEE International Conference on Data Mining, pp. 431-434, 2004 18] M. Kaya, Multi-objective genetic algorithm based approaches for mining optimized fuzzy association rules, Soft computing, Vol. 10 pp. 578-586, 2006 19] C. Kuok, A. Fu and M. Wong, Mining fuzzy association rules in databases, SIGMOD Record, Vol. 27, No. 1, pp. 41-46, 1998 20] Y. C. Lee, T. P. Hong and W. Y. Lin, Mining fuzzy association rules with multiple minimum supports using maximum constraints, Lecture Notes in Computer Science, Vol. 3214, pp. 1283-1290, 2004 21] H. Roubos and M. Setnes, Compact and transparent fuzzy models and classifiers through iterative complexity reduction, IEEE Transactions on Fuzzy Systems, Vol. 9, No. 4, pp. 516-524, 2001 22] J. D. Schaffer, Multiple objective optimization with vector evaluated genetic algorithms, The International Conference on Genetic Algorithms, pp. 93-100, 1985 23] C. H. Wang, T. P. Hong and S. S. Tseng, Integrating membership functions and fuzzy rule sets from multiple knowledge sources, Fuzzy Sets and Systems, Vol. 112, pp. 141-154, 2000 24] S. Yue, E. Tsang, D. Yeung and D. Shi, Mining fuzzy association rules with weighted items, The IEEE International Conference on Systems 


Man and Cybernetics, pp. 1906-1911, 2000 25] E. Zitzler, M. Laumanns and L. Thiele, "SPEA2: Improving the strength Pareto evolutionary algorithm for multiobjective optimization," Proc. Evolutionary Methods for Design, Optimization and Control with App. to Industrial Problems \(Barcelona, Spain, 2001 pp. 95-100 


9] Y. Gong, S. Mabu, C. Chen, Y. Wang, and K. Hirasawa, "Intrusion detection system combining misuse detection and anomaly detection using genetic network programming," in Proc. of the SICE-ICASE International Joint Conference, 2009, pp. 3463-3467 10] "Kddcupl999 data. " [Online]. Available: kdd.ics.uci.eduldatabases kddcup99/kddcup99.htrn1 11] R. P. Lippmann, D. J. Fried, I. Graf, J. Haines, K. P. Kendall, D. Mc Clung, D. Weber, S. Webster, D. Wyschogrod, R. K. Cunningham and M. A. Zissman, "Evaluating intrusion detection systems: The 1998 darpa offline intrusion detection evaluation," in Proc. of DARPA Information Survivability Conference and Exposition 2000, vol. 2 IEEE Computer Society Press, 2000 12] K. Shimada, K. Hirasawa, and J. Hu, "Class association rule mining with chi-squared test using genetic network programming," in Proc. of the IEEE International Conference on Systems, Man and Cybernetics 2006, pp. 5338-5344 


n-dimension data cube\( I1  I k Support=sup_count/total_count 2 3 4. Performance Analysis Example 2 Lets talk about a practical problem just like the status of sales. Assume that we will mine the association rules involved 4 dimension attributes of sales, the minsup=25%. First of all, using OLAP technology to build a 4-D data cube and the 4 dimension attributes are: time location, item, and supplier. For location dimension which contains area, country and so on, we choose province level We use brand level for item dimension, company level for supplier dimension. Time dimension can be divided as Q1 1-3 4-6 7-9 10-12 location\(P1,P2,L1,L2 York, item\(B1,B2,B3,B4 C1,C2,C3 sales data cube can be generalized like this Graphic 2: The 4-Dimension Data Cube of Sales The details of this sales data cube are in the table follow: The amount of cells is 100 Location Time Item Supplier Count Cell-1 P1 Q1 B1 C2 5 Cell-2 P1 Q3 B1 C1 2 Cell-99 L1 Q3 B1 C1 3 Cell-100 L2 Q4 B1 C3 11 Table 2: The details data table of sales data cube We use original Apriori_Cube Algorithm to find frequent predicate set with minsup_num= 25%*100=25 According the data table we calculate that sup_count of every member of dimension L is \(P1:8, P2:5, L1:1, L2:24 and also T, I, S. So there is the process Graphic 3: The processes of old algorithm As we know, through comparing with the minsup_num dimension location has no one frequent 1-predicate set, so that there have no frequent 4-predicate set in the output by the original algorithm. But users are interested in the Candidate 1-Predicate set L T I S P1 P2 L1 


L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate set T I S Q1 Q3 B2 B3 C1 C2 I1 I 2 C1              C2                 C3 2 12 1 3 5 1 6 2 12 8 11 Q 1  Q 2  Q 3  Q 4 P 1 P 2 L 1 L 2 Candidate 2-Predicate set Q1,B2},{Q1,B3 Q1,C1},{Q1,C2 Q3,B2},{Q3,B3 


Q3,C1},{Q3,C2 Candidate 3-Predicate set Q1,B2,B3}{Q 1,Q3,B2},{Q3 C1,B2 Frequent 2-Predicate set Q1,B2}{Q1,B 3},{Q3,B2 Q3,C1 Output: 1L U 2L U 3L Frequent 3-Predicate set Q1,B2,C1},{Q1,B3,B2 Q1,B2,B3  6 level from the same dimension and SP to pruning Candidate 3 Set, pruning {Q1,Q3,B2}, keep {Q1,B2,B3} due to the special requirement on location B by users 7 1 max_support of every dimension 1 frequent 4-predicate set which contains location dimension And the candidate 3-predicate set {Q1,Q3,B2} is useless because when we mine multi-dimension association rules we should follow that one predicate set contains no more than one different level from the same dimension even the sup_count >minsup_num when there are no requirements from users. But this time, users require that different members from dimension item can exist at the same time So we redo this mining process by the improved Algorithm Graphic 4: The processes of improved algorithm At last, we got the frequent 4-predicate set which the users are interested in, and then can format the rules 5. Conclusion In this paper, we have studied issues and methods on efficient mining of multi-dimension association rules based on data cube. With the development of technology, the size 


of database has become much huge than ever before unimaginable. So the multi-dimension model of database based on data cube has become useful and popular relatively how to mine useful multi-dimension association rules based on this model has been important too. Among the algorithms for the problems we choose the most classify one to improve, to make it more efficient and useful An efficient algorithm, Apriori_Cube_Improved, has been developed which explores the multi-dimension association rules. First, we use max_support and min_support of every frequent 1-predicate set to check the levels of dimensions, and then adjust the data cube by roll-up and drill-down operation immediately. This step is embed in the process of mining association rules, so it makes the rules much more useful and flexible; second applying one predicate set contains no more than one different level from the same dimension and SP pruning can help reduce the amount of predicate set, especially with the great growing of the number. So the speed of algorithm improved is faster than ever At last, there are also many other interesting issues and flaws of the algorithm which call for further study including efficient mining multi-dimension association rules of complex measures and so on. Finally, I should thank all the people who have give me help for this paper Reference 1] Jiawei Han, Micheline Kamber. Data Mining Concepts and Techniques. China Machine Press, 2007 2] Agrawal R, Imielinski T and Swami A. Mining association rules between sets of items in large database. Proc. of the ACM SIGMOD Conf., Washington DC, 1993 3] Gao Xuedong, Wang Wenxian and Wu Sen. Multidimensional Association Rule Mining Method Based on Data Cube Computer Engineering, China,2003 4] Sheng Yingying, Yan Ren, Wang Jiamin and Li Jia. Research Multi-dimensional Association Rule Ming Based on Apriori Science Technology and Engineering, China, 2009 5] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. Proc. of the 20th Int.Conf. on VLDB Santiago, Chile, 1994 6] Guozhu Dong, Jiawei Han, Joyce Lam, Jian Pei and Ke Wang Mining Multi-Dimension Constrained Gradients in Data 


Cubes. Proc. of the 27th Int.Conf. on VLDB, Roma, Italy 2001 7] M. J. Zaki. Scalable Algorithms for Association Mining. IEEE Transactions on Knowledge and Data Engineering, 2000 Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Candidate 1-Predicate set L T I S P L Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Frequent 1-Predicate L T I S P L 


Q1 Q3 B2 B3 C1 C2 Frequent 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3 Output L1 U L2 U L3 U L4 The Dimension need to be adjust Dimension: Location Operation: Roll-up to increase the level of this dimension Result P\(P1,P2 L1,L2 2 3 2 3 The Features Of Every 1-Predicate Set Location: min_support=13 max_support=25 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 4 5 4 max_support of every dimension 5 minsup_num=25, and no one has to be adjusted Frequent 3-Predicate set Q3,C1,B2},{Q1,B2,L Q1,B3,L},{Q3,P,B2 Candidate 4-Predicate set 


Q3,C1,B2,P},{Q1,B2,B3,L}\(7 The Features of Every 1-Predicate Set Location: min_support=1 max_support=24 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 Candidate 2-Predicate set Q1,B2},{Q1,B3},{Q1,C1 Q1,C2},{Q3,B2},{Q3,B3 Q3,C1},{Q3,C2},{P,Q1 P,Q3},{P,B2},{P,B3},{P,C1 P,C2},{L,Q1},{L,Q3 L,B2},{L,B3},{L,C1},{L,C2 Frequent 2-Predicate set Q1,B2}{Q1,B3},{Q3,B2 Q3,C1},{P,Q3},{L,Q1 6 Candidate 3 -Predicate set Q3,C1,B2},{Q3,C1,P},{Q1,B 2,L},{Q1,B3,L},{Q3,P,B2},{Q 1,B2,B3  6 level from the same dimension and SP to pruning Candidate 3 Set, pruning {Q1,Q3,B2}, keep {Q1,B2,B3} due to the special requirement on location B by users 7 1 max_support of every dimension 1 frequent 4-predicate set which contains location dimension And the candidate 3-predicate set {Q1,Q3,B2} is useless because when we mine multi-dimension association rules we should follow that one predicate set contains no more than one different level from the same dimension even the sup_count >minsup_num when there are no requirements from users. But this time, users require that different 


members from dimension item can exist at the same time So we redo this mining process by the improved Algorithm Graphic 4: The processes of improved algorithm At last, we got the frequent 4-predicate set which the users are interested in, and then can format the rules 5. Conclusion In this paper, we have studied issues and methods on efficient mining of multi-dimension association rules based on data cube. With the development of technology, the size of database has become much huge than ever before unimaginable. So the multi-dimension model of database based on data cube has become useful and popular relatively how to mine useful multi-dimension association rules based on this model has been important too. Among the algorithms for the problems we choose the most classify one to improve, to make it more efficient and useful An efficient algorithm, Apriori_Cube_Improved, has been developed which explores the multi-dimension association rules. First, we use max_support and min_support of every frequent 1-predicate set to check the levels of dimensions, and then adjust the data cube by roll-up and drill-down operation immediately. This step is embed in the process of mining association rules, so it makes the rules much more useful and flexible; second applying one predicate set contains no more than one different level from the same dimension and SP pruning can help reduce the amount of predicate set, especially with the great growing of the number. So the speed of algorithm improved is faster than ever At last, there are also many other interesting issues and flaws of the algorithm which call for further study including efficient mining multi-dimension association rules of complex measures and so on. Finally, I should thank all the people who have give me help for this paper Reference 1] Jiawei Han, Micheline Kamber. Data Mining Concepts and Techniques. China Machine Press, 2007 2] Agrawal R, Imielinski T and Swami A. Mining association rules between sets of items in large database. Proc. of the ACM SIGMOD Conf., Washington DC, 1993 3] Gao Xuedong, Wang Wenxian and Wu Sen. Multidimensional Association Rule Mining Method Based on Data Cube 


Computer Engineering, China,2003 4] Sheng Yingying, Yan Ren, Wang Jiamin and Li Jia. Research Multi-dimensional Association Rule Ming Based on Apriori Science Technology and Engineering, China, 2009 5] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. Proc. of the 20th Int.Conf. on VLDB Santiago, Chile, 1994 6] Guozhu Dong, Jiawei Han, Joyce Lam, Jian Pei and Ke Wang Mining Multi-Dimension Constrained Gradients in Data Cubes. Proc. of the 27th Int.Conf. on VLDB, Roma, Italy 2001 7] M. J. Zaki. Scalable Algorithms for Association Mining. IEEE Transactions on Knowledge and Data Engineering, 2000 Candidate 1-Predicate set L T I S P1 P2 L1 L2 Q1 Q2 Q3 Q4 B1 B2 B3 B4 C1 C2 C3 Candidate 1-Predicate set L T I S P L Q1 Q2 Q3 Q4 B1 B2 


B3 B4 C1 C2 C3 Frequent 1-Predicate L T I S P L Q1 Q3 B2 B3 C1 C2 Frequent 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3 Output L1 U L2 U L3 U L4 The Dimension need to be adjust Dimension: Location Operation: Roll-up to increase the level of this dimension Result P\(P1,P2 L1,L2 2 3 2 3 The Features Of Every 1-Predicate Set Location: min_support=13 max_support=25 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 4 5 


4 max_support of every dimension 5 minsup_num=25, and no one has to be adjusted Frequent 3-Predicate set Q3,C1,B2},{Q1,B2,L Q1,B3,L},{Q3,P,B2 Candidate 4-Predicate set Q3,C1,B2,P},{Q1,B2,B3,L}\(7 The Features of Every 1-Predicate Set Location: min_support=1 max_support=24 Time: min_support=5 max_support=30 Item: min_support=10 max_support=25 Supplier: min_support=11 max_support=27 Candidate 2-Predicate set Q1,B2},{Q1,B3},{Q1,C1 Q1,C2},{Q3,B2},{Q3,B3 Q3,C1},{Q3,C2},{P,Q1 P,Q3},{P,B2},{P,B3},{P,C1 P,C2},{L,Q1},{L,Q3 L,B2},{L,B3},{L,C1},{L,C2 Frequent 2-Predicate set Q1,B2}{Q1,B3},{Q3,B2 Q3,C1},{P,Q3},{L,Q1 6 Candidate 3 -Predicate set Q3,C1,B2},{Q3,C1,P},{Q1,B 2,L},{Q1,B3,L},{Q3,P,B2},{Q 1,B2,B3  


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


