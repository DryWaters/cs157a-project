Image Fusion for Object Tracking Using Factor Graphs Francesco Castaldo and Francesco A N Palmieri DIII Seconda Universit 036 a degli Studi di Napoli 050SUN\051 Via Roma 29 Aversa 050CE\051 Italy francesco.castaldo@unina2.it francesco.palmieri@unina2.it Abstract\227 In recent years there has been an increasing demand of systems that automatically manage and control the state of large critical areas such as airports harbors parking lots etc The framework of the Bayesian Factor Graphs to target fusion seems to be quite promising with respect to classical approaches because of its modularity and because it can naturally integrate very heterogeneous sources of information The system presented in this paper fuses real-time data coming from various sensors along with estimates coming from the tracked object models 050if available\051 All the information is merged within environmental constraints in order to provide the best estimate of the state of a moving object Factor graphs allow the information to 003ow bidirectionally to predict the future or to strengthen our knowledge of the past In this paper we focus on camera sensors deployed along the area of interest The information is merged into the factor graph after geometric inversion and covariance estimate The problem of automatic localization of moving objects on the images is also addressed The framework has been tested on a parking area where states are estimated accuracy is assessed and considerations about the framework are provided T ABLE OF C ONTENTS 1 I NTRODUCTION                                   1 2 T HE F ACTOR G RAPH F RAMEWORK             1 3 E XPERIMENTS                                    5 4 C ONCLUSION                                     5 A PPENDICES                                      6 A C AMERA C ALIBRATION                          6 B D ERIVATIVES                                     7 R EFERENCES                                     8 B IOGRAPHY                                      8 1 I NTRODUCTION Probability propagation in graphs provides an excellent framework for fusion systems that need to merge any available information at any time and need to provide on-line inferences in a dynamic fashion Propagation and computation rules are purposely designed to k eep into account messages current reliability and provide a much desired modularity Computation can be distributed among various blocks so that we can easily expand our system as new sources of information become available Tracking systems that seek to reconstruct the state of objects moving into a complex scene may greatly bene\002t from using this framework because graphical models can be made to be robust simple to handle and modular  978-1-4799-1622-1/14$31.0 c 015 2014 IEEE This project has been partially sponsored by Ministero Infrastrutture e Trasporti PON01-01936 Harbour Traf\002c Optimization System 050HABITAT\051 with Consorzio Nazionale Interuniversitario per le Telecomunicazioni 050CNIT\051 Complex fusion systems that combine information from many types of sensors such as Radars multiple cameras IR sensors Lidar etc must rely on the partial information that each sensor may provide to achieve reliable global estimates  The mathematics of the dynamic model 4 of an object that the system is trying to follow should be considered just one of the many factors that contribute to system's knowledge of the environment Tracking algorithms proposed in the literature differ in reconstruction quality capacity of handling appearing and disappearing targets types of sensors etc Standard approaches are Kalman Filters Particle Filtering Bayesian Networks along with new and promising techniques as PHD 002lters A complete survey on target tracking systems is beyond the purpose of this paper for which we refer the readed to classic references as 2 and 4 In this paper we propose to design a single target tracking system using the Normal Factor Graphs framework Dynamic probability propagation in graphs seems to re\003ect naturally our need to handle complex scenarios when information comes from various sources that are of different nature may become available only occasionally and asynchronously and may carry only partial information In this work we focus on multi-camera sensor systems Cameras have been intensively studied in computer vision as they are available at very low cost and can be deployed in networks that monitor complex and changing scenarios Accurate estimations with such systems requires that 1 cameras are correctly calibrated 2 data from multiple sensors are reliably associated 3 best use is made of partial information for global estimates We consider cameras as stand-alone systems that exchange bi-directionally their estimates with the other sensors and with the fusion center No images are transferred to the fusion center to maintain the computational complexity to a bare minimum In Section 2 the fusion factor graph framework is described with details about the integration of the camera information 003ow In Section 3 experimental results are presented for tracking a car in a parking lot Section 4 draws conclusions and suggestions for future developments 2 T HE F ACTOR G RAPH F RAMEWORK Figure 1  A simpli\002ed version of the tracking system 1 


Figure 1 shows the framework for a single target tracking where s k  050 X k  Y k  _ X k  _ Y k 051 T is the object state with 050 X k  Y k 051 positional coordinates 050 _ X k  _ Y k 051 speed variables and k 2 N the discrete time step The system fuses information from previous estimation steps and from heterogeneous sensors that can be visual cameras Radar infrared sensors etc Information can also 003ow towards the sensors to increase their accuracy or to ease their work in locating objects in the image For simplicity we limit ourselves here to objects associated single points 050the object's barycenter or the last point in touch with the ground etc.\051 moving at almost constant speed and with limited maneuvering capabilities The factor graph tracking system is shown in Figure 2 The system is synchronous and at each discrete-time step k all the information coming from the dynamic model and the vision systems is embedded into a unique pipeline The great feature of factor graphs specially in their Forney-Style normal form 10 is that through the use of di v erter blocks ne w information can be easily added without having to re-work out the rest of system In a factor graph edges represent variables and nodes represent factors There is a direction for each branch that allows unambiguous de\002nition of forward 050 f 051 and backward 050 b 051 messages Each factor block describes the conditional probability function that maps the input to the output variable The diverter blocks express the equal 050  051 constraint and become our fusion buses that merge bi-directionally all the information coming from the different branches of the system Our architecture is built to be cycle-free to avoid indeterminacies and it is globally a tree We assume that all messages are Gaussian pdfs i.e fully described by a mean vector and a covariance matrix The means typically represent our best estimates of the variables and the covariances the multidimensional regions of con\002dence Forward and backward messages for a multidimensional variable s are denoted as f s  f m f s  006 f s g and b s  f m b s  006 b s g  respectively Messages f s and b s at each edge can be combined with the product rule to obtain a Gaussian pdf p s  b s f s with mean m s  006 s 050\006 000 1 f s m f s  006 000 1 b s m b s 051 and covariance 006 s  050\006 000 1 f s  006 000 1 b s 051 000 1  As usual in factor graphs messages are distributions except for a scale factor that generally is irrelevant for the estimations Our model describes a 002xed time window on time steps k  0   K  Therefore we have to inject into the graph an initial forward message f s 0  f m f s 0  006 f s 0 g at the beginning of the chain and a backward message b s K  f m b s K  006 b s K g at the end If a priori information about the tracked object is not available the message f s 0 can be assumed having m f s 0  0 and a very high-valued covariance matrix 050 006 f s 0  1 051 The same can be done for b s K  The system includes N cameras Distributions with very large variance are injected when there is no available data As stated in an important issue is the grade of dependence between tracks In classic target tracking two tracks of the same target obtained from different 050independent\051 sensors share the same process noise therefore they are somewhat correlated and cannot be fused without considering the mutual cross-covariance On the contrary in our framework the sensors provide only 050independent\051 estimates of the target state that are combined as independent messages into the fusion bus As emphasized later in the paper a certain correlation between the messages may be added when backward messages are used to reduce the search region on the image 050bounding box\051 However we consider this effect negligible In the following we expand on the details of propagation and combination rules for each part of the system Figure 2  The k-stage of the Factor Graph The Track Model The recursions used here assume a standard nearly-constantvelocity model with accelerations along X and Y modeled as small white noises 8       X k  X k 000 1  T V X k 000 1  w X k  Y k  Y k 000 1  T V Y k 000 1  w Y k  _ X k  V _ X k 000 1  w _ X k  _ Y k  V _ Y k 000 1  w _ Y k  0501\051 where T is the sampling period and w k  050 w X k  w Y k  w V X k  w V Y k 051 T  is a white Gaussian noise that models uncertainties in object's position and speed Noise covariance allows us to tune the con\002dence we have on the model Equations 0501\051 can be rewritten in matrix form as 0 B  X k Y k _ X k _ Y k 1 C A  A 0 B  X k 000 1 Y k 000 1 _ X k 000 1 _ Y k 000 1 1 C A  w k  0502\051 2 


Figure 3  A focus on the Model block in the factor graph with A  0 B  1 0 T 0 0 1 0 T 0 0 1 0 0 0 0 1 1 C A  0503\051 The corresponding factor graph is more speci\002cally drawn in Figure 3 Forward and backward messages are computed as follows  f s M 0 k  f Am f s k 000 1  A 006 f s k 000 1 A T g 0504\051 f s M k  f m f s M 0 k  m f w k  006 f s M 0 k  006 f w k g  0505\051 b s M 0 k  f m b s M k 000 m f w k  006 b s M k  006 f w k g  0506\051 b s k 000 1  f 050 A T 006 000 1 b s M 0 k A 051 000 1 A T 006 000 1 b s M 0 k m b s M 0 k  050 A T 006 000 1 b s M 0 k A 051 000 1 g  0507\051 More detailed models may tak e into account the possibility for the tracked object to maneuver change course and speed However more interestingly the factor graph framework allows for different models to be run in parallel as separate 224experts\224 and be seamlessly fused into the same framework We leave such an extension to future papers The Camera Subsystems To fuse the information coming from the camera systems we have to translate the pixel information into the state variables We assume a standard pinhole geometry that de\002nes the mapping between world points 050 X k  Y k 051 and the 2D pixel coordinates 11 As usual in tracking the en vironment into which targets operate and maneuver is limited and with a predictable geometry In particular for common scenarios such as harbours parking lots etc we can assume that all the targets are constrained to move on a 2D surface 050the sea plane for harbours the street for parking lots\051 If this hypothesis holds we can de\002ne the relationship between world points and pixel correspondences on the camera image planes with homography matrices We assume that the homography matrices betweeen the world points and the camera coordinates have been computed for each camera with the procedure detailed in using 002x ed reference points The reference points are known correspondences between world and pixel points and are usually extracted by hand using GPS data 050Google Maps AIS\051 or other techniques not reported here for space reasons Further information about the pinhole model and the camera calibration algorithms used in this paper can be found in Appendix A It is important to emphasize that since the modeling for each camera is an homography even a single camera can do the tracking However at least a twoor three-cameras setup is suggested to have redundant information on the target for better performance and to cover the a wider scene under surveillance According to the pin-hole geometry each camera C i  i 2 f 1   N g  corresponds to the following equation 025 i k 040 x i k y i k 1   H i 040 X k Y k 1   0508\051 where x i k and y i k are pixel coordinates 050assumed real\051 and H i is the camera 3x3 homography matrix  The scale parameters 025 i k is a factor that depends on 050 X k  Y k 051 and renders the transformation non linear Manipulating 0508\051 with H i  0  h i 11 h i 12 h i 13 h i 21 h i 22 h i 23 h i 31 h i 32 h i 33 1 A  0509\051 we obtain 8             x i k  025 i k x i k  025 i k  h i 11 X k  h i 12 Y k  h i 13  h i 31 X k  h i 32 Y k  h i 33  f i 1 050 X k  Y k 051  y i k  025 i k y i k  025 i k  h i 21 X k  h i 22 Y k  h i 23  h i 31 X k  h i 32 Y k  h i 33  f i 2 050 X k  Y k 051  _ x i k  dx i k  dt  f i 3 050 X k  Y k  _ X k  _ Y k 051  _ y i k  dy i k  dt  f i 4 050 X k  Y k  _ X k  _ Y k 051  05010\051 Explicit expressions for f 3 and f 4 are reported in Appendix B The four equations 05010\051 link the object position and speed in the world cartesian coordinate system with their 224pixel\224 position and speed into the image plane framed by the sensor Going in the opposite direction i.e from pixels to world points the equation is rewritten as 0  X k 025 i k Y k 025 i k 1 025 i k 1 A  R i 040 x i k y i k 1   05011\051 where R i  H i 000 1  Manipulating 05011\051 with R i  0  r i 11 r i 12 r i 13 r i 21 r i 22 r i 23 r i 31 r i 32 r i 33 1 A  05012\051 we get 8             X k  X k 025 i k  1 025 i k  r i 11 x i k  r i 12 y i k  r i 13  r i 31 x i k  r i 32 y i k  r i 33  g i 1 050 x i k  y i k 051  Y k  Y k 025 i k  1 025 i k  r i 21 x i k  r i 22 y i k  r i 23  r i 31 x i k  r i 32 y i k  r i 33  g i 2 050 x i k  y i k 051  _ X i k  dX i k  dt  g i 3 050 x i k  y i k  _ x i k  _ y i k 051  _ Y i k  dY i k  dt  g i 4 050 x i k  y i k  _ x i k  _ y i k 051  05013\051 with explicit expressions for g 3 and g 4 reported in Appendix B Camera forward 003ow\227 In our factor graph the forward 003ow goes from the camera to the fusion bus 050the 003ow direction is just a convention since messages travel in both directions\051 Assuming that a pixel extraction algorithm provides an estimate for the target location and speed 050as detailed later in the paper\051 we have at time k a message f p i k  f m f p i k  006 f p i k g  with mean m f p i k  050 013 i k  014 i k  _ 013 i k  _ 014 i k 051 T  and covariance 006 f p i k  3 


Unfortunately the non linear nature of the transformation 05010\051 makes it impossible to propagate exactly the gaussian distribution though the block Therefore we have to resort to an approximation Calling g i 050 x y _ x _ y 051  0 B  g i 1 050 x y 051 g i 2 050 x y 051 g i 3 050 x y _ x _ y 051 g i 4 050 x y _ x _ y 051 1 C A  05014\051 and using 05013\051 we can write a 002rst-order approximation around the current estimate 050 013 i k  014 i k  _ 013 i k  _ 014 i k 051 T as 2 6 4 X k Y k _ X k _ Y k 3 7 5  g i 050 013 i k  014 i k  _ 013 i k  _ 014 i k 051 J i 050 013 i k  014 i k  _ 013 i k  _ 014 i k 051 0 B  x 000 013 i k y 000 014 i k _ x 000 _ 013 i k _ y 000 _ 014 i k 1 C A  05015\051 with the Jacobian matrix J i 050 x y _ x _ y 051  2 6 6 6 4   x g i 1   y g i 1 0 0   x g i 2   y g i 2 0 0   x g i 3   y g i 3    _ x g i 3    _ y g i 3   x g i 4   y g i 4    _ x g i 4    _ y g i 4 3 7 7 7 5  05016\051 The derivatives are straightforward to compute and are reported in Appendix B Having assumed a Gaussian error model on the image 0 B  x 000 013 i k y 000 014 i k _ x 000 _ 013 i k _ y 000 _ 014 i k 1 C A 030 N 050 0  006 f p i k 051  05017\051 the world coordinates estimates are passed to the fusion bus via the forward message f s i k  f g i 050 013 i k  014 i k  _ 013 i k  _ 014 i k 051  J i 050 013 i k  014 i k  _ 013 i k  _ 014 i k 051\006 f p i k J i 050 013 i k  014 i k  _ 013 i k  _ 014 i k 051 T g  05018\051 Camera backward 003ow\227 Regarding the backward direction from the fusion bus a message b s i k  f m b s i k  006 b s i k g reaches the camera system with mean m b s i k  050 030 i k  021 i k  _ 030 i k  _ 021 i k 051 and covariance 006 b s i k  Using again a 002rst-order approximation for the backward 003ow we obtain b p i k  f f i 050 030 i k  021 i k  _ 030 i k  _ 021 i k 051  000 i 050 030 i k  021 i k  _ 030 i k  _ 021 i k 051\006 b s i k 000 i 050 030 i k  021 i k  _ 030 i k  _ 021 i k 051 T g  05019\051 with f i 050 X Y _ X _ Y 051  0 B B  f i 1 050 X Y 051 f i 2 050 X Y 051 f i 3 050 X Y _ X _ Y 051 f i 4 050 X Y _ X _ Y 051 1 C C A  05020\051 and the Jacobian 000 i 050 x y 051  2 6 6 4   X f i 1   Y f i 1 0 0   X f i 2   Y f i 2 0 0   X f i 3   Y f i 3    _ X f i 3    _ Y f i 3   X f i 4   Y f i 4    _ X f i 4    _ Y f i 4 3 7 7 5  05021\051 The derivatives are straightforward to compute and are reported in Appendix B The Fusion Bus This block has to merge model and sensors estimates by applying the product rule More speci\002cally  the product q of M gaussian messages q j  f m j  006 j g  j  1   M  is a new gaussian message denoted as q  q 1 014 q 2 014  014 q M  014 M j 1 q j  f m q  006 q g 05022\051 with 006 q  050\006 000 1 1  006 000 1 2    006 000 1 M 051 000 1  05023\051 m q  006 q 000 006 000 1 1 m 1  006 000 1 2 m 2    006 000 1 M m M 001  05024\051 Therefore we have for the messages around the fusion bus f s k  f s A k 014 f s M k 014 050 014 N j 1 f s j k 051  b s A k  b s k 014 f s M k 014 050 014 N j 1 f s j k 051  b s M k  b s k 014 f s A k 014 050 014 N j 1 f s j k 051  b s i k  b s k 014 f s A k 014 f s M k 014 050 014 N j 1 j 6  i f s j k 051  i  1   N 05025\051 Note that through covariance combination the fusion rule automatically keeps into account message reliability component by component For example information on velocity may be very rough from an image sensor The covariances are always combined to provide the best response in terms of estimates and con\002dence Pixel extraction from video 003ow The video 003ow of a camera C i is analyzed frame-by-frame by an algorithm of background subtraction  as detailed in  F or each time step k  the algorithm provides a pixel 050 013 i k  014 i k 051 representing the moving object reference point 050the barycenter or the rightmost point in touch with the ground or else\051 We enclose this information into a forward message f p i k  f m f p i k  006 f p i k g  with m f p i k  050 013 i k  014 i k  _ 013 i k  _ 014 i k 051 T  05026\051 and 006 f p i k  diag 050 033 2 013 i k  033 2 014 i k  033 2 _ 013 i k  033 2 _ 014 i k 051  05027\051 with 050 033 2 013 i k  033 2 014 i k 051 accuracy parameters 050variances\051 chosen according the camera quality and algorithm precision Also image-plane velocity estimates and con\002dence values could be provided if available If no speeds have been estimated on the image plane to the means 050 _ 013 i k  _ 014 i k 051 we can associate arbitrary values and to the variances 050 033 2 _ 013 i k  033 2 _ 014 i k 051 very large values Given the bi-directional nature of the framework probabilistic messages travel also towards the cameras in the form 4 


of backward messages b p i k  f m b p i k  006 b p i k g  with i  1  2   N  These messages can be used to resolve the data association problem that is to select the measure from the tar get of interest discarding clutter and other targets detections More speci\002cally assuming to be at time k  we can predict the state at time k  1  propagating f s k through the track model to have f s M k  1  Using 05025\051 we get backward messages b s i k  1  i  1  2   N  Using 05019\051 we obtain b p i k  1  f m b p i k  1  006 b p i k  1 g  where m b p i k  1  f  x k 1 i  y k 1 i g represents the next 050predicted\051 position of the moving object into the image plane of camera C i  and 006 b p i k  1 represents how reliable is the estimate We can construct for each camera C i a prediction window W i around 050 x k 1 i  y k 1 i 051 into the image plane with dimensions determined by 006 b p i k  1  These windows can be seen as the frame portions which restrict the background subtraction algorithm operations In this way if the object is moving without sudden changes in speed and direction 050nearlyconstant-velocity hypothesis\051 the target can be found in the window We have noticed that this solution reduces both the probability to have a noisy estimate 050a very common issue in scenarios full of moving targets\051 and the computational load of the background subtraction algorithm 050an important feature for a real-time implementation of the algorithm\051 The use of backward messages to enhance the sensor data extraction seems to add some dependence between the sensor estimates and the target track However we use such messages only to restrict the portion of image into which to look for the measurement and we do not merge the two estimates In this way we reasonably say that the correlation effect of this procedure is negligible 3 E XPERIMENTS We have tested our framework with real data in various scenarios such as parking lots harbours busy streets etc In this section we report an experiment of state reconstruction of a car maneuvering in a busy parking lot We have deployed three cameras along the walls 050at almost 10-meters height\051 surrounding the parking of our Department in Aversa Italy The cameras are GE E1035 Samsung D70 and Fuji\002lm FinePix S3200 Cameras have been calibrated with data measured by hand in the parking lot and with world points calculated into the reference system showed in Figure 5 Pixel and world coordinates can vary in the following ranges  x y   1  640  1  for Camera 1 2 and 3 and  X Y    000 29  2900  000 40  7900  16  8300  71   for world points coordinates The three homography matrices have been computed using the procedure described in Appendix A and are H 1  040 2  3619 19  3920 000 303  5301 2  4994 1  1149 000 128  0305 000 0  0156 0  0141 000 0  6470   05028\051 H 2  040 000 15  2914 12  2913 000 209  1322 0  8658 0  1043 000 165  8650 000 0  0201 000 0  0117 000 0  6633   05029\051 H 3  040 11  9505 2  3571 171  9991 3  0587 3  7881 138  3809 0  0203 0  0283 0  6983   05030\051 We focus our attention on a red car performing a parking maneuver Figures 4 5 and 6 show the experimental results More speci\002cally Figure 4 regards a step of the pixel extraction algorithm Figure 5 shows the views the three cameras have on the scene and Figure 6 depicts the state trajectory on a Google map image of the parking lot This experiment on real data con\002rms that if the object is moving without strange changes in speed or acceleration the tracker 224hooks\224 the object and follow its state The nearlyconstant-velocity model does not produce highly-reliable estimates in this experiment 050the car is changing its direction\051 but the probabilistic fusion allows it to automatically rely almost only on camera estimates yielding a global reconstruction of the car trajectory The factor graph parameters are T  1  033 0  10 4  050we do not know the initial state of the car\051 033 w  10 000 7  033 b s K  10 5  and 050 033 013 i k  033 014 i k 051  10  i 2 f 1  2  3 g  4 C ONCLUSION The paper has presented the application of the factor graph framework to single-target tracking The paper has focused on fusing data from heterogeneous sources where our a priori knowledge about target dynamics is only one block of the system Information 003ows bi-directionally into the whole system following the rules of gaussian belief propagation where for the non linear blocks we have computed local 002storder approximation We have focused on a con\002guration with multiple cameras However the system is very general and can be easily upgraded to include other sensor modalities such as AIS radar lidar IR camers etc The 003exibility of the factor graph in its normal form is such that when new data sources become available they can be just added to the fusion bus that acts as a probability pipeline Propagation of messages in both forward and backward directions in the graph enhances the quality of the state trajectory estimation There are many directions for future improvements The natural system enhancement is the provision for handling multiple tracks via mixture propagation Camera placement is also a crucial issue for scene coverage and for 3D tracking that will be addressed elsewhere Handling of multiple moving objects requires a merge of pattern recognition and tracking and is a topic of current research work The inclusion of simultaneous multiple track models into the graph is another important issue that will be addressed in the future 5 


Figure 4  The three 002gures on the left represent what the three cameras see in the parking lot The three 002gures on the right show the operations the pixel extraction algorithm perform on the framed scene The point that de\002nes the moving ofbject 050the car\051 is the object barycenter We construct prediction windows in the way shown in Section 2 050the dotted squares in the images on the right\051 and consequently apply the algorithm only to the subparts of the frames into the windows Such operation reduce both dimensions of the examined area and computation load The results are good because the car is moving without sudden changes in speed or direction Figure 5  In this Google Maps image of the parking lot under consideration are depicted the positions and the 002elds of view of the three cameras and the reference system of the world points 050centered in O 051 Figure 6  Tracking of the maneuvering car in the parking The ellipses around the estimates are proportional to the covariance matrices of the data and indicate how reliable the estimates are They are scaled with a 10 2 factor in order to make them clearly visible Due to the combination of forward and backward messages in the graph the uncertainty modi\002es and the ellipses changes A PPENDICES A C AMERA C ALIBRATION According to the pinhole camera model  11 12 the mapping between a 3D world points 050 X Y Z 051 and 2D image coordinates 050 x y 051 is described by the equation 025 040 x y 1   P 0 B  X Y Z 1 1 C A  05031\051 where P is a 3 002 4 matrix called camera projection matrix P   p 11 p 12 p 13 p 14 p 21 p 22 p 23 p 24 p 31 p 32 p 33 p 34   05032\051 and 025 is a scale parameters that depends on the speci\002c point The projection matrix contains parameters corresponding to camera features 050 intrinsic parameters\051 sensor position and orientation 050 extrinsic parameters\051 The model shown in Figure 7 is derived from a sequence of system rotation and translations Since intrinsic and extrinsic parameters are rarely available it is necessary to go through a calibration procedure to estimate P  In the simpler case in which the world points lay on a plane equation 05031\051 becomes 025 040 x y 1   H 040 X Y 1   05033\051 where H is a 3 002 3 homography matrix 6 


Figure 7  Simpli\002ed pinhole camera model Camera calibration 11 consists in estimating P or H given a certain number of correspondence points between the world and the image We detail the procedure for 002nding P  since H can be found by straightforward substitution in 05031\051 setting Z  z 0 050taken to be the homography world plane\051 Given the calibration points f 050 x i  y i 051  050 X i  Y i  Z i 051 g  i  1   N we can re-write 05031\051 as 050 x i  025x i  025  p 11 X i  p 12 Y i  p 13 Z i  p 14  p 31 X i  p 32 Y i  p 33 Z i  p 34  y i  025y i  025  p 21 X i  p 22 Y i  p 23 Z i  p 24  p 31 X i  p 32 Y i  p 33 Z i  p 34  i  1   N 05034\051 Finding P can be reduced to the solution of the homogeneous linear system  A p  0  05035\051 with  A  2 6 6 6 6 6 6 6 6 4 X 1 Y 1 Z 1 1 0 0 0 0 000 x 1 X 1 000 x 1 Y 1 000 x 1 Z 1 000 x 1 0 0 0 0 X 1 Y 1 Z 1 1 000 y 1 X 1 000 y 1 Y 1 000 y 1 Z 1 000 y 1 X 2 Y 2 Z 2 1 0 0 0 0 000 x 2 X 2 000 x 2 Y 2 000 x 2 Z 2 000 x 2 0 0 0 0 X 2 Y 2 Z 2 1 000 y 2 X 2 000 y 2 Y 2 000 y 2 Z 2 000 y 2                                     X N Y N Z N 1 0 0 0 0 000 x N X N 000 x N Y N 000 x N Z N 000 x N 0 0 0 0 X N Y N Z N 1 000 y N X N 000 y N Y N 000 y N Z N 000 y N 3 7 7 7 7 7 7 7 7 5 and p   p 11 p 12 p 13 p 14 p 21 p 22 p 23 p 24 p 31 p 32 p 33 p 34  T  We need at least 5 world-image point matches because P has 10 degrees of freedom As showed in we can obtain the least-squares solution in p by applying singular values decomposition 050SVD\051 on  A and then taking the last column of the last matrix that is returned Rearranging p we get P  B D ERIVATIVES We calculate explicitly the 002rst derivatives de\002ned in 05010\051 Named D h  050 h i 31 X k  h i 32 Y k  h i 33 051  _ D h  050 h i 31 _ X k  h i 32 _ Y k 051  C 11  050 h i 11 _ X k  h i 12 _ Y k 051  C 12  050 h i 11 X k  h i 12 Y k  h i 13 051  C 21  050 h i 21 _ X k  h i 22 _ Y k 051  and C 22  050 h i 21 X k  h i 22 Y k  h i 23 051  we get 050 _ x i k  1  D 2 h  C 11 D h 000 _ D h C 12   f i 3 050 X k  Y k  _ X k  _ Y k 051  _ y i k  1  D 2 h  C 21 D h 000 _ D h C 22   f i 4 050 X k  Y k  _ X k  _ Y k 051  05036\051 In the following we report extended calculations of 000 i elements As stated in 05021\051 Jacobian matrix 000 i is de\002ned as 000 i 050 x y _ x _ y 051  2 6 6 4   X f i 1   Y f i 1 0 0   X f i 2   Y f i 2 0 0   X f i 3   Y f i 3    _ X f i 3    _ Y f i 3   X f i 4   Y f i 4    _ X f i 4    _ Y f i 4 3 7 7 5  05037\051 with elements   X f i 1  1  D 2 h 050 h i 11 D h 000 h i 31 C 12 051    Y f i 1  1  D 2 h 050 h i 12 D h 000 h i 32 C 12 051    X f i 2  1  D 2 h 050 h i 21 D h 000 h i 31 C 22 051    Y f i 2  1  D 2 h 050 h i 22 D h 000 h i 32 C 22 051    X f i 3  1  D 4 h 050\050 C 11 h i 31 000 _ D h h i 11 051 D 2 h 000 2 D h h i 31 050 C 11 D h 000 _ D h C 12 051\051    Y f i 3  1  D 4 h 050\050 C 11 h i 32 000 _ D h h i 12 051 D 2 h 000 2 D h h i 32 050 C 11 D h 000 _ D h C 12 051\051     _ X f i 3  1  D 2 h 050 h i 11 D h 000 h i 31 C 12 051     _ Y f i 3  1  D 2 h 050 h i 12 D h 000 h i 32 C 12 051    X f i 4  1  D 4 h 050\050 C 21 h i 31 000 _ D h h i 21 051 D 2 h 000 2 D h h i 31 050 C 21 D h 000 _ D h C 22 051\051    Y f i 4  1  D 4 h 050\050 C 21 h i 32 000 _ D h h i 22 051 D 2 h 000 2 D h h i 31 050 C 21 D h 000 _ D h C 22 051\051     _ X f i 4  1  D 2 h 050 h i 21 D h 000 h i 31 C 22 051     _ Y f i 4  1  D 2 h 050 h i 22 D h 000 h i 32 C 22 051  05038\051 Calculating the 002rst derivatives of 05013\051 named D r  050 r i 31 x k  r i 32 y k  r i 33 051  _ D r  050 r i 31 _ x k  r i 32 _ y k 051  C 31  050 r i 11 _ x k  r i 12 _ y k 051  C 32  050 r i 11 x k  r i 12 y k  r i 13 051  C 41  050 r i 21 _ x k  r i 22 _ y k 051  and C 42  050 r i 21 x k  r i 22 y k  r i 23 051  we get 050 _ X i k  1  D 2 r  C 31 D r 000 _ D r C 32   g i 3 050 x k  y k  _ x k  _ y k 051  _ Y i k  1  D 2 r  C 41 D r 000 _ D r C 42   g i 4 050 x k  y k  _ x k  _ y k 051  05039\051 As stated in 05016\051 Jacobian matrix J i is de\002ned as J i 050 x y _ x _ y 051  2 6 6 6 4   x g i 1   y g i 1 0 0   x g i 2   y g i 2 0 0   x g i 3   y g i 3    _ x g i 3    _ y g i 3   x g i 4   y g i 4    _ x g i 4    _ y g i 4 3 7 7 7 5  05040\051 7 


with elements   x g i 1  1  D 2 r 050 r i 11 D r 000 r i 31 C 32 051    y g i 1  1  D 2 r 050 r i 12 D r 000 r i 32 C 32 051    x g i 2  1  D 2 r 050 r i 21 D r 000 r i 31 C 42 051    y g i 2  1  D 2 r 050 r i 22 D r 000 r i 32 C 42 051    x g i 3  1  D 4 r 050\050 C 31 r i 31 000 _ D r r i 11 051 D 2 r 000 2 D r r i 31 050 C 31 D r 000 _ D r C 32 051\051    y g i 3  1  D 4 r 050\050 C 31 r i 32 000 _ D r r i 12 051 D 2 r 000 2 D r r i 32 050 C 31 D r 000 _ D h C 32 051\051     _ x g i 3  1  D 2 r 050 r i 11 D r 000 r i 31 C 32 051     _ y g i 3  1  D 2 r 050 r i 12 D r 000 r i 32 C 32 051    x g i 4  1  D 4 r 050\050 C 41 r i 31 000 _ D r r i 21 051 D 2 r 000 2 D r r i 31 050 C 41 D r 000 _ D r C 42 051\051    y g i 4  1  D 4 r 050\050 C 41 r i 32 000 _ D r r i 22 051 D 2 r 000 2 D r r i 31 050 C 41 D r 000 _ D r C 42 051\051     _ x g i 4  1  D 2 r 050 r i 21 D r 000 r i 31 C 42 051     _ y g i 4  1  D 2 r 050 r i 22 D r 000 r i 32 C 42 051  05041\051 R EFERENCES   H.-A Loeliger 223An introduction to factor graphs,\224 Signal Processing Magazine IEEE  vol 21 no 1 pp 28\226 41 2004   Y Bar-Shalom P Willett and X Tian Tracking and Data Fusion A Handbook of Algorithms  YBS Publishing 2011 A v ailable http://books.google.it/books?id=2aOiuAAACAAJ   J L David L Hall Martin Liggins II Handbook of Multisensor Data Fusion Theory and Practice  C P I Llc Ed 2008   X Li and V Jilkov 223Survey of maneuvering target tracking part i dynamic models,\224 Aerospace and Electronic Systems IEEE Transactions on  vol 39 no 4 pp 1333\2261364 2003   R Mahler 223Multitarget bayes 002ltering via 002rst-order multitarget moments,\224 Aerospace and Electronic Systems IEEE Transactions on  vol 39 no 4 pp 1152\226 1178 2003   Y Bar-Shalom T Kirubarajan and X.-R Li Estimation with Applications to Tracking and Navigation  New York NY USA John Wiley  Sons Inc 2002   J Forney G.D 223Codes on graphs normal realizations,\224 Information Theory IEEE Transactions on  vol 47 no 2 pp 520\226548 2001   R Hartley and A Zisserman Multiple View Geometry in Computer Vision 2ed  Cambridge 2003   F Kschischang B Frey and H.-A Loeliger 223Factor graphs and the sum-product algorithm,\224 Information Theory IEEE Transactions on  vol 47 no 2 pp 498\226 519 2001   H.-A Loeliger J Dauwels J Hu S Korl L Ping and F Kschischang 223The factor graph approach to modelbased signal processing,\224 Proceedings of the IEEE  vol 95 no 6 pp 1295\2261322 2007   E Trucco and A Verri Introductory Techniques for 3-D Computer Vision  Prentice Hall 1998   F Palmieri F Castaldo and G Marino 223Harbour surveillance with cameras calibrated with ais data,\224 in Aerospace Conference 2013 IEEE  2013 pp 1\2268   M Piccardi 223Background subtraction techniques a review,\224 in IEEE International Conference on Systems Man and Cybernetics  2004   G Golub and C Reinsch 223Singular value decomposition and least squares solutions,\224 Numerische Mathematik  vol 14 no 5 pp 403\226420 1970 On A v ailable http://dx.doi.or g/10.1007/BF02163027 B IOGRAPHY  Francesco Castaldo received his Laurea 050Bachelor's Degree\051 in 2009 and his Laurea Magistrale 050Master's Degree\051 in 2012 both in Ingegneria Informatica from Seconda Universit 264 a degli Studi di Napoli 050SUN\051 He has cooperated during his thesis work with the PRISMA lab at Universit 264 a degli Studi di Napoli Federico II and he has been involved as post-graduated fellow in the European project HABITAT 050HArBour traffIc optimization sysTem\051 He is currently a PhD student with the Dipartimento di Ingegneria Industriale e dell'Informazione at SUN His research interests are in image processing computer vision and data fusion applied to robotics and target tracking Francesco A N Palmieri received his Laurea in Ingegneria Elettronica from Universita degli Studi di Napoli Federico II Italy in 1980 In 1981 he served as a 2nd Lieutenant in the Italian Army in full\002llment of draft duties In 1982 and 1983 he was with the ITT 002rms FACE SUD Selettronica in Salerno 050currently Alcatel\051 Italy and Bell Telephone Manufacturing Company in Antwerpen Belgium as a designer of digital telephone systems In 1983 he was awarded a Fulbright scholarship to conduct graduate studies at the University of Delaware 050USA\051 where he received a Master's degree in Applied Sciences and a PhD in Electrical Engineering in 1985 and 1987 respectively He was appointed Assistant professor in Electrical and Systems Engineering at the University of Connecticut Storrs 050USA\051 in 1987 where he was awarded tenure and promotion to Associate Professor in 1993 In the same year he was awarded the position of Professore Associato at the Dipartimento di Ingegneria Elettronica e delle Telecomunicazioni at Universit 264 a degli Studi di Napoli Federico II Italy where he has been until October 2000 when he became Professore Ordinario di Telecomunicazioni and moved to the Dipartimento di Ingegneria dell'Informazione  Seconda Universit 264 a di Napoli Aversa Italy His research interests are in the areas of signal processing data fusion communications information theory and neural networks 8 


                                     


                                                        


                           


                                        


                  


  


                                               


   


                                


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


