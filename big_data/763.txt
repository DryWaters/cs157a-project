Framework for the Analysis of the Adaptability, Extensibility, and Scalability of Semantic Information Integration and the Context Mediation Approach   Thomas Gannon MITRE Corporation tgannon@mitre.org    Michael Siegel MIT Sloan School of Management msiegel@mit.edu Stuart Madnick MIT Sloan School of Management MIT School of Engineering smadnick@mit.edu  Marwan Sabbouh 
MITRE Corporation ms@mitre.org  Allen Moulton MIT Sloan School of Management amoulton@mit.edu   Hongwei Zhu Old Dominion University hzhu@odu.edu    Abstract  Technological advances such as Service Oriented Architecture \(SOA\ have incr eased the feasibility and importance of effectively integrating information from an ever widening number of systems within and across enterprises. A key difficulty of achieving this goal 
comes from the pervasive heterogeneity in all levels of information systems. A robust solution to this problem needs to be adaptable, extensib le, and scalable. In this paper, we identify the deficiencies of traditional semantic integration approaches. The COntext INterchange \(COIN\ approach overcomes these deficiencies by declaratively representing data semantics and using a mediator to create the necessary conversion programs from a small number of conversion rules. The capabilities of COIN is demonstrated using an example with 150 data sources where COIN can automatically generate the over 
22,000 conversion prog rams needed to enable semantic interoperability usin g only six parametizable conversion rules. This paper presents a framework for evaluating adaptab ility, extensibility, and scalability of semantic integration approaches. The application of the framework is demonstrated with a systematic evaluation of COIN and other commonly practiced approaches   1. Introduction  In the report Making the Nation Safer 
the National Research Co found t h at  Although there are many private and public databases that contain information potentially relevant to counter terrorism programs, they lack the necessary context definitions \(i.e., metadata\ss tools to enable interoperation with other databases and the extraction of meaningful and timely information Despite the fact that nearly 30% of IT dollars are spent on Enterprise Information Integration \(EII\ganizations are still plagued by the lack of effective integration and 
interoperation. NIST found that lack of interoperability costs the U.S. capital fa cilities industry $15.8 billion  n terprise s becom e  increasingly information intensive and the web continues to grow in the range and number of sources semantic information integration is critical for effective exchange and utilization of valuable information. A viable solution to large scale integration has to be adaptable, extensible, and scalable Technologies already exist to overcome heterogeneity in hardware, software, and syntax used in different systems \(e.g., the ODBC standard, XML 
based standards, web services and SOA-Service Oriented Architectures.\While these capabilities are essential to information in tegration, they do not address the issue of heterog eneous data semantics that exist both within and across enterprises. The data receiver still needs to reconcile semantic differences such as converting pounds an d ounces into kilograms or vice versa, depending on how the receiver wants to interpret the data. Hand-cod ing such conversions is only manageable on a small scale; alternative solutions are needed as the number of systems and the 
complexity of each system increase There have been significant efforts devoted to developing robust semantic integration so   such as the COntext INterchange \(COIN\h  not until recently that attention is paid on evaluating the robustness of various solutions. Two frameworks are provided in [4 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 1 978-0-7695-3450-3/09 $25.00 © 2009 IEEE 


 capability of a solution. Each framework consists of scenarios with a range of heterogeneous sources and a set of testing queries to see if an integration solution can correctly answer the queries. Neither framework provides criteria for evaluating other important properties of a solution e.g., adaptability, extensib ility, and scalability. These properties, or rather these terms, appear frequently in works that describe certai n integration approaches, but they are often used informally. For example, a comparative analysis is gi p are several integration approaches; certain misinterpretations exist because of a lack of a uniform framework to evaluate these properties In contrast, in this paper, we will provide a framework for evaluating adaptability, extensibility and scalability of semantic in tegration solutions, and to use the framework to evaluate the COIN approach and several commonly practiced approaches  2. Examples and Challenges of Intelligence Information Integration  As a motivating example let us consider intelligence information which is usually gathered by different agencies in multiple countries Since no single agency is expected to have comp lete information, integration is necessary to perform various intelligence analyses including basic questions such as who did what where, and when Significant challenges exist when different agencies organize and report information using different conventions. We illustrate the challenges using several examples from the counterterrorism domain. Similar issues exist in most other application domains where information integration is required, especially if heterogeneous semi-structured web sources are involved Person Identification  Identifying a person in a corporate database can be as simple as assigning a unique identification number, e.g., employee_id, for each person. This cannot be easily done across multiple agencies. Other attributes of a person are often used to help identify the records related to the person in different data sources Name of a person is a good candidate attribute, but different sources may record names differently, e.g William Smith in one source and Bill Smith in another. Name spelling becomes more complicated when a foreign name is tran slated into English. For example, the Arabic name has been shown to have over 60 romanizati ons including: Gadaffi Gaddafi, Gathafi, Kadafi, Kaddafi, Khadafy, Qadhafi and Qathafi. There are numerous romanization and transliteration standards. But different agencies may choose different standards Other attributes such as weight and height of a person can be used conjunctively to help with identification matching. Again, different agencies may choose different standards for these attributes, e.g., a British agency may report weight in stones, while a U.S. agency might use pounds and a German agency might use kilograms. Similarly, these agencies may use feet, inches, and centimeters, respectively, for height It would be impossible to perform any useful intelligence analysis wh en the information from different sources is put together without reconciling these differences. To illustr ate the difficulties, consider three records from three different sources shown in Table 1 Table 1. Data from three different sources Source Name Weight Height Place Time Event UK Gadaffi 12.14 5.67 London 12/11/2004 13:15 Plane arrives US Kadafi 170 68 London 11/15/2004 19:30 Meeting Germany Qadhafi 77 173 Vienna 12/11/2004 11:30 Plane departs In their present form, the three records apparently refer to three different people. However, an important pattern will be revealed when the data from different sources are transformed into a uniform standard. For example, if the three records are converted to the standard used by the U.S. agency, we can relate the three records to the same person because after the conversion, the three records have the same Name Weight and Height \(e.g., 12.14 stones is equal to 179 lbs or 77 kg\ and discover a pattern that a person named Kadafi, who weighs 170 lbs and measures 68 inches high, flew from Vienna to London on November 12, 2004 and la ter on November 15, 2004 had a meeting Location Representation Location information is often represented using place names, codes, and various geographic coordinates. Place names are not unique. A search for Cambridge at Weather.com returns eight cities located in Canada, UK, and U.S Thus it is necessary to qualify a place name with other place names at different geographical granularities e.g., Cambridge, MA, US or Cambridge, Ontario, CA Here, country codes are used to indicate the country in which the city is located. Al though country codes are compact and can eliminate pr oblems with spelling and translation of country names, the same code sometimes represents different countries in different standards The frequently used standards include the FIPS 2character alpha codes and the ISO3166 2-character alpha codes, 3-character alpha codes, and 3-digit numeric codes. Confusions will arise when different agencies use different coding standards. For example Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 2 


explosion heard in the capital of BG  is it in Bulgaria \(if ISO 3 166 2-charcter alpha code was used or in Bangladesh \(if FIPS code was used\. Similarly BD stands for Bermuda in FIPS, while it stands for Bangladesh in ISO 3166; and BM stands for Bermuda in ISO 3166 and for Burma in FIPS There are also multiple standards for airport codes The two major ones are IATA and ICAO. For example the code for Heathrow airport is LHR in IATA standard, EGLL in ICAO sta ndard. A system that uses one code standard will not be able to correctly recognize an airport designate d with another standard One may contemplate that we should be able to identify a location by its geographical coordinate on earth. That turns out to be very complicated there are over 40 geographic coordinate systems widely used around the world. Even within the U.S. Department of Defense \(DoD\ standards are used by different branches of the armed forces, e.g., parts of the US Army and Marine Corps use the Universal Transverse Mercator \(UTM\d and Military Grid Reference System \(MGRS\ while parts of the US Navy use latitude and longitude expressed in degrees minutes and seconds, and parts of the US Air Force express them in degrees and decimal degrees Misinterpretation of these different representations can lead to ineffective coordina tion in the battle field or tactic operations in the war on terror Time Representation The representations for other data elements coul d vary significantly among data sources. Time representation is particularly important for many applications. For example, date may be expressed using different calendars \(e.g besides the Gregorian calendar, there are others, such as the Jewish/Israeli calendar and Chinese/lunar calendar\ven when only the Gregorian calendar is used, year, month, and day can be arranged in different orders and using different punctuations, e.g 11/12/2004 versus 12-11-2004, etc The time of day values can be at GMT time or local time \(with different conventions for how to encode the time zone\rd time or daylight savings time using either 12-hour or 24-hour format, etc. There is considerable variety of comb inations and permutations  3. Integration Scenario  To further illustrate the challenges of integrating information from diverse sources, let us consider a scenario that involves many of the data elements discussed earlier After September 11, it became imperative that different agencies in the U.S. and among coalition countries share coun ter-terrorism intelligence information. Suppose there are a total of 150 such agencies, e.g., two dozen countries each having, on average, half dozen agencies \(or possibly different parts of the same agency\assume that the shared information consists of person name, height weight, airport, country, geo-coordinate of location date, and time which, of course, is a small sample of the wide array of information actually used. To further simplify explication, we assume that person name and time data have been standardized across the sources For the rest of the attributes different agencies may use different conventions. The varieties of these conventions are summarized in Table 2 Table 2. Semantic Differences in Data Sources Data Types Semantic varieties Height 4 different units of measure: ft, in, cm, m Weight 3 different units of measure: lbs, kg, stone Airport 2 different coding standards: IATA, ICAO Country 4 different coding standards: FIPS, ISO 2Alpha, ISO 3-Alpha, ISO 3-digit Geocoordinate 4 different reference systems and datum parameters: MGRS_WGS84, BNG_OGB7 Geodetic_WGS84, UTM_WGS84 Date 4 different formats: mm/dd/yyyy dd/mm/yyyy, dd.mm.yyyy, dd-mm-yyyy There are a total of 1,536 \(i.e., 4*3*2*4*4*4 combinations from these varieties. We use the term contexts to refer to these different ways of representing and interpreting data there are potentially 1,536 unique contexts in this scenario. Let us assume that each of the 150 data sources uses a distinct context as its data representation conventi on. For example, a U.S agency may choose to use inches for height, lbs for weight, IATA code for airport, etc., while a U.K agency may choose to use feet for height, stones for weight, ICA for airport, etc An analyst from any of the 150 agencies may need information from all the other agencies to perform intelligence analysis As shown in Table 1, when information from other agencies is not converted into the analyst s context, it will be difficult to identify important patterns. Therefore a total of 22,350 \(i.e 150*149\onversion programs would be required to convert data from any source s context to any other source s context, and vice versa In practice, any specific analyst or analytical tool used by the analyst can have a context different from the agency s, e.g., an analyst from the CIA may use a tool that assumes height is in feet while the agency s databases use inches. Theref ore, every data source and data receiver could have their own contexts, so that in reality, there can be more than 150 information exchanging entities in the 150 agencies. For explication purposes, we continue the example with the assumption that there are on ly 150 sources/receivers Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 3 


Implementing tens of thousands of data conversions is not an easy task; but maintaining them to cope with changes in data sources and receiver requirements over time is even more challenging. We will describe and discuss various approaches to this problem in the next two sections  4. Traditional Approaches to Achieving Semantic Interoperability  Brute-force Data Conversions BF The BF approach directly implements all necessary conversions in hand-code d programs. With N data sources and receivers, N\(N-1\uch conversions need to be implemented. When N is large, these conversions become costly to implemen t and difficult to maintain The BF process is labor-intensive because many semantic differences have to be identified by humans and the conversions need to be implemented and maintained over time to account for changes in the underlying sources. This explains why nearly 70% of integration costs come from the implementation of these data conversion programs The BF approach might appear sufficiently inefficient that one might be surprised at how common it is.  The reason is that conversion programs are usually written incrementally. Each individual conversion program is produced in response to a specific need.  Writing only one conversion program does not seem like a bad idea, but over time this process leads to N\(N-1 conversion programs that must be maintained Global Data Standardization \(GS In the example different data standards are used in the 150 agencies that need to exchange information. If they could agree on a uniform standard, e.g standardizing height data to centimeters in all systems, all the semantic differences would disappear and there would be no need for data conversion. Unfortunately, such standardization is usually infeasible in practice for several reasons Often there are legiti mate needs for storing and reporting data in different forms. For example, while height in centimeters makes sense to an agent in other NATO countries such as Germany, a U.S. agent may not find it useful until it has been converted to feet and inches. Since most integr ation efforts involve many existing systems, agreeing to a standard often means someone has to change current implementation, which creates disincentives and makes the standard setting and enforcement process extremely difficult. This difficulty is exacerbated when the number of the data elements to be standardized is large. For example, in 1991 the DoD initiated a da ta administration program that attempted to standardize nearly one million data elements. By the year 2000, DoD had only managed to register 12,000 elements, most of which were infrequently reused. After a decade of costly effort, the DoD realized its infeasibility and switched to an alternative approach to allow different communities of interest to develop their own standards [1   The change in approach by the DoD manifests the reality of standards development, i.e., there are often competing or otherwise co-exi sting standards. As seen in the examples in Section 2, there are multiple standards for airport code s and for country codes Different systems can pote ntially choose different standards to implement. Thus, in most cases, we cannot hope that semantic differences will be completely standardized away; data conversion is inevitable Interchange Standardization IS The data exchange parties sometimes can agree on the format of what is to be exchanged, i.e., standardizing a set of concepts as well as interchange formats. The underlying systems do not need store the data according to the standard; it suffices as long as each data sender generates the data according to the standard. Thus each system still maintains its own autonomy. This is different from the global data standardization, where all systems must store data according to a global standard. With N parties exchanging information, the Interchange Standardization approach requires 2N conversions This is a significant improvement over the brute-force approach that might need to implement conversions between every pair of systems This approach has been used for various business transactions, e.g., EDI and various B2B trading standards. In the military setting, the U.S. Message Text Format \(MTF\NATO equivalent, Allied Data Publication-3, have over 350 standard messages that support a wide range of military operations. This standard has been used for over 50 years and currently an XML version is being de As a recent  example, the DoD standardized the exchange format of weather related data, which consists of about 1,000 attributes. This standard ha s been successfully used by several systems that exchange weather data Similarly, the XML-based Cursor-On-Target \(COT standard, which consists of 3 entities and 13 attributes has been used successfully by over 40 systems to exchange targeting information T h e U.S  A r m y  also succeeded in rapidly integrating a dozen diverse stovepipe battlefield systems using limited scope XML-based interchange standards  Although the IS approach has certain advantages e.g., local autonomy and a smaller number of conversions required, it also has serious limitations First, all parties have to have a clear understanding about the domain, decide what data elements should go into the standard, and reach an agreement on the data Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 4 


format. This can be costly and time consuming. It took the DoD five years to standardize the weather data interchange format. Furthermore, in many cases it is difficult to foresee what data needs to be exchanged or changes to requirements over time, which makes it inappropriate to have a fixed standard. When the significant information is not specified in the standard ad-hoc conversions have to be implemented. Lastly any change to the interchange standard affects all systems and the existing conversion programs Summary of Tr aditional Approaches Each of the three traditional approaches has certain drawbacks that make them inappropriate for integrating information from a large number of data sources. These weaknesses are summarized below  Brute-force data conversions \(BF\requires a large number of hand-written conversions that are difficult to maintain  Global Data Standardization \(GS sometimes impossible to de velop a global standard In addition to legitimate r easons of having multiple standards, there are technological difficulties and organizational resistance for a single standard  Interchange Standardization \(IS\ndard is static, only suitable for routine data sharing and it still requires a large number of hand-written conversions In addition, these approach es lack flexibility to adapt to changes because the data se mantics is hard-coded in the conversions for BF, in the standard in GS, and in both the conversions and the standard in the case of IS A suitable approach needs to overcome these shortcomings. In the next section, we will discuss such an approach that automates code generation for conversions and requires no data standardization  5. Ontology-based Context Mediation  Most of the shortcomings of the traditional approaches can be overcome by declaratively describing data semantics and separating knowledge representation from conversion implementation. There have been a number of research projects that utilize ontology to represent data semantics and to facilitate reconciliation of semantic differe ontology is essentially an agreement on conceptual models approaches that require a single, i.e. global, ontology have shortcomings similar to the data standardization approach. Therefore it is desirable to lower or eliminate the reliance on reaching a global agreement on the details of every data element. In the following we introduce the COntext INterchange \(COIN 2,6,10 h  whic h allows eac h data source  and receiver to describe its local ontology using a common language and also provides reasoning service to automatically detect and reconcile semantic differences  5.1 The COIN Approach  The COIN approach consists of a deductive objectoriented data model for knowledge representation, a general purpose mediation reasoning service module that determines semantic differences between sources and receivers and generates a mediated query to reconcile them, and a query processor that optimizes and executes the mediated query to retrieve and transform data into user context \(see Figure 1  COIN Mediator  Executioner  Optimizer  Receivers User Apps Conversion Libraries Mediated query explication User query Data in user context Data sources Knowledge Representation  F-Logic based data model Ontology  define types and relationships Context definitions  define source and receiver contexts by specifying  modifier values Mappings  assigning correspondence between data elements and the types in ontology Mediation service Graphic/Web-based modeling tool wrapper wrapper   Figure 1. Architecture of COIN System The COIN knowledge representation consists of three components. An ontol ogy is used to capture common concepts and their re lationships such as one concept being a property \(i.e., attribute\ or a subconcept \(i.e., is_a relationship\another. A concept is roughly equivalent to a class in object-oriented models and entity type in Entity-Relationship conceptual models. Each concept may have one or more modifiers as a special kind of property to explicitly represent specializations of the concept in the sources and receivers. We call the collection of declarative specifications of modifier values context. For each modifier, a rule or a set of rules are used to specify the conversions between different values of the modifier The semantic mappings es tablish the correspondence between data elements in the sources and the concepts in the ontology. These components are expressed in the object-oriented deductive language F-Logic   which can be translated into Horn logic expressions that we use internally, or Web Ontology Language OWL for the Semantic Web The core component in the mediation service module is the COIN mediator implemented in abductive constraint logic programmi  h ere  constrai nts are concurrently solved using Constraint Handling Rules \(CHR h e m e diator ta kes a user query a n d produces a set of mediated queries \(MQs\hat resolve semantic differences. This is accomplished by first Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 5 


translating the user que ry into a Datalog query and using the encoded knowledge to derive the MQs that incorporate necessary conversions from source contexts to receiver context. The query processor optimizes the MQs using a simple cost model and the information on source cap abilities, obtains the data performs the conver sions, and returns the final datasets to the user Within the COIN approach, the users are not burdened with the diverse and changing semantics in data sources, all of which are recorded in the knowledge representation component and are automatically taken into account by the mediator Adding or removing a data source is accomplished by adding and removing declarations, which does not require any changes to the mediator or query processor they will use the new knowl edge to produce the new correct conversion programs, as needed  5.2 Information Integration using COIN  To apply COIN to the intelligen ce information integration scenario, none of the agencies need to change their current systems; they only need to record their context definitions by using the terms in a shared ontology. An excerpt of the ontology is shown in Figure 2  is_a relationship attribute modifier Legend is_a relationship attribute modifier Legend height date time location geoCoord countryCode cityName airport person personName event weight basic ctryCodeStd geoCoordCode aptSymType dateFormat weightUnit lengthUnit  Figure 2. Excerpt of ontology In the ontology, concepts, i.e., types, are in rectangular boxes. There is a special type called basic which has no modifier and serves as the parent of all the other types. We do not show the is_a relationship between the type basic and the rest of the types to avoid cluttering the graph The shared ontology is completely different from a data standard in that it only contains basic concepts and their relationships, which are much easier to agree on than the representations of the types that are usually specified in a data standard For example, the ontology only states that a person has weight, keeping silent about in what unit the weight should be. With this ontology, each data source and receiver can define their local ontologies by specifying modifier values to obtain desired specializations to the common types, e.g., specializing weight to weight in lbs These specifications are called context definitions Table 3. Example contexts Modifier USA context UK context NATO context Analyst context dateFormat mm/dd/yyyy dd/mm/yyyy  dd.mm.yyyy dd-mm-yyyy ctryCodeStd FIPS ISO3166 2-alpha ISO3166 3-digit ISO3166 3-alpha aptSymType IATA ICAO ICAO IATA geoCoordCode MGRSWGS84 BNG-OGB7 GeodeticWGS84 UTM-WGS84 lengthUnit inches feet cm m weightUnit pounds stones Kg kg Table 3 shows four example contexts that will be used later for a demonstration Both the ontology and the context definitions are declaratively defined and can be manipulated using graphic tools. Using the internal repres entation, the following F-Logic formula states that in context c_USA  the weight unit is lb       _     _      lb USA c value Y Y USA c weightUnit X basic Y weight X  The modifiers of a type are represented as methods of the type. The value method returns a value in the context specified by the parameter. This method is implemented by the mediator to compare the modifier values between the source context and the receiver context; if they are different, conversions are introduced to reconcile the differences Conversions are defined for each modifier between different modifier values; they are called component conversions. The mediator automatically composes a composite conversion using the component conversions defined for releva nt modifiers to reconcile all semantic differences invo lved in a user query. In many practical cases, a component conversion can be parameterized to convert from any given context to any other given context for that modifier. For example, the following internal component conversion definition converts between any two contexts of weight units \(a user-friendly interface can be used to define conversions      2      _ _   2     1     1   2      2 2 r u v r C value R C T C F R T F conv unit C C weightUnit X C C weightUnit X v u c C weightUnit cvt X weight X t C f C t f  Once all contexts are defined and the component conversions for each modifier are specified, a receiver in any context can query any data source in other context as if they were in the same context. The mediator automatically recognizes context differences and dynamically composes a conversion using the component conversions We will demonstrate the key features of COIN using the intelligence information integration scenario. In Figure 3 we show the mediation of a query to two intelligence data sources, one in the USA context, the other in the UK context.  In addition to converting data Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 


from different contexts into the desired context, the mediator also has explicati on tools such as reporting detected semantic differenc es and generating mediated queries as intensional answe rs to the original query For example, when the receiver in the Analyst context issues the following query to combine data from two sources select personName, height, weight geoCoord, cityName, airport, countryCode eventDate, eventType from cti_reports_UK union select personName, height, weight geoCoord, cityName, airport, countryCode eventDate, eventType from cti_reports_USA at the Conflict Detection stage, the mediator reports all detected semantic differences as shown in Table 4 The first table in Table 4 displays the semantic difference between the UK context and the Analyst context, while the second table shows the differences between the USA context and the Analyst context Comparing the detected differences here with those summarized in Table 3 indicates that all semantic differences are correctly identified. For example weight is expressed in ston es in the UK context while it is in kg in the Analyst context; because both the USA context and the Analyst context use the same airport code standard, the airport code difference shown in the first table does not appear in the second table. In fact for the same query, if the desired context is USA context, the second table will be empty Table 4. Semantic differences detected by mediator  SemanticType Modifier Modifier value in source context Modifier value in target context eventDate dateFmt c_UK European Style c_Analyst American Style countryCode ctryCodeStd c_UK ISO3166A2 c_Analyst ISO3166A3 airportCode aptSymType c_UK : ICAO c_Analyst : IATA geoCoord geoCoordCode c_UK : BNGOGB7 c_Analyst : UTMWGS84 Weight weightUnit c_UK : stone c_Analyst : kg Height lengthUnit c_UK : ft c_Analyst : m a  Differences between UK and Analysts contexts  SemanticType Modifier Modifier value in source context Modifier value in target context eventDate dateFmt c_USA American Style c_Analyst American Style countryCode ctryCodeStd c_USA : FIPS c_Analyst ISO3166A3 geoCoord geoCoordCode c_USA : MGRSWGS84 c_Analyst : UTMWGS84 weight weightUnit c_USA : lb c_Analyst : kg height lengthUnit c_USA : in c_Analyst : m b  Differences between USA and Analyst contexts  The mediated query in the internal Datalog syntax is shown in Figure 3. All semantic differences shown in Table 4 are reconciled by the conversions automatically composed by the mediator. For example the unit of measure difference for weight between UK context and the Analyst context is reconciled by using the unit_conv conversion function, which returns a conversion ratio \(V15 indicated by a rectangle\. The weight value in UK is V14 \(indicated by an oval which is multiplied by the conversion ratio to obtain V24 \(in double-lined rectangle\h is kg as desired by the Analyst.  Other semantic differences are reconciled similarly answer\('V26', 'V25', 'V24', 'V23', 'V22', 'V21', 'V20', 'V19', 'V18 unit_conv\("ft", "m", 'V17  V25' is 'V16' * 'V17  unit_conv\("stone", "kg", 'V15  V24' is 'V14' * 'V15  cti_geoTran_convert2\("BNG-OGB7-X", 'V13', "MGRS-WGS84-X", 'V23 airporticao\('V12', 'V21', 'V11  cti_ctrycode\('V10', 'V9', 'V8', 'V20', 'V7  datexform\('V6', "European Style /", 'V19', "American Style  cti_reports_UK\('V5', 'V4', 'V8', 'V22', 'V12', 'V3', 'V13', 'V26 V16', 'V14', 'V18', 'V6', 'V2', 'V1  answer\('V24', 'V23', 'V22', 'V21', 'V20', 'V19', 'V18', 'V17', 'V16 unit_conv\("in", "m", 'V15  V23' is 'V14' * 'V15  unit_conv\("lb", "kg", 'V13  V22' is 'V12' * 'V13 cti_geoTran_convert2\("geodetic-WGS84-X", 'V11', "MGRS-WGS84X", 'V21  cti_ctrycode\('V10', 'V9', 'V8', 'V18', 'V7  datexform\('V6', "American Style /", 'V17', "American Style  cti_reports_USA\('V5', 'V4', 'V9', 'V20', 'V19', 'V3', 'V11', 'V24 V14', 'V12', 'V16', 'V6', 'V2', 'V1   Figure 3. Mediated query When the same query is issued by a receiver in other contexts, the appropriate mediated query will be generated accordingly. For example, Figure 4 shows the mediated query when the desired context is USA Note that first sub-query now consists of necessary conversions between the UK context and USA context e.g., weight conversion conve rts from stone to lb. The second sub-query does not include any conversion at all, because the source is already in the receiver context answer\('V26', 'V25', 'V24', 'V23', 'V22', 'V21', 'V20', 'V19', 'V18 unit_conv\("ft", "in", 'V17  V25' is 'V16' * 'V17  unit_conv\("stone", "lb", 'V15  V24' is 'V14' * 'V15  cti_geoTran_convert2\("BNG-OGB7-X", 'V13', "geodetic-WGS84-X", 'V23  airporticao\('V12', 'V21', 'V11  cti_ctrycode\('V10', 'V20', 'V9', 'V8', 'V7  datexform\('V6', "European Style /", 'V19', "American Style  cti_reports_UK\('V5', 'V4', 'V9', 'V22', 'V12', 'V3', 'V13', 'V26 V16', 'V14', 'V18', 'V6', 'V2', 'V1  answer\('V14', 'V13', 'V12', 'V11', 'V10', 'V9', 'V8', 'V7', 'V6  cti_reports_USA\('V5', 'V4', 'V8', 'V10', 'V9', 'V3', 'V11', 'V14 V13', 'V12', 'V6 V7', 'V2', 'V1  Figure 4. Mediated query when receiver is in USA context We have shown with this example how the COIN approach overcomes the shortcomings of traditional approaches. That is, with COIN, the sources are not required to make any change or commit to any standard; they only need to record data semantics declaratively. Only a small number of component conversions need to be defined declaratively, which are used by the mediator to compose necessary Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


conversions automatically. Changes in the sources can be accommodated by updating context definitions, no hand-written code need to be maintained. These features will be discussed further in the next section  6. Framework for Analyzing Adaptability Extensibility, and Scalability  The framework presented he re is an extension to the preliminary work reported in  motivated by   base d on an obse rvation of nece ssary tasks  involved to en able semantic interoperation. We categorize the tasks into three main categories  Knowledge acquisition to acquire knowledge about all systems engaging in information exchange  Implementation to encode the acquired knowledge and implement necessary conversions as a set of instructions on how to reconcile semantic differences between systems  Execution to fulfill a given information exchange task by determining and executing all necessary conversions to obtain data instances This task breakdown allows us to separate two aspects that need to be co nsidered when evaluating semantic integration approaches. One concerns human efforts involved, the other concerns the performance of the software algorithm. In tuitively, the two aspects distinguish between how hard humans have to work  and how hard computers have to work to achieve semantic interoperability. Tradeoffs can be made between human efforts and computer efforts For example, the global standard approach requires all systems implement the standard, in which case all systems are semantically intero perable by design. With this approach, most of the work is upfront human effort on developing and impl ementing the standard A set of criteria can be developed to evaluate different approaches for each task. By far the second task, i.e., implementation, is the most labor-intensive and requires significant amo unt of human efforts, so our framework will focus on assessing human efforts involved in carrying out the second task. The evaluation criteria consider three properties  Adaptability is the capability of accommodating changes, such as semantic changes within a data source with minimal effort  Extensibility is the capability of adding \(or removing\data sources with minimal effort  Scalability refers to the capab ility of achieving and maintaining semantic inter operability with the amount of effort not growing dramatically with the number of participating sources and receivers Although a direct measurement of human efforts involved can be obtained through ex periments, it will be costly to set up such experiments appropriately to reliably test different integ ration solutions. We take an alternative analytical approach that indirectly measures human efforts using the number of conversion programs to be manually de veloped, and maintained over time, as a proxy  Adaptability number of conversions to be updated when data semantics changes  Extensibility number of conversions to be added \(or removed\hen a source is added \(or removed  Scalability number of conversions needed for semantic interoperability among all sources and receivers The global standard approach eliminates the need for writing conversions, thus we will provide general discussions about its propert ies instead of using the proxy measurements.  In the preceding discussions we used the term conversion quite loosely. To be more precise, we distinguish four types of conversion  a component conversion is defined for a modifier between two modifier values in the COIN approach it reconciles one aspect of semantic differences of a single data type \(e.g., a conversion that only reconciles differences in unit of weight  a compound conversion reconciles all aspects of semantic differences of a single data type \(e.g suppose that aside from weightUnit modifier, type weight also has a modifier to represent different scale factors in different contexts, a compound conversion reconciles differences in all aspects of weight, i.e., weight unit as well as scale factor  a composite conversion combines multiple component conversions to reconcile the semantic differences of all data types involved in a specific user query, which may access multiple data sources e.g., a conversion that reconciles differences in weight, height, and geo-coo rdinate, supposing these data types are requested in a user query\nd  a comprehensive conversion reconciles the semantic differences of all data types in two sources or between a source and an interchange standard \(e.g., a conversion that reconciles the differences in all data types of two systems, whic h could have dozens or even hundreds of data types Conceptually, a conversion of a latter type consists of multiple conversions of preceding types in the order shown above. Thus, when a conversion is implemented by writing code manually, on average it takes more human efforts for a latter type than for a previous type In the next subsections, we illustrate the application of this evaluation framework by analyzing the integration approaches introduced in sections 3 and 4    Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


6.1. Adaptability and Extensibility Analysis  Since both adaptab ility and extensibility are concerned with changes, we use the term flexibility to collectively refer to the two pr operties in the ensuring discussion The Brute-force BF data conversion approach has the least flexibility. With N so urces, a change in any source would affect 2\(N-1 conversion programs, i.e N-1 conversion programs converting from the changing source to the othe r sources and vice versa Adding or removing a source has similar effects This problem is somewhat reduced with the Interchange Standa rdization \(IS requires re-programming to handle changes, which can be tedious and error-prone. Furthermore, when the interchange standard is changed, all the N sources need to be updated to accommodate the change. All hardwiring approaches require the reconciliation of all semantic differences to be pre-determined and implemented in conversion pr ograms. As a result, they lack flexibility The Global Data Standardization \(GS\pproach also lacks flexibility because any change requires agreement by all sources, which is difficult and extremely time consuming. Because it requires all systems to implement the changes, it sometimes causes disruption in operations In contrast, the ontology and context based COIN approach overcomes this problem. COIN has several distinctive features  It only requires that th e individual contexts and individual conversions between a modifier s values e.g., how to convert between weight units\be described declaratively. Thus it is flexible to accommodate changes because updating the declarations is much simpler than rewriting conversion programs \(e.g it is merely necessary to indicate that a source now reports in kilograms instead of stones  The customized conversion between any pair of sources \(as many conversion programs as are needed\posed au tomatically by the mediator using conversions of the relevant modifiers  COIN is able to generate all the conversions in BF but without the burden of someone having to manually create and keep up-to-date all the pair-wise conversion programs  The COIN approach also avoids the multiple or unnecessary conversions that arise from the IS approach since the conversion programs that it generates only includes the minimally required conversions, including no conversions fo r certain \(or all\odifiers, if that is appropriate As we will see from the next subsection, the COIN approach significantly reduces the number of predefined component conversions so that it can scale well when a large number of sources need to exchange information 6.2. Scalability Analysis  As discussed earlier, our scalability analysis will focus on the number of conversions needed in each approach.  The GS approach is scalable because it does not need any conversion at all. But it is often impossible to establish a global standard in large scale integration effort. We have informally discussed the scalability of the two other traditional approaches. We will summarize them followe d by a detailed analysis on the scalability of the COIN approach  Scalability of BF With N data sources, the number of conversions for BF is N\(N-1 which is O\(N 2   Explanation Each source needs to perform translations with the other N-1 sources; there are N sources, thus a total of N\(N-1 translations need to be in place to ensure pair-wise information exchange, which is O\(N 2    Scalability of IS With N data sources, the number of conversions for IS is 2N which is O\(N  Explanation For each source there is a conversion to the standard and another conv ersion from the standard to the source. There are N sources, so the total number of conversions is 2N  O\(N   Scalability of COIN With N data sources and an ontology that has m modifiers with each having n i  unique values   1  m i the number of conversions for COIN is   2 k mn O where   1   max m i n n i k when m is fixed, the number of conversions defined in COIN is   2 k n O  Explanation As seen earlier, conversions in COIN are defined for each modifier, not between pair-wise sources. Thus the number of conversions depends only on the variety of contexts, i.e., number of modifiers in the ontology and the number of distinct values of each modifier. In worst case, the number of conversions to be defined is m i i i n n 1  1  where n i  is the number of unique values of the i th modifier in the ontology, which is not to be confused with the number of sources m is the number of modifiers. This is because in worst case for each modifier, we need to write a conversion from a value to all the other values and vice versa, so the total number of conversions for the i th modifier is n i  n i 1\Let n k max n 1   n m n both m and n k  approach infinity    1  2 1 k m i i i mn O n n for k k m i i i n n O n n m as    1   2 1  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


However, in the intelligence information example and in many practical cases, the conversion functions can be parameterized to convert between all values of a context modifier. For instance, the weight unit conversion given in Section 4 can convert between any two units of measure using the external relation unit_conv. The conversion functions for many other modifiers are also of this nature. Thus, only 6 of these parameterized conversion functions are necessary for converting between contexts that differ in weight height, airport code, country code, geo-coordinate and/or date format. The COIN approach can take advantage of these general functions because the overall conversion program between any two contexts is automatically generated When parameterization is impossible, we can still exploit certain relationships among component conversion functions.  In cases where the set of component conversions are essentially a set of interrelated equations, COIN can generate undefined conversions using its symbolic equations solver [5,6  to reduce the number of conversion component declarations needed. Thus the number of conversion definitions for a modifier can be reduced from n\(n-1 to n-1, where n is the number of unique values of the modifier, leading to  Scalability of COIN \(parameterization and invertible conversion When conversions can be parameterized, COIN requires m conversions Otherwise, if the conversi ons are invertible functions COIN needs m i i n 1  1   conversions Furthermore, declaring the contexts can be simplified since contexts can be inherited with optional overriding in COIN. This significantly reduces the number of necessary declarations. For example, we can define a context k for a country because most agencies in the same country share the same context. If an agency in the country differs from the other agencies only with regard to say, weight unit, we can define its context as k and specify only the particular weight unit in k by declaring k as a sub-context of k, k inherits all the other context definitions for context k. This keeps the size of the knowledge base compact when the number of sources grows. In additi on, subtypes in the ontology inherit the modifiers and th e conversion definitions of their parent types, which also helps keep the number of conversion component definitions small Table 5 summarizes the scalability of different approaches in terms of the number of conversions that need to be specified. Even in the worst case, the COIN approach requires significantly less conversions than the BF or IS approaches   Table 5. Number of conversions to achieve semantic interoperability among 150 sources Approach General case The scenario Brute Force BF N  N 1 N number of sources and receivers 22,350 Interchange Standard \(IS 2 N  N number of sources and receivers 300 Context Interchange COIN 1\orst case  m i i i n n 1  1   n i number of unique values of i th  modifier m number of modifiers in ontology 2  m i i n 1  1  when equational relationships exist 3 m if all conversions can be parameterized 1     2\tual number: 6 Recent research [21 d CO IN to re prese nt  and reason about semantics that change over time. For example, when comparing historic stock prices in different exchanges, some of them changed the currency assumed in the reporting \(e.g., changed from reporting in French Francs to Euros\e temporal changes can be captured and the semantic differences at different times \(in addition to between different sources\atically recognized and reconciled at run time. With these advanced features and its flexibility and scalability, COIN is ideal for large scale information integration  7. Conclusion  Integrating information from diverse heterogeneous systems is one of the key challenges today Technological advances, such as web services and XML are reducing the cost of connecting software components both within the enterprise and across enterprise boundaries. At the same time the relative importance of achieving semantic interoperability has risen and alternative approach es to this problem may have vastly different long term costs and effectiveness For example, it is not clear that the Army s success in limited scope integration of a dozen systems will scale to the demands of current efforts o apply similar techniques to integration acr oss the whole enterprise Any viable solution must be flexible and scalable in reconciling semantic differences among information sources and must be able to support adaptation as requirements change As new solutions continue to emerge, it is important that practitioners and researchers understand the characteristics of different solution approaches. In this paper, we presented an evaluation framework and applied it to analyze seve ral integration approaches Our analysis shows that the COIN approach can efficiently handle a large number of semantic conflicts Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


and it is flexible and scalable to meet the evolving requirements We believe a systematic evaluation of integration solutions is an important area for future research. The current framework uses the measurement of human efforts involved in the implementation phase of information integration to evaluate the characteristics of an integration approach. Future research will develop mechanisms for assessing the amount of human efforts involved in knowledge acquisition phase  Acknowledgements  This work has been supported in part, by MITRE Corp., the MIT-MUST project, the Singapore-MIT Alliance, and Suruga Bank  References  1  2008 http://data.army.mil 2  S. Bressan, C. Goh, N. Levina, S. Madnick, A. Shah, M Siegel \(2000\Context Knowledge Representation and Reasoning in the Context Interchange System", Applied Intelligence: The International Journal of Artificial Intelligence, Neutral Networks, and Complex ProblemSolving Technologies, 12\(2\, pp. 165-179 3  D. Doughty \(2004 The Achilles Heal of ServiceOriented Architectures Business Integration Journal September, 44-47 4  H.T. El-Khatib, M.H. Williams, L.M. MacKinnon, D.H Marwick \(2000 A Framework and Test-suite for Assessing Approaches to Resolving Heterogeneity in Distributed Databases Information & Software Technology, 42\(7 5  A. Firat, S.E. Madnick, B. Grosof \(2002 Information Integration In the Presence of Equational Ontological Conflicts", Proceedings of the Workshop on Information Technology and Systems \(WITS Barcelona, Spain, December 14-15, pp. 211-216 6  A. Firat \(2003\ using Contextual Knowledge and Ontology Merging," PhD Thesis, MIT 7  T. Frühwirth \(1998\y and Practice of Constraint Handling Rules," J. of Logic Programming, 37, 95-138 8  M.P. Gallaher, A.C. O Connor, J.L. Dettbarn, and L.T Gilday \(2004\ "Cost Analysis of Inadequate Interoperability in the U.S. Capital Facilities Industry GCR 04-867, NIST 9  H. Greene & R. Mendoza \(2005 Lessons Learned From Developing the ABCS 6.4 Solution Defense AR Journal \(April-July 2005 189 10  C.H. Goh \(1997\g and Reasoning about Semantic Conflicts in Heterogeneous Information Systems", PhD Thesis, MIT 11  C.H. Goh, S. Bressan, S. Madnick, M. Siegel \(1999 Context Interchange: New Features and Formalisms for the Intelligent Integration of Information", ACM Trans 3 12  M. Hammer, M. Stonebraker, O. Topsakal \(2005 THALIA: Test Harness for th e Assessment of Legacy Information Integration Approaches ICDE 05, 485486 13  A.C. Kakas, A. Michael, and C. Mourlas \(2000 ACLP: Integrating Abduction and Constraint Solving Journal of Logic Programming, 44, pp. 129-177 14  V. Kashyap, A.P. Sheth \(2000 Across Heterogeneous Digital Data: A Metadata-based Approach, Springer 15  M. Kiffer, G. Laussen, J. Wu \(1995 Foundations of Object-Oriented and Frame-based Languages", J. ACM, 42\(4\. 741-843 16  R. Miller, M.A. Malloy, E. Masek \(2003 Transforming Tactical Messaging: Exploiting Web Information Standards for Interoperability Intercom 44\(1 17  National Research Council \(2002 Making the Nation Safer: The Role of Science and Technology in Countering Terrorism The National Academies Press 18  A. Rosenthal, L. Seligman, S. Renner, F. Manola \(2001 Data Integration Needs an Industrial Revolution Int l Workshop on Foundations of Models for Info Integration \(FMII 01\ntaly 19  A. Rosenthal, L. Seligman, S. Renner \(2004 From Semantic Integration to Semantics Management: Case Studies and a Way Forward ACM SIGMOD Record 33\(4 20  H. Wache, T. Vogele, U. Vi sser, H. Stuckenschmidt, G Schuster, H. Neumann, S. Hubner \(2001\tologyBased Integration of Information A Survey of Existing Approaches", Proceedings of the IJCAI-01 Workshop on Ontologies and Information Sharing, Seattle, USA August 4 5 21  H. Zhu, S. Madnick, M. Siegel \(2004 Reasoning about Temporal Context using Ontology and Abductive Constraint Logic Programming Workshop on Principles and Practices of Semantic Web Reasoning PPSWR04\ Saint Malo, France, Sep 8-9, in LNCS 3208, 90-101 22  H. Zhu, S.E. Madnick \(2004 Context Interchange as a Scalable Solution to Interoperating Amongst Heterogeneous Dynamic Service 3rd Workshop on EBusiness, December 11, 2004, Washington, D.C., 150161 23  H. Zhu \(2005 Effective Information Integration and Reutilization: Solutions to Technological Deficiency and Legal Uncertainty PhD Thesis, MIT   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 11 


Concept Medical diagnostics features highest case The example cited from Luo et al 17 is the case-based data fusion methods used to support clinical decision support The detection and classification process might involve can grasp the concepts and processes that underpin the widest possible range of fusion applications 12  I Selection Criteria C4ISR Remote Medical Intelligent Condition reference Sensing Diagnostics Transportation Monitoring Level 1 Detection 0 0    Orientation 0 0 0  Classification 0    Identification 0     Level 2 Object Aggregation 0    Event  Activity Aggregation 0 0  0 Contextual Interpretation Fusion 0 0 0 Level 3 Capability Estimation 0 0 Prediction of Enemy Intent 0 0 Identification of Threat 0 0   Multi-perspective Assessment 0  0 0  Offensive  Defensive Analysis 0 0 0 Level 4 Evaluations 0     Fusion Control 0 0 0  0 Source Requirements Processing 0  0   Mission Management 0 0 0  0 Sum s 0 7 7 7 8 Sum O's 16 7 8 1 5 Sum s 0 2 1 8 3 Net Score 0 5 6 1 5 Rank 3 2 1 4 2 Continue Yes Yes Yes No Yes error in Pugh methods In the example above error have mainly to do with the bias of the selection criteria to the remote sensing and defense lexicon of data fusion This highlights the importance of developing upper-level ontologies which as a JDL implementation option which sources of means that it is best suited to leverage the algorithms and processes from the C4ISR reference a cardiac event including ventricular and atrial activity The process being controlled referred to automatic rhythm monitoring through integration of electrocardiogram and hemodynamic signals Although it is a quantitative tool for comparing concepts there is inherent subjective 


REFERENCES 1 Steinberg A 2000 Data Fusion System Engineering International Society of Information Fusion 2]Klein L 1999 Sensor and Data Fusion Concepts and Applications 2nd ed SPIE Optical Engineering Press Bellingham WA 3]Gruber T 1993 A Translation Approach to Portable Ontologies Knowledge Acquisition 5\(2 199-220 4]Boury-Brisset A 2003 Ontology-based Approach for Information Fusion International Society of Information Fusion 5 Martins J and Pinto H 2004 Ontologies How can they be built Knowledge and Information Systems Springer-Verlag London 14 de Weck 0 and Simchi-Levi D 2006 HaughtonMars Project Expedition 2005 Interplanetary Supply Chain Management and Logistics Architectures MITINASA Exploration Systems Mission Directorate and Research  Technology Program NASA/TP-2006214196 15 Kobayashi K Munekata F and Watanabe K 1994 Accurate navigation via differential GPS and vehicle local sensors in Proc IEEE Int Conf Multi sensor Fusion Integration Intell Syst pp 9-16 16 Mirabadi A Mort N and Schmid F 1996 Application of sensor fusion to railway systems in Proc IEEE/SICE/RSJ Int Conf Multi sensor Fusion Integration Intell Syst pp 185-192 17 Luo R Chih-Chen Y and Su K 2002 Multi-sensor Fusion and Integration Approaches and Future Research Directions IEEE Sensors Journal Vol 2 No 2 pp 102119 6 Van Heist G Schreiber A and Wielinga B 1997 Using explicit ontologies in KBS development Int J Hum Computer Stud 46\(2/3 pp 183-292 7 Guarino N 1998 Formal ontology and information systems In Guarino N ed Formal ontology in information systems IOS Press Amsterdam pp.3-15 8 Wald L 1999 Some Terms of Reference in Data Fusion IEEE Transactions on Geoscience and Remote Sensing Vol 37 No 3 9 Waltz E and Llinas J 1990 Multi sensor Data Fusion Artech House Norwood MIA 10 Bowman C 2004 The Dual Node Network DNN Data Fusion  Resource Management DF&RM Architecture AIAA Intelligent Systems Conference Chicago 11 Hall D and Llinas J 1997 An Introduction to Multisensor Data Fusion IEEE Proceedings Vol 85 No 1 12 Russell S and P Norvig 2003 Artificial Intelligence A Modern Approach 2nd ed Pearson Education Inc New Jersey 13 Lin G et al 2005 Transforming the military through sense and respond IBM Business Consulting Services White Paper IBM Global Services Somers NY BIOGRAPHY Atif Mirza is a Senior Consultant with Booz Allen Hamilton He specializes in corporate strategy and operations advisory for clients in the Global Aerospace and Defense Practice Prior to joining Booz Allen Mr Mirza was a graduate student at MIT where he was a System Design and Management Fellow in the Engineering Systems Division He holds a joint SM degree in Engineering and Management from MIT/Sloan and a BEng with Honors in Mechanical Engineering from The University of Edinburgh Scotland 13 


Time 50 350   10 1                   10 0                   10 1  14         300 0.005  s D 2 k,8 Velocity   100 150 200 250 10 2                    0.5 50  Figure 13 \226 Position consistency, filter tuning example 5  C ONCLUSION  Calculating and observing the behavior of covariance consistency at different levels  in the radar signal processing chain represents a very powerfu l tool that can be used to assess the accuracy and softwa re implementation of radar signal-processing algorithms.  Analyzing covariance consistency is applicable to radar systems both in the field and in simulations.  The primary challenge in both arenas comes down to properly accounting for the true target states that contribute to detections, detection primitives measurements, and state estimates For a fielded radar syst em, achieving covariance consistency is usually a s econdary consideration behind achieving and maintaining track s.  Indeed, until recently radar specifications did not even include requirements for covariance consistency.  Recent covariance consistency requirements stem from the fact that the use of radar systems in sensor netting applications is on the rise Currently the combined e ffects of off-beam-center measurements, atmospheric correction, bias correction clustering and centrioding, data association, and filtering on state covariance consistency throughout a target\222s trajectory are not well known.  This is particularly true for radars using wideband waveforms and multiple hypotheses or multiple frame trackers.  Numerical results presented here indicate that algorithms early in the radar signal processing chain can significantly degrad e covariance consistency and that some errors are better tolerated than others For a simulated target in a modeled system, truth relative to some global reference is known. However, transforming truth through different refere nce frames and accounting for changes that occur during various radar processing algorithms is not as simple as it appears.  The techniques in this paper help expose this hidden complexity and provide a framework for discussing and expanding the future development of covariance consistency techniques.  Such future developments include issues related to mapping truth through the convolution operation typically used to simulate wideband signal processing and the fast Fourier transforms typically used in pulse-Doppler processing.  Sophisticated tracking algorithms that carry multiple hypotheses, associate across multiple frames, or weight the association of multiple targets within a single frame pose significant challenges in properly associating truth with state estimates.  Additional work, including an investigation of track-to-truth assignment, is needed before covariance consistency techniques can be applied to these algorithms Another area that needs furthe r analysis is the use of a sliding window to approximate the covariance behavior expected during a set of Monte-Carlo trials.  Various timedependent variables such as the target\222s range and orientation, the transmit waveform, the radar\222s antenna patterns toward the target, missed detections, and false alarms could easily viol ate the assumption that measurement conditions are nearly stationary over the time of the window.  It is importa nt to understand the conditions when this assumption is violated Finally, the examples presented here included a relatively benign arrangement of targets.  Further analysis in dense target environments with the related increase in merged detections, merged measurements, and impure tracks is needed.  Further analysis for targets traveling over different trajectories is also needed Even so, the techniques presented here can be extended to many of these analyses R EFERENCES  1  S. Blackman and R. Popoli Design and Analysis of Modern Tracking Systems Artech House, 1999 2  Y. Bar-Shalom and X. R. Li Multitarget-Multisensor Tracking: Principles and  Techniques YBS Publishing, Storrs, CT, 1995 3  Y. Bar-Shalom, Editor Multi-target-Multi-sensor Tracking: Advanced Applications and  Vol. I Artech House, Norwood, MA, 1990 4  D. B. Reid, \223An Algorithm for Tracking Multiple Targets,\224 IEEE Trans. on Automatic Control Vol. 24 pp. 843-854, December 1979 5  T. Kurien, \223Issues in the Design of Practical Multitarget Tracking Algorithms,\224 in Multitarget-Multisensor Tracking Y. Bar-Shalom \(ed.\43-83, Artech House, 1990 6  R.P.S. Mahler, Statistical Multisource-Multitarget Information Fusion, Artech House, 2007 


 15 7  B.-N. Vo and W.-K. Ma, \223The Gaussian Mixture Probability Hypothesis Density Filter,\224 IEEE Trans Signal Processing Vol. 54, pp. 4091-4104, November 2006 8  B. Ristic, S. Arulampalam, and N. Gordon Beyond the Kalman Filter Artech House, 2004 9  Y. Bar-Shalom, X. Rong Li, and T. Kirubarajan Estimation with Applications to Tracking and Navigation, New York: John Wiley & Sons, pg. 166 2001 10  X. R. Li, Z. Zhao, and V. P. Jilkov, \223Estimator\222s Credibility and Its Measures,\224 Proc. IFAC 15th World Congress Barcelona, Spain, July 2002 11  M. Mallick and S. Arulampalam, \223Comparison of Nonlinear Filtering Algorithms in Ground Moving Target Indicator \(GMTI Proc Signal and Data Processing of Small Targets San Diego, CA, August 4-7, 2003 12  M. Skolnik, Radar Handbook, New York: McGrawHill, 1990 13  A. Gelb, Editor Applied Optimal Estimation The MIT Press, 1974 14  B. D. O. Anderson and J. B. Moore Optimal Filtering  Prentice Hall, 1979 15  A. B. Poore, \223Multidimensional assignment formulation of data ass ociation problems arising from multitarget and multisensor tracking,\224 Computational Optimization and Applications Vol. 3, pp. 27\22657 1994 16  A. B. Poore and R. Robertson, \223A New multidimensional data association algorithm for multisensor-multitarget tracking,\224 Proc. SPIE, Signal and Data Processing of Small Targets Vol. 2561,  p 448-459, Oliver E. Drummond; Ed., Sep. 1995 17  K. R. Pattipati, T. Kirubarajan, and R. L. Popp, \223Survey of assignment techniques for multitarget tracking,\224 Proc  on Workshop on Estimation  Tracking, and Fusion: A Tribute to Yaakov Bar-Shalom Monterey CA, May 17, 2001 18  P. Burns, W.D. Blair, \223Multiple Hypothesis Tracker in the BMD Benchmark Simulation,\224 Proceedings of the 2004 Multitarget Tracking ONR Workshop, June 2004 19  H. Hotelling, \223The generalization of Student's ratio,\224 Ann. Math. Statist., Vol. 2, pp 360\226378, 1931 20  Blair, W. D., and Brandt-Pearce, M., \223Monopulse DOA Estimation for Two Unresolved Rayleigh Targets,\224 IEEE Transactions Aerospace Electronic Systems  Vol. AES-37, No. 2, April 2001, pp. 452-469 21  H. A. P.  Blom, and Y. Bar-Shalom, The Interacting Multiple Model algorithm for systems with Markovian switching coefficients IEEE Transactions on Au tomatic Control 33\(8  780-783, August, 1988 22  M. Kendall, A. Stuart, and J. K. Ord, The Advanced Theory of Statistics, Vol. 3, 4th Edition, New York Macmillan Publishing, pg. 290, 1983 23  T.M. Cover and P.E. Hart, Nearest Neighbor Pattern Classification, IEEE Trans. on Inf. Theory, Volume IT-13\(1 24  C.D. Papanicolopoulos, W.D. Blair, D.L. Sherman, M Brandt-Pearce, Use of a Rician Distribution for Modeling Aspect-Dependent RCS Amplitude and Scintillation Proc. IEEE Radar Conf 2007 25  W.D. Blair and M. Brandt-Pearce, Detection of multiple unresolved Rayleigh targets using quadrature monopulse measurements, Proc. 28th IEEE SSST March 1996, pp. 285-289 26  W.D. Blair and M. Brandt-Pearce, Monopulse Processing For Tracking Unresolved Targets NSWCDD/TR-97/167, Sept., 1997 27  W.D. Blair and M. Brandt-Pearce, Statistical Description of Monopulse Parameters for Tracking Rayleigh Targets  IEEE AES Transactions, Vol. 34 Issue 2,  April 1998, pp. 597-611 28  Jonker and Volgenant, A Shortest Augmenting Path Algorithm for Dense and Sparse Linear Assignment Problems, Computing, Vol. 38, 1987, pp. 325-340 29  V. Jain, L.M. Ehrman, and W.D. Blair, Estimating the DOA mean and variance of o ff-boresight targets using monopulse radar, IEEE Thirty-Eighth SSST Proceedings, 5-7 March 2006, pp. 85-88 30  Y. Bar-Shalom, T. Kirubarajan, and C. Gokberk 223Tracking with Classification-Aided Multiframe Data Association,\224 IEEE Trans. on Aerospace and Electronics Systems Vol. 41, pp. 868-878, July, 2005   


 16 B IOGRAPHY  Andy Register earned BS, MS, and Ph  D. degrees in Electrical Engineering from the Georgia Institute of Technology.  His doctoral research emphasized the simulation and realtime control of nonminimum phase mechanical systems.  Dr. Register has approximately 20 years of experience in R&D with his current employer, Georgia Tech, and product development at two early-phase startups. Dr. Register\222s work has been published in journals and conf erence proceedings relative to mechanical vibration, robotics, computer architecture programming techniques, and radar tracking.  More recently Dr. Register has b een developing advanced radar tracking algorithms and a software architecture for the MATLAB target-tracking benchmark.  This work led to the 2007 publication of his first book, \223A Guide to MATLAB Object Oriented Programming.\224  Mahendra Mallick is a Principal Research Scientist at the Georgia Tech Research Institute \(GTRI\. He has over 27 years of professional experience with employments at GTRI \(2008present\, Science Applications International Corporation \(SAIC Chief Scientist \(2007-2008\, Toyon Research Corporation, Chief Scientist 2005-2007\, Lockheed Martin ORINCON, Chief Scientist 2003-2005\, ALPHATECH Inc., Senior Research Scientist 1996-2002\, TASC, Principal MTS \(1985-96\, and Computer Sciences Corporation, MTS \(1981-85 Currently, he is working on multi-sensor and multi-target tracking and classification bas ed on multiple-hypothesis tracking, track-to-track association and fusion, distributed filtering and tracking, advanced nonlinear filtering algorithms, and track-before-detect \(TBD\ algorithms He received a Ph.D. degree in  Quantum Solid State Theory from the State University of New York at Albany in 1981 His graduate research was also based on Quantum Chemistry and Quantum Biophysics of large biological molecules. In 1987, he received an MS degree in Computer Science from the John Hopkins University He is a senior member of the IEEE and Associate Editor-inchief  of the Journal of Advances in Information Fusion of the International Society of Information Fusion \(ISIF\. He has organized and chaired special and regular sessions on target tracking and classific ation at the 2002, 2003, 2004 2006, 2007, and 2008 ISIF conferences. He was the chair of the International Program Committee and an invited speaker at the International Colloquium on Information Fusion \(ICIF '2007\, Xi\222an, China. He is a reviewer for the IEEE Transactions on Aerospa ce and Electronics Systems IEEE Transactions on Signal Pr ocessing, International Society of Information Fusion, IEEE Conference on Decision and Control, IEEE Radar Conference, IEEE Transactions on Systems, Man and Cybernetics, American Control Conference, European Signal Processing Journal and International Colloquium on Information Fusion ICIF '2007   William Dale Blair is a Principal Research Engineer at the Georgia Tech Research Institute in Atlanta, GA. He received the BS and MS degrees in electrical engineering from Tennessee Technological University in 1985 and 1987, and the Ph.D. degree in electrical engineering from the University of Virginia in 1998. From 1987 to 1990, he was with the Naval System Division of FMC Corporation in Dahlgren, Virginia. From 1990 to 1997, Dr Blair was with the Naval Surface Warfare Center, Dahlgren Division NSWCDD\ in Dahlgren, Virg inia. At NSWCDD, Dr Blair directed a real-time experiment that demonstrated that modern tracking algorithms can be used to improve the efficiency of phased array radars. Dr Blair is internationally recognized for conceptualizing and developing benchmarks for co mparison and evaluation of target tracking algorithms Dr Blair developed NSWC Tracking Benchmarks I and II and originated ONR/NSWC Tracking Benchmarks III and IV NSWC Tracking Benchmark II has been used in the United Kingdom France, Italy, and throughout the United States, and the results of the benchmark have been presented in numerous conference and journal articles. He joined the Georgia Institute of Technology as a Se nior Research Engineer in 1997 and was promoted to Principal Research Engineer in 2000. Dr Blair is co-editor of the Multitarg et-Multisensor Tracking: Applications and Advances III. He has coauthored 22 refereed journal articles, 16 refereed conference papers, 67 papers and reports, and two book chapters. Dr Blair's research interest include radar signal processing and control, resource allocation for multifunction radars, multisen sor resource allocation tracking maneuvering targets and multisensor integration and data fusion. His research at the University of Virginia involved monopulse tracking of unresolved targets. Dr Blair is the developer and coordinator of the short course Target Tracking in Sensor Systems for the Distance Learning and Professional Education Departmen t at the Georgia Institute of Technology. Recognition of Dr Blair as a technical expert has lead to his election to Fellow of the IEEE, his selection as the 2001 IEEE Y oung Radar Engineer of the Year, appointments of Editor for Radar Systems, Editor-InChief of the IEEE Transactions on Aerospace and Electronic Systems \(AES\, and Editor-in- Chief of the Journal for Advances in Information Fusion, and election to the Board of Governors of the IEEE AES Society,19982003, 2005-2007, and Board of Directors of the International Society of Information Fusion   


 17 Chris Burton received an Associate degree in electronic systems technology from the Community College of the Air force in 1984 and a BS in Electrical Engineering Technology from Northeastern University in 1983.  Prior to coming to the Georgia Institute of Technology \(GTRI\ in 2003, Chris was a BMEWS Radar hardware manager for the US Air Force and at MITRE and Xontech he was responsible for radar performance analysis of PAVE PAWS, BMEWS and PARCS UHF radar systems Chris is an accomplished radar-systems analyst familiar with all hardware and software aspects of missile-tracking radar systems with special expertise related to radar cueing/acquisition/tracking for ballistic missile defense ionospheric effects on UHF radar calibration and track accuracy, radar-to-radar handover, and the effects of enhanced PRF on radar tracking accuracy.  At GTRI, Chris is responsible for detailed analysis of ground-test and flight-test data and can be credited with improving radar calibration, energy management, track management, and atmospheric-effects compensation of Ballistic Missile Defense System radars   Paul D. Burns received his Bachelor of Science and Masters of Science in Electrical Engineering at Auburn University in 1992 and 1995 respectively. His Master\222s thesis research explored the utilization of cyclostationary statistics for performing phased array blind adaptive beamforming From 1995 to 2000 he was employed at Dynetics, Inc where he performed research and analysis in a wide variety of military radar applications, from air-to-air and air-toground pulse Doppler radar to large-scale, high power aperture ground based phased array radar, including in electronic attack and protection measures. Subsequently, he spent 3 years at MagnaCom, Inc, where he engaged in ballistic missile defense system simulation development and system-level studies for the Ground-based Midcourse defense \(GMD\ system. He joined GTRI in 2003, where he has performed target tracking algorithm research for BMD radar and supplied expertise in radar signal and data processing for the Missile Defense Agency and the Navy Integrated Warfare Systems 2.0 office.  Mr. Burns has written a number of papers in spatio-temporal signal processing, sensor registration and target tracking, and is currently pursuing a Ph.D. at the Georgia Institute of Technology  


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


