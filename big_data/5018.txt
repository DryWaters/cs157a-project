Mining Multidimensional Fuzzy Association Rules of Alarms in Communication Networks  Wu Jian School of Communication and Information Engineering University of Electronic Science and Technology of China Chengdu, 611731, China okmrj@hotmail.com  Li Xing-ming School of Communication and Information Engineering University of Electronic Science and Technology of China Chengdu, 611731, China xingmingl@uestc.edu.cn    Abstract 227In the modern communication network, a fault will bring more than one alarm, an alarm may be caused by different faults vice versa. The relationship between faults and alarms is not accurate but fuzzy, which can not be described and 
understood using traditional Boolean logic. Also, crisp association rules use sharp partitioning to transfer numerical attributes to binary ones, and can potentially introduce loss of information due to these sharp ranges. As fuzzy sets provide a smooth transition between member and non-member of a set fuzzy association rules use fuzzy logic to convert numerical attributes to linguistic terms. We first analyze the meanings of each field of network alarms. Then define a fuzzy judge language set as domain set to describe the relationship between the field and the root alarm. After that we integrate fuzzy membership function values and weights of every field of alarm to change alarm database to fuzzy alarm database. At last, we come up with 
a new fuzzy association rules mining algorithm, which generalizes the popular frequent itemsets based algorithm. The advantages and efficiency of the new algorithm are shown by experiments on a communication network database with alarm transaction records Keywords- Communication Networks; Fuzzy Set; Fuzzy Membership Function; Fuzzy Association Rule I   I NTRODUCTION  Data mining is the science of extracting implicit, previously unknown and potentially useful information from the content of large databases. Mining association rules in networks can find out effective alarm rules which reflect the faults in networks rapidly. It is also validated that the system which 
provides the user the compact alarm rules with complete information is valuable and useful for fault diagnosis localization as well as recovery in networks. To the best of our knowledge, this is the first attempt to utilize fuzziness in alarm correlation analysis mining. Applying fuzzy association rules to realize automatic and intelligent of network management can accurately diagnose and locate root alarms, as a result shorten the system recovery time, therefore has a significant meaning The rest of this paper is organized as follows. Section covers the background and the related work of our research Section presents the proposed multidimensional fuzzy weighted association rule mining \(MFWARM\ethod Experimental and comparison results are reported and 
discussed in Section The conclusion and future work are described in Section  II  B ACKGROUND AND R ELATED W ORK  A  Association Rules Mining association rules is one of the most intensively studied models for data mining. The basic task in mining for association rules is to determine the correlation between items belonging to a transactional database. In general, every association rule must satisfy two user specified constraints support and confidence. So, the target is to find all association rules that satisfy user specified minimum support and confidence values. We also call them strong rules to distinguish 
from the weak ones B  Degree of Membership Given an attribute x and let 12     k xxx x F ff f 
 be a set of k fuzzy sets associated with x. Each fuzzy set j x f in x F  has a corresponding membership function, denoted x j f 265  which represents a mapping from the domain of x into the interval 0,1 
Formally 0,1 x j fx D 
265 002  For a given value v of x, if  x j f v 265 1 then v totally and certainly belongs to fuzzy set j x f On the other hand x j f 
265  means that v is not a member of fuzzy set j x f All other values between 0 and 1, exclusive, specify a \223partial membership\224 
degree for v C  Fuzzy Association Rule Definition After mapping quantitative alarm databases into fuzzy ones the fuzzy support of the itemset X is defined as  1 1 1   j l j m N f ij i jlF F Support X v N 
265  003 212 
004  1  2326 978-1-4244-9763-8/11/$26.00 \2512011 IEEE 


006\006 006 006  is corresponding fuzzy node sets 2  The links where transmit alarms in the network The position of alarm links in network topology is one of the most important factor, we also use the routing number get through Links to illustrate the state of links in the network When a link fails, data transmitted by the link will be affected the more the routing number is, the greater the impact on the network will be Let 12     l L LL L 005 where N is the number of all records, m is the length of X and  jl f ij v  utilizing the membership function in its domain to get fuzzy set like 2327 007\007 007 265\265  003 003 212  and 2 12     m Yyy y   the fuzzy support and the confidence for the fuzzy association rule X=>Y are defined as  12 1 1 1 1     jj l l jj mm N fij fij i jlF jlF F Support X Y v v N 212  be a set of nodes, and the associated routes with them are 12     Nn R rr r  where i r  is the routing number of node i N As for fuzzy linguistic terms 12     Nk F ff f  each fuzzy set based on number of routes go through every node is  1  N k n kfjj j f rr 265    4  On the other hand, the fuzzification of each node represent as 1    1 2 N j k ifij j N rfi n 265  006   5  and 12     n N NN N  be a set of links and the related route set is 12     Ll R rr r  For the fuzzy region defined before, the fuzzy sets depend on routing number should be  1  L k l kfjj j f rr 265    6  Also we can get fuzzy link sets as  1    1 2 L j k ifij j L rfi l 265  006   7  3  The level of alarms In general, there are four levels of alarms: critical, major minor and warning, which we use number 4, 3, 2, 1 respectively to show the relative significance among different values of them Intuitively, alarm with higher level is closer to the root alarm, i.e. more likely to be the root alarm. If level set is 12     p L vLvLv Lv  the membership function of it for a given fuzzy set is 1  Lv k p kfjj j f lv lv 265    8  With the same process, fuzzy alarm level sets are 1    1 2 Lv j k ifij j L vlvfip 265  006   9  4  The probability density of alarms in a time window Since our alarm transactions are extracted by time windows with fixed length of 5 seconds, alarm probability densities namely numbers of the same alarm occurs in unit time are used to illustrate the relationship between root alarm and them Suppose alarm i happens c times within t seconds, the density of it will be i ct 007  times per second\. As for density set 12     q P  212 3  III  T HE P ROPOSED M ULTIDIMENSIONAL F UZZY W EIGHTED A SSOCIATION R ULES M INING A PPROACH  This section includes two parts. The first part is to fuzzify each numerical attribute for alarms into several linguistic terms specified by their membership functions. Then convert the initial data set to fuzzy database by combining membership degrees and weights of all the attributes. The second part provides a fuzzy association rule mining algorithm which can be broken into two steps find the set of all frequent itemsets with fuzzy support over a threshold generate all fuzzy association rules with fuzzy confidence over the minimum confidence A  Attributes of Alarms As our research is based on the communication system there are specific evaluation attributes selected to describe the alarm in communication network. Here we choose 1  The nodes which occur alarms in the network Obviously, the alarm sent by node which has more communication routes need to get more attention. By using routing number of nodes to describe the status of them in communication network, we consider both dynamic and distribution of the network. When network topology changing the relationship between alarm node and root node will change accordingly Let 12     n N NN N 004\004 2  and    F Support X Y F Confidence X Y F Support X 212 265 means the \223 th l 224 fuzzy set membership degree for the \223 th j 224 item of X in the \223 th i 224 transaction Given two itemsets 1 12     m X xx x 


212\212  11 1 1  1  1  1 jj jj jj jj s ajksaj asajk asaj sajksaj  which means extremely near, near, medium near, not near and far away root alarm from left to right, respectively. Therefore, all the k values aforementioned equal to 5 C  Integrate Fuzzy Membership Degrees and Weights of Attributes to Set up Fuzzy Alarm Database After calculating proper membership functions of all attributes, an alarm with multidimensional attributes is represented as \223Composite Item\224 in fuzzy database. However it increases the complexity from  On to  Ok n compare with crisp database In modern networks, some of the alarms' attributes that have different types and QoS requests result in different treatment of alarms. Therefore, the weight of alarms may affect the accuracy and validity of the results directly. Since it is important to know both the degree of relative significance among different values of one attribute and the degree of relative significance among all evaluation attributes, we propose a way to integrate grades of membership as well as weights of attributes together and build a single dimension fuzzy set of each alarm to denote its proximity degree with the root alarm. The suggested method enables to construct objective and effective fuzzy membership sets of alarms which are more flexible, natural and understandable Here we format fuzzy relationship matrix of an alarm with evaluation attributes as follows 12345 12345 12345 12345 nnnn n lllll R lv lv lv lv lv  265\007\007    10  Accordingly, the fuzzy alarm density sets express as 1    1 2 P j k ifij j f iq 1  P k q kfjj j f 212 and min f conf 212  212 b\t\n\t  t  t b=\n  12  The only parameters need to be determined are the k centers 12     k aa a At the initial time, we set k centers distribute evenly on the range of S. Later, the centers will be adjusted like 1  Randomly take a value s from S and search for an integer m that satisfy  1 nn nn mj s aMinsa jk 212\b min sup f 007\007 007\007 007 007\265\007  006   11  B  Fuzzifying Numerical Attribute into Linguistic Terms One way of determining membership functions of these linguistic terms is by expert opinion or by people's perception However it depends on the knowledge and experience of field experts, which have the subjective and man-made elements are not always precise and complete, and usually change a lot Fuzzy clustering based on self-organized learning can also be used to generate membership functions. Following is an algorithm for generating certain type of membership functions Let S be the considered data set of attribute x. We intend to cluster S into k linguistic terms j x f j=1,2,\205,k. In this paper we assume the type of membership to be triangular. Each linguistic term j x f will have the triangular membership functions as follows  11 11 0      1 x j jjj f jjj asaa s s aaa 265  212\212 013   212 and keep other centers unchanged, where n is the iteration time and 013 is the learning rate. Usually, we set   the iteration ends when   j sS dSA Mins a 003 212  14  converges For simplicity, all attributes\222 membership functions subject to the same linguistic terms of fuzzy sets, i.e      Attribute F JEHMNF   15  For the main information fields of alarms, we consider to determine their weights  nllv 007 f 006 r where r is the fuzzy relationship operator and we apply Zedah operator in this paper D  Fuzzy Association Rule  Mining Algorithm In this section, we proposed an algorithm for mining fuzzy association rules called MFWARM, which has the following inputs and outputs Inputs: A database D of the transactions T, each has alarm itemset with m attribute values and corresponding weights f  two threshold values min sup f 212 a set of membership functions F Output: A list of fuzzy association rules 1  MFWARM\(D D 006  212  2  D f F 3  1 C Set of 1 itemsets in fuzzy database D 006  4  L  1 L  1 XX C 003 and  F Support X 013 to 0.5 Let 12     k Aaa a 212  min f conf 006 Mapping\(D f\f\f\f\f  by the experience and knowledge of network experts. With these weights, we realize a comprehensive evaluation of all information to obtain the overall distribution of fuzzy relationship between fuzzy alarm and root alarm as AR 212 212 n\n  13  2  Put 1  nn nn mm m aa sa 212\212 212  2328 f F min sup f 


020  11  End 12  lL 212 t\017 k++\begin 7  k C generate candidates 1 k L 212  8  Prune transaction t in D 5  Delete item i from D 212  10  k L LL 212\b min f conf 006 if 1 iL 016  6  For\(k=2 1 k L 006 if tk   9  k L  k YY C 003 and  F Support Y 006   13  R  rr R 006 003 and  F Confidence r 212  IV  PERFORMANCE STUDY  All the experiments were performed on a 1.66GHz Core 2 PC with 1GB of main memory, running under Windows XP professional. The algorithm is coded in Java on Eclipse 3.1 development platform A  Generation of Synthetic Alarm Database In our research, we use synthetic database which has specific topology of communication network to evaluate the performance of different algorithms. After generating alarms attributes that reflect the faults were picked out to form an alarm item in transactions and redundant alarms were got rid of by alarm compressing. Here also comes to a problem of alarm synchronization which settled by setting time window and slip length. The alarm pretreatment is carried out to transform the alarm database into fuzzy alarm database and get data ready for fuzzy association rules mining For each alarm, it has two aspects: one is fixed information such as alarm node \(source\, alarm link \(address\ alarm level etc. and the other is fuzzy information i.e. the distribution for linguistic terms of fuzzy sets      J EHM NF  Table 1 shows example alarms in fuzzy database as combination items B  Experimental Results In order to control different parameters in the experimental setup, we use membership functions and weights formed above in the experiment. We study the effect of different values of min sup f f  The third one uses the same fuzzy alarm database with our method but traditional level wise mining way. The last one finds Boolean association rules in original alarm database with usual minimum support and minimum confidence First of all, Fig.1 compares the number of frequent itemsets for different min sup f 212 The number of rules represented by each of the four curves decreases as the value of the minimum confidence increases. This is quite consistent with our intuition. In addition it can be easily noted that the curve labeled LBCARM is smoother than all the other curves, i.e., the min f conf 212 values have large effect on the number of rules mined from databases with fuzzy sets 2329 021\003 generate set of candidate rules 12     n R rr r 212 number of transactions and items, on the processing time for proposed approach to analyze its scalability TABLE I  EXAMPLE OF ALARMS IN FUZZY DATABASE  Name source Address Level Count Fuzzy membership A 2 202.11.11.1 Major 1 0.5,0.6,0.5,0.4,0.1 B 15 202.11.25.2 Critical 3 0.9,0.7,0.4,0.3,0.1 C 3 202.11.14.1 Major 1 0.6,0.7,0.8,0.3,0.2 D 11 202.11.13.1 Warning 1 0.2,0.3,0.3,0.4,0.6 E 10 202.11.19.2 Minor 2 0.3,0.4,0.5,0.8,0.3 A performance study is carried out for the four algorithms general fuzzy association rules mining algorithms\227 GFARM weighted fuzzy association rules mining algorithm\227MFWARM level-based fuzzy association rules mining algorithm\227LBFARM, and level-based crisp association rules mining algorithm\227LBCARM The first one treats every attribute of alarms equally and defines the membership degree of alarms as the lowest values among all attributes in each fuzzy set. The second one is suggested algorithm with expert predefined attribute weights 0.6 0.4 0.8 0.5 212 As the value of minimum support increases, the number of frequent itemsets decreases. On the other hand, the curves that correspond to standard ARM extract higher number of itemsets than those of fuzzy ARM. This is because the frequent itemsets generated more accurately reflect the true patterns in the data set than the numerous artificial patterns using crisp boundaries in standard ARM Then, Fig.2 displays the decreasing trend of the execution time when the support threshold increases. The execution time decreases because of less searching time. We experienced MFWARM is about 13.3% to 18.4% and 17.5% to 32.7 faster than LBFARM and LBCARM, respectively. Besides GFARM is more sensitive to the change of min sup f 212 than MFWARM since unweighted items affected more easily In the following, we generate transactions ranging from 1K to 10K to examine how methods scale up and compare the performance of them. Fig.3 indicates that not only the whole execution time of MFWARM is better than LBFARM, but also with increase of transactions the increasing scope of execution time of MFWARM is less than that of LBFARM, which implies our algorithm is more scalable than others After that, the next experiment deals with relationship between the number of frequent itemsets involving some given alarms and their weights are plotted in Fig.4. Here we let the respective weight of the three items \(alarm1, alarm2, alarm3 vary, while weights of other alarms hold constantly. Figure 4 shows the number of frequent itemsets grows with the increase of the average attribute weights, which is as we expected. This also means users' subjective preference can be added into the procedure of data mining by adjusting weights At last, the curves plotted in Figure 5 demonstrate the change in the number of association rules for different values of min f conf 212\b min sup f 


      Figure 1  Number of frequent itemsets vs min sup f 212       Figure 2  Execution time with min sup f 212 scale-up     Figure 3  Execution time with transactions scale-up       Figure 4  Number of frequent itemsets vs weights      Figure 5  Number of association rules vs min f conf 212  V  S UMMARY AND C ONCLUSIONS  The contributions in this paper are 225 The concept of \223Composite Item\224 namely multidimensional attributes of alarms in communication network 225 Reduce the size of alarm databases using novel pruning strategy which enables us to generate the interesting frequent itemsets with fewer passes and higher speed 225 Design proper fuzzy membership function of each attribute by triangular functions to cluster numerical values into k linguistic terms 225 Given unique fuzzy set for every alarm by integrating fuzzy membership function values and weights of all attributes Experimental results have demonstrated our MFWARM outperforms the classical level-based algorithm. We also found that different rules could be discovered according to weighted or unweighted inputs by users. In the result, the number of rules having alarms with heavy weights is much more than that of rules having light ones. Obviously, we use the algorithm to obtain the interesting rules from the user\222s point of view Besides, how to build a knowledge representation model by combining fuzzy theory and fuzzy inference technology to get the root alarms in communication network can be further considered R EFERENCES  1  Verlinde, H., De Cock, M., Boute, R., \223Fuzzy Versus Quantitative Association Rules: A Fair Data-Driven Comparison,\224 IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics, vol. 36, pp 679-683, 2006 2  Mangalampalli, A., Pudi, V., \223Fuzzy Logic-based Preprocessing for Fuzzy Association Rule Mining\224, Technical Report IIIT/TR/2008/127 International Institute of In formation Technology, 2008 3  Chen G., Yan P., Kerre E.E., \223Computationally Efficient Mining for Fuzzy Implication-Based Association Rules in Quantitative Databases\224 International Journal of General Systems, vol. 33, pp.163-182, 2004 4  C.C.Chang, Y.C.Li, and J.S.Lee, \223An Efficient Algorithm for Incremental Mining of Association Rules\224, In Proc. 2005 RIDE-SDMA IEEE International Workshop on Rese arch Issues in Data Engineering Stream Data Mining and Application, 2005 5  Rolly Intan, \223A Proposal of Fuzzy Multidimensional Association Rules\224, Jurnal Informatika Vol. 7 No. 2 \(Terakreditasi SK DIKTI No 56/DIKTI/Kep/2005\, November 2006 6  M. Kaya and R. Alhajj Extending OLAP with Fuzziness for Effective Mining of Fuzzy Multidimensional Weighted Association Rules  Proc. of ADMA, Lecture Notes in Computer Science, Springer-Verlag 2006 7  Dubois, D., H\374llermeier, E., Prade, H., \223A Systematic Approach to the Assessment of Fuzzy Association Rules\224, DM and Knowledge Discovery Journal vol. 13\(2\, pp.167\226192, 2006 8  Muyeba, M., Sulaiman, M., Malik, Z., Tjortjis, C., \223Towards Healthy Association Rule Mining \(HARM\, A Fuzzy Quantitative Approach\224 In: Corchado, E., Yin, H., Botti, V., Fyfe, C. \(eds.\ IDEAL 2006. LNCS vol. 4224, pp. 1014\2261022. Springer, Heidelberg 2006 9  Dubois, D., Prade, H., Sudkamp, T., \223On the representation measurement, and discovery of fuzzy associations\224,  IEEE Transactions on Fuzzy Systems vol. 13\(2\, pp. 250\226262, 2005   H\374llermeier, E., \223Fuzzy sets in machine learning and data mining: Status and prospects\224, Fuzzy Sets and Systems vol. 156\(3\, pp.387\226406, 2005   Y. C. Lee, T. P. Hong and W. Y. Lin, "Mining fuzzy association rules with multiple minimum supports using maximum constraints", The Eighth International Conference on Knowledge-Based Intelligent Information and Engineering Systems, 2004, Lecture Notes in Computer Science, Vol. 3214, pp. 1283-1290, 2004 2330 


 978-1-61284-212-7/11/$26.00 \2512011 IEEE  0 10 20 30 40 50 60 70 80 90 20 40 60 80 100 K-LENGTH K-LENGTH generated MAV APRIORI FILTER_AP  Figure 6  Comparison between MAV, AP and FP in generating the maximum k_length The other explanation that could be derived from our experiment is that MAV algorithm able to produce an equal length with comparison algorithm for all 17 dataset. Our performance are consistently derived the comparative result with AP and FP except on minimum support 20%. This can be explain on dataset ZOO,LYM,HDE and HPT we are not generating any values due to our computation time > 30 minute as in figure 5 and 6  V  CONCLUSION  Frequent itemset mining is one of the most important areas of data mining specifically in association. Existing implementation of Apriori based algorithms focus on binary dataset. In this paper, we revi m pl e m en t a t i o n  f o r n onbinary dataset. We consider the item values in generating the frequency based on attribute. Each attribute in the dataset may consist of multiple values. The computation is based on Apriori concept.  We also introduce the multiple attribute value function in generating frequent items.  Our experiments show that the improved Apriori offers a significant improvement in terms of identifying frequent items for more meaningful result over existing solutions Generally MAV algorithm for the frequent mining process is able to stand a line with comparison algorithm at certain minimum support such as at 80% with 0.04 t-tests While in producing maximum k-length we are able to get 0.02 and 0.05 significance changes. Table below show the ttest results  Table VI. Statistical Test algorithm Result Statistic Min_supp Mean T 2-t Item 40 35714 1.794 .096  80% .57143 2.280 055 007  80% .57143 2.280 040 007  K-Length 40 50000 2.463 029  007   We will further enhance the algorithm to reduce the complexity cost as we believe our computation is higher since we evaluate each value independently. We will view the optimization perspective in the algorithm for improving the complexity costs  R EFERENCES   1   J. Han, and M. Kamber Data Mining: Concepts and Techniques  Morgan Kaufmann, 2001 2   I.H.W.E. Frank Data Mining Practical Machine Learning Tools and Techniques Morgan Kaufmann Publishers, 2005 3   A. Ceglar, and J.F. Roddick, \223Association mining,\224 ACM Computing Surveys \(CSUR 2006 4   J. Han, H. Cheng, D. Xin, and X. Yan, \223Frequent pattern mining current status and future directions,\224 Data Mining and Knowledge Discovery 2007, pp. 55-86 5   R. Agrawal, T. Imieli ski, and A. Swami, \223Mining association rules between sets of items in large databases,\224 ACM SIGMOD Record 1993, pp. 207-216 6   S. Brin, R. Motwani, J.D. Ullman, and S. Tsur, \223Dynamic itemset counting and implication rules for market basket data,\224 ACM, 1997, pp. 264 7   Y. Bastide, R. Taouil, N. Pasquier, G. Stumme, and L. Lakhal 223Mining frequent patterns with counting inference,\224 ACM SIGKDD Explorations Newsletter 2000, pp. 66-75 8   8 Q. Zhao, and S.S. Bhowmick, \223Association rule mining: A survey,\224 Nanyang Technological University Singapore 2006 9   H. Liu, Y. Lin, and J. Han, \223Methods for mining frequent items in data streams: an overview,\224 Knowledge and Information Systems 2009, pp. 1-30    G. Cormode, and M. Hadjieleftheriou, \223Methods for finding frequent items in data streams,\224 The VLDB Journal 2010, pp. 320    R. Agrawal, and R. Srikant, \223Fast algorithms for mining association rules,\224 1994, pp. 487499    J. Han, and J. Pei, \223Mining frequent patterns by pattern-growth methodology and implications,\224 ACM SIGKDD Explorations Newsletter 2000, pp. 14-20    S. Chai, J. Yang, and Y. Cheng, \223The Research of Improved Apriori Algorithm for Mining Association Rules,\224 2007, pp. 1-4    D. Sun, S. Teng, W. Zhang, and H. Zhu, \223An Algorithm to Improve the Effectiveness of Apriori,\224 2007, pp. 385-390    Y. Xie, Y. Li, C. Wang, and M. Lu, \223The Optimization and Improvement of the Apriori Algorithm,\224 IEEE Computer Society, 2008, pp. 1101-1103    W. Yu, X. Wang, F. Wang, E. Wang, and B. Chen, \223The research of improved apriori algorithm for mining association rules,\224 2008, pp. 513-516    T. Hu, X. Wang, Q. Fu, and S.Y. Sung, \223Mining Maximum Length Frequent Itemsets: A Summary of Results,\224 2006, pp 505-512    T. Hu, S.Y. Sung, H. Xiong, and Q. Fu, \223Discovery of maximum length frequent itemsets,\224 Information Sciences 2008, pp. 69-87 


 978-1-61284-212-7/11/$26.00 \2512011 IEEE     Y. Ye, and C.C. Chiang, \223A parallel apriori algorithm for frequent itemsets mining,\224 2006, pp. 87-94    H. Hamilton, \223Apriori Implementation,\224 2009 http://www2.cs.uregina.ca/~dbd/cs831/notes/itemsets/itemset_pr og1.html     F. Shaari, \223Outlier Detection Method Based on Non-Reduct Computation using Rough Sets Theory,\224 Faculty of Technology and Information System, UNIVERSITI KEBANGSAAN MALAYSIA, BANGI, 2008    X. Yan, H. Cheng, J. Han, and D. Xin, \223Summarizing itemset patterns: a profile-based approach,\224 In Proceeding of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining 2005, pp. 314-323    H. Cheng, X. Yan, J. Han, and C.W. Hsu, \223Discriminative frequent pattern analysis for effective classification,\224 2007, pp 716-725    J. Besson, C. Rigotti, I. Mitasiunaite, and J.F. Boulicaut 223Parameter Tuning for Differential Mining of String Patterns,\224 IEEE Computer Society Washington, DC, USA, 2008, pp. 7786    M. Boley, \223Intelligent Pattern Mining via Quick Parameter Evaluation \(Extended Abstract\.\224    A.K. Poernomo, and V. Gopalkrishnan, \223Efficient computation of partial-support for mining interesting itemsets,\224 SDM, 2009    A.R.H. Zalizah A.L, Azuraliza Abu Bakar, \223Parameter Setting Procedure via Quick Parameter Evaluation in Frequent Pattern Mining for Outbreak Detection,\224 Proc. Data Mining and Optimization IEEE, 2009     


a,b,e On the other hand, all frequent itemsets are a}, {b}, {c}, {e a,b}, {a,c}, {a,e}, {b,c}, {b,e a,b,c}, {a,b,e  Figure 2. An example of using the necessary condition Using this method, we reduced the cost of searching for frequent itemsets. This method can be used to improve the search strategy implemented by the Apriori algorithm. A time series fs  of a frequent itemset f  is an ordered sequence of time stamps of the covering transactions. This set will be helpful in finding out what kinds of changes are occurring in which time periods. This can give an indication of the behavior of users with respect to time. We can find why some pages are frequently visited and why others not Through that, we can get a better view about our pages, and also about the users visiting our website. We can predict the future behavior of users depending on the periodical behavior of the users that we have already extracted V. EXPERIMENTAL WORK For the experiments we used a dataset that contains the preprocessed and   filtered sessionized data for the main DePaul CTI Web server \(http://www.cs.depaul.edu data is   based on a random sample of users visiting this site for a 2 week period during April of 2002. Each session begins with a line of the form SESSION #n \(USER_ID = k Where n in the session number and k is the USER_ID There may be multiple   consecutive sessions corresponding to the same USER_ID \(repeated users with a dashed line. Within a given session, each line corresponds to one page view access. Each line in a session is a tab delimited sequence of 3 fields: time stamp, page view accessed, and the referrer. The time stamp represents the number of seconds relative to January 1, 2002 In order to illustrate what we made in our experimental work, let us take a sample of three sessions that could be found in the web log file SESSION #1 \(USER_ID = 11 9374553 /news/default.asp /news 9374590 /people/search.asp?sort=pt /news/default.asp 


9374610 /people/facultyinfo.asp people/search.asp?sort=pt 9374685 /news/default.asp /people/facultyinfo.asp 9374720 /courses/ /news/default.asp  SESSION #2 \(USER_ID = 22 9185108 /admissions/ /programs 9185138 /news/default.asp /news/default.asp  SESSION #3 \(USER_ID = 33 9226945 /people/search.asp?sort=pt /people 9226975 /people/facultyinfo.asp people/search.asp?sort=pt 9227072 /advising/ /courses 9227098 /people/search.asp?sort=pt /news/default.asp  After preprocessing which includes the removal of redundant URLs within every session, considering the time stamp of the first request in every session as the time stamp of the session, and the removal of the referrer we get SESSION #1 \(USER_ID = 11 9374553 /people/search.asp?sort=pt people/facultyinfo.asp news/default.asp courses  SESSION #2 \(USER_ID = 22 9185108 /admissions news/default.asp  SESSION #3 \(USER_ID = 33 9226945 /people/search.asp?sort=pt people/facultyinfo.asp advising news/default.asp Then we build the set of items which consists of the set of unique URLs of all sessions Set of items I = {/people/search.asp?sort=pt people/facultyinfo.asp news/default.asp, /courses/, /admissions/, /advising We then used the Apriori algorithm to mine for frequent itemsets. We set the minimum support to be 20% which is 


equivalent to minimum support count equal to 2 Candidate 1-itemset news/default.asp \(support count =3 people/search.asp?sort=pt\( support count =2 people/facultyinfo.asp\(support count =2 Candidate 2-Itemset news/default.asp, /people/search.asp?sort=pt}\(support count = 2 news/default.asp, /people/facultyinfo.asp}\(support count 2 people/search.asp?sort=pt, /people/facultyinfo.asp support count= 2 Candidate 3-Itemset news/default.asp, /people/search.asp?sort=pt people/facultyinfo.asp\(support count= 2 and because the available dataset was not suitable to run our experiments, the main goal of our future work is to find a real life representative dataset that fits our experimental needs or use one of the available dataset generators to generate a suitable one Candidate 3-Itemset represents the frequent itemset f . The set of covering transactions for the frequent itemset f  are in session #1 and session #3 which have the time  stamps 9374553 and 9226945 respectively. The time stamps are then ordered to get the time series corresponding to the frequent itemset f :  REFERENCES 1] Agrawal, R., Imieliski, T., and Swami, A. Mining association rules between sets of items in large databases. In: SIGMOD NY, USA, ACM Press \(1993 207-216 S = {9226945, 9374553 The experimental results we have got were not representative enough to reflect the applicability of our approach. Such bad results were expected because the dataset we used was not the one we need. On one hand because the time interval of the transactions is too small \(2 weeks other hand, the transactions represent user sessions in a university website, and usually the transactions recorded in the web log file of university websites are not more than course registration, search for an assignment, etc.  The dataset we need to run an effective experiment should have 


a long time interval for example a dataset that represents customers transactions in a retail website within two years As far as we know, such dataset does not exist especially when we talk about time stamped datasets. This lack of such datasets amplifies the need to develop time stamped transactional datasets generators which is discussed in [2 2]  Asem Omari, Regina Langer, and Stefan Conrad. TARtool: A Temporal Dataset Generator for Market Basket Analysis. In Proceedings of the 4th International Conference on Advanced Data Mining and Applications \(ADMA 2008 2008. Springer Lecture Notes in Artificial Intelligence \(LNAI August 2008 3] Asem Omari. Data Mining for Improved Website Design and Enhanced Marketing.  In Yukio Ohsawa and Katsutoshi Yada editors, Data Mining for Design and Marketing,  volume 5 of Chapman Hall/CRC Data Mining and Knowledge Discovery Series, chapter  6. Chapman Hall/CRC, First edition, November 2008 4] Asem Omari, Alexander Hinneburg, and Stefan Conrad Temporal Frequent Itemset Mining. In Proceedings of the Knowledge Discovery, Data Mining and Machine  Learning workshop \(KDML 2007 5]  B. Ozden, S. Ramaswamy and A. Silberschatz: Cyclic Association Rules. In: ICDE  98: Proceedings of the  Fourteenth International Conference on Data Engineering,  Washington, DC USA, IEEE Computer Society \(1998 412421 6] C. Antunes and A. Oliveira: Temporal Data Mining: An Overview. In: Proceedings of the Workshop on Temporal Data Mining, of Knowledge Discovery and Data Mining KDD01, San Francisco, USA \(2001 VI. APPLICATION FIELDS This method can be applied in different fields. One application field is in search engine log files for example to find out the most frequently searched keywords in the last time period. Another application field is in web usage mining for example to find out the most visited web pages in the last 3 months in some website. It can also be applied to a transaction dataset in a physical store or business to find out the most frequently bought products or used services in the last time period. Any other problem that needs to study the behavior of some items with respect to time can be a good application field 


7] Ding-An Chiang, Shao-Lun Lee, Chun-Chi Chen, and MingHua Wang. Mining Interval Sequential Patterns. International Journal of Intelligent Systems, 20\(3 359373 8] J. Han and M. Kamber: Data Mining Concepts and Techniques Morgan Kaufmann Publishers, San Francisco \(2001 9] Kaidi Zhao and Bing Liu. Visual Analysis of the Behavior of Discovered Rules. In Workshop Notes in ACM SIGKDD-2001 Workshop on Visual Data Mining, San  Francisco, CA, 2001 10] Mannila, H., Toivonen, H.: Multiple uses of frequent sets and condensed representations. In: KDD, Portland, USA \(1996 pp\(189-194 11] Q. Yang and X. Wu: 10 Challenging Problems in Data Mining Research. Volume 5, World Scientific Publishing Company International Journal of Information Technology and Decision Making \(2006 597604  VII. SUMMARY AND FUTURE WORK In this paper, we presented a new measure to mine for interesting frequent itemsets. This measure is based on the idea that interesting frequent itemsets are mainly covered by many recent transactions.  This measure reduces the cost of searching for frequent itemsets by minimizing the search interval. Furthermore, it can be used to improve the search strategy implemented by the Apriori algorithm 12] Sheikh, L.M. Tanveer, B. Hamdani, M.A. Interesting Measures for Mining Association Rules. In proceedings of the 8th International Multi-topic Conference INMIC, 2004. pp \(641 644 13] W. Lin, M. A. Orgun and G. Williams: An Overview of Temporal Data Mining. In:  Proceedings of the 1st Australian Data Mining Workshop, Canberra, Australia,   University of Technology, Sydney \(2002 8390  


processed, until all attributes in A have been exhausted and we get the final fuzzy version of the dataset E. At the end, all attributes would have categorical values for each record in the fuzzy dataset E. Thus, by applying the aforementioned pre-processing, given any dataset D with initial crisp attributes \(set A fuzzy records. And each of these is further iteratively converted to generate more fuzzy records, until each crisp attribute has been taken into account and we get our final fuzzy dataset E  VI. COUNTING IN FUZZY ASSOCIATION RULES Crisp ARM algorithms calculate support of itemsets in various ways Record-by-record counting; as in Apriori Counting using tidlists; for example, ARMOR Tree-style counting; as in FPGrowth In this section, we describe how counting is done in various fuzzy ARM algorithms using membership functions and how our pre-processing technique can be used to generate fuzzy datasets which can be used by any fuzzy ARM algorithm Table I. t-norms in Fuzzy sets  t-norm TM\(x, y x, y TP\(x, y TW\(x, y x + y ? 1, 0  A. Counting in Fuzzy Apriori The first pass of Apriori counts item occurrences to determine the large 1-itemsets. Any subsequent pass k consists of two phases. First, the large itemsets found in the k-1 the kth pass. Next, the database is scanned and the supports of candidate itemsets are counted. In any pass k, each record is selected in a sequential manner and the supports for the candidate itemsets, occurring in that particular record, are increased by one. Thus, the counting in Apriori is done in a record-by-record manner Fuzzy Apriori is a modified version of the original Apriori algorithm, and can deal with fuzzy records. Fuzzy 


Apriori counts the support of each itemset in a manner similar to the counting in Apriori; the only difference is that it calculates sum of the membership function corresponding to each record where the itemset exists. Thus the support for any itemset is its sum of membership functions over the whole fuzzy dataset. This calculation is done with the help of a suitable t-norm \(see Table I We generated the fuzzy dataset required for Fuzzy Apriori using our pre-processing methodology. The crisp dataset \(FAM95 sections 4 and 5, and the resultant fuzzy dataset was used as input to the Fuzzy Apriori algorithm. More details of how FPrep was used for pre-processing before Fuzzy Apriori can be found in [21] \(though the pre-processing methodology used in [21] is not explicitly names as FPrep  B. Counting in Fuzzy ARMOR Each record in the dataset is marked by a unique number called transaction id \(tid order. A tid-list of an itemset X is an ordered list of TIDs of transactions that contain X. ARMOR is based on the Oracle algorithm and is totally different from Apriori in that it calculates the support of each itemset by creating its tidlist and counting the number of tids in the tidlist. The count of any itemset is equal to the length of its corresponding tidlist The tidlist of an itemset can be obtained as the intersection of the tidlists of its mother and father \(for example, ABC is generated by intersecting AB and BC started off using the tidlists of frequent 1-itemsets In a similar manner, Fuzzy ARMOR also creates the tidlist for each itemset by intersecting the tidlists of its mother and father itemsets. And for each tid in the tidlist, it calculates the membership function  \(again using a suitable t-norm support for an itemset is thus the sum of the membership functions associated with each tid in its tidlist We have also developed an initial implementation of Fuzzy ARMOR [21]. This algorithm uses the same fuzzy dataset as input as that was used for Fuzzy Apriori. There is no change, whatsoever, made to this fuzzy dataset after it was generated initially \(for Fuzzy Apriori processing technique. Even though Fuzzy Apriori and Fuzzy 


ARMOR operate in different ways and process data differently, the fuzzy dataset created using our preprocessing technique can be used as input for both the algorithms. This is because the fuzzy dataset is generated in a standard manner of fuzzy data representation \(as described in section 5 ARM algorithm. More details of how FPrep was used for pre-processing before Fuzzy ARMOR can be found in [21 C. Counting in Fuzzy FPGrowth FPGrowth uses a compact data structure, called frequent pattern tree \(FP-tree structure and stores quantitative information about frequent patterns. Only frequent length-1 items will have nodes in the tree, and the tree nodes are arranged in such a way that more frequently occurring nodes will have better chances of sharing nodes than less frequently occurring ones. FP-treebased pattern fragment growth mining starts from a frequent length-1 pattern, examines only its conditional pattern base constructs its \(conditional recursively with such a tree. The support of any itemset can be calculated from its conditional pattern base and from the nodes in the FP-tree, which correspond to the itemset Fuzzy FPGrowth also works in a similar manner by constructing an FP-tree, with each node in the tree corresponding to a 1-itemset. In addition, each node also has a fuzzy membership function  corresponding to the 1itemset contained in the node. The membership function for each 1-itemset is retrieved from the fuzzy dataset while constructing the FP-tree, and the sum of all membership function values for the 1-itemset is its support. The support for a k-itemset \(where k ? 2 corresponding to the itemset by using a suitable t-norm  VII. RELATED WORK 3] describes the current status and future prospects of applying fuzzy logic to data mining applications. In [4] and 5], the authors discuss two facets of fuzzy association rules namely positive rules and negative rules, and describe briefly a few rule quality measures, especially for negative rules. The authors in [6] take this discussion further by describing in detail the theoretical basis for various rule quality measures using various t-norms, t-conorms, S 


implicators, and residual implicators. [8] and [9] illustrate quality measures for fuzzy association rules and also show how fuzzy partitioning can be done using various t-norms, tconorms, and implicators. The authors in [8] go a step further and do a detailed analysis of how implicators can be used in the context of fuzzy association rules Last, [7] and [10] take diametrically opposing stands on the usefulness of fuzzy association rules. The authors of [7 do a data-driven empirical study of fuzzy association rules and conclude that fuzzy association rules, after all, might not be as useful as thought to be. But the authors of [10 defended the usefulness of fuzzy association rules, by doing more experimental work, and then corroborating their stand through the successful results of their empirical research In addition to the fuzzy clustering based methodology briefly mentioned in [7], [19] and [20] describe methodologies for generating fuzzy partitions \(using nonfuzzy hard clustering original dataset into a fuzzy form. [19] uses k-Medoids CLARANS  CURE for the same. The hard clusterings so generated are then used to derive the fuzzy partitions. In such cases, where hard clustering is used, typically the middle point of each fuzzy partition is taken as reference \(membership  = 1 with respect to which the memberships for other values belonging to that partitions are calculated. [22] goes even a step further, and uses Multi-Objective Genetic Algorithms in the process for finding fuzzy partitions. Such methodologies which use hard clustering, or non-fuzzy methods are one way to obtain fuzzy versions of original datasets before any fuzzy ARM can ensue. But, with FPrep we use only fuzzy methods, fuzzy clustering to be more specific, in order to ensure consistency, and to have the notion of fuzziness maintained throughout. The main motive behind doing so is to ensure that any processing preceding the actual fuzzy ARM process, also involves fuzzy methods. Thus, the whole end-to-end process, right from the moment the processing of original crisp dataset starts till the time the final frequent itemsets are generated, involves only fuzzy methods and is holistic in nature  VIII. EXPERIMENTAL RESULTS 


The experimental results of FPrep as compared to other such non-fuzzy methods, on the basis of various parameters, are described below  A. Results from First Dataset We have tested FPrep against the automated methods for generating fuzzy partitions proposed in [19], [20]. These use hard clustering algorithms CLARANS \(k-Medoids CURE respectively. The main tangible metric to compare our approach to the ones proposed in [19], [20] is the time taken for execution. And, the dataset used for doing so is the USCensus1990raw dataset http://kdd.ics.uci.edu/databases/census1990 has around 2.5M transactions, and we have used nine attributes present in the dataset, of which five are quantitative and the rest are binary. The attributes, with their respective number of unique values, on which the evaluation was done, are as follows Age - 91 unique values Hours  100 unique values Income1  55089 unique values Income2  13707 unique values Income3  4949 unique values  Using each of the three methodologies being evaluated three fuzzy partitions were generated for each of these attributes. The results are illustrated in fig. 7, which has the y-axis in log10 form for ease of perusal.  The same are also available in Table II. As far as speed is concerned, for attributes having very low number of unique values \(~ 100 there is no big difference among the three methods. FPrep and CURE perform five times better than CLARANS for the attributes Age and Hours, both of which have around 100 unique values. But, the real differences become apparent for higher number of unique values. For attribute Income3, with 4949 unique values, we see that FPrep is nearly nine times faster than CURE, and nearly 2672 times faster than CLARANS, and for attribute Income2, with 13707 unique values, it is 27 times faster than CURE, and 13005 times faster than CLARANS. For attribute Income1, having 55089 unique values, FPrep is 46 times faster than CURE. No comparison was done with CLARANS for this attribute, as 


the time needed for execution exceeded 100000 seconds Thus, from this analysis we see that FPrep, which uses FCM clustering, clearly outperforms the CLARANS and CURE based methods on the basis of speed. The execution times for CLARANS and CURE mentioned in fig. 7 and Table II do not include the time required to create fuzzy sets, and calculate the membership value  for each numerical data point in every fuzzy set for the numerical attribute under consideration. These times also do not take into account the time required to transform crisp numerical attributes to fuzzy attributes, and derive the fuzzy dataset from the original crisp dataset The fuzzy partitions generated for each of the five numerical attributes for the USCensus1990raw dataset are shown in Table III. Coincidentally, generating three fuzzy partitions for each numerical attribute seemed a perfect fit In addition to the superior speeds achieved by FPrep, as illustrated in fig. 7 and Table II, Table III indicates the semantics and the quality of the fuzzy partitions generated by FPrep. Moreover, the number of frequent itemsets generated by a fuzzy ARM algorithm \(like fuzzy ARMOR and fuzzy Apriori minimum support threshold, is illustrated in fig. 8   Fig. 7. Algorithm, numerical attribute comparison based on speed \(log10 seconds   Fig. 8. Number of frequent itemsets for various minimum support values  B. Results from Second Dataset We have also applied FPrep on the FAM95 dataset http://www.stat.ucla.edu/data/fpp transactions. Of the 23 attributes in the dataset, we have used the first 18, of which six are quantitative and the rest are binary. For each of the six quantitative attributes, we have generated fuzzy partitions using FPrep. A thorough analysis with respect to execution times, has already been performed on the USCensus1990raw dataset \(which is manifolds bigger in size than the FAM95 dataset both on the basis of number of transactions and number of unique values for numerical 


attributes dataset has been done solely to provide further evidence of the quality and semantics of the fuzzy partitions generated by FPrep. The details of the same are in Table IV. In this case, the number of fuzzy partitions is different for different numerical attributes. Thus, the number and type of fuzzy partitions to be generated is totally dependent on the attribute under consideration. A graphical representation of the fuzzy partitions generated for the attribute Age has already been provided in fig. 5, and clearly shows the Gaussian nature of the fuzzy partitions. The nature and shapes of fuzzy partitions for the rest of the attributes are also similar. Last, the number of frequent itemsets generated for different minimum support values is illustrated in fig. 8  C. Analysis of Results With FPrep, we can analyze and zero in on the number and type of partitions required based on the semantics of the numerical attributes, which the methods detailed in [19 20] do not necessarily facilitate. Then, FPrep, backed by FCM clustering, takes care of the creating the fuzzy partitions, especially assigning membership values for each numerical data point in each fuzzy partition. In section 8.A we have already shown that FPrep is nearly 9 to 44 times faster than the CURE-based method, and 2672 to 13005 times faster than the CLARANS-based method. FPrep is not only much faster than other related methods, but also generates very high quality fuzzy partitions \(Table III and IV much user-intervention. We have created a standard way of representing any fuzzy dataset \(converted from any type of crisp dataset efficacy of the same is corroborated by the successful implementation of Fuzzy Apriori and Fuzzy ARMOR on the fuzzy dataset \(converted from crisp version of FAM95 dataset an initial implementation of Fuzzy ARMOR, are very encouraging. FPrep, when used in conjunction with these fuzzy ARM algorithms, generates a pretty good number of high-quality frequent itemsets \(fig. 8 frequent itemsets generated for a particular minimum support is same, irrespective of the fuzzy ARM algorithm 


used IX. CONCLUSIONS In this paper we have highlighted our methodology, called FPrep, for ARM in a fuzzy scenario. FPrep is meant for seamlessly and holistically transforming a crisp dataset into a fuzzy dataset such that it can drive a subsequent fuzzy ARM process. It does not rely on any non-fuzzy techniques and is thus more straightforward, fast, and consistent. It facilitates user-friendly automation of fuzzy dataset 1 0 1 2 3 4 5 Age - 91 Hours - 100 Income3 4949 Income2 13707 Income1 55089 Ti m e lo g1 0 se co nd s Numerical Attribute - Number of Unique Values FCM CURE CLARANS 0 500 1000 1500 2000 2500 


3000 0.075 0.1 0.15 0.2 0.25 0.3 0.35 0.4 N o o f F re qu en t I te m se ts Minimum Support USCensus1990 FAM95 generation through FCM, and subsequent steps in preprocessing with very less manual intervention and as simple and straightforward manner as possible. This methodology involves two distinct steps, namely creation of appropriate fuzzy partitions using fuzzy clustering and creation of fuzzy records, using these partitions, to get the fuzzy dataset from the original crisp dataset FPrep has been compared with other such techniques, and has been found to better on the basis of speed. We also illustrate its efficacy on the basis of quality of fuzzy partitions generated and the number of itemsets mined by a fuzzy ARM algorithm which is preceded by FPrep. This preprocessing technique provides us with a standard method of fuzzy data \(record that it is useful for any kind of fuzzy ARM algorithm irrespective of how the algorithm works. Furthermore, this pre-processing methodology has been adequately tested with two disparate fuzzy ARM algorithms, Fuzzy Apriori and Fuzzy ARMOR, and would also work fine with other fuzzy ARM algorithm REFERENCES 1] Zadeh, L. A.: Fuzzy sets. Inf. Control, 8, 338358 \(1965 2] Chen G., Yan P., Kerre E.E.: Computationally Efficient Mining for Fuzzy Implication-Based Association Rules in Quantitative Databases. International Journal of General Systems, 33, 163-182 


2004 3] Hllermeier, E.: Fuzzy methods in machine learning and data mining Status and prospects. Fuzzy Sets and Systems. 156, 387-406 \(2005 4] De Cock, M., Cornelis, C., Kerre, E.E.: Fuzzy Association Rules: A Two-Sided Approach. In: FIP, pp 385-390 \(2003 5] Yan, P., Chen, G., Cornelis, C., De Cock, M., Kerre, E.E.: Mining Positive and Negative Fuzzy Association Rules. In: KES, pp. 270-276 Springer \(2004 6] De Cock, M., Cornelis, C., Kerre, E.E.: Elicitation of fuzzy association rules from positive and negative examples. Fuzzy Sets and Systems, 149, 7385 \(2005 7] Verlinde, H., De Cock, M., Boute, R.: Fuzzy Versus Quantitative Association Rules: A Fair Data-Driven Comparison. IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics 36, 679-683 \(2006 8] Dubois, D., Hllermeier, E., Prade, H.: A systematic approach to the assessment of fuzzy association rules. Data Min. Knowl. Discov., 13 167-192 \(2006 9] Dubois, D., Hllermeier, E., Prade, H.: A Note on Quality Measures for Fuzzy Association Rules. In: IFSA, pp. 346-353. Springer-Verlag 2003 10] Hllermeier, E., Yi, Y.: In Defense of Fuzzy Association Analysis IEEE Transactions on Systems, Man, and Cybernetics - Part B Cybernetics, 37, 1039-1043 \(2007 11] Agrawal, R., Imielinski, T., Swami, A.N.: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Record, 22, 207216 \(1993 12]  Agrawal, R., Srikant, R.: Fast Algorithms for Mining Association Rules. In: VLDB, pp. 487-99. Morgan Kaufmann \(1994 13] Han, J., Pei, J., Yin, Y.: Mining Frequent Patterns without Candidate Generation. In: SIGMOD Conference, pp. 1-12. ACM Press \(2000 14] Han, J., Pei, J., Yin, Y., Mao, R.: Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree Approach. Data Mining and Knowledge Discovery, 8, 5387 \(2004 15] Pudi V., Haritsa J.R.: ARMOR: Association Rule Mining based on Oracle. CEUR Workshop Proceedings, 90 \(2003 16] Dunn, J. C.: A Fuzzy Relative of the ISODATA Process and its Use in Detecting Compact, Well Separated Clusters. J. Cyber., 3, 32-57 1974 17] Hoppner, F., Klawonn, F., Kruse, R, Runkler, T.: Fuzzy Cluster Analysis, Methods for Classification, Data Analysis and Image Recognition. Wiley, New York \(1999 


18] Bezdek J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms. Kluwer Academic Publishers, Norwell, MA \(1981 19] Fu, A.W., Wong, M.H., Sze, S.C., Wong, W.C., Wong, W.L., Yu W.K. Finding Fuzzy Sets for the Mining of Fuzzy Association Rules for Numerical Attributes. In: IDEAL, pp. 263-268. Springer \(1998 20] Kaya, M., Alhajj, R., Polat, F., Arslan, A: Efficient Automated Mining of Fuzzy Association Rules. In: DEXA, pp. 133-142. Springer \(2002 21] Mangalampalli, A., Pudi, V. Fuzzy Association Rule Mining Algorithm for Fast and Efficient Performance on Very Large Datasets In FUZZ-IEEE, pp. 1163-1168. IEEE \(2009 22] Kaya, M., Alhajj. Integrating Multi-Objective Genetic Algorithms into Clustering for Fuzzy Association Rules Mining. In ICDM, pp. 431434. IEEE \(2004  Table II. Algorithm, numerical attribute comparison based on speed \(seconds  Algorithm Age - 91 Hours - 100 Income3 - 4949 Income2 - 13707 Income1 - 55089 FCM 0.27 0.3 3.13 6.28 79.4 CURE 0.25 0.25 28.67 163.19 3614.13 CLARANS 1.3 1.34 8363.53 78030.3 Table III. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions Age Old Middle Aged Young Hours More Average Less Income1 High Medium Low Income2 High Medium Low Income3 High Medium Low  Table IV. Attributes and their fuzzy partitions  Attribute Fuzzy Partitions AGE Very old Around 25 Around 50 Around 65 Around 35 HOURS Very High Zero Around 40 Around 25 INCHEAD Very less Around 30K Around 50K Around 100K INCFAM Around 60K Around 152K Around 96K Around 31K Around 8K TAXINC Around 50K Around 95K Around 20K Very less FTAX Around 15K Very less Around 6K Very high Around 33K  


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


