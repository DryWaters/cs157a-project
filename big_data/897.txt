A Comprehensive Feature-Oriented Traceability Model for Software Product Line Development   Liwei Shen, Xin Peng  and Wenyun Zhao School of Computer Science Fudan University,Shanghai,China 061021062, pengxin, wyzhao}@fudan.edu.cn   Abstract  Feature-oriented traceability is essential for efficient Software Product Line \(SPL\ development including product derivation and SPL evolution Widely-used feature based method has been proved to be effective in domain anal ysis and modeling. However it cannot support the traceability naturally due to the big gap between the problem space and the solution space. In this paper, we propose a comprehensive feature-oriented traceability model for SPL development, which provides mechanisms for various features and implementation types throughout the four levels of goal model, fe ature model, feature implementation model and program implementations In it, the feature implementation model is introduced as the intermediate level between features and implementation artefacts. The feature interactions are captured in the finer role level, and they help to clarify the complex mapping between features and program implementations. The traceability meta-model for SPL development is introduced and an example on the library management domain is demonstrated  1. Introduction  The purpose of SPL method is rapidly producing cost-efficient and high-quality application products with domain core assets, including domain model domain-specific architecture and domain components etc. The ideal mode of product derivation is constructing the final product by configuring and tailoring of core assets, follo wing a prescribed process and complemented by application-specific implementation of some part del y u s e d f eat u r e  based method has been proved to be effective in domain analysis and modeling. In these feature-based methods, proper mechanism of feature-oriented traceability throughout domain analysis, design and implementation is expected to be established to ease feature-based core assets selection, configuration and composition Feature-oriented traceability is the embodiment of requirement traceability in SPL development, which has been widely studied in several areas in software engineering, such as requirement management software evolution, program comprehension, etc. Gotel et al. [2 e f i n e req u ire m e n t t r aceab ilit y as  t h e ab ilit y  to describe and follow the life of a requirement, in both a forwards and backwards direction”. In SPL development, requirement traceability is to identify and explicitly represent the deriving relationships among different domain artefacts, usually from domain feature model to design decisions, and further down to program implementations In SPL development, feature-oriented traceability is expected to map features to SPL designs and implementation elements, including commonalities and variations, to enable feature-oriented product derivation and SPL evolution, etc. However, existing featurebased methods do not provide the traceability naturally due to the big gap between the problem space and the solution space [5, 6  w h ic h is g e n e rall y called th e  feature tangling and scattering. Usually, there may be several components contributing to a single feature. A single component may also contain implementations for several features. As stated in f eatu r e tan g l i n g  and scattering have negative impacts on the system's maintainability. On the other hand, there exist various kinds of implementation artefacts for user-visible features. Component is the most acceptable that a system can be constructed by several components working together. Other implementation types such as code fragment, configuration file, data structure also make sense in the SPL development. Usually they cannot be encapsulated like components but they are used to collaborate with components to provide additional functions and controls. Thus, the traceability is especially difficult in SPL development due to the inherent demand of variability analysis, design and implementation 
2009 Australian Software Engineering Conference 1530-0803/09 $25.00 © 2009 IEEE DOI 10.1109/ASWEC.2009.27 210 


In this paper, we propose a comprehensive featureoriented traceability model for SPL development which provides mechanisms for various feature types considering commonality/variability, diverse binding time and differences among composition generalization and atomic features, etc. The model focuses on the low-end user’s perspective of modeling requirement dependencies, allocation of requirements to model and implementation elements which are included in the work by Ramesh and Jark It  provides visualized and comprehensive traceability representations for SPL development, throughout the four levels of goal model, feature model, feature implementation model and program implementations In particular, feature implementation model is a novel concept which represents the developers’ design decisions and helps to clarify the complex mapping between features and program implementations including components and other kinds of program units  res f eat u r e interaction s i n c l u ding cros s cutting interactions\ the finer role level, and explains how the functions of features are splitted as well as the intrinsic semantic of the feature interaction. The model contributes to an ideal traceability mechanism upon which developers can understand how the requirements are realized and how the product derivation and SPL evolution can be implemented The remainder of this paper is organized as follows Section 2 introduces related works on feature-oriented traceability in SPL development. Section 3 presents the meta-model of traceability, including the four levels and diverse traceability links. A representation example on the library management domain is demonstrated in section 4. Then in section 5, some discussions related to the feature-based traceability model is placed Finally, section 6 concludes the whole paper and discusses our future works  2. Related work  There is much seminal work in the area of traceability representation for SPL. They utilize modelbased approaches which introduce several hierarchical layers to visually model the product line artefacts as well as the traceability links among the artefacts. For example, Lago et al. define a product-oriented model to represent features, design decisions and implementation assets at both the Product Family Level and the Product Level in T h e m odel su pport s cros s level traceability and has been used to extend a tool to support traceability in product families. In i ebi s ch introduces traceability links among software development artifacts grouped and categorized in different abstraction levels including feature level architecture component level, class level and implementation artifact level. He also introduces feature model as an intermediate element for linking requirements to design models in [7 h e m o d e l is  useful to eliminate the huge difference between requirements and design elements, and features are regarded to offer advantages such as the reduction of links’ number, easier verification, maintenance and comprehension. Similar with Riebisch’s work, we follow the idea of representing various traceability links between artefacts in di fferent abstraction levels The difference is that we extend the semantic of the feature model as well as the implementation types Furthermore, we introduce an innovative feature implementation model as the bridge between features and implementation artefacts. It contains the design decisions which assign the requirements \(by features to the implementations in a finer granularity level. As a result, it can clearly describe the rationale of traceability between features and implementations which is ignored by prior publications  3. Traceability meta-model  3.1. Overview  The feature-oriented traceability meta-model for SPL is presented in figure 1. It consists of four hierarchical layers with different stakeholder concerns including domain requirement goals, feature model feature implementation model and program implementations. Artefacts and trace links within each level are modeled: goal level involves functional and non-functional requirements; feature level includes different kinds of features such as Action, Facet as well as the related relationships [3 eat u r e i m p l e m e n tatio n  level consists of roles, inter-role relationships and interactions; program level includes domain/application components and other kinds of implementation units for variant features, such as configuration file items code segments, data structures, etc. Traceability links are divided into explicit ones and implicit ones as stated in T h e f o rm er a r e v i s i ble in t h e m odel represented by the lines with solid arrows with annotations. Contrarily, the latter are invisible and they are derived from the explicit traceability links  3.1.1 Goal level Goals capture stakeholder intentions   m eans of hi ghl e vel fun c t i o n a l an d n onfunctional requirements \(FRs and NFRs\ our model they are described in natural language. FRs are desired operations or abilities for the realization of the SPL and an example can be “accomplish the functions of borrow and return in a library system”. NFRs are 
211 


  Figure 1. Feature-oriented traceability meta-model  usually on quality aspects of the SPL and those can be satisfied by system functions are included. Example of them can be “have good traceability in book circulation” and “ensure the reliability of borrow&return of valued books” in the library management domain. Both FRs and NFRs can be refined into sub-goals by decomposition and specialization. For example, FRs can usually be decomposed according to the expected menu hiberarchy while the NFRs can be continually decomposed until the requir ement engineers consider the soft-goals can be satisfied [8  3.1.2 Feature level Domain feature model formally structures domain-specific requirements by various kinds of features and inter-feature relationships. It is the result of domain analysis with careful considerations on commonalities among applications and differences between applications in the domain Feature level discussed in this paper is represented by the ontology-based feature model proposed in our previous work on feature modeling th e m odel features and inter-feature relationships are subdivided into several categories. Their concepts are exemplified by the function BorrowBook on the library management domain whose feature model is illustrated in figure 2 Action An action represents a business operation or function\ of various granularities with domainspecific semantics. In figure 2, actions are represented by rectangles IfOptional This attribute denotes whether an action is optional or not, and the value ‘True’ means the element action is optional for its parent action. The optional actions in figure 2 are blocks with a circle HasElement   It is a type of specialization relationship and it divides the function of a parentfeature into the functions of its sub-features Composition Action It is a kind of action which can be decomposed into sub-actions and has HasElement relations with its partitions. In particular there is a new attribute isControlProcess that is introduced to determine whether the composition action should manage the execution sequence of its sub-actions. For example, the root action BorrowBook  is a composition action but not controls process  ifControlProcess=false not labeled\. Contrarily, the Borrow action specifies the execution sequence of its sons ifControlProcess=true that the operation can be performed in a correct behavior subClassOf It is the self-defined ontology property between two actions or other ontology concepts representing direct specialization relationship between them 
212 


Generalization Action   It is another kind of action whose sub-actions are specialized from it and have subClassOf relations with it. In figure 2, the action RewardPointCaculation is a generalization action. It has two concrete calculation types which can both take effect in products Atomic Action It is the leaf of a feature model and could not be decomposed further Facet Facets are defined as perspectives viewpoints, or dimensions of precise descriptions for certain action, providing details of business semantics For example, the action BorrowControl in figure 2 has a facet Minimal Storage which acts as a parameter in one of the action’s dimensions to decide the feasibility of the borrowing operation ConfigDepend It represents configuration constraints, which are static dependencies on bindingstates of variable features. Usually it is shown as require and exclude between two actions BindTime It represents the time when the binding status of an optional feature will be decided. Three kinds of BindTime are distinguished: the first is compile-time which means to make decision at program compiling phase; the second is load-time  which indicates the decision is made when the system starts up; the last is run-time which means whether the function is used depends on the runtime context. In the example, the bind time of action RewardPointCaculation is compile-time, which means only one of its sub-action is included when deriving a product     Figure 2. Ontology-based feature model example  Readers can refer to [3 o r de tailed des c ription s f o r the feature model 3.1.3 Feature implementation level In this level feature implementation model is introduced as an intermediate level between features and program implementations Feature implementation model can also be called role model. Role is the basic element. It is a logical unit a responsibility which should be taken by program fragment for feature implementation. Compared with features, roles reserve the knowledge of features as well as endow features with the implementation respects, so it can support the mapping from features to program implementations in a more natural way. The concept of role here is similar to the responsibility in  d t h e role in  A role can be specialized into a concrete role or an abstract role An action in feature model can be realized by one or more concrete roles while abstract role does not refer to any feature but inherited by other concrete roles. The introduction of abstract role makes sense because it represents a set of concrete roles that share common characteristics so that interactions between a role and one in that set can be reduced, i.e to provide expression for crosscutting interactions. For example borrow and return are roles that will both change the storage of a book, so interactions between a role doing log and these two roles can be simplified as one interaction between the role log and an abstract role bookChange generalized from them In addition, two kinds of concrete roles are distinguished: one is operational role and the other is resource role An operational role is a functional segment while a resource role represents specific internal or external entity necessary for the implementation of a feature. A concrete role can also be optional for representing variability. Some are identified by inheriting from the feature model. Others are found as the so-called internal variability [1 w h ich  is necessary for technical reasons. Besides it property  is introduced to denote the perspective of a specific role, which is similar to the definition of Facet  The interactions between roles are important because they indicate how roles cooperate with each other to implement a feature’s function \(action in our feature level\gether, and they are also used to guide the composition of corresponding implementation artefacts. The interactions include those between roles from the same feature, as well as those between roles from different features. In our method, five kinds of role interactions are identified in table 1  Table 1. Interaction types between roles Interaction Description Involve An operational role activates another operational role in a synchronous mode and makes it a part of the host operation Inform An operational role informs another operational role to activate in an asynchronous mode Determine Execution result of an operational role can determine the execution of another operational role, including whether execute or not and choosing a variant from several choices Access An operational role reads or writes a resource role, or both, to fulfill its responsibility in specific feature implementation Introduce A resource role is introduced into implementation unit of another operational role to be a sub-element 
213 


3.1.4 Program level Components and other kinds of implementation artefacts are modeled in this level They come from asset depository and are reused to derive software applications. In the SPL context, a product line has a mandatory part which is regarded as the base programs while the variable part is weaved into the base programs In our mechanism, we assume that all the commonalities for a SPL \(base programs constituted by the components which collaborate with each other. On the other hand, the variations for a SPL are supposed to be implemented by all kinds of program artefacts which can be component, code fragment, configuration file, or data-structure Components are black-box entities whose interfaces are exposed. Thus, an application is derived based on component composition through interfaces and other implementation artefacts which complement the system functions. These program implementations are attached to the base programs by means of specific weaving techniques such as AOP, if the corresponding variant is bound. Among them, the configuration file which is usually in the form of text-file or ini-file, contains formalized information as well as different parameter values organized in specific items. Data-structure denotes the database schema supporting the implementation of specific functions. For instance, a table for storing log information should be created if a function doing log takes effect  3.2. Explicit traceability in the meta-model  Explicit traceability links represent the developers knowledge about requirements and their realization in the developed system [1 h ey are us ual l y manual l y  specified. In our meta-model we separate the explicit traceability links into two categories intra-level traceability and inter-level traceability Next we first briefly introduce the intra-level traceability Intra-level traceability is the associations between elements in the same level: the decompose relation between FRs or between NFRs in the goal model; the HasElement  subClassOf and configDepend relations in the feature model; five kinds of interactions between roles in the feature implementation model. Usually, in each level, abstract and composition elements may first be refined, and then the atomic elements can generally be mapped to elements in another level. On the other hand, interactions between roles are modeled to clarify how a user-visible feature is implemented by several logical sides and to guide the program-level customization and composition Inter-level traceability is more complex for it is the associations between elements of different abstraction levels. In our mechanism, we assume that the explicit inter-level traceability links only exist between adjoining levels. We group the links into the following categories with different names and semantics  3.2.1 Traceability between the goal level and the feature level We define the traceability between FRs and actions as support Usually, we analyze a textual requirement specification and map it to a set of actions Some of the actions may be optional because of the variability in the requirements. On the other hand, the BindTime attribute of each optional action depends on how the requirements express. For example, function X  takes effect if the physical memory is bigger than Y  The BindTime of the corresponding action should be set to load-time which is to decide the binding status of the action according to the computer’s physical configuration when the system starts The traceability related to NFRs follows the mechanism in [8 h ic h  id en t i f i es th e traceab ilit y f r o m  NFRs to actions as dynamic-operationalize and traceability from NFRs to facet as  static-operationalize  respectively. Dynamic-operationalize means that such kinds of NFRs require actual operations to accomplish them while static-operationalize means perspectives of an action which can be adjusted to satisfy the specific quality requirements 3.2.2 Traceability between the feature level and the feature implementation level This kind of traceability is called realize It denotes the associations from the elements in feature model to the elements in role model \(table 2 An atomic action can be realized by a set of concrete roles. These roles may include operational roles as well as resource roles which divide the action’s function into finer logic units. The variability of these roles does not follow the action all along, i.e. some of them are the newly defined internal variations which are always technique-related and unconcerned by customers. Besides roles, interactions \(table 1\ between the roles are also modeled. These roles cooperate with each other according to the in teraction rules to realize the action’s function in a correct way The traceability from a composition action is different. It exists only if the isControlProcess attribute of the composition action is true, i.e. it managers the execution sequence of its partitions. Under the situation the functional part is decomposed and delegated by its sub-elements, but the part relating to process controlling still remains to be traced. In our mechanism we map such a composition action to an operational role which takes the responsibility of controlling the execution flow of the other related roles. On the contrary, if the composition action doesn’t involve the execution sequence, the traceability makes no sense 
214 


Table 2. The realize traceability  Origination Precondition Target General Realization Units Illustration atomic action N/A operational resource role\(s component interface code fragment, data structure, component  composition action isControlProcess true operational role component interface that controls the execution flow isControlProcess false N/A N/A generalization action BindTime  compile-time N/A makefile \(in C build.xml \(Ant for Java   BindTime  load-time operational role resource role configuration file system parameter method that readers the parameters BindTime  run-time operational role runtime logic user choice facet N/A property component interface accessing property configuration file   and it can be ignored The traceability from a generalization action differs a lot due to the BindTime attribute. Usually, variability lies in it because it determines the different binding and realization mechanisms. Suppose that a generalization action X is specialized into action Y and Z last column in table 2\, we will give three traceability situations based on the different BindTime settings. Firstly, if the BindTime of X is compile-time only one of the subactions in Y and Z will be chosen to compose the final product. In such circumstances X is modeled as a placeholder so that it will be replaced by one of its concrete actions in product derivation. Sometimes the replacement can be realized by means of makefile in C build.xml Ant for Java\hich configures the program units. However, they are not software implementation artefacts in the common sense, thus not contained in our meta-model. Secondly, if the  BindTime of X is load-time the binding decision depends on the actual environment when the system is starting. It is usually decided by reading a configuration file containing startup settings or reading from system parameters. So we can map X to an operational role and a resource role. The former makes the decision through accessing the latter. Thirdly, if the BindTime of X is run-time, the binding decision is undetermined and it may change when system is running. The choice can be made by user or by program logics. Therefore X can be regarded as a proxy and further realized by an operational role that is able to accept user choice or take effect by prescribed logics On the other hand realize also resides in traceability from facet in feature model to property in role model. Since facet can be regarded as a kind of variability towards a feature, we reserve the variability on implementation level and it will ultimately be implemented by specific program units such as component property \(accessed through interface\d configuration file’s segment 3.2.3 Traceability between the feature implementation level and the program level  
215 


Instantiate denotes the traceability from concrete roles to program artefacts Operational roles, representing the business logics are usually instantiated by component interfaces, code fragments or data structures. Interfaces of components offer the business operations. Code fragments always provide operational roles with additional implementations which are combined with the base programs. Data structures are necessary if an operational role requires tables to store information in a database. In our model, the mandatory roles are always mapped to the component interfaces which will be composed to derive the common part of a SPL Resource roles are usually instantiated by components, code fragments and segments in configuration files. For example, if a resource role denotes a visible widget, it should be instantiated by a widget component and some code fragments to initialize it in the UI. If the resource role represents a parameter, it should be mapped to the relevant segment in a configuration file Property denotes the perspectives of a role. It can be accessed from outer files or coded in programs Thus, we usually instantiate them as the configuration file segments or the components properties which are actually accessed through the interfaces  3.3. Implicit traceability in the meta-model  Implicit traceability links are automatically derived using explicit links to gain a more complete insight into a system by showing relationships between artefacts that the developer may not have been aware of  They are not visually represented in the meta-model Using criteria like transitivity, we can derive implicit traceability links which will bring benefits. For example, the traceability between requirements and the implementation artefacts is derived by combining the inter-level traceability links to check if all the requirements are implemented as well as to ease the comprehension of the SPL. Similarly, elements in the same level can also have implicit traceability if they refer to the same program units so that the traceability is helpful when performing change impact analysis  4. A representation example  Represented by the model above, an example on library management domain is illustrated in figure 3 which proves the scalability of the mechanism. The top layer is the goal level representing FRs and NFRs of the domain. The next layer is the feature level containing the ontology-based feature model. In it white rectangles represent mandatory features while grey-filled ones represent variable features \(for the sake of clarity, we use color instead of circle\ole model is described in next level. Similarly, variable roles are represented by grey-filled rectangles and roles surrounded by a pane are traced from a same feature. In the bottom level, components and other kinds of implementations such as configuration file and data structure are illustrated. For the sake of clearness, each program implementation has a prefix whose legend is on the right-bottom corner In the goal level, FRs and NFRs are expressed in natural language specifications Reader Management  Book Management and Borrow&Return Operations are separated functional requirements Storage Safety the storage of a book cannot be less than a minimal value to ensure the book is available in library\d Operation Traceability record borrow&return operations of readers to avoid influence by system failure\on-functional requirements decomposed from the goal of System Safety  The FRs are supported by actions, e.g Borrow&Return is supported by BorrowBook and ReturnBook Next we discuss BorrowBook in detail BorrowBook is decomposed into SearchBook  Borrow  and Borrow-Control control book borrowing by prescribed policy\ by HasElement relationship. The first two are mandatory actions possessed by each application while the third one is dispensable SearchBook has an optional action BookPicShow show the book picture when browsing\whose BindTime is compile-time Borrow as a composition action controlling execution sequence isControlProcess true further decomposed into Borrow Operation  Calculate ReturnDate and RewardPoint Calculation  RewardPoint Calculation here is a generalization action whose BindTime is also compile-time which indicates only one calculation type will be bound and included in an application As for the NFRs Operation Log is an action dynamic operationalized from Operation Traceability  and another NFR Storage Safety is static operationalized to a facet Minimal Storage belonging to BorrowControl to specify the parameter In the next layer, an action is realized by a single role or a set of roles surrounded by a pane. For example the action SearchBook refers to the role BookSearch  while the action BorrowOperation is realized by two roles SetBookAmt update the storage of a book\d AddReaderRec add a book to the reader’s borrowing list\ particular, the action Borrow is realized by a role BorrowProcess because its isControlPrcess  attribute is true. And the action RewardPoint Calculation doesn’t trace further because it acts as a placeholder \(comp ile-time binding 
216 


The interactions between roles are also modeled. In the figure BookPicShow is realized by three roles PicShowControl  ImageFetch and ImageContainer  ImageContainer is an image container for picture visualization ImageFetch is to fetch image data, and PicShowControl takes the responsibility of controlling image fetching and showing. So, we can see that ImageFetch is involved by PicShowControl which means the former is activated by the latter, and PicShowControl accesses ImageContainer to set the image. They are the interactions between roles from a same feature. On the other hand PicShowControl is involved by BookSearch to take effect. It’s the interaction between roles from different features. In particular, the role BorrowProcess controls the execution process, so it involves all the roles mapped from the sub-actions in the composition action Borrow  We also model the role interactions related to abstract roles \(see in section 3.1.3\This kind of interactions usually crosscut multiple parts of a system For example, in figure 3 Log will be informed to activate by all the events of readers, e.g AddReaderRec  DelReaderRec etc. So, abstract role ReaderEvent is modeled to generalize the two roles and to simplify the interactions towards them Roles are instantiated by different kinds of assets in the bottom level. For instance, the role SetBookAmt is instantiated by the setAmount interface in the component Book The role Penalty Param corresponds to the penalty segment in a configuration file which is assessed by the Calculate Penalty role. The role ImageContainer as a resource role, is instantiated by a component Canvas as well as a code fragment to be used for its initialization in UI. The two RewardPoint Calculation Type roles are optional roles and they are realized by two methods which are not included in any component but only one of the methods will be composed into the base programs. The property Minimal Storage can be instantiated into an interface accessing the related variable in component Book The role log is instantiated as a log method to perform its function as well as a schema to create the log table in database for storing operation records         Figure 3. The traceability model on library management domain 
217 


5. Discussion about th e feature-oriented traceability model  In this section, we will briefly discuss the featureoriented traceability model: why we adopt the feature implementation model and ignore the architecture, how the model instructs the product derivation and SPL evolution phases. On the other hand, related tools are presented  5.1. Feature implementation model instead of SPL architecture  A product line architecture \(PLA\pecifies the architecture for a set of closely related software products u s u a l l y i n t e r m s of co m p o n en t s  connectors and configurations A P L A  f o cu s e s on modeling the variability for a domain and promotes the reuse in the SPL development. However, in the traceability context, feature tangling and scattering still exist between the feature model and the PLA, i.e several components may contribute to a single feature and a single component may contain implementations for several features. Therefore, it is difficult to explain how the functions of features are splitted as well as what is the intrinsic semantic of the feature interaction In our model, the feature implementation model is introduced to replace the complex many-to-many traceability between features and implementation artefacts with two sets of clear trace links. Roles that decompose features and the inter-relationships between the feature parts \(role interactions\o defined Role model provides the design decisions from the viewpoint of system designers, which are not concerned by the requirement analysers. It also captures the inner structure of a feature and records the semantic of feature implementation \(reason for splitting feature functions\ and feature interactions \(the intrinsic relationships between features\ a finer level Furthermore, roles are explicitly assigned to different implementations artefacts including components and other forms of implementations. Thus, the semantic for traceability from requirements to implementations is complemented and extended  5.2. Traceability-based product derivation and SPL evolution  Traceability is the basis of product derivation because we need to find out the variability-related program implementations according to the variable requirements and combine them with the base implementations [9  I n  o u r m ech an ism  r a th e r th a n  feature-driven customization and composition, we involve traceability-based role level customization and program-level composition. The first step is to decide whether the variable roles will be included in the final product. Some can be determined directly by the related features’ bounding status, others especially the internal variability-r elated roles should be customized by developers in role-level since they are unconcerned by clients. The next step is to select and configure the variability-related implementation artefacts according to the role instantiation traceability. In particular, role interaction, as an important kind of traceability, guides the program-level composition, which is to instruct what kinds of program implementations should be composed and how they can be composed using AOP mechanis   Traceability also plays an important role in SPL evolution. We are able to locate the features, roles and program implementations that are involved in the evolution through traceability and then make decisions on the evolution of the models, the implementations, as well as the traceability links. Usually evolution is driven by two factors. One is the requirement changes the other is bug fixing. Our mechanism is useful in these perspectives for it assists change impact analysis to identify the involved part of a SPL driven by a specific evolution request  5.3. Tool support  The tools OntoFeature  a n d FDAPD featuredriven and aspect-based product derivation tool   are developed to support the manual traceability capturing and visualizing. They are now separated and are used by different stakeholders \(requirement analyzer, system designer\n the SPL development process OntoFeature is a graphic feature modeling tool used to accomplish the ontology-based feature modeling which includes actions, facets and the relationships between these elements. FDAPD is a role modeling tool and is integrated with OntoFeature. It captures all the features and dependencies in the previous tool and provides graphic editing space for each of the features, where the roles, interactions and the corresponding implementation artefacts can be modeled. In practice, the two tools are cooperated, i.e OntoFeature for representing requirements and FDAPD for further design and implementation FDAPD is also developed to accomplish the role customization and program-level composition by invoking the AOP mechanism i.e. the variabilityrelated implementations are selected as aspects to be woven into the base programs according to the interaction types. The detailed composition process can be referred to  
218 


6. Conclusion and future work  In this paper, we focus on the visualized representation of the traceability in a software product line, and introduce a comprehensive feature-oriented traceability model. The model explicitly represents the product line artefacts in different abstraction levels. It also contains various kinds of traceability links explicit/implicit\mong the artefacts. Based on it, the traceability information from requirements to implementations is extended, described in finer-grained levels. In the whole model, the feature implementation model is innovative that it integrates the knowledge of both business logics and implementation techniques, i.e how the features are splitted and what is the intrinsic semantic of the feature interaction. This level helps to reduce the key problem of the mapping between the problem space and the solution space by providing evidences that the relationships between features and programs can be reasonably obtained through a third party ‘role’ rather than simply connecting the two sides As a result, the new concep t contributes to the ideal mechanism that the traceability of a system can be clearly presented However, the method we propose is far from mature in the following perspectives 1\The model is an informal representation model Now the models and the traceability links are manually identified and recorded by means of the developed tools. However, the automatic traceability capture deduction and validation relying on a formal basis are not available thus they are expected in the future work 2\When the product line grows larger, the traceability model will be farther complex and difficult to define. However, the problem cannot be avoided perfectly unless we make the model focus on the core part of a product line rather than the whole boundary This tailored model can be expected to describe the variable part in detail while the base program curtly. It will also be our future work that we think it will be helpful in the variability management domain especially tracing the variabil ity at different abstraction levels 3\The meta-model can be extended to other SPL development perspectives. For example, traceability related to testing is necessary in many development processes. Thus, we will complement the current traceability on design and implementation with the trace to testing artefacts in the future work  Acknowledgments This work is supported by National Natural Science Foundation of China under Grant No 60703092, 90818009, and National High Technology Development 863 Program of China under Grant No 2007AA01Z125  7. References  1 D  M  We is s a nd C  T. R   La i S of tw a r e Pr o duc t Line  Engineering: A Family-Based Software Development Process”, Addison-Wesley, 1999  te l a n d A.Fi nke lste i n  A n Ana l ysis of t h e  Requirements Traceability Problem”, in Proceeding of 1st International Conference on Requirement Engineering, 1994  Pe n g We ny un Zha o Y unjia o Xue a n d Yijia n W u   Ontology-Based Feature Modeling and ApplicationOriented Tailoring”, in Proceedings of International Conference on Software Reuse \(ICSR2006\, pp.87-100 4 n P e ng  L i wei S h en W e n y u n Z h ao F eat u r e Implementation Modeling based Product Derivation in Software Product Line”, in Proceedings of International Conference on Software Reuse\(ICSR2008  b isc h R.Brc i na  O pti m iz ing De sign f o r Va ria b ilit y  Using Traceability Links”, in Proceedings of International Conference on Engineering of Computer Based Systems ECBS2008\, pp.235-244 6  L a go  E  Ni emel a H Van Vl i e t  T o o l S u pp o r t f o r  Traceable Product Evolution”, in Proceedings of the Eighth European Conference on Software Maintenance and Reengineering \(CSMR2004\, pp.261-269 7 M.R i e b is c h  S u p p o r t i ng E v oluti ona r y D e ve lo pm e n t b y  Feature Models and Traceability Links”, in Proceedings of IEEE International Conference and Workshop on the Engineering of Computer-Based Systems \(ECBS2004\ pp 370-377  y sne i ros  J.Le ite   N onf un c tiona l Re q u ire m e n ts: Fr om  Elicitation to Conceptual Models”, in IEEE Transactions on Software Engineering, Vol. 30, No. 5, May, 2004 9 A  G  J  J a ns e n R  Sm e d i nga  J  va n G u r p a n d J  B o s c h   First class feature abstractions for product derivation”, in IEE Proc.-Softw., Vol. 151, No. 4, August 2004 10 W e i Z h ang  H o ng  M e i  H a i y an Z h ao   F eat u r ed r i v e n  requirement dependency analysis and high-level software design”, in Requirements Eng \(2006\ Vol.11, pp: 205–220  M  A l e k s y  T.H ile n b r a nd C  O b e r gf e ll, M  Sc hw i nd A  Pragmatic Approach to Traceability in Model-Driven Development”, in Proceedings of the Multikonferenz Wirtschaftsinformatik 2008 \(MKWI2008  B  R a m e s h  M.J a r k e   T ow a r ds r e f e r e nc e m ode ls f o r  requirements traceability”, in IEEE Transactions on Software Engineering, 2001, 27\(1\:58 14 J Bo sch Desi g n an d Use o f S o ft ware Arch i t ect u r es  Adopting and Evolving a Product Line Approach”, Pearson Education \(Addison-Wesley & ACM Press\, May 2000 15 N. M e dv i d o v i c R  N T a yl o r  A C l assi f i cat i o n and Comparison Framework for Software Architecture Description Languages”, in IEEE Transactions on Software Engineering, 2000. 26\(1\: p. 70-93 16 Yi j u n Yu  Yi q i ao W a ng  J M y l o p o u l o s  et al  Rev erse  Engineering Goal Models from Legacy Code”, in Proceedings of IEEE International Conference on Requirements Engineering \(RE2005\, pp.363-372  
219 


Automated Windowed Coefficient Trending Earlier we attempted to find explanatory models whose coefficients changed over time in a predictable way This would then let us predict future coefficient values for these models and apply these trended models towards future estimates The problem was that we were not able to find such stable models at least manually We automated our search for stable models by building a system we refer to as the Automated Coefficient Trend Search ACTS tool A patent is currently pending for the method implemented in this software Against the Far Out data set ACTS is designed to automatically permute fields run chronologically-windowed regressions with the chosen fields establish linear and nonlinear trends for the resulting coefficients and then use these trended coefficients to estimate a future set of data ACTS also automatically tries up to seven different transformations on any given dependant or independent variable The length of calibration windows and the increment between them are automatically varied ACTS establishes most promising coefficient trends and also rates the performance of predictions done using trended-coefficient models With so many parameters transformations windows length and intervals to evaluate ACTS needed to search a huge problem space Despite assembling a cluster of 8 computers to run ACTS if necessary for months the problem still required pruning Each run of ACTS was therefore restricted to producing models with a different number of variables between 3 and 15 We quickly found that the best models contained very few parameters due to over-fitting on wider models permitting us to pare down the problem Neural Netv 1.00 0.75 0.50 0.25 I  M The system produced literally thousands of forecasts for each project If the distribution of project costs and project costs estimates were known a confidence interval could be computed using the cost estimates and a method such as Bayesian model averaging used to integrate them 5 Not having a priori knowledge of this distribution we chose instead to use the inter-quartile range of the estimates that is we looked at whether the estimate fell between the first and third quartile of the estimates The top and bottom 25 were disregarded because there are always outliers and it was presumed the system could work well even when many of the individual estimates were not very good Given the very large number of estimates produced by the system literally thousands or tens of thousands depending on the settings we could afford to squander a few Results were best with only three variables per equation In eight out of 31 projects 26 of the projects the project's actual costs fell within the inter-quartile range of the forecasts of costs for that project produced by the ACTS models For 30 out of 31 projects the inter-quartile range of estimates was made up of over a thousand estimates while for the 3 1st project only 446 estimates were part of the inter-quartile range The significance of these numbers is revealed in the next paragraph When equations with four variables were used only one of the true project costs fell within the inter-quartile range of forecasts produced by ACTS When more than 4 variables were used none of the true project costs fell within the interquartile range From this we concluded that ACTS performed best when only 3 variables were included in each model Using more variables appears to produce over-fitted models Aork MRE By Lauch Month Chronologic 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Launch Month Since 1960 Figure 10 Neural network magnitude of relative error sorted chronologically 11 space 


incapable of accurate forecasts Of course the wider the inter-quartile range of forecasts the more likely it is that the true forecast will fit within that range But how parsimonious is the range As figure 11 shows in about 55\2600 of the inter-quartile ranges created by ACTS the 3rd quartile of estimates is 50%0 larger than the Ist quartile of estimates For about 90\2600 of the ranges created the difference between the I't and 3rd quartile is less than 75\2600 and for 97\2600 of the projects the top of the range is 100 or less than the bottom of the range 45\2600 of the true project costs falling into the new range When the top and bottom of the range were changed by 50\2600 the range estimates encompassed 77\2600 of the true project costs In the end however we felt that the adjusted ranges were too wide for practical use Adaboosting using binomial logit classifiers Short for adaptive boosting the Adaboost algorithm 6 is a type of machine learning algorithm where a set of classifiers are adjusted to favor previous misclassifications These classifiers are actually other estimating algorithms Percentage of Time 3rd Quartile no more than XO/o Greater than 1st Quartile 120 100 80 60 40 20 0 25 50 75 100 Figure 11 Variation in ACTS-produced estimates measured between 3rd and 1st quartile Percentage of time True Cost Falls in Range Produced by ACTS 3 variable ersion 90 8070 60 5040 30 20 10 l 0 Interquartile  or10  or25  or50  or75 Figure 12 The effect of widening the estimating range measured by  of estimates now lying within Of the eight projects that fell within the average percentage difference between the 1st and 3rd quartile was 55 with median of 52 This indicated that the inter-quartile range might be too restrictive We therefore proceeded to widen the range of cost estimates for each project multiplying the ISt quartile estimate by 1 X%0 and the 3rd quartile estimate by 1  X using various Xs In each instance we compared actual project costs to estimates produced and determined how frequently the actual fell into the range of estimates Results are summarized in figure 12 When the top and bottom of the range were changed by 10 32 of the actual estimates fell within the new range Extending each of the range boundaries by 25 resulted in Adaboost works by iteratively weighting observations so that poorly classified ones are given more weight during the next iteration With the algorithm focused on improving estimates for exceptional cases it is somewhat more sensitive to noisy data and outliers though less likely to over fit We chose to test this approach due to the latter property The boosting algorithm seemed structured particularly for binary decisions though we are seeking to estimate a continuous variable cost We therefore staged the method by estimating cost bands so that the estimate for each band reverted to a binary decision The algorithm would separately estimate whether an observation was 12 


our evaluation method tested potential models using either ordinary least squares least absolute above or below X dollars then Y dollars then Z such that instead namely that we were examining a proportion rather X  Y  Z than a binary outcome but this was impractical to use and we judged that the problem was not strongly biased towards The boosting algorithm still required a classifier and model either context For a classifier we implemented binary logit78 a discrete Adaboost MRE By COST Lu was similar to ACTS but with 0 1.00 0.75 0.50 0.25 0.00 0.25 0 50 0 75 1 00 1 25 1 50 1 75 2 00 Cost Figure 13 Adaboost magnitude of relative error sorted by actual cost Adaboost MRE CHRONOLOGICALLY Year Figure 14 Adaboost magnitude of relative error sorted chronologically choice model specifically intended for a binary dependant variable is the cost above or below this point There may have been some support for using the probit model 7 This was done by a statistical programmer with output checked against the statistics program STATA 8 we obtained from this first stage system We also attempted to use OLS as a classifier but our experiments showed no significant effect from adding relatively orthogonal i.e unrelated models Since using more than one model did not make a difference boosting using OLS was little more than a single OLS model with estimates divvied up in bands no dynamic re-weighting of classifiers and thus no boosting effect to speak of To obtain models some changes and improvements and again put were discarded and resolved by the latter 13 Lu 1.00 0.75 0.50 0.25 0.00 0 0.50 0.75 1 00 1.25 1 50 1 75 2 00 error which better discounts outliers and weighted least squares The models which we built another windowed test system that our computing cluster to use trying to find good ones from were then used to bootstrap the binary logit method although coefficients a sea of potential transformations This time 


This exercise required a number of decisions about how many classifiers to use how many bands to estimate and how many variables should be used in the models All these choices were made empirically Models with 3 variables were marginally more accurate than those with 4 More models seemed generally better to a point so we used 99 models We also used 22 cost bands checking whether an estimate was above or below 12M 35M 44M and so on through 519M It is interesting to note that there was no data scarcity-induced limit on the number of bands we could have used since each was estimated using all observations lying above and below and not just between bands As with the neural network training was carried out using 52 missions from the 1960s through 1996 and testing occurred on 58 missions from 1996 through 2007 The trend in estimate deviation would again permit us to gauge whether predictive accuracy decayed over time Results are shown sorted by cost in figure 13 and chronologically in figure 14 Figure 13 shows that MREs are large and negative for missions at 56M actual mission cost and below Above this point results are mixed although not as poor as at the very low range There seems to be no degradation in predictive accuracy as projects increase in scale The correlation between estimates and actual values is approximately 0.66 Sorted chronologically in figure 14 no trend can be seen as estimates go farther out up to 11 years as was seen with the neural network Again this may have been due to the normalization process in which we mapped each variable to its percentage ranking based on future and past minimum and maximum values basically embedding the effect of time within the factors being input to the model It also may be due to a relatively stable era Weighted combinations of simple models A persistent problem in multivariate estimation methods which has been mentioned is that a model may be produced that over fits the observations used for calibration It would thus not be adequately generalized to estimate out of that sample Techniques such as jack-knifing and outlier removal can mitigate this We chose another approach developing models that were little more than simple rules of thumb and then combining these in a weighted manner so each would have a say in the estimate The weightings were formulated using Excel's Solver to minimize estimation error on a set observations lying 15 to 25 years before the test set We refer to this method as continuous boosting because as with the Adaboost method it too involves a mix of classifiers that are combined according to their performance against a set of test observations Unlike Adaboost which solves a binary problem the algorithms are here estimating on a continuous scale Also unlike Adaboost the models are re-weighted rather than the observations to maximize prediction Before presenting results it is worth noting that we ran a control test where all variables from the simple rules of thumb were combined into a single model and run on the same windows performance was nowhere near as good We chose the models by hand guided by the criteria that they should be relatively orthogonal rely on differing parameters so they could each contribute a unique influence to the combined result Figure 15 shows that MREs are mostly positive though for lower cost missions errors are regularly negative and large Continuous Boost MRE By COST M M 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Cost Figure 15 Continuous boosting magnitude of relative error sorted by actual cost 14 


CONCLUSIONS Given that this project was intended to estimate missions lying 10-15 years out we structured it differently than one intended to estimate contemporary projects A variety of conventional techniques were not used as we felt they would over fit training observations and thus not be suitable for prediction We also were not sure which approach would work so we tried many We heavily favored ensemble methods where models are combined because we surmised that any one model could not be guaranteed to have the best view of the future However a single neural network ultimately yielded the most competitive results Another finding was that methods built upon simple models such as with three variables generally did work best not surprisingly because they were less likely to over fit calibration data A graph of the three best methods is shown in figure 18 with estimates for the same missions sorted by cost For the neural network calibration occurred until just before the results shown for continuous boosting calibration occurred no more recently than 15 years prior to the results and for Adaboost calibration also occurred up until the results shown A graph sorted by year is not shown we think it instructive that no results seemed to degrade over time As mentioned but obvious from figure 17 the neural network performs best followed by Adaboost and then continuous boosting It is interesting to note that the three cases in which the neural network goes haywire and predicts too low also correspond to worst performances for the Adaboost method pointing to exceptional data points We will be further investigating these regularly errant results and other outliers which may result in estimating improvements ACKNOWLEDGEMENTS This work was carried out under Small Business Innovation Research contract FS9453-05-C-0023 with the Air Force Research Lab The authors wish to gratefully acknowledge Judy Fennelly and later Ross Wainwright our technical points of contact at AFRL for their continuous support and encouragement Our contract officer Timothy Provencio also provided invaluable assistance Our critical seed stock of data was provided by Joseph Hamaker who previously was Director of NASA Headquarters Cost Analysis Division and now is Senior Cost Analyst with SAIC Ainsley Chong and Dale Martin USAF Ret also lent considerable assistance with data gathering APPENDIX A MISSIONS COLLECTED Active Cavity Radiometer Irradiance Monitor Satellite Active Magnetospheric Particle Tracer Explorer Adeos Advanced Communications Technology Satellite Advanced Composition Explorer Alexis Amos-I AMSC-1 Anik El Anik E2 Applications Technology Satellite-I Applications Technology Satellite-2 Applications Technology Satellite-5 All MREs Lu 1.00 0.75 l 0.50l 0.25 l 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75-2.00oi v 1   Cost o N X  X~~~~~Cl Figure 16 Comparison of estimating error for three best methods 15 


Applications Technology Satellite-6 Aqua Argos Atmospheric Explorer AURA Aurora 2 Calipso Cassini Cassini Spacecraft  Huygens Probe Chandra X Ray Observatory CHIPSat Clark Clementine CloudSat COBE Columbia 5 Contour CRRES DART Dawn DBS-1 Deep Impact Flyby Spacecraft  Impactor Deep Space 1 Deep Space 2 Defense Meteorological Satellite Program-5D Defense Meteorological Satellite Program-5D3 Defense Support Program DSCS 3 FIO DSCS 3 F7 DSCS I DSCS-II DSCS-IIIA DSCS-IIIB Dynamics Explorer-I Dynamics Explorer-2 Earth Observing Satellite 1 Earth Radiation Budget Experiment EchoStar 5 Extreme Ultraviolet Explorer Far Ultraviolet Spectroscopic Explorer FAST FLTSATCOM 6 Galaxy 5 Galaxy 11 Galaxy Evolution Explorer Galileo Orbiter  Probe Gamma Ray Large Area Space Telescope GE 1 GE 5 Genesis GFO 1 Globalstar 8 Glomr GOES 3 GOES 9 GOES N GPS-1 GPS-IIR GPSMYP GRACE Gravity Probe-B GRO/Compton Gamma Ray Observatory GStar4 Hayabusa HEAO-1 HEAO-2 HEAO-3 HESSI-II High Energy Transient Explorer-II HETE HST ICESat Ikonos IMAGE IMP-H Inmarsat 3-F5 Intelsat K INTELSAT-II INTELSAT-IV International Ultraviolet Explorer Iridium James Webb Space Telescope Jason 1 JAWSAT KEPLER KOMPSAT LANDSAT1 LANDSAT-4 LANDSAT-7 Lewis Lunar Orbiter Lunar Prospector Magellan Magsat Mariner-4 Mariner-6 Mariner-8 Mariner1 0 MARISAT Mars Exploration Rover Mars Express/Beagle 2 Mars Global Surveyor Mars Observer Mars Odyssey Mars Surveyor 2001 Orbiter Mars Pathfinder  Sojourner Rovers Mars Polar Lander Mars Reconnaissance Orbiter Mars Telecommunication Orbiter Mars Climate Orbiter Messenger Meteor Mid-course Space Experiment MightySat Milstar 3  Adv EHF Model-35 Morelos NATO III Near Earth Asteroid Rendezvous NEAR Shoemaker New Horizons 16 


NIMBUS NOAA 8 NOAA 15 NPOESS Preparatory Project Orbital Maneuvering Vehicle Orbcomm Orbiting Carbon Observatory Orbiting Solar Observatory-8 Orbview 2 Orion 1 P78 Pas 4 Phoenix Pioneer P-30 Pioneer Venus Bus/Orbiter Small Probe and Large Probe Pioneer-I 0 Polar QuikSCAT Radarsat Reuven High Energy Solar Spectroscopic Imager REX-II Rosetta Instruments Sage III SAMPEX Satcom C3 Satcom C4 SBS 5 SCATHA Seastar SMART-1 SNOE Solar Dynamics Observatory Solar Maximum Mission Solar Mesosphere Explorer Solar Radiation and Climate Experiment Solar Terrestrial Relations Observatory Space Interferometry Mission Space Test Program Small Secondary Satellite 3 Spitzer Space Telescope SPOT 5A Stardust  Sample Return Capsule STEPO STEPI STEP3 STEP4 STRV ID Submillimeter Wave Astronomy Satellite Surfsat Surveyor Swift Gamma Ray Burst Exporer Synchronous Meterological Satellite-I TACSAT TACSAT 2 TDRS F7 Tellura Terra Terriers Tethered Satellite System Thermosphere Ionosphere Mesosphere Energetics and Dynamics TIMED TIROS-M TIROS-N TOMS-EP Total Ozone Mapping Spectrometer TOPEX TRACE Triana Tropical Rain Measuring Mission TSX-5 UFO I UFO 4 UFO 9 Ulysses Upper Atmosphere Research Satellite Vegetation Canopy Lidar VELA-IV Viking Viking Lander Viking Orbiter Voyager Wilkinson Microwave Anisotropy Probe WIND WIRE XMM X-Ray Timing Explorer XTE XSS-iO XSS-ii APPENDIX B FIELDS COLLECTED Mission Mission Scenario Mission Type Launch Year Launch Vehicle Bus Model Bus Config Bus Diameter Location Expected Life Flight Profile Flight Focus Technical Orbit Currency Total Cost Including Launch Unit Sat Costs Average Sat Cost Production Costs Launch Costs Operations Cost Orbit Weight Wet Weight Dry Weight Max Power EOL Power BOL Power  of Batts 17 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


