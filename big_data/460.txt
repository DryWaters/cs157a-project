Interpretations of Association Rules by Granular Computing Yuefeng Li a and Ning Zhong b a School of Software Engineering & Data Communications Queensland University of Technology, Brisbane, OLD 4001, Australia E-mail y2.li@qut.edu.au b Department of Systems and Information Engineering Maebashi Institute of Technology, Maebashi 371-0816, Japan E-mail zhong@maebashi-it.ac.jp Abstract This paper presents interpretations for association k\222s method, and the corresponding algorithm of finding decision rules \(a kind of association rules\. It then uses extended random sets to teresting rules. It 
proves that the new algorithm is faster than Pawlak\222s algorithm. The extended random sets are easily to include more than one criterion for determining interesting rules They also provide two measures for dealing with uncertainties in association rules 1. Introduction ntion currently [7 T h e freq u e n c y  o f o ccu rr en ce i s a w e l l accepted criterion for mining association rules. Apart from the rules should reflect real world patterns 1 4 It i s d e s i rab l e  t o u s e s o m e  m a t h em at i cal m o d e l s t o  rpret association rules in order to obtain real world patterns The patterns hidden in data can be characterized by 
rough set theory [6  in w h ic h t h e  p r e m i s e s o f  a sso c i a t io n rules \(or called decision rules in [6  are i n t e rp ret ed  as  condition granules, and the post-conditions are interpreted as decision granules. The measure of uncertainties for decision rules is based on well-established statistical models. This only reveals the objective aspect of decision rules. However, knowledge in some applications is based on \215subjective\216 judgments In this paper, we use granular computing to interpret k\220s method [6 and formally describe the corresponding algorithm for determining strengths and certainty factors of decision 
rules. We then present a new interpretation of association rules using extended random sets [3  A n  e f f e c t ive  algorithm of finding interesting rules is proposed using the new interpretation. We also show that an extended random set can be interpreted as a probability function which can provide an \215objective\216 interpretation\r a belief function \(which can provide a \215subjective\216 interpretation 2. Databases to Decision Tables Let U be a non-empty finite set of objects \(a set of records\, and A be a set of attributes \(or fields\. We call a pair S  U  A  information table if there is a function 
for every attribute a 001 A such that a  U 002 V a where V a is the set of all values of a We call V a the domain of a  Let B be a subset of A  B determines a binary relation I  B n U such that x  y  001 I  B if and only if a  x  a  y  for every a 001 B where a  x enotes the value of attribute a for element x 001 U it is easy to prove that 
I  B s an nce relation, and the family of all equivalence s of I  B that is a partition determined by B is denoted by U  I  B or simply by U  B s in U  B are referred to B-granules or ts The class h contains x is called B-granule by x and is by B  x  A user may use some attributes of a database. We can divide the user used attributes into two groups: condition attributes and decision attributes, respectively. We call the 
tripe U  C  D  decision table of U  A f C 003 D 004 and C 005 D 006 A where U  C  D s and each class is the representative of a group of records For example, we assume that there is an information table \(relation\ that includes 1000 records of vehicle t of attributes is A  driver  vehicle type  weather  road  time  accident If the user t C  weather  road 
and D  time  accident  C 005 D determines a binary relation I  C 005 D n U and U is classified into 7 equivalence classes, as shown in Table 1 \(i.e. a decision table\ where N is the number of records in the corresponding class t of condition granules U  C 1,7}, {2,5}, {3,6}, {4}},  and decision granules U  D 1}, {2,3,7}, {4}, {5,6}}, respectively Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


In the following we let U  C  c 1  c 2  c 3  c 4 and U  D   d 1  d 2  d 3  d 4   Table 1 A decision table Class Weather Road Time Accident N 1 Misty Icy Day Yes 80 2 Foggy Icy Night Yes 140 3 Misty Not icy Night Yes 40 4 Sunny Icy Day No 500 5 Foggy Icy Night No 20 6 Misty Not icy Night No 200 7 Misty Icy Night Yes 20 3. Pawlak\222s Method Every class in a decision table can be mapped into an association rule \(or called decision rule in [6   e.g   cl as s  ble 1 can be read as 215 if the weather is foggy and road is icy then the accident occurred at night 216 in 140 cases For our convenience, we assume A  a 1 203 a k  a k+1  203 a m  C  a 1 203 a k and D  a k+1 203 a m where k 0, and m  k Every class f determines a sequence a 1  f  203 a k  f  a k+1  f 203 a m  f The sequence can determine a decision rule a 1  f  a k  f  002 a k+1  f 203 a m  f  or in short f  C  002 f  D  The strength of the decision rule f  C  002 f  D s defined as C  f  003 D  f  U and the certainty factor of the decision rule is defined as C  f  003 D  f  C  f  can use the following algorithm to calculate strengths and certainty factors for all decision rules, where we assume N i denotes the number of records in class i and UN denotes the total number of records in U  Algorithm 1 1 let UN 0 2 for i 1 to n  n is the number of classes UN  UN  N i  3 for i 1 to n   strength  i  N i  UN  CN = N i  for j 1 to n  if j 007 i d f j  C  f i  C   CN = CN + N j   certainty_factor  i  N i  CN  If we assume the basic operation is the comparison between two classes \(i.e f j  C  f i  C then the time complexity is n 1 327 n O n 2 where n is the number of s in the decision table r algorithm to determine interesting rules for Pawlak\220s method 4. Extended Random Sets Let U  C t of condition granules and U  D be the set of decision granules. To describe the relationship between condition granules and decision granules, we can rewrite the decision rules in Table 1 as follows c 1 002  d 1 80/100 d 2 20/100 c 2 002  d 2 140/160 d 4 20/160 c 3 002  d 2 40/240 d 4 200/240 c 4 002  d 3 500/500 These determine a mapping b from U  C to  1  0     2 327 DU  such that 1   000\246 b\001 snd i c snd fst for all c i 001 U  C where b  c i is a set of decision-granule numeral pairs  Now we consider the support degree for each condition granule. The obvious way is to use the frequency in the decision table, that is 000\246 001  i cx xi Ncw  1 for every condition granule c i where N x is the number of records in class x By normalizing, we can get a probability function P on U  C such that  000\246 001  CUc j i i j cw cw cP     for all c i 001 U  C   b  P  to represent what we can obtain from an information table We call the pair b  P n extended random set  According to the definitions in the previous section we can obtain the following decision rules and    2,1 i ciiiiii fst c fst c fst c b 002\002\002   for a given condition granule c i where       1,1 ii ciciiii snd  fst  snd fst c b\b b 2 We call snd cP   snd cP i ciiii 1  b 327\327 the strengths of these decision rules, respectively; and     1 i cii sndsnd b the certainty factors, respectively From the above definitions, we have     i jii ji c fst c snd 003   The above definitions about strengths and certainty factors are the same as Pawlak\220s definitions 5. Determining Interesting Rules Given an extended random set b  P it can provide a new s. Figure 1 shows a such Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


nting the extended random set that is obtained from Table 2, where U  C is the set of condition granules, and b  c i is the set of conclusions of premise c i  i 1, \203 U  C the process of ts, and the process of calculating of strengths and certainty factors of the decision rules U  C b  c i  c 1 002  d 1 0.8  d 2 0.2 c 2 002  d 2 0.875  d 4 0.125 c 3 002  d 2 1/6  d 4 5/6 c 4 002  d 3 1.0  Figure 1 An extended random set Algorithm 2 1 let UN 0 U  C  004  2 for i 1 to n  UN  UN  N i  3 for i 1 to n do \/ create the data structure if f i  C  001 U  C  insert  f i  D  N i o b  f i  C  else add  f i  C into U  C and set b  f i  C  004  4 for i 1 to U  C  P  c i 1 UN  327  snd i c snd fst 000\246 b\001   5 for i 1 to U  C normalization  temp 0 for \(j = 1 to b  c i   temp  temp  snd i,j  for \(j = 1 to b  c i   snd i,j  snd i,j  temp  6 for i 1 to U  C calculate rule strengths  for \(j = 1 to b  c i    strength  c i 002 fst i,j  P  c i  327 snd i,j   certainty_factor  c i 002 fst i,j  snd i,j   steps 4, 5, and 6 all traverse pairs in b  c i   i 1 203 U  C and the number of pairs in all b  c i   i 1, \203  U  C is just n the number of classes in the decision table\, the time complexity of this algorithm is determined by step 3. In step 3, checking f i  C  001 U  C takes O U  C  so the time complexity of the algorithm is O n 327  U  C  where the basic operation is still the comparison between classes. Since U  C  000\224 n Algorithm 2 is better than Algorithm 1 for the time complexity  jii fst c  002 is an interesting rule if   jiiji fst pr c fst pr 212 is greater than a suitable constant From the definition of mapping b we have jiiji snd c fst pr    To decide the probability on the set of decision granules, we present the following function  1  0      002 DU pr such that 000\246 b\001\001 327     ii c snd dCUc i snd cPd pr   3 We can prove that pr is a probability function on U  D  The algorithm of determining pr is only to traverse the data structure as showed in Figure 1 Table 2 Probability function on decision granules Decision Granule Description pr d 1  Accident occurred at night 0.08 d 2  Accident occurred in daytime 0.20 d 3  Accident not occurred in daytime 0.50 d 4  Accident not occurred at night 0.22 Table 2 shows the probability function on the set of decision granules. From Figure 1 and Table 2 we can obtain the probability of pr  fst i,j  c i for every decision rule c i 002 fst i,j where fst i,j 001  d 1  d 2  d 3  d 4 We can get 4 interesting rules \(seeTable 3\ if we assume that a decision rule jii fst c  002 is an interesting rule iff   jiiji fst pr c fst pr 212 0  Table 3 Interesting rules Rule Description pr  fst i,j  c i  pr  fst i,j  Interesting rule c 1 002 d 1 0.8 0.08 Yes c 2 002 d 2 0.875 0.20 Yes c 3 002 d 2 0.167 0.20 No c 4 002 d 3 10.5 Yes c 2 002 d 4 0.125 0.22 No c 3 002 d 4 0.833 0.22 Yes c 1 002 d 2 0.20 0.20 No 6. Discussions In this section, we discuss other advantages of our approach except the time complexity.  We first discuss the weight functions for condition granules. We also introduce another uncertain measure on decision granules Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


t Functions for Condition Granules The extended random sets are easily to include other criteria apart from the well-accepted criterion 215frequencies\216 when determining association rules Although the frequency is a well-accepted criterion for data mining, it is not the only criterion for support degree some condition granules with high frequencies may be meaningless. For example, when we use keywords usually consider both keywords frequency and inverse document frequency \(e.g., the popular technique tf  idf in information retrieval\because some words \(like 215information\216\ may have high frequencies in a document nts in a collection e.g., \215information\216 may appears in most documents in the information table collection In order to use the above idea, we assume there is a collection which contains many databases. Given a decision table U  C  D of a database, we can define a new weight function w on U  C to instead of the weight function in Eq. \(1\, which satisfies w  c i  000\246 001 i cx x N  327 log  M  n i  for every c i 001  U  C where M is the total number of databases, and n i is the number of databases which contain the given condition granule c i It is easy to do so because tion of decision rules 6.2 Uncertain Measures on Decision Granules t of decision granules is using a probability function. For a given set of decision granules X  d 1 203 d s we may use 000\246 001 Xx x pr  to represent the probability of d 1 or \203 or d s However, this measure is very sensitive to the frequencies of records To consider a relative stable measure, we consider a random set t  P see [2 5 w h i c h  i s  d e ri v e d  f r o m  t h e extended random set b  P  DU CU  2 002\t such that   ii c snd fstfst c b\001=\t for every c i 001  U  C  The random set t  P etermines a Dempster-Shafer mass function \(see [2 m P on U  D such that      XcCUccPXm iiiP t\001  4 for every X 006 U  D   This mass function can decide a belief function and plausibility function \(see [2  as  w e l l  T h ey  ar e d e f i n e d  as  follows  1  0  2   002 DU m bel   1  0  2   002 DU m pl and 000\246 006  XY Pm YmX bel   000\246 004\007\003  XY Pm YmX pl  5 for every X 006 U  D  We can prove that bel m  X  n 000\246 001 Xx x pr  n pl m  X r every X 006 U  D  Domain experts can use the interval bel m  pl m  o  check if their \215subjective\216 judgments for some descriptions are correct.  Table 4 shows the uncertainty measures for some descriptions Table 4 Uncertain measures Description Subset Pr m P  bel m  pl m  215 d 1 or d 2 216 {d 1 d 2 0.28 0.1 0.1 215 d 3 or d 4 216 {d 3 d 4 0.72 0.0 0.5 215 d 2 or d 3 216 {d 2 d 3 0.70 0.0 0.5 215 d 1 or d 4 216 {d 1 d 4 0.30 0.0 0.0 7. Conclusions This paper uses granular computing to interpret association rules. The main contribution of this paper is ts is used to describe the relationships between condition granules and decision granules. It presents a new efficient algorithm to from the 215frequencies\216, the extended random sets are easily to include other criteria when determining association rules ts also provide more than one measure for dealing with uncertainties in the association rules significantly References  ohe n, e t al  Fi nding i n te r e st i n g as s o c i a t ions w i t h out support pruning IEEE Transactions on Knowledge and Data Engineering 2001, 13\(1 64-78 2 R  Kru s e, E  S c h w ec ke an d J Hei n s o l n   y and vagueness  in knowledge based systems \(Numerical Methods  Springer-Verlag, New York, 1991  L i  Ex te nde d r a ndom s e ts f o r  k n ow le dg e  di sc ov e r y  in information system, in Proc the 9 th International Conference on Rough Sets, Fuzzy Sets, Data Mining and Granular Computing China, 2003, 524-532 4 T. Y  Li n  Th e lat ti ce st ru ctu r e o f  d a t a b a se an d  m i n i n g  multiple level rules Bulletin of International Rough Set Society 2002, 6\(1/2\: 11-16    L i u a n d Y L i  T h e  int e rpr e t at i on of g e ne r a li z e d  e v i d e n ce  theory Chinese Journal of Computers 1997  20\(2 158-164  P a w l ak   I n purs ui t of pa tt e r ns in da ta re a s oni ng  f r om  d a t a  the rough set way 3 rd International Conference on Rough Sets and Current Trends in Computing USA, 2002, 1-9    Ra s t og i a nd K S h im  M i ni ng  opt im i z e d a s soc i a ti on r u l e s with categorical and numeric attributes IEEE Transactions on Knowledge and Data Engineering 2002, 14\(1 29-50 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


Fear Dur.Contr 4 i Solemnity fast/lom I DurConb 0  Anger 222  DurConb 2  221x   slow soft Factor 1 222  Tenderness  DurContr 4 222 Sadness DurConb 2 1 F U 10 c 3  25 B 20  m Figure 4 Two-dimensional space of the substantives derived from principal component analysis of the different emotional rule setups a Ekom o Mazurka Anger Happiness i a U  5 H  10 m q I Tenderness I Sadness 15 1 I I 100 50 0 50 100 Average 101 Deviation 224 Figure 5 Average 101 deviations versus average sound level deviations for all six performances of each piece The bars represent the standard deviations REFERENCES Gabrielsson A Intention and emotional expression in music performance In A. Friberg J lwarsson E Jansson  J Sundberg \(Eds Proceedings of the Stockholm Music Acoustics Conference 1993 Stockholm Royal Swedish Academy of Music, 1994, 108- 1 1 1 Juslin P.N Emotional communication in music performance a functionalist perspective and some data Music Perception 1997 I4 4 383-418 Juslin P.N Can results from studies of perceived expression in musical performances be generalized across response formats Psychomusicology in press Canazza S De Poli G Rinaldin S  Vidolin A Sonological analysis of clarinet expressivity In M. Leman Ed Music gestalt and computing: studies in cognitive and systematic musicology Berlin, Heidelberg New York Springer Verlag 1997 43 1-440 Battel G.U  Fimbianti R How communicate expressive intentions in piano performance In A Argentini  C Mirolo Eds Proceedings of the XII Colloqium on Musical Informatics Udine AIMI 1998 De Poli G Rod4 A  Vidolin A A model of dynamic profile variation depending on expressive intention in piano performance of classical music In A Argentini  C Mirolo Eds Proceedings of the XI1 Colloqium on Musical Informatics Udine: AIMI 1998,79-82 Orio N  Canazza S How are expressive deviations related to musical instruments? Analysis of tenor sax and piano performances of 223How High the Moon\224 theme In Argentini A  Mirolo C Eds Proceedings of the XII Colloqium on Musical Informatics Udine AIMI 1998 Juslin P.N Perceived emotional expression in synthesized performances of a short melody capturing the listener\222s judgment policy Musicae Scientiae 1997b 1 2 225 256 Canazza S De Poli G Di Sanzo G  Vidolin A Adding expressiveness to automatic musical performance In A Argentini  C Mirolo \(Eds Proceedings of the XU Colloqium on Musical Informatics Udine AIMI 1998 7 1-74 Gabrielsson A Expressive intention and performance In R Steinberg Ed Music and the Mind Machine the Psychophysiology and the Psychopathology of the Sense of Music Berlin Heidelberg New York Springer Verlag 1995,35-47 Friberg A A Quantitative Rule System for Musical Expression Doctoral dissertation Stockholm Royal Institute of Technology 1995a Bresin R Friberg A Emotional expression in music performance: synthesis and decoding TMH-QPSR Speech Music and Hearing Quarterly Progress and Status Report 411998 Stockholm pp 85-94 Friberg A Generative for music performance a formal description of a rule system Computer Music Journal Friberg A Bresin, R Frydtn L  Sundberg J Musical punctuation on the microlevel Automatic identification and performance of small melodic units Journal of New Music Research 1998,27 3 271-292 Friberg A Matching the rule parameters of Phrase arch to performances of 223Tr2umerei\224 A preliminary study In A Friberg  J Sundberg Eds Proceedings of the KTH Symposium on Grammars for Music Performance Stockholm Speech Music and Hearing Department Friberg A  Sundberg J Does music performance allude to locomotion A model of final ritardandi derived from measurements of stopping runners Journal of the Acoustical Society ofAmerica 1999 IO5 3 1469 1484 Cope D Computer modeling of musical intelligence in experiments in musical intelligence Computer Music Journal 1992,16 2 69-83 Granqvist S Enhancements to the Visual Analogue Scale VAS for listening tests Quarterly Progress and Status Report Stockholm Royal Institute of Technology  Speech Music and Hearing Department 1996,4,61-65 67-70 75-78 1991 I5\(2 56-71 1995b 37-44 LINKS Sound and MIDI fires of the seven versions of Ekorrn and Mazurka http://www.speech.kth.se/-robertolemotion KTHperformance rules description and sofmare http://www speech.kth.se/music/performance IV-321 


Fear 180  7 c 120 m 60 a U    0 eo z 60 0 200 400 6M 800 1000 Nominal 101 ms Anger 180 0 MO 4Gu 600 Boo Nominal 101 ms Happiness 180   120 0 V  m  d60 0 Eo 2  a 40 0 400 6w lo00 Nominal 101 ms Solemnity 0 200 400 600 800 loo0 Nominal 101 ms Sadness 180 a Eo s 60 Nominal 101 ms 1 00 Tenderness 180  4 120 0 e   IM P so 3 a m 60 0 400 600 Boo lo00 Nominal 101 ms Figure 6 Relative deviations of IO1 and for all six performances of each piece Negative values imply a tempo faster than the non expressive versions, while positive deviations indicate lengthening of tone duration and thus a slower tempo. The bars show the standard deviations IV-322 


main\(atgc. argv  cvm_startup\(argc, argv spacep  cvm-alloc\(size cvm-create-procs\(apriori apriorio cvm-finish 1 number of 1 nodes 2 4 VISIONA 1 2 type CVM apriori  r Apriori algorithm 221I k=l while \(candidate item sets of k-th pass are not emply  r step of k-th pass  while \(\(read data  emply  while item sets of k items are selectable from the data  search the itemset in the hash tree cvm-lock\(&mt[fieldp  4001 222fieldp cvm_unlock\(\(Lmt[fieldp  4001 1 1 cvm-barrier\(0 make the large itemsets of k by comparing the support values of the candidate itemsets with the minimum support value make the candidate item-sets of k+l out of the large itemsets of k k I I pass-1 pass-2 pass-3 pass-4 total 1 4 1 rscc sec ec e 15 21 4 1 41 39 44 9 1 93 1 4 2 1 8 1 2 1 1 5 step-I step-9 step-3 step-4 step-5 step4 step-7 step-8 step-9 step-IC Figure 5 Pseudo code for association rule mining executed in CVM consistency mechanism on the DSM for scalable data mining in both parallel and distributed computing en vironments l and 2 show the basic algorithm of as sociation rule mining which is popular among the data mining research community 9 shows a modification of l and 2 to decrease the number of the candidate item-sets to increase the performance lo shows the parallel algorithm of l and a for cluster-type dis tributed systems with message passing programming model 51 shows bayesian network generation which is a new technique of data mining New techniques for data mining and speedup are important research areas In the past new techniques or speedup is performed for each application indepen dently Our approach is different from the past re search We use DSM as a common tool for scalable data mining programs The objective is to develop scalable Table 6 Performance of association rule mining in CVM and VISIONA I I 4 1 21 11 1 5 data mining programs efficiently To decrease the over head of the DSM we proposed the LBC mechanism on the DSM. Also this paper described VISIONA a pro totype to implement the DSM with LBC To evaluate the effectiveness of the DSM with LBC we have implemented VISIONA both in PC clusters and UNIX computer clusters Programs of association rule mining and bayesian network generation has been also implemented in VISIONA According to t,he results of the evaluation of the DSM with LBC speedup by increasing the number of processors on the DSM with LBC is greater than or equal to that of SMP-type parallel computer In the future we will improve the VISIONA as a DSM system with multiple consistency protocol including the LBC mechanism The goal is to increase the adaptability to many different data mining programs References l R Agrawal T Imielinski A Swami 224Min ing Association Rules between Sets of Items in Large Databases,\224 Proceedings of ACM SIGMOD pp.207-216 May 1993 a R Agrawal R Srikant 224Fast Algorithms for Min ing Association Rules,\224 Proceedings of the 20th VLDB Conference pp.487-499 September 1994 3 C Amza A L Cox S Dwarkadas P Keleher H Lu R Rajamony W Yu W Zwaenepoel 224Tread Marks Shared Memory Computing on Networks of Workstations,\224 IEEE COMPUTER Vol 29 No 2 pp.18-28 February 1996 4 P Keleher 224The relative importance of concur rent writers and weak consistency models,\224 Pro ceedings of the 16th International Conference on Distributed Computing Systems pp 9 1-98 Nl ay 1996 5 D Heckerman 224Bayesian Networks for Knowl edge Discovery,\224 Advances in Knowledge Discov 149 


ery and Data Mining AAA1 Press/The MIT Press pp.273-305 1996 6 D Lenosla J Laudon T Joe D Nakahira L Stevens A Gupta J Hennessy 224The DASH Pro totype Implementation and Performance,\224 Pro ceedings of the 19th International Symposium on Computer Architecture pp.92-103 May 1992 7 F T Chong B Lim R Bianchini J Kubiatow icz 224Application Performance on the MIT Alewife Machine,\224 IEEE COMPUTER Vol 29 No 12 pp.57-64 December 1996 8 P Keleher A.L Cox S Dwarkadas W Zwaenepoel 224An Evaluation of Software-Based Release Consistent Protocols 224 Journal of Parallel and Distributed Computing Vol 29 pp.126-141 October 1995 9 J S Park M Chen P S Yu 224An Effective Hash Based 4lgorithni for Mining Association Rules,\224 Proceedings of ACM SIGMOD pp.175-186 June 1995 lo E Han, G Karypis V Kumar 224Scalable Parallel Data Mining for Association Rules,\224 Proceedings of ACM SIGMOD pp.277-288 May 1997 150 


Figure 8 Visual interface for Moridou system Search EngineTest Page 0 UI 0 5 5 Keyword plealet Figure 9 Prototype system in hcterogeneous environment 283 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


