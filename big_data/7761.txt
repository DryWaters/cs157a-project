2016 International Conference on Circuit, Power and Com    Parallel Frequent Itemset Mining with Spark RDD   Framework for Disease Prediction   Rini Joy Department of Information Technology Toc H Institute of Science and Technology Kochi Kerala India   rinijoy20@gmail.com     Sherly K.K Assoc. Prof. Department of Information Technology Toc H Institute of Science and Technology Kochi  Kerala India  sherly.shilu@gmail.com    Abstract The aim behind frequent itemset mining is to find all common sets of items defined as those itemsets that have at least a minimum support.  There are many well known algorithms for frequent itemset mining.  Some of which are Apriori, Éclat RElim, SaM, and FP-Growth.  Although each of these algorithms is well formed and works in different scenarios, the main drawback of these algorithms is that they were designed to perform on small chunks of data. These limitations were imposed based on time that they were developed. The notion of big data was not up and running at these times. So in the present scenario these algorithms won’t perform well on the current statistics of data present.  So we propose a new approach of implementing these well known algorithms on a parallelized manner so that it can handle the data perfectly. The proposed work parallelizes, dynamic frequent itemset mining algorithm Faster-IAPI with spark RDD framework. The main goal of selecting Apache Spark is that it overcomes the limitations of the Hadoop architecture which was basically designed to handle big data processing in a parallelized manner. The main drawback of the architecture was that it doesn’t handle the Iterative algorithms very well. This drawback is rectified in spark which handles it well. In this approach this algorithm is applied to find correlation between different symptoms of patients in faster and efficient manner and provides the support for the prediction of occurrence of disease based on the symptoms Keywords Frequent itemset mining, Faster-IAPI algorithm Spark RDD, Parallel computing, Disease prediction  I.       INTRODUCTION  The world is cornered by issues that affect the daily life of mankind. Amongst these issues the health of a person promotes itself to be of paramount concern. Many diseases have existed and continue to evolve in the world today. The only means to diagnose a disease before it becomes fatal to a human being is by analyzing its respective symptoms. Even a newly formed disease leaves behind a trail of symptoms by which its future occurrence can be depicted This work aims at collectively mining the information obtained from the diagnosis of the symptoms of various patients and using that information a prognosis is made as to what disease a patient is affected by. The architecture of the work includes a framework by which all the doctors around the world can contribute their diagnostic expertise   to   a centralized   repository The   Faster-IAPI algorithm is applied to this repository to find the correlation between different symptoms of the disease and map it on to the disease. Faster-IAPI algorithm has many noteworthy features compared to other frequent itemset mining algorithm [1 h is alg o r ith m  co m e s up  w i t h  generating frequent itemset from incremental massive dataset in two database scan. Other benefits of this algorithm are it is efficient, scalable, faster and dynamic. Another remarkable feature of this   algorithm is that it supports   incremental updating of frequent itemset without rescanning entire database. Faster-IAPI algorithm come up with four phases during first phase it generates all the frequent itemset from large data sore, In second phase it holds the newly arrived transactions   to the existing set and updates the frequent itemset to provide incremental mining, Third phase covers eliminating old transaction after a preset time period and modifies the patterns to accommodate the behavioral change At the last phase it serves a facility to adjust minimum support value as per the user’s requirement. The Spark RDD framework is used to parallelize the Faster-IAPI Algorithm  Batch p r o c e s s i ng w o r k lo ad can b e  e x ec ut e d eff e c tiv e l y  in Spark In Hadoop where batch processing workload is not present, optimized result cannot be delivered due to two reasons; first is lack of iteration support and second is high latency due to persisting intermediate data onto disk .By taking into account the strengths and weakness of Hadoop Spark has contributed to bigdata community by promising parallelization for big data analytics. With the help of Spark applications can be made quickly by Java, Scala, python Spark endorses flexible DAG -based \(directed acyclic graph dataflow and in-memory data sharing across DAG due to which different jobs can work with the same data. The disadvantage of Hadoop is overcome in Spark by holding intermediate result in memory instead of writing them back to disc. By this the benefit of Spark RDD framework is that it can work on the same dataset multiple times . RDD \(Resilient Distributed Dataset\is the heart of Spark framework.RDD can be compared to a table in a database that can hold any type of data. Data in RDD is stored in different partition by Spark. By this, rearrangement of computation and optimization of data processing can be achieved. Since a RDD knows how to recreate and recomputed the datasets, it is fault tolerant. On modification of a RDD a new RDD is obtained but the original RDD remains the same. A transformation function takes a RDD and return a single RDD. There are the two operations of RDD.  A transformation function which takes a RDD and returns a single RDD and an Action function computes all the data processing queries present and the result value is returned[15,16    978-1-5090-1277-0/16/$31.00 ©2016 IEEE 


2016 International Conference on Circuit, Power and Com    II.       RELATED WORK  The huge research efforts devoted to a variety of sophisticated and efficient algorithms to find frequent item sets.  Among the  best-known approaches are  Apriori Eclat FP-growth,SaM,RElim. Apriori algorithm [3  is t h e m o st  classic and most widely used algorithm for mining frequent itemsets which generate Boolean association rules. Apriori 2 ith m  fi nds all the f r eq u e nt ite m s et s  b y  sca n ni n g  t h e database repeatedly and it will consume a lot of time and memory space. When scanning large dataset, which will become the difficulty of Apriori algorithm [4 EC LA T  algorithm [5 de ri v e s f r o m A p r i ori  a l g o ri t h m   s o bot h  t w o  algorithms share the same theoretical base. ECLAT algorithm scans a continually updating database and then it updates the database. Each item in the databases represented by set of transaction IDs \(called a tidset\, which adopts a vertical layout and also implements a depth first search to represent the database. Tidset of an itemset is generated by intersecting tidsets of its items. The depth first search makes the downward closure strenuous in utilization. However; using tidsets has an advantage that there is no need for counting support. The size of the tidset affects the running time and memory usage of éclat because the main operation of the éclat is intersecting tidset. More time and memory are needed for bigger tidset. FP-Growth [6  f o llo w s  a d i v i d e a n d c o n qu e r  strategy and a FP-tree data structure   to   achieve   a compact   representation   of   the transaction database. It   is currently one   of   the   fastest algorithms for frequent itemset mining. It uses a sophisticated and rather complex data structure and thus requires loading the transaction database into main memory.  SaM split and merge\[7 al gor i th m utilizes only a single transaction list which is purely a horizontal representation and it stored as an array. This array is processed with a simple split and merge scheme and it computes a conditional database. The conditional database is processed recursively and finally eliminates the split item from original database. Advantage of this scheme is its simple data structure and processing scheme, easy to implement, and very convenient to execute on external storage.  Limitation of this scheme is, SaM struggles on sparse” data sets, In RElim \(Recursive Elimination\8 algorithm the transaction database is preprocessed in essentially the same way as for the SaM algorithm. The main difference is instead of listing all transactions in one array, they are grouped according to their leading item These lists are sorted in descending order with respect to the frequency of their associated items in the transaction database: the first list is associated with the most frequent item, the last list with the least frequent item.  Faster-IAPI algorithm provides some added features like incremental and interactive mining support and another feature is that it is capable of generating frequent itemset mining from incremental massive dataset in two database scans  MapReduce is a progra m m i n g m o d e l w h ic h was introduced by Google. It includes two functions map and reduces which is used to preserve data generated by real world application on to the disc. The underlying system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. This has proved to be very easy in use for a wide variety of users but the disadvantage is that it does not implement full memory in hadoop cluster. The iterative structure of frequent itemset mining does not fit well into the MapReduce framework [14  b e c a us e in e a ch rou n d a n e w MapReduce job need to read the data from and write back to HDFS which will create a lots of I/O overhead and increase time cost as w m p r ove the  performance we propose Spark RDD framework  f o r  parallelize Faster-IAPI   algorithm   and   can   achieve better   memory utilization and can avoid many overheads  III.       PROPOSED WORK  The proposed work is divided into 3 sections 1.  Data Preprocessing 2.  Frequent Pattern Mining 3. Correlation  As illustrated in Fig. 1 first, evaluation of all the symptoms pertaining to each disease known today, doctor can add   their   evaluation   of   the   disease   to   the preprocessing framework.  Data preprocessing framework provide a standard format for data submitted by doctor. The processed data is fed to the Faster-IAPI algorithm that run in spark RDD framework, then frequent patterns generated by  the  system  is  correlated  to  predict  the disease    Fig .1 Proposed architecture   A  Medical Data Preprocessing  Dataset is taken from UCI repository. Data set has both numeric and nominal values. The table 1 shows the result before preprocessing the data set. The data preprocessing framework provides an endpoint for doctors around the world to  submit  the  patient  details  for  the evaluation  of  their diseases. The data submitted by the doctors from around the world is saved into the data preprocessing framework. The framework provides a standard format for submitted data so that everyone can use the endpoint, integrate it into their application. From the above 14 attributes, the listed features such as age, trestbps Chol, thalach and oldpeak are numeric attributes and  the remaining  9  comes  under  nominal.  The primary  step  of preprocessing  involves  the  conversion  of numeric attribute  to  nominal  attribute and  fill  the  missing values in the dataset. After cleaning, dataset is trained for accuracy. Last step is to validate the dataset to acquire optimal redundant feature for prediction [10    


2016 International Conference on Circuit, Power and Com    TABLE 1 HEART ATTACK PREDICTION ITEMS IN THE DATASET  Age Thalach\(maximum heart rate achieved Sex Exang\(exercise induced angina \(1 = yes; 0 = no Cp\(chest pain type Oldpeak\( ST depression induced by exercise relative to rest Trestbps\( resting blood pressure \(in mm Hg on admission to the hospital Slope\(the slope of the peak exercise ST segment Chol\(serum cholestoral in mg/dl Ca\(number of major vessels 0-3\olored by flourosopy Fbs\(\(fasting blood sugar 120 mg/dl\\(1 = true; 0 false Thal\( 3 = normal; 6 = fixed defect; 7 = reversable defect Restecg\(resting electrocardiographic results N um\(diagnosis of heart disease \(angiographic disease status   A  Frequent Pattern Mining  Faster-IAPI Algorithm  Faster-IAPI algorithm come up with four phases during first phase it generates all the frequent itemset from large data sore, In second phase it holds the newly arrived transactions to the existing set and updates the frequent itemsets to provide incremental mining, Third phase covers eliminating old transaction after a preset time period and modifies the patterns to accommodate the behavioral change At last phase it serve a facility to adjust minimum support value as per the user’s requirement. In this algorithm transaction items are pre-processed and arranged according to the  item  code so  that  individual item counting and  count comparisons are made faster. Another remarkable feature of this algorithm is that it uses a range of support values Sl, Sh  for making the dynamic and the interactive mining faster. If any  existing  frequent  itemsets  is  found  to  be  infrequent remove it from  the Fset list 1 1  T h en i n cl u d e it w ith the NFset list if it’s support  Sl along with the current partition number. Further find the count of its subset and if found to be frequent include it in the Fset Then find the new frequent itemsets having support Sh in the new partition. If any, then find its count in other partitions by searching it in the NFset  list.  If  not obtained, conduct a possibility test, if there is possibility to be frequent, search its count in the entire dataset. If found to be frequent include it in the Fset list else if nearly frequent, include  in NFset list  else  discard it.  Here  the  dataset  is logically divided into small sized non-overlapping horizontal partitions of user specified sizes so that each partition can be   accommodated in   the main   memory.   To reduce the computational cost, I/O overhead as well as space complexity, each frequent item transaction group is collected separately and four level filtering is done to remove infrequent items. In first level it removes all the global infrequent items from the selected transaction, at the second level it remove items whose support count less than the selected frequent item, during third level it select the itemsets which have length k or more to obtain the frequent k-itemset, at the last level it consider only the itemset which are frequent in a set where a frequent item belongs to In the first scan, Faster IAPI generates frequent 1 itemsets. To generate higher frequent itemsets, do the second scan of database and collect only frequent items from each transaction. The drawback is that it requires more memory multiple memory buffers\to hold the different frequent item transaction groups. Prepare a co-occurring item list Cf r each frequent item, i.e. items having support greater than or equal to the support of that item which is considered for the frequent itemset selection. Then form separate transaction set groups of each frequent item and keep only the corresponding co-occurring items of each transaction group in separate buffers. Also eliminate the similar transaction entry in each transaction group \(compress the transaction set\by recording the occurrence count, then find the frequent items in the selected group and eliminate others. Then the higher frequent itemsets of each transaction groups are obtained sequentially Frequent itemset generation steps of Faster-IAPI algorithm are given below 1  Faster-IAPI Algorithm \(PhaseI Input  D Transaction database contain N transactions T 1 T 2  T N horizontally partition D into n non-overlapping partitions  P 1 P 2 P n d sort the items of each transaction in the order of item code  Sl low minimum support value Sh user selected min. support Sh  Sl  Output: Complete set of frequent itemsets 1.    For each partition do  Read each transaction and find frequency flocal\(i for each item i  2.    Identify frequent 1-itemsets F 1-itemset i  flocal i  Sh for each item i  3.    Sort F 1-itemset in ascending order F 1-sorted  f 1  f 2 f m   4.    Prepare co-occurring itemset list  Cf i   f i+1 f i+2 f m frequency f i+1 f i+2 f m   frequency f i for each f i   5.    Read each transaction of D and do Collect transactions contain each f i in to separate buffers and remove items that are not in the Cf i list from each transaction  6.    For each f i transaction group  i Find frequency of each Cf i item in the selected f i  transaction group  ii If frequency Cf i   Sl   F 2-itemset  Cf i for each Cf i item iii Else remove Cf i from the selected buffer iv Sort F 2-itemset in ascending order v To obtain higher frequent itemsets of F 2  


2016 International Conference on Circuit, Power and Com     7.  for each item in F 2-itemset do  F itemset  Higher-frequentItemset-Generate  f i  transactions \(Buffer 1 F 2-itemset   If support \(F itemset   Sh then Fset F itemset   Else NFset  F itemset   Procedure Higher-frequentItemset-Generate  f i transactions Buffer 1  Fn  Fn p p th item of F n-itemset   1. Collect fi transactions contain Fn p to a newtemporary buffer n and remove items having count  Fn p from each transaction in buffer n  p initialized to 0  2    Find frequency of each item in the selected Fn p  transaction group  3 F n+1\set  Fn p+k frequency Fn p+k   Sl for each Fn p+k item where k 1 to m-p   4.    else remove Fn p+k from the Fn p transactions  5.    Sort F n+1\set in ascending order  6.    To obtain higher frequent itemsets of f i do  7.    if  F n+1\mset  hen  n = n+1 & Repeat above steps  8.    else if p < size\(F n-itemset n  p = p+1 & repeat above steps  9.    else remove  buffer n content n=n-1, p=1 10   if n 2 repeat above steps  11  else return F itemset   2  Incremental mining \(Phase II   Faster-IAPI algorithm support incremental mining by utilizing NFset nearly frequent itemsets\it is possible to add new transaction to existing frequent itemset without rescanning the entire database. The initial step of this process is to add new transaction to a separate partition and count of items in the new partition is calculated and added with the existing count to update the  frequent items. Frequent item transaction groups of the newly added partition are collected to separate buffers and find their higher frequent itemsets in the new partition. Then update the count of the existing Fset frequent itemset belongs to each group. If any existing frequent item/itemsets is found to be infrequent, remove it from the Fset list and include it with the NFset list if it’s support  Sl along with the current partition number. If any new frequent itemset obtained from new partition, obtain its global count using NFset If not obtained from NFset then conduct a possibility test using \(1\and if possible to be frequent find their global count from the old transaction set  S*Z+\(Pc-Pi-1\\(U*Z-1\+L*Pi*Z-1 Pc*U*Z 1  Where Pi no. of partitions used for initial pattern generation Pc current  partition  no Z Partition  size L Lower minimum support U Upper minimum support S New Fset support in new partition The Apache Spark as the base to create a framework that parallelizes the computational process [13  T h e f r a m e w o r k is sh o w n in  t h e f i g 2      Fig .2 Apache Spark Framework    B  Correlation  The  correlation  phase  reads  the  values  from  the output of the algorithm and computes the relation between the disease and symptom. For example: the association rule generated from the identified frequent itemset {\(cp, thalach thal Num\, with support count  Sh correlates as there is more possibility for heart condition. This method also can also apply to predicting  the  chance  of  occurrence of  Jaundice Cancer and Kidney failure  IV IMPLEMENTATION   Proposed method can be implemented with Apache Hadoop with Spark-1.2.0 in Ubundu platform. Dataset can be collected from UCI repository. Only 14 attributes from the dataset are required for prediction. The priority of each attribute is to be considered. The Relative importance of attributes can be listed as cp,thalach,thal,oldpeak,ca,age exang[10 o s ed m e th o d can be implemented with 3 modules A  User Module In this approach authorized user can add symptoms of patient to system. System will compare added symptoms value with symptoms of different patients stored in dataset and will predict the chance of occurrence of heart attack B  Frequent Pattern Mining Faster-IAPI algorithm is implementing with SparkRDD framework. For that first we need to load the transaction data from HDD into Spark RDD framework. For the purpose of loading spark allows direct transition of flat files into RDD and is done using textFile method in spark context. Once the file is loaded, the count of each individual element symptoms\d using flatMap and reduceByKey methods respectively. Once the element count of each symptom is identified each symptom is taken from the list and is filtered based on a thre shold. The resultant symptoms and its co-occurring symptoms ar e identified and the count is 


2016 International Conference on Circuit, Power and Com    calculated. The symptoms and co-occurring symptoms is combined to find higher frequent item set \(set of frequently occurring symptoms unt is calculated. The filtering based on threshold is applied once again to eliminate the infrequent items. The result is written back to file suing save As Text File method of spark context C  Association rule mining In this module, need to discover interesting rules using frequent patterns generated by Spark RDD framework Once the Frequent pattern mining is completed each element in the frequent pattern is iterated to find the confidence. Once the support and co nfidence is calculated the rules are generated based on the minimum threshold. The rules are written back to the file. This rule helps to predict the occurring chance of heart attack  V CONCLUSION    This paper proposes a mechanism for disease prediction based on Faster-IAPI algorithm. The algorithm support's incremental mining and it is capable of generating frequent patterns from a massive data store in two database scan Our work guarantees the advantage o f i n memory computation, over other models using spark RDD framework  REFERENCES   1     S h e r ly   K  K   Ne dunc h e z h ia n   E f f i c i e n t A d a p tiv e  F r e quent  P a t t e r n  Mining Techniques for Market Analysis in Sequential and Parallel Systems International Arab Journal of Information Technology in press \(scheduled to publish in July 2017, 14\(4  2      N i ng  L i   L i  Ze ng   Q i ng  H e    Z h o n g z hi  S h i    P a r a l l e l  Im plem e n ta ti o n  of  Apriori Algorithm Based on MapReduce Proceedings   of the 13th ACIS   International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing May 2012  3    R    A g r a w a l    T   I m i e l i ns k i      A  N   S w am i   M i ni n g a s s o c i a t i o n  rules between sets of items in large databases in ACM SIGMOD International Conference on Management of Data, Washington 1993   4  R    A g r a w a l   R   S r i k a n t   F a s t  a l g o r i t h m s  f o r  m i n i n g a s s o c i a t i o n  rules 20th International Conference on Very Large Data Bases, Washington  in 1994  5 M J  Z a k i  S   P a r t h a sa r a t h y   M  O g ih a r a    W   L i   N e w al g o r i t h m s  for fast discovery of association rules   Third International Conference on Knowledge Discovery and Data Mining in 1997  6  Ha n  J  P e i   Y Yi n   M i n in g f r e q ue nt  pa tt e r n s  w ith ou t c a n d id at e  generation   ACM  SIGMOD International Conference on Management of Data, Texas in 2000  7    Chr is t i a n   Bo r g e l t  X i a o m e ng W a ng    S a M   A  Sp li t  a n d     Merge Algorithm    for Fuzzy Frequent Item Set Mining  2010    8 C h r i s t i a n B o r g e l t   S i m pl e A l go ri t h m s  fo r F r eque nt I t e m  S e t M i nin g  IFSA/EUSFLAT conf 2009  9 Je ffr ey D e a S a n j ay G h emaw at  M a p Re du c e  S i m p l i fi e d  D a t a  Processing    On Large Clusters communication of ACM/vol. 51,No.1 in January 2008    r a i r a j  M   S i vago w r y  S  A Prag m a ti c A p p r oa c h   o f      Preprocessing the Data Set for Heart Disease    Prediction international journal of Innovative Research in computer and communication Engineering vol. 2,Issue 11,November 2014  11   S h e r ly   K   K   N e dunc he z h ia n  M Ra jal a k s h m i   I A P I  Q u a d Fil t e r  An Interactive And Adaptive Partitioned Approach For Incremental Frequent Pattern Mining Journal of Theoretical and Applied Information Technology May 2014   Hon g j i a n  Q i u R o n g  G u  C h unfe n g  Yu a n   Yih u a  Hua n g    Y A F IM  A Parallel Frequent Itemset Mining Algorithm with Spark IEEE 28 th  International Parallel and Distributed Processing Symposium Workshop May 2014   M a t e i Z a ha r i a  M o sh a r af  C h ow d h u r y   T a t h ag at a Das   A n ku r Dave  Justin Ma, Murphy McCauley, Michael J. Franklin, Scott Shenker &  Ion Stoica, “Resilient  Distributed  Datasets:  A  Fault-Tolerant Abstraction for In-Memory Cluster Computing Technical Report No.UCB/EECS-2011-82  in 2011   a s i lik i K a lavr i Vlad i m i r  Vlas s ov  M ap R e du c e  Li m i ta ti on s   Optimizations and Open Issues Trust, Security and Privacy in Computing and Communications \(TrustCom\ Conference on 2013   Spar k http://spark.apache.org   16  Big Data Processing with Apache Spark – Part 1: Introduction   http://www.infoq.com/articles/apache-spark-introduction  17  Y. Bu et al  HaLoop: Efficient iterative data processing on large clusters Proceedings of the VLDB Endowment, 3\(1-2\6 2010  


collaborate collaborate assert  Call pattern Failing test cases  Passing test cases  we tested all four fault locators mentioned in Table 2 in and Ochiai Equation 6 came out as the best performing one For our running example the suspiciousness gets a suspiciousness with the highest suspiciousness Equation 7 We choose the maximum instead of average for the suspiciousness score because the technique is looking for exceptional traces one unique and highly suspicious pattern is more important than several unsuspicious ones Those methods without call patterns set have suspiciousness 0 The suspiciousness for method  we can now compare the e\002ectiveness of these two heuristics from the perspective of a continuous integration scenario We give some details about the dataset used for the comparison  xisting ontrolled ava programs The database contains meta info about each fault including the source classes modi\223ed to 223x the fault the test cases that expose the fault and the test cases that trigger at least one of the modi\223ed classes Although the framework does not explicitly list the modi\223ed methods we could reverse engineer those by means of the patches that come with the framework Note that we excluded 3 faults of Apache Commons Lang 2 and  of Table 5 listing the average and standard deviation per project respectively The high number of for the Closure project is an indication that the Closure tests exercise a lot of the base code yet the high standard deviation of 407 compared to an average of 306 indicating a few outlier tests which cover a lot of the base code 006 007 b t The Defects4J dataset does not distinguish between unit tests or integration tests However one project Closure Compiler gravitates towards integration tests Therefore the results of the Closure Compiler should serve as circumstantial evidence during the comparison As mentioned earlier we compare by means of the wasted e\002ort 7  the evaluation metric Wasted E\002ort to 223nish with the research questions and protocol driving the comparison a 003  and the variant proposed in this paper referred to as S m 006 is the number of non-faulty methods with equal rank to the faulty method This deals with ties in the ranking The comparison is driven by the following research questions 279 Suspiciousness per method W Table 4 An Example Test Coverage Matrix for Method e e s signals the presence of unit tests as well On the other hand the low number of  T T for the other project hints at mostly unit tests yet Chart has a standard deviation dataset does not distinguish between unit tests or integration tests As argued in the Scenario Section 3 this is a crucial factor when assessing a fault localisation heuristic in a continuous integration context We therefore manually inspected a sample of test methods and noticed that four projects Apache Commons Math Apache Commons Lang Joda-Time and JFreeChart mainly contain unit tests they have a small often empty set-up method and test methods contain only a few 3 Wasted E\002ort 003 2 8  Where      X X X X X X X X X X X X X    W e n e W  003 265 265 m n  W metric commonly adopted in recent research 41 42 T he w asted e\002ort i ndicates the n um b er of non-faulty methods to inspect in vain before reaching the faulty method wasted e\002ort  esting that de\223nes a few template methods which are the entry point to several classes in the base code of the project To corroborate this manual inspection we calculated the number of methods triggered in each fault spectrum analysis The assumption here is that integration tests exercise several methods in various classes consequently the fault spectrum analysis should trigger many methods as well Thus projects which gravitate towards integration testing should trigger many methods while projects gravitating towards unit tests should trigger far fewer The results are shown in the last two columns  9 9  1  with highest suspiciousness Table 4   6 6 6 6        Ranking  is the number of non-faulty methods ranked strictly higher than the faulty method m CompilerTestCase patterned spectrum analysis patterned spectrum analysis  which is the suspiciousness of the call pattern       1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0       http://defects4j.org faults of Apache Commons Math and 1 fault of Joda-Time since the fault was not located inside a method Unfortunately the 265 Defects4J Defects4J Defects4J   max X\003 Finally a ranking of all executed methods is produced using their suspiciousness W\(m The suspiciousness of the method indicates its likelihood of being at fault Those methods with the highest suspiciousness appear a the top in the ranking 5 CASE STUDY SETUP X        n X  200 m 200 n s One project however Closure Compiler relies on integration tests The test cases there are a subclass of W atabase of aults to collaborate Each method 5 in our running example is 1.0 which is the suspiciousness of the call pattern  Given the current state of the art referred to as We use 351 real faults from 5 open source java projects Apache Commons Math Apache Commons Lang Joda-Time JFreeChart and Google Closure Compiler The descriptive statistics of these projects are reported in Table 5 These faults have been collected by Just et al into a database called nable tudies for 1.0 0.7 0.7 0.7 is given in Table 4 raw spectrum analysis W 6   6 11 15 11 13 11 15 11 11 15 e e n n W t t T\007 T\007 F P f p f p f f f f p Dataset d f c t J       005  007  for each call pattern of method e e m m  1 2 3 


Project 153.1 89.3 586.0 306.9 One dimension of variation in spectrum based fault localisation and Naish2 for raw spectrum analysis Tarantula  or the same  and list the absolute numbers per project See Table 7 Project Lang Chart 101 76  238 68  T 201 8\(6 6\(5 4\(3 7\(27 7\(27 2\(2   85 19 140.8 Lang   22 2,245 55.2  28 53 209.5 Chart   96 50 7 407.5  90 83 5 2043.0 1228.9  Ochiai 46 35 against the state of the art raw spectrum analysis  22 21 13 13 14 23 12 19 3\(12 3\(12 30 23  80 23  351  Table 5 Descriptive Statistics for the Projects Used in Our Experiments 204 Defects4J  http://defects4j.org  Protocol 013 10  Time  for all relevant test cases i.e all test classes which trigger at least one of the source classes modi\223ed to 223x the fault as recorded in the Defects4J Time 1 2 3 4  Apache Commons Lang 205 http://commons.apache.org/lang   Google Closure Compiler 205 http://code.google.com/closure/compiler 201 ofTests  is strictly less  Motivation 202 is the fault locator Table 2 lists the most popular ones As explained in Section 4.5 for comparison purpose we use Ochiai for patterned spectrum analysis 202  Apache Commons Math 205 http://commons.apache.org/math   Joda-Time 205 http://joda.org/joda-time   JFreeChart 205 http://jfree.org/jfreechart  Table 7 Comparing Wasted E\002ort Patterned Spectrum Analysis vs Raw Spectrum Analysis Source KLoC  Methods triggered  3,602 6 4,130 2,205 Closure  7,927 205 Which ranking results in the lowest wasted e\002ort raw spectrum analysis or patterned spectrum analysis  205 How often do raw spectrum analysis and patterned spectrum analysis rankings result in a wasted e\002ort 205 How does the number of triggered methods a\002ect the wasted e\002ort of raw spectrum analysis and patterned spectrum analysis   we actually tested all four fault locators Table 2 Naish2 performed the best on the Defects4J 50 38 75 56 81 61 20 15 109 82 To run the fault spectrum analysis we check out a faulty version   for each project Then we run the actual spectrum based fault localisation dataset Given the continuous integration context for this research this is the most logical way to minimise the number of tests which are fed into the spectrum based fault localisation 205 Which ranking results in the lowest wasted e\002ort raw spectrum analysis or patterned spectrum analysis  accordingly The result can be seen in Figures 1a 1b 1c 1d and 1e Next we count all the faults for which the wasted e\002ort in patterned spectrum analysis 69 66 36 58 16 62 16 62 Closure To illustrate how the rankings of the heuristics di\002er we inspect fault 40 of the Closure project where the wasted e\002ort for patterned spectrum analysis  On the other hand just marking whether or not the method is executed is not discriminating in raw spectrum analysis the wasted e\002ort is 183 This is due to the fact that the faulty method has a call pattern which is unique in all failing test cases hence is easily picked up by patterned spectrum analysis Average number of methods triggered by the Standard deviation  Test KLoC Total Total  1 2 3 4 5 RQ1 RQ2 RQ3 RQ1   Fault Lo cator inthecasestudy  Methods triggered  In this section we address the three research questions introduced in Section 5 This allows for a multifaceted comparison of the e\002ectiveness of patterned spectrum analysis To determine the best performing heuristic we plot the wasted e\002ort for all of the faults for both heuristics To allow for an easy exploration of the nature of the di\002erence we sort the faults according to the wasted e\002ort of raw spectrum analysis Age years Faul Locator Math  Math is 0.5 the faulty method is ranked 223rst while for raw spectrum analysis  of Bugs 106 11 65 12 27 11 26 133 Based on the scenario Section 3 we investigate how many times the location of the fault is ranked in the 223rst 10 items Again based on the scenario Section 3 we gauge the impact of integration tests The number of methods triggered by the fault spectrum analysis acts as a proxy for the degree of integration tests in the test suite dataset with method level granularity as can be seen in Table 6 There we compare the wasted e\002ort of Naish2 against the wasted e\002ort of other fault locators using the 133 defects in the Closure project For most defects Naish2 results in a better or equal ranking only for a few defects is the ranking with other locators better For space reasons we do not show the comparison on other projects but there as well Naish2 was the best Hence we choose Ochiai for patterned spectrum analysis and plot the wasted e\002ort for patterned spectrum analysis 104 62 26 26 133 33 9  However for the optimal con\223guration of raw spectrum analysis  Note that this explains why the number of methods triggered by a fault spectrum is a good indicator for the integration tests since the tests are chosen such that they cover all changes made to 223x the defect 6 RESULTS AND DISCUSSION   strictly more  265 003  V fault   5 This is the 223rst step of the comparison assessing which of the two fault localisation methods provides the best overall ranking Motivation Motivation 204\204 Table 6 Naish within Raw Spectrum Analysis vs Tarantula Ochiai and T Thenumberof failing test cases covering the faulty method and non-faulty methods is the same 169 Yet the non-faulty methods have 280 Spectrum based fault localisation 


Project Lang Chart Table 8 shows for each project the number of faults where the wasted e\002ort is within the range of 10 with both heuristics For three projects Lang Time and Chart the performance of the patterned spectrum analysis is noticeably better These 223ndings con\223rm that patterned spectrum analysis patterned spectrum analysis PSA Time 10 e Closure 59 57 104 54 87 1 14 54 2 13 50 3 56 42 30 23 133 170 48 351 201 PSA  patterned spectrum analysis  202 RSA  raw spectrum analysis  e  is comparable but still better than the one of the raw spectrum analysis n 201 202 RQ2 patterned spectrum analysis raw spectrum analysis raw spectrum analysis 013 013 How often do How does the number of triggered methods a\002ect the wasted e\002ort of and 10   Moreover this improvement is a lot better for the Closure project the one system in the data set which gravitates towards integration tests where we see an improvement for 76 of faults 101 out of 131 more suspiciousness than faulty method because the number of passing test cases covering the non-faulty methods is less Since more passing test cases cover the faulty method high value of p 013 f r 016 017 020 021 is lower than rankings result in a wasted e\002ort  205 Inspired by the scenario in Section 3 we count how many times the location of the fault is ranked in the top 10 To deal with ties in the ranking especially at position 10 we identify these as having a wasted e\002ort 73 70 55 89 16 62 16 62 Closure 216 62 ranks more faults in the top 10 However there are still a large amount of faults where the ranking is poor wasted e\002ort 205 heuristic should obtain a good ranking for a particular fault regardless of the number of triggered methods Again based on the scenario Section 3 we gauge the impact of integration tests Therefore for each fault we calculate the number of methods triggered by the fault spectrum analysis We then sort the faults according to the number of methods and inspect the trend with respect to the number of triggered methods Unfortunately the standard deviation for the number of triggered methods is really high see the 003 patterned spectrum analysis Total Total raw spectrum analysis raw spectrum analysis 201\212\202 Figure 1 The comparison plots of all the rankings in each Lang 013 10 Especially for the Closure project less than half 42 of the faults are ranked in the top 10 Hence there is still room for improvement which we will cover in Section 7 Figure 2  Triggered Methods vs Wasted E\002ort RQ3                                              For 68 faults in the dataset the wasted e\002ort with RSA succeeds in ranking the root cause of the fault in the top 10 for 62 of the faults against 48 for and a Math Math b Lang c Time d Chart  it renders the faulty method less suspicious 10 14 62 26 26 26 46 In Section 5 we argued that the number of methods triggered by the fault spectrum analysis is an indicator of the gravitation towards integration tests see also the last two columns in Table 5 If that is the case a good spectrum based fault localisation column in Table 5 and a normal scatterplot mainly showed the noise Therefore we group the faults according to the triggered methods into 11 bins of 32 elements As these numbers did not divide well there were two bins having 30 and 33 triggered methods respectively This binning was decided as a trade-o\002 for having an equal number of elements per bin and enough bins to highlight a trend in the 281  Whereas for the remaining two projects Math and Closure the performance of the patterned spectrum analysis Patterned Spectrum Analysis Raw Spectrum Analysis Patterned Spectrum Analysis Raw Spectrum Analysis Patterned Spectrum Analysis Raw Spectrum Analysis Patterned Spectrum Analysis Raw Spectrum Analysis Patterned Spectrum Analysis Raw Spectrum Analysis The wasted effort 16 46 99 34 35 70 91 3 8 15 17 27 47 53 55 59 63 75 89 90 94 102 9 10 52 5 49 65 72 100 105 106 73 98 25 31 45 54 93 96 84 92 95 36 41 66 77 101 88 69 83 67 79 86 97 103 42 2 81 26 11 57 13 61 85 43 14 38 1 30 4 33 68 78 64 6 37 56 22 21 60 80 40 28 76 62 50 87 82 23 24 44 51 48 39 29 58 71 19 7 18 32 20 74 1 26 51 76 101 126 151 1 26 51 76 101 126 151 wasted effort 12 22 31 15 21 24 25 29 33 35 36 43 49 51 61 2 14 37 40 46 48 60 4 26 44 45 58 1 5 11 19 28 30 39 53 54 59 62 65 55 63 3 13 16 20 34 41 52 47 42 17 64 38 32 9 6 7 27 10 50 8 18 1 26 51 76 101 1 26 51 76 101 wasted effort 3 15 2 8 9 23 1 14 17 5 18 16 4 7 26 21 25 22 6 13 24 10 12 19 20 27 3 15 2 8 9 23 1 14 17 5 18 16 4 7 26 21 25 22 6 13 24 10 12 19 20 27 1 51 101 151 201 251 301 351 401 451 1 51 101 151 201 251 301 351 401 451 wasted effort 18 24 3 9 10 11 17 5 8 16 20 22 4 13 21 7 6 1 12 2 14 19 26 15 23 25 1 51 101 151 201 251 301 351 401 451 501 1 51 101 151 201 251 301 351 401 451 501 wasted effort 7 46 56 122 48 2 104 124 74 6 109 9 83 68 116 90 86 27 32 97 128 16 115 81 52 57 60 42 62 63 69 75 23 26 88 65 77 84 73 1 133 51 113 76 117 53 61 38 34 105 29 72 78 96 43 103 33 4 87 49 30 85 15 112 25 118 125 82 20 110 132 39 45 89 71 10 114 5 24 64 35 54 40 3 127 70 28 120 98 121 37 19 100 11 99 108 14 17 131 50 95 66 58 41 80 92 93 47 44 91 67 8 126 31 36 79 55 13 21 22 106 119 123 130 102 111 129 12 18 101 94 107 59 7 46 56 122 48 2 104 124 74 6 109 9 83 68 116 90 86 27 32 97 128 16 115 81 52 57 60 42 62 63 69 75 23 26 88 65 77 84 73 1 133 51 113 76 117 53 61 38 34 105 29 72 78 96 43 103 33 4 87 49 30 85 15 112 25 118 125 82 20 110 132 39 45 89 71 10 114 5 24 64 35 54 40 3 127 70 28 120 98 121 37 19 100 11 99 108 14 17 131 50 95 66 58 41 80 92 93 47 44 91 67 8 126 31 36 79 55 13 21 22 106 119 123 130 102 111 129 12 18 101 94 107 59 1 151 351 551 751 951 1151 1401 1651 1901 1 151 351 551 751 951 1151 1401 1651 1901 0 200 400 600 800 1000 bin Wasted effort 4\21243 44\21271 72\21291 92\212134 137\212202 204\212397 423\212892 917\2121262 1273\2121721 1752\2122464 2523\2125825 Table 8  Faults where Wasted E\002ort is patterned spectrum analysis Raw Spectrum Analysis Patterned Spectrum Analysis 16 46 99 34 35 70 91 3 8 15 17 27 47 53 55 59 63 75 89 90 94 102 9 10 52 5 49 65 72 100 105 106 73 98 25 31 45 54 93 96 84 92 95 36 41 66 77 101 88 69 83 67 79 86 97 103 42 2 81 26 11 57 13 61 85 43 14 38 1 30 4 33 68 78 64 6 37 56 22 21 60 80 40 28 76 62 50 87 82 23 24 44 51 48 39 29 58 71 19 7 18 32 20 74 12 22 31 15 21 24 25 29 33 35 36 43 49 51 61 2 14 37 40 46 48 60 4 26 44 45 58 1 5 11 19 28 30 39 53 54 59 62 65 55 63 3 13 16 20 34 41 52 47 42 17 64 38 32 9 6 7 27 10 50 8 18 18 24 3 9 10 11 17 5 8 16 20 22 4 13 21 7 6 1 12 2 14 19 26 15 23 25 


4-43 13.0 11.5 17.6 15.5 23.5 20.0 51.4 70.8 14.0 20.8 11.2 24.0 is often able to push the faulty method high in the ranking however there are several cases where it never reaches the top 10 A nice example is fault 126 in Closure where the wasted e\002ort for in class com.google.javascr patterned spectrum analysis Bin 1.0 1.5 1.5 1.5 1.5 1.5 2.0 3.5 1.9 3.5 5.8 8.2 2.5 5.0 is 85.5 This value is still lower than the one given by Q1 Q3 Q3 PSA Q1 1.5 2.5 1.0 1.8 2.9 3.0 6.8 2.2 2.8 8.5 2.8 9.1 2.4 5.2 2.8 3.8 3.2 9.1 1.5 3.2 8.0 73.0 5.0 9.0 38.5 10.4 263.0 511.6 56.4 33.9 97.8 203.1 40.9 12.4 50.0 196.0 77.5 11.0 115.5 561.1 patterned spectrum analysis raw spectrum analysis patterned spectrum analysis patterned spectrum analysis Table 9  Triggered Methods vs Wasted E\002ort  is an itemset hence the call pattern is not order preserving and has no repetitive method calls Note that the importance of the call-order was also pointed out by Lo et al  016 patterned spectrum analysis patterned spectrum analysis patterned spectrum analysis patterned spectrum analysis its\(Node,int,String number of triggered methods if any For each of the bins we calculated the 223rst quartile median and the third quartile listing them all in Table 9 and plotting them in a series of boxplots Figure 2 Table 9 and Figure 2 illustrate that the number of methods triggered has little e\002ect on Distribution Second ipt.jscomp.MinimizeExitPoints reveals that the developers removed the 215if check\216 with a finally Listing 2 Code snippet for a sample method 1 public boolean contains char ch  2 char  thisBuf  buffer 3  Correct code 4 for in ti=0;i this.size i  5  Incorrect code 6 for  int i  0;i  thisBuf.length;i  7 if   ch  8 return true  9  10  11   205 do we measure what was intended  216in class 215 org.apache.commons.lang.text.StrBuilder 44-71 72-91 92-134 137-202 204-397 423-892 917-1262 1273-1721 1752-2464 2523-5825  however quite a lot on is because the order of method calls is crucial Indeed the call pattern in  In this research we adopted the wasted e\002ort metric to compare raw spectrum analysis 216in class\215 distribution.UniformReal patterned spectrum analysis As a future improvements of heuristic we can infer some suggestions for improvement regarding future variations First of all an inherent limitation is that a faulty method which does not call any other methods will always be ranked at the bottom Indeed such methods don\220t have a call pattern which is the primary coverage element appearing in the test coverage matrix thus the method gets suspiciousness 0 In our case study we noticed a few cases where none of the faulty methods had any call pattern More speci\223cally there are 4 such cases in the Math project 3 in the Chart project 2 in the Time and Lang projects and only 1 in the Closure project The best example corresponds to the highest wasted e\002ort on fault 60 of the Lang project See Listing 2 Indeed the faulty method 215 contains\(char performs better for integration tests 7 POSSIBLE IMPROVEMENTS  we might incorporate statements or branches into the hitspectrum The call-order of methods as well is relevant information to incorporate into the hit-spectrum 8 THREATS TO VALIDITY ndInclusive 202 201 Upon closer inspection of those faults ranked high by the 216 gets suspiciousness 0 because the for loop only performs direct accesses to memory and never calls any methods Similarly the highest wasted e\002ort for fault 22 in the Math project is due to the faulty method 215 isSupportUpperBou  we found a unique call pattern Listing 3 which is only called in the failing tests The bug 223x 4 RSA block This 215if check\216 involves the last 3 calls in Listing 3 lines 4-6 Despite being unique the reason why this call pattern was not picked up by Median Median  The last four bins in particular contain faults which trigger more than thousand methods The median wasted e\002ort for 216 which again never calls any other methods In this case the method body contained a single statement 215 return false 216 the bug 223x replaced it by 215 return true 216 A last example is fault 22 in Time project where the fault resided in a faulty constructor hence did again not have any method call pattern 532.5 yet it is too high to ever be considered in a realistic scenario Manually analysing the traces of the faulty method tryMinimizeEx As with all empirical research we identify those factors that may jeopardise the validity of our results and the actions we took to reduce or alleviate the risk Consistent with the guidelines for case studies research see 37 w e organise them into four categories against  However in information retrieval rankings where users do not want to inspect all outcomes other measures are considered such as Mean Reciprocal 4 https://github.com/google/closure-compiler/commit/bd2803 282 patterned spectrum analysis 201 202 is four to eighteen times lower than raw spectrum analysis raw spectrum analysis patterned spectrum analysis Construct validity The better rankings for Closure in Table 7 and Table 8 are inconclusive as one case is not enough to generalise upon Yet based on an analysis of the number of methods triggered by the fault spectrum there is at least circumstantial evidence that Wasted E\002ort 006 007 b t 017 020 021 raw spectrum analysis Listing 3 Unique call sequence in faulty method tryMinimizeExits\(Node,int,String 1 Node.getLastChild 2 NodeUtil.getCatchBlock\(Node 3 NodeUtil.hasCatchHandler\(Node 4 NodeUtil.hasFinally\(Node 5 Node.getLastChild 6 tryMinimizeExits\(Node int String 


Thanks to prof Martin Monperrus for reviewing an early draft of this paper This work is sponsored by i the Higher Education Commission of Pakistan under a project titled 215Strengthening of University of Sindh Faculty Development Program ii the Institute for the Promotion of Innovation through Science and Technology in Flanders through a project entitled 215Change-centric Quality Assurance CHAQ with number 120028 patterned spectrum analysis patterned spectrum analysis patterned spectrum analysis patterned spectrum analysis raw spectrum analysis 205 is the result dependent on the tools  All the tools involved in this case study i.e creating the traces calculating the  hence there as well the risk of faults is small is a class of heuristics known to be e\002ective for localising faults in existing software systems These heuristics compare execution traces of failing and passing test runs to produce a ranked list of program elements likely to be at fault The current state of the art referred to as  comprises several variants typically classi\223ed according to two dimensions the granularity statement 204 block 204 method 204 class and the fault locator function Tarantula Ochiai T and Naish2 In this paper we explore a third dimension the hitspectrum More speci\223cally we propose a variant referred to as and http://www.philippe-fournier-viger.com/spmf Defects4J performs better for integration tests Despite this improvement we collect anecdotal evidence from those situations where the 5 Internal validity patterned spectrum analysis 205towhatextentisitpossibletogeneralise the 223ndings  In our study we experimented with 351 real faults drawn from 223ve representative open source object oriented projects from 9 CONCLUSION Spectrum based fault localisation 216 The test case which exposes the defect calls both methods yet the test case fails on the 223rst assertion calling the 215 205 are there unknown factors which might a\002ect the outcome of the analyses  patterned spectrum analysis 216 method The question then is what a fault localisation should report one location or all locations  Furthermore how should we assess the ranking of multiple locations In this research inspired by earlier work 39 w e to ok the assumption that rep orting one location is su\003cient and use the highest ranking of all possible locations However one could make other assumptions patterned spectrum analysis renderer.category.MinMaxCategoryRenderer setGroupStroke\(Stroke CorrectnessoftheOracle Defects4J Defects4J raw spectrum analysis Defects4J 10 ACKNOWLEDGMENTS ranking is less adequate and derive suggestions for future improvements 283 216 T h e 223rst change is to override 215 Also equals\(Object setGroupStroke\(Stroke equals\(Object Fault Masking External validity Reliability Multiple faults  One often heard critique on fault localisation heuristics in general and raw spectrum analysis spectrum based fault localisation patterned spectrum analysis raw spectrum analysis raw spectrum analysis patterned spectrum analysis 5  One particular phenomenon which occurs in a few faults in the dataset is 215fault masking\216 This i s a fault whic h is spread o v er m ultiple locations and where triggering one location already fails the test The 223x for fault 23 of project Chart for instance comprises two changes in two separate methods of the class 215  The continuous integration scenario in Section 3 makes the assumption that the test oracle itself is infallible However this does not hold in practice Christophe et al observed that functional tests written in the Selenium library get updated frequently  We ignore the e\002ects of the tests being at fault in this paper but here as well point out that this is something to be studied in future work rankings have been created by one of the authors They have been tested over a period of 2 years thus the risk of faults in the tools is small Moreover for the calculation of the  which extends the hitspectrum with patterns of method calls extracted by means of frequent itemset mining The motivation for the is more e\002ective in localising the fault For 68 faults in the dataset the wasted e\002ort with is lower than succeeds in ranking the root cause of the fault in the top 10 for 63 of the defects against 48 for patterned spectrum analysis using the raw spectrum analysis 216 method and the second involves changes in method 215 is indeed a lot better on integration tests in other systems dataset the most recent defect dataset currently available Obviously it remains to be seen whether similar results would hold for other defects in other systems In particular there is a bias towards unit test in the variant stems from a series of contacts with software developers working in Agile projects and relying on continuous integration to run all the tests Complex systems with multiple branches and staged testing could really bene\223t from fault localisation Faults in integration tests in particular are very relevant they seldom occur but if they do they have a big impact on the team productivity Inspired by the continuous integration motivational example we compare Rank MRR or Mean Average Precision MAP 39 It is unclear whether the use of these relative evaluation metrics would alter the results Nevertheless the use of an absolute metric alleviates other concerns 33 T herefore the impact is minimal 216 method thereby masking the 215 in particular is that when multiple faults exist the heuristic will confuse their e\002ects and its accuracy will decrease Two independent research teams con\223rmed that multiple faults indeed in\224uence the accuracy of the heuristic however it created a negligible e\002ect on the e\002ectiveness 12  W e i gnore t he p o ten tial e\002ect of m ultiple faults in this paper Nevertheless future research should study the e\002ect of multiple faults dataset with only the Closure project gravitating towards integration tests Further research is needed to verify whether the rankings we compared as best as possible against the results reported in earlier papers The algorithm for frequent itemset mining was adopted from open source library SPMF against dataset This dataset contains 351 real faults drawn from 223ve representative open source java projects Despite a bias towards unit tests in the dataset we demonstrate that  Moreover this improvement is a lot better for the Closure project the one system in the data set which gravitates towards integration tests There we see an improvement for 76 defects 101 out of 131 The better rankings for Closure are inconclusive one case is not enough to generalise upon yet based on an analysis of the number of methods triggered by the fault spectrum there is at least circumstantial evidence that 


 ASE 2012 pages 378\205381 NewYork,NY,USA,2012.ACM 8 L Christophe R Stevens C D Roover and W D Meuter Prevalence and maintenance of automated functional tests for web applications In  89:51\20562 Mar 2014 31 A Miller A hundred days of continuous integration In  ISSTA 2014 pages 437\205440 New York NY USA 2014 ACM  F Khomh B Adams T Dhaliw al and Y Zou Understanding the impact of rapid releases on software quality The case of 223refox SIGSOFT Softw Eng Notes Proceedings of the 20th IEEE/ACM International Conference on Automated Software Engineering Proceedings of the 34th International Conference on Software Engineering J Syst Softw Leaders of Tomorrow Future of Software Engineering Proceedings of the 23rd IEEE International Conference on Software Analysis Evolution and Reengineering SANER Proceedings of the 2014 IEEE International Conference on Software Maintenance and Evolution Proceedings of the 2014 International Symposium on Software Testing and Analysis Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering  82\(11 Nov 2009 2 R A b r e u P Z o e t e w e i j a n dA J  C v a nG e m u n d O nt h e accuracy of spectrum-based fault localization In  39\(5 Sept 2014  M Beller G Gousios A P anic hella and A Zaidman When how and why developers do not test in their IDEs In  ISSTA 22011 pages 210\205220 New York NY USA 2011 ACM 13 H Do S Elbaum and G Rothermel Supporting controlled experimentation with testing techniques An infrastructure and its potential impact  ICSE 22094 pages 191\205200 Los Alamitos CA USA 1994 IEEE Computer Society Press  J A Jones a nd M J Harrold Empirical ev aluation o f t he tarantula automatic fault-localization technique In  ICSE 22002 pages 467\205477 New York NY USA 2002 ACM 21 R Just D Jalali and M D Ernst Defects4j A database of existing faults to enable controlled testing studies for java programs In  20\(2 2015 23 G Laghari A Murgia and S Demeyer Localising faults in test execution traces In  20\(5 Oct 2015  T.-D B Le R J Oen tary o and D Lo I nformation retrieval and spectrum based bug localization Better together In  ESEC/FSE 2015 pages 579\205590 New York NY USA 2015 ACM 26 C Le Goues M Dewey-Vogt S Forrest and W Weimer A systematic study of automated program repair Fixing 55 out of 105 bugs for 8 each In  20\(3 Aug 2011 33 C Parnin and A Orso Are automated debugging techniques actually helping programmers In Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering Proceedings of the 19th European Conference on Object-Oriented Programming  ICSE 22015 pages 483\205493 Piscataway NJ USA 2015 IEEE Press  M Hutc hins H F oster T Goradia and T  Ostrand Experiments of the e\002ectiveness of data\224owand control\224ow-based test adequacy criteria In 1 R A b r e u P Z o e t e w e i j R G o l s t e i j n a n dA J C v a n Gemund A practical evaluation of spectrum-based fault localization 11 REFERENCES  TAICPART-MUTATION 22007 pages 89\20598 Washington DC USA 2007 IEEE Computer Society 3 B Adams and S McIntosh Modern release engineering in a nutshell 205 why researchers should care In  Addison-Wesley Longman Publishing Co Inc Boston MA USA 1999 7 J Campos A Riboira A Perez and R Abreu Gzoltar An eclipse plug-in for testing and debugging In  ICSME 22014 pages 141\205150 Washington DC USA 2014 IEEE Computer Society 9 V D a l l m e i e r C L i n d i g a n dA Z e l l e r L i g h t w e i g h td e f e c t localization for java In  ASE 22007 pages 433\205436 NewYork,NY,USA,2007.ACM 11 B Daniel V Jagannath D Dig and D Marinov Reassert Suggesting repairs for broken unit tests In  10\(4 2005  P  M Duv a ll S Mat y as and A Glo v er 284 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering Proceedings of the Twenty-second IEEE/ACM International Conference on Automated Software Engineering Proceedings of the Int\220l Conference on Automated Software Engineering ASE Proceedings of the 37th International Conference on Software Engineering Volume 1 Proceedings of the 16th International Conference on Software Engineering Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  KDD 22007 pages 460\205469 New York NY USA 2007 ACM  Lucia D Lo L Jiang and A Budi C omprehensiv e evaluation of association measures for fault localization In Empirical Softw Engg Empirical Software Engineering Software Maintenance ICSM 2010 IEEE International Conference on ACM Trans Softw Eng Methodol  IWPSE 2015 pages 1\2058 New York NY USA 2015 ACM 24 T.-D B Le D Lo and F Thung Should i follow this fault localization tool\220s output Proceedings of the 2011 International Symposium on Software Testing and Analysis Osaka Japan March 2016 4 P Agarwal and A P Agrawal Fault-localization techniques for software systems A literature review ASE\22005 pages 273\205282 New York NY USA 2005 ACM  J A Jones M J Harrold a nd J Stask o  Visualization o f test information to assist fault localization In ICSE 22012 pages 3\20513 Piscataway NJ USA 2012 IEEE Press  D Lo S.-C Kho o and C Liu E\003cien t mining o f iterativ e patterns for software speci\223cation discovery In ASE\22014 pages 127\205138 New York NY USA 2014 ACM  X Mao Y Lei Z Dai Y Qi a nd C W ang Slice-based statistical fault localization  Addison-Wesley 2007  S Elbaum G  Rothermel and J P enix T ec hniques for improving regression testing in continuous integration development environments In  pages 179\205190 ACM 2015  R  V Binder  pages 433\205444 IEEE CS 2009 12 N DiGiuseppe and J A Jones On the in\224uence of multiple faults on coverage-based fault localization In  pages 1\20510 Sept 2010  Lucia D Lo a nd X Xia F usion f ault lo calizers In  pages 289\205293 Aug 2008  L Naish H J Lee and K  Ramamohanarao A m o del for spectra-based software diagnosis Journal of Systems and Software Proceedings of the 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering ESEC/FSE Testing Object-oriented Systems Models Patterns and Tools Continuous Integration Improving Software Quality and Reducing Risk Empirical Software Engineering Proceedings of the 14th International Workshop on Principles of Software Evolution Proceedings of the 2011  ECOOP\22005 pages 528\205550 Berlin Heidelberg 2005 Springer-Verlag  V Dallmeier and T Z immermann Extraction of bug localization benchmarks from history In Proceedings of the Testing Academic and Industrial Conference Practice and Research Techniques MUTATION Proceedings of the 24th International Conference on Software Engineering  FSE 2014 pages 235\205245 New York NY USA 2014 ACM 16 M Fowler and M Foemmel Continuous integration original version http://http://www.martinfowler.com  Sept 2010 Accessed April 1st 2016  K Herzig M G reiler J Czerw o nk a and B Murph y  The art of testing less without sacri\223cing quality In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering Agile 2008 AGILE 22008 Conference 


Case Study Research Design and Methods 3 edition 285 Proceedings of the 2013 International Symposium on Software Testing and Analysis pages 345\205355 Nov 2013  D St 036 ahl and J Bosch Modeling continuous integration practice di\002erences in industry software development pages 1105\2051112 New York NY USA 2006 ACM  J Zhou H Zhang and D Lo W here should the bugs b e 223xed more accurate information retrieval-based bug localization based on bug reports In ICSE 22012 pages 14\20524 Piscataway NJ USA 2012 IEEE Press IEEE Transactions on Software Engineering International Symposium on Software Testing and Analysis Automated Software Engineering ASE 2013 IEEE/ACM 28th International Conference on  pages 113\205122 Oct 2013  R K Yin IEEE Software IEEE Software Proceedings of the 34th International Conference on Software Engineering Empirical Softw Engineering Proceedings of the Second SIAM International Conference on Data Mining Arlington VA USA April 11-13 2002 SIGSOFT Softw Eng Notes Proceedings of the the 6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering  ISSTA 22011 pages 199\205209 New York NY USA 2011 ACM  Y Qi X Mao Y Lei and C  W ang Using a utomated program repair for evaluating the e\002ectiveness of fault localization techniques In  ISSTA 2013 pages 191\205201 New York NY USA 2013 ACM  S Rao H Medeiros and A K ak Comparing i ncremen tal latent semantic analysis algorithms for e\003cient retrieval from software libraries for bug localization  40\(1 Feb 2015  P  Runeson A surv e y o f u nit t esting practices  14\(2 2009  R Saha M Lease S Kh urshid a nd D P erry  Impro ving bug localization using structured information retrieval In  87\(0 204 59 2014  F Steimann and M F renk el Impro ving co v erage-based localization of multiple faults using algorithms from integer linear programming In  ISSTA 2013 pages 314\205324 New York NY USA 2013 ACM  S H T an and A Ro yc houdh ury  R eli\223x Automated repair of software regressions In  ICSE 22015 pages 471\205482 Piscataway NJ USA 2015 IEEE Press  N Tillmann a nd W Sc h ulte Unit tests reloaded parameterized unit testing with symbolic execution  2016  J Xuan and M Monp errus Learning to com bine m ultiple ranking metrics for fault localization In  pages 191\205200 Sept 2014  X Xue and A S Namin Ho w s igni\223can t is the e\002ect of fault interactions on coverage-based fault localizations In  23\(4 2006  P  Runeson a nd M H 250 ost Guidelines for conducting and reporting case study research in software engineering  pages 345\205355 Nov 2013  R K Saha M Lease S  Kh urshid and D E P e rry  Improving bug localization using structured information retrieval In  pages 121\205130 Nov 2012  F Steimann M F renk el and R Abreu Threats to t he validity and value of empirical assessments of the accuracy of coveragebased fault locators In  23\(4 July 2006  J T u  L  C hen Y Zhou J Zhao and B Xu Lev eraging method call anomalies to improve the e\002ectiveness of spectrum-based fault localization techniques for object-oriented programs In  ESEC-FSE 22007 pages 35\20544 New York NY USA 2007 ACM 47 J.Xuan,M.Martinez,F.Demarco,M.Cl\264 ement S Lamelas T Durieux D Le Berre and M Monperrus Nopol Automatic repair of conditional statement bugs in java programs  Sage Publications 2002  A Zaidman B V Rompaey  v a n Arie v an Deursen and S Demeyer Studying the co-evolution of production and test code in open source and industrial developer test processes through repository mining  16\(3 2011  M J Zaki and C J Hsiao CHARM an e\003cien t a lgorithm for closed itemset mining In  pages 457\205473 2002 53 A X Zheng M I Jordan B Liblit M Naik and A Aiken Statistical debugging Simultaneous identi\223cation of multiple bugs In Proceedings of the 2013 International Symposium on Software Testing and Analysis Journal of Systems and Software Software Reliability Engineering ISSRE 2012 IEEE 23rd International Symposium on Proceedings of the 23rd International Conference on Machine Learning ICML 22006 Software Maintenance and Evolution ICSME 2014 IEEE International Conference on Proceedings of the 2012 12th International Conference on Quality Software 2013 ACM  IEEE International Symposium on Empirical Software Engineering and Measurement QSIC\22012 pages 1\2058 Washington DC USA 2012 IEEE Computer Society  A W a sylk o wski A Zeller and C  L indig Detecting ob ject usage anomalies In Empirical Software Engineering Automated Software Engineering ASE 2013 IEEE/ACM 28th International Conference on Proceedings of the 37th International Conference on Software Engineering Volume 1 


LI et al  PRIVACY-PRESERVING-OUTSOURCED ASSOCIATION RULE MINING ON VERTICALLY PARTITIONED DATABASES 1859 the publication of this seminal work a number of privacypreserving association rule mining or frequent itemset mining solutions have been published in the literature see 11   1 3   28]Ð[31 The most relevant work is the privacy-preserving association rule mining solution presented in I n t hi s s ol ut i on a d at a owner known as the master is responsible for the mining The other data owners known as slaves insert ctitious transactions to their respectiv e datasets and send the datasets to the master Each data owner will also send his set of real transactions IDs to a semi-trusted third-party server The third-party server is assumed not to be colluding with any data owner but it cannot be trusted to hold the raw data The master generates association rule candidates from the joint database containing ctitious data For each rule candidate X  Y  the master sends the ID lists of the transactions containing X  Y and the transactions containing X to the third-party server The server veriÞes if the rule is qualiÞed or not Similar to our solutions a semi-trusted third-party is utilized for the mining However unlike our solutions a data owner i.e the master does the majority of the computational work Therefore we can hardly say that such a solution is an outsourced mining solution Though ctitious data are added in datasets to lower data usability the master is able to learn signiÞcant information about other data owners raw data from the received datasets In cont rast our solutions do not leak such information as we do not rely on one particular data owner to undertake the computations and we also encrypt the datasets All existing solutions with the exception of  d o not utilize a third-party server to server to compute the mining result Some solutions  13 us e a s y mmet r i c encryption to compute the supports of itemsets while other solutions 28]Ð[30 us e a s ecure s cal ar product p rot o col  a s et intersection cardinality protocol or a secret sharing scheme to perform these computations A majority of these solutions expose exact supports to all data owners resulting in the leakage of information about the data owners raw data  The only exception is one of s s o l u t i ons  I n  13 t here are two privacy-preserving solutions for frequent itemset mining The rst solution exposes exact supports which is not desirable The second solution does not expose exact supports However association rules cannot be mined based on the result of second solution because conÞdences cannot be computed without the exact supports In addition this solutionÕs method cannot be used to mine association rules because securely computing conÞdence is more complicated than computing support In comparison with this solution our frequent itemset mining solutionÕs computational complexity is signiÞcantly lower Our solutions do not expose exact supports or conÞdences to data owners Different from existing solutions based on homomorphic encryption we use symmetric homomorphic encryption instead of asymmetric homomorphic encryption and the manner in which we use homomorphic encryption also differs from existing solutions In our approach we use homomorphic encryption to create ERVs and build our secure outsourced comparison scheme B Privacy-Preserving Outsour ced Association Rule Mining and Frequent Itemset Mining Privacy-preserving outsourced frequent itemset mining and association rule mining have been studied in the setting of a single data owner  19]Ð[21 I n e xi s t i n g s ol ut i ons  the data owner outsources thei r data and the mining task to the cloud but at the same time wish to keep the raw data secret from the cloud Generally data items in the database are encrypted using a substitution cipher prior to outsourcing Reference  propos ed a s ol ut i o n t o c ount er frequency analysis attack on substitution cipher However a later work demons t r at ed t h at 19 s s o l u t i o n i s not s ecure Giannotti et al proposed a solution based on k anonymity frequency   21 T o c ount er frequenc y a nal y s i s a t t ack the data owner inserts ctitious transactions in the encrypted database to conceal the item frequency After inserting the ctitious transactions any item in the encrypted database will share the same frequency with at least k  1 other items The data owner sends the encrypted database of both the real and ctitious transactions to the cloud The cloud runs a classic frequent itemset mining algorithm and returns the result frequent itemsets and their supports to the data owner The data owner revises these itemsets supports by subtracting them with these itemsets corresponding occurrence counts in the ctitious transactions respectively Finally the data owner decrypts the received itemsets with the revised supports higher than the frequency threshold an d generates association rules based on found frequent itemsets Our solutions use their techniques to conceal the raw data from the cloud and mitigate frequency analysis attack that can be undertaken by the cloud Using these techniques alone however is not sufÞcient to protect data privacy in the vertically partitioned database setting To cancel out ctitious transactions both 21 an d  1 6  require the data owner to count itemset occurrences in ctitious transactions In the vertically partitioned database setting data owners are unable to perform such calculation using the techniques described in and  16 I n our s o l u t i ons  the cloud rather than the data owners cancels out ctitious transactions in a privacy-preserving manner and the underlying techniques are our homomorphic encryption secure comparison and ciphertext tag schemes Another recent work  propos ed a p ri v a c y pres e rving outsourced association rule mining solution based on predicate encryption This solution is resilient to chosen-plaintext attacks on encrypted items but it is vulnerable to frequency analysis attacks Applying this solution to vertically partitioned databases will also result in the leakage of the exact supports to data owners In this paper our adversary model is different We assume the cloud has knowledge of the item frequencies instead of chosen plaintext-ciphertext pairs and our solutions are resilient to frequency analysis attacks C Other Related Work Other than the settings of vertically partitioned databases and cloud/third-party-aided mining privacy-preserving frequent itemset mining and association rule mining have 


1860 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY VOL 11 NO 8 AUGUST 2016 been studied in the settings of horizontally partitioned databases   33]Ð[35 d at a publ i s hi ng 36 and d i f ferent i a l privacy  T hes e s e t t i ngs are b e yond t h e s cope of t h i s paper  IX C ONCLUDING R EMARKS In this paper we proposed a privacy-preserving outsourced frequent itemset mining solution for vertically partitioned databases This allows the data owners to outsource mining task on their joint data in a privacy-preserving manner privacypreserving manner Based on this solution we built a privacypreserving outsourced association rule partitioned databases Our solutions protect data ownerÕs raw data from other data owners and the cloud Our solutions also ensure the privacy of the mining results from the cloud Compared with most existing solutions our solutions leak less information about the data owners raw data Our evaluation has also demonstrated that our solutions are very efÞci ent therefore our solutions are suitable to be used by data owners wishing to outsource their databases to the cloud but require a high level of privacy without compromising on performance To realize our solutions an efÞcient homomorphic encryption scheme and a secure outsourced comparison scheme were presented in this paper Both schemes have potential usage in other secure computation applications such as secure data aggregation beyond the data mining solutions described in this paper Demonstrating the u tility of the p roposed homomorphic encryption scheme and outsourced comparison scheme in other settings will be the focus of future research A PPENDIX I NSERTING F ICTITIOUS T RANSACTIONS  s A LGORITHM  An algorithm to counter frequency analysis attacks on the outsourced database encrypted with a substitution cipher was proposed in F or the purpos e o f c oncealing t he item frequency this algorithm inserts ctitious transactions in the database to be oursourced The goal is to ensure that each item share the same frequency with at least k  1 items The algorithm is summarized as follows also see  Firstly the data owner scans the database to count each individual itemÕs support  Secondly the data owner groups items considering the supports and co-occurrence of items The data owner sorts items in decreasing order of support Starting from the rst of the sorted item list i.e the item with the highest support the data owner assigns every k adjacent items to a new created group If there are less than k unassigned items remaining these items will be assigned to the last created group The data owner swaps items from different groups to ensure that all items in the same group do not occur together in the same transaction  Thirdly for each item in each group the data owner calculates the difference between the itemÕs support and the highest support in the group The difference is deÞned as the noise of the item  Fourthly to achieve k anonymity frequency the data owner generates ctitious transactions based on the result of the third step The number of an itemÕs occurrences in the ctitious transactions is equal to its noise calculated in the third step After inserting the ctitious transactions all items in the same group share the same support A CKNOWLEDGMENT The authors would like to thank Quach Vinh Thanh the Associate Editor and the three anonymous reviewers for providing constructive and gen erous feedback Despite their invaluable assistance any erro rs remaining in this paper are solely attributed to the authors R EFERENCES  T  B rijs  G  S winnen K V a nhoof a nd G W e ts   Us ing a s s o ciation r ules for product assortment decisions A case study in Proc SIGKDD  1999 pp 254Ð260  S  E  B ros s e tte A  P  S prague J  M  H ardin K B W a ites  W  T  J ones  and S A Moser Association rules and data mining in hospital infection control and public health surveillance J Amer Med Inform Assoc  vol 5 no 4 pp 373Ð381 1998 3 B  M obas h er  H  D ai T  L uo and M  N akaga w a E f f ecti v e p er s onalization based on association rule discovery from Web usage data in Proc WIDM  2001 pp 9Ð15  C  C reighton and S  H anas h Mining g ene e xpres s i on databas e s f or association rules Bioinformatics  vol 19 no 1 pp 79Ð86 2003 5 X  Y in and J  H an  CP A R  C las s i  cation b as ed on pr edicti v e as s o ciation rules in Proc SIAM SDM  2003 pp 1Ð5 6 R  A gr a w al and R  S r i kant  F a s t algor ithm s f o r m ining a s s o ciation rules in Proc VLDB  1994 pp 1Ð13 7 M  J  Z aki S calable algor ithm s f o r a s s o ciation m ining  IEEE Trans Knowl Data Eng  vol 12 no 3 pp 372Ð390 May/Jun 2000  J  H an J  P ei a nd Y  Y i n Minin g frequent patterns without candidate generation in Proc ACM SIGMOD  pp 1Ð12 2000 9 J  V aidya and C  C lif ton P r i v a c y pr es er ving as s o ciation r ule m ining i n vertically partitioned data in Proc SIGKDD  2002 pp 639Ð644  M Kantarcioglu a nd C Clifton Pri vacy-preserving distributed mining of association rules on horizontally partitioned data IEEE Trans Knowl Data Eng  vol 16 no 9 pp 1026Ð1037 Sep 2004  B Rozenber g and E  G udes   A s s o ciation r ules m i ning in v e rtically partitioned databases Data Knowl Eng  vol 59 no 2 pp 378Ð396 2006  J  Z h an S  M atwin and L  C hang Pri v a c y pres e rving c ollaborati v e association rule mining in Proc DBSEC  2005 pp 153Ð165  S Z hong Pri v a c y pres e rving a lgorithm s for d is trib uted m i ning of frequent itemsets Inf Sci  vol 177 no 2 pp 490Ð503 2007  P  P a illier  P ublick e y cr yptos ys tem s bas e d o n c om pos ite de gr ee r e s i duosity classes in Proc EUROCRYPT  1999 pp 223Ð238  R Cram er  R  G ennaro a nd B Schoenm ak ers   A s ecure and optim ally efÞcient multi-authority election scheme Eur Trans Telecommun  vol 8 no 5 pp 481Ð490 1997  F  G i annotti L  V  S  L a ks hm anan A  M onr eale D  P e dr es chi and H Wang Privacy-preserving mining of association rules from outsourced transaction databases IEEE Syst J  vol 7 no 3 pp 385Ð395 Sep 2013  B Dong R L i u and H  W ang Res ult i nte g rity v e riÞcation o f outsourced frequent itemset mining in Proc 27th Annu IFIP WG Conf Data Appl Secur Privacy DBSec  Newark NJ USA Jul 2013 pp 258Ð265 O A v a ilable http://dx doi o r g 10 1007/978-3-64 239256-6_17  R L i u a nd H W a ng Res ult i nte g rity v e riÞcation o f outs ourced pri v ac ypreserving frequent itemset mining in Proc SIAM Int Conf Data Mining  Vancouver BC Canada Apr./May 2015 pp 244Ð252 Available http://dx.doi.org/10.1137/1.9781611974010.28  W  K W ong D W  Cheung E  Hung B Kao and N  M am oulis  Security in outsourcing of association rule mining in Proc VLDB  2007 pp 111Ð122  I  M o llo y  N  L i  a nd T  L i   O n the  in s ecur ity and  im  p r acticality of outsourcing precise association rule mining in Proc ICDM  Dec 2009 pp 872Ð877  F  G i annotti L  V  S  L a ks hm anan A  M onr eale D  P e dr es chi and W Wang Privacy-preserving data mining from outsourced databases in Proc CPDP  2011 pp 411Ð426 


LI et al  PRIVACY-PRESERVING-OUTSOURCED ASSOCIATION RULE MINING ON VERTICALLY PARTITIONED DATABASES 1861 22 FIPS Publication 180-1 Secure Hash Standard  Nat Inst Standards Technol Gaithersburg MD USA 1995 23 FIPS Publication 180-2 Secure Hash Standard  Nat Inst Standards Technol Gaithersburg MD USA 2002  T  E l Gam a l  A public k e y c ryptos ystem and a signature scheme based on discrete logarithms IEEE Trans Inf Theory  vol 31 no 4 pp 469Ð472 Jul 1985 O A v a ilable http://dx doi o r g 10 1109 TIT.1985.1057074  N  Cour tois  A  K lim o v  J  P atar in a nd A  S h am ir   E f  c ient algor ithm s for solving overdeÞned systems of multivariate polynomial equations in Proc EUROCRYPT  2000 pp 392Ð407  P  F ournier V iger  Real-life Datasets in SPMF Format  accessed on Apr 6 2016 O A v a ilable http://w w w  philippe-fournier viger.com/spmf/index.php?link=datasets.php  P  F ournier V iger  A  G om ariz T  G ueniche A Soltani C  W  W u and V S Tseng SPMF A Java opensource pattern mining library J Mach.Learn.Res  vol 15 no 1 pp 3389Ð3393 2014  J  V a idya and C  C lif ton S ecur e s e t i nter s ection car dinality w ith application to association rule mining J Comput Secur  vol 13 no 4 pp 593Ð622 2005  X Ge L  Y an J  Z hu and W  S hi  Pri v ac y-pres erving dis t rib u ted association rule mining based on the secret sharing technique in Proc SEDM  Jun 2010 pp 345Ð350  R K h ar at M  K um bhar  and P  B ham r e E f  cient p r i v a c y pr es er ving distributed association rule mining protocol based on random number in Intelligent Computing Networking and Informatics  Raipur Chhattisgarh India Springer 2014 pp 827Ð836  C Dong and L  C hen  A f a s t s ecure dot product p rotocol with application to privacy preserving association rule mining in Proc 18th PaciÞc-Asia Conf Adv Knowl Discovery Data Mining PAKDD  Tainan Taiwan May 2014 pp 606Ð617 Available http://dx.doi.o rg/10.1007/978-3-319-06608-0_50  J  L a i Y  L i  R  H  D eng J  W e ng C Guan a nd Q Y a n T o w ards semantically secure outsourcing of association rule mining on categorical data Inf Sci  vol 267 pp 267Ð286 May 2014  T  F ukas a w a  J  W ang T  T a kata a nd M  M i yazaki  A n e f f ecti v e distributed privacy-preserving data mining algorithm in Proc 5th Int Conf IDEAL  2004 pp 320Ð325  C Su and K  S akurai  A d is trib ut ed privacy-preserving association rules mining scheme using frequent-pattern tree in Proc ADMA  2008 pp 170Ð181  M  G  K a os ar  R  P aulet and X  Y i S ecur e tw opar t y a s s o ciation r ule mining in Proc ACSW-AISC  2011 pp 15Ð22  J  L  L in and J  Y  C L i u Pri v a c y pres erving item s et m i ning through fake transactions in Proc ACM Symp Appl Comput SAC  Seoul South Korea Mar 2007 pp 375Ð379 A v a ilable http://doi.acm.org/10.1145/1244002.1244092  B N K e s h a v am urthy  A M Khan a nd D T o s hniw a l Pri v a c y preserving association rule mining over distributed databases using genetic algorithm Neural Comput Appl  vol 22 no 1 pp 351Ð364 2013 Lichun Li received the bachelorÕs degree in information engineering from the Beijing University of Posts and Telecommunications in 2002 the masterÕs degree in communication and information systems from the China Academy of Telecommunication Technology in 2006 and the Ph.D degree in computer science from the Beijing University of Posts and Telecommunications in 2009 He is currently a Postdoctoral Research Fellow with the INFINITUS Laboratory School of Electrical and Electronic Engineering Nanyang Technological University Singapore His research interests include privacy and security in cloud and big data Rongxing Lu SÕ09ÐMÕ11ÐSMÕ15 received the Ph.D degree in computer science from Shanghai Jiao Tong University Shanghai China in 2006 and the Ph.D degree in electrical and computer engineering from the University of Waterloo Waterloo ON Canada in 2012 From 2012 to 2013 he was a Postdoctoral Fellow with the University of Waterloo Since 2013 he has been an Assistant Professor with the School of Electrical and Electronic Engineering Nanyang Technological University Singapore His research interests include computer network security mobile and wireless communication security and applied cryptography He was a recipient of the Ca nada Governor General Gold Metal Kim-Kwang Raymond Choo SMÕ15 received the Ph.D degree in information security from the Queensland University of Technology Australia in 2006 He is currently a Cloud Technology Endowed Associate Professor with the University of Texas at San Antonio an Associate Professor with the University of South Australia and a Guest Professor with the China University of Geosciences He was named one of 10 Emerging Leaders in the Innovation category of The Weekend Australian Magazine MicrosoftÕs Next 100 series in 2009 and is a recipient of the ESORICS 2015 Best Research Paper Award the 2015 Winning Team of GermanyÕs University of Erlangen-Nuremberg Digital Forensics Research Challenge the 2014 Australia New Zealand Policing Advisory AgencyÕs Highly Commende d Award the 2010 Australian Capital Territory Pearcey Award the Fulbright Scholarship in 2009 the 2008 Australia Day Achievement Medallion and the British Computer SocietyÕs Wilkes Award Anwitaman Datta is an Associate Professor with the School of Computer Science and Engineering NTU Singapore He lead s the Self and Algorithmic aspects of Networked Distributed Systems Research Group at NTU Jun Shao received the Ph.D degree from Shanghai Jiao Tong University Shanghai China in 2008 He was a Postdoctoral Fellow with the School of Information Sciences and Technology Pennsylvania State University State College PA USA from 2008 to 2010 He is currently a Full Professor with the Department of Information Security Zhejiang Gongshang University Hangzhou China His research interests include network security and applied cryptography 


