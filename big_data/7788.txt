Fast 1-itemset Frequency Count using CUDA Roger Luis Uy Computer Technology Department De La Salle University Manila, Philippines Nelson Marcos Software Technology Department De La Salle University Manila, Philippines   Abstract Frequent itemset mining is one of the main and compute-intensive operations in the field of data mining.  The said algorithm is use in finding frequent patterns in transactional databases.  The 1-itemset frequent count is used as basis for finding succeeding k itemset mining.  Thus there is a need to 
speed-up this process.  One of the techniques to speed-up the process is using the Single Instruction Multiple Thread \(SIMT architecture.   This architecture allows a single instruction to be applied to multiple threads at the same time.  Current graphics processing unit \(GPU\, which contains multiple streaming processing units, uses SIMT architecture.  In order to abstract the GPU hardware from the programming model, NVIDIA introduces the compute unified device architecture \(CUDA\as an extension to existing programming languages in order to support SIMT. This paper discusses how 1-itemset frequent count is 
implemented in SIMT using CUDA KeywordsÑFrequent itemset mining; CUDA programming graphics processing unit; data mining; big data I  I NTRODUCTION  In the last few decades, CPU processor technology has improved by leaps and bounds.  This is due to the motivation to improve the performance of single-threaded application The processor speed has almost breach the 4-GHz barrier if it is not due to power and thermal constraints.  Those obstacles led to the change in design philosophy of processor technology which espouses lower processor speed but with increased number of processing cores  The introduction of dual-core and multi-core processors 
provide noticeable benefits for applications with dynamic workloads that are composed of short sequences of computations operations, high data locality and unpredictable control flow.  This is typical in running multiple multimedia applications   T h u s th e design of CP U is f o cused  m o re on  having large cache memory, multiple branch predictors and complex instruction set decoders Another improvement in the processor technology is the increase in the processing speed and the number of processing cores of the graphics processing unit \(GPU\  Originally, the GPU is designed as a non-programmable 3D graphics accelerator.  But it has gradually evolved and later on become a programmable processing unit Each GPU is composed of hundreds of processing units 
performing simple task.  This exhibits the characteristic of a parallel computing platform akin to single instruction multiple thread \(SIMT\  Having this architectural characteristic, GPU is suitable for high performance computing \(HPC\ applications that have longer sequences of computations instructions and very minimal control flow instru The compute unified device architecture \(CUDA\was developed by NVIDIA as a programming model in order to abstract the GPU hardware so that programmer can focus more on application development.  CUDA provides development environment to support parallel computing extension to such programming languages as C, C++, Fortran and OpenCL as well 
Frequent itemset mining \(FIM\ sometimes known as frequent pattern mining \(FPM\ is a fundamental task in the field of data mining especially in association rule mining ARM\ [4 FIM aim s to extract h i dden pattern s i n larg e volume of data by discovering frequently co-occurring groups of items in a dataset.  Once the hidden patterns are extracted and strong association are found among them, useful information can then be derived from the patterns Generating frequent itemset is a time demanding task.  Its operation starts from finding initial frequent 1-itemset and later on generating combinations of frequent k itemsets for a given 
k items. This operation leads to an exponential time complexity \(i.e  the realm of dataintensive scientific discovery \(DISD\more commonly known as çbig data n is a potentially large number which can lead to a long execution time With improvements on hardware technology to support parallel processing, much of the existing FIM works still focuses on algorithmic improvements 6 7 8  rath er than parallelizing existing algorithms.  As of now, most of the FIM algorithms are generally serial in nature This paper compares the speedup improvement of 
generating frequent 1-itemset using parallel computation via GPU as compare to its sequential processing counterpart Section II provides an overview of the 1-itemset frequency count while section III provides an overview of CUDA and GPU.  Section IV and V respectively discuss the sequential and CUDA implementation of 1-itemset frequency count Section VI describes the datasets used in the experiment with the result and analysis are presented in section VII.  Finally conclusion and recommendation are discussed in Section IX  210 978-1-5090-2597-8/16/$31.00 c  2016 IEEE 


II  1-I TEMSET F REQUENCY C OUNT   There have been many researches on algorithm to find frequent itemsets efficiently.  Most of the previously proposed algorithms are categorized either as candidate generation based or pattern g r o w t h  based 10  T h e can didate generation is based on Apriori algorithm.  This algorithm introduces the concept of downward closure property known as Apriori or anti-monotone.  The property states that a k itemset is frequent only if all of its sub-items \(i.e k 1\ are frequent.  Thus, the frequent itemsets are generated by first scanning the datasets to find the initial frequent 1-itemset From this 1-itemset, the candidate k 1\itemsets are generated and their corresponding support count is counted On the other hand, the pattern growth method avoids the need for candidate generation by constructing complex structure that contains sufficient information within the dataset.  First proposed by th e frequent pattern  FP  growth algorithm instead generates a FP-growth tree which stores the compressed version of the database.  The frequent itemsets are then generated based on the FP-growth tree using a method known as FP-growth pattern. This algorithm scans the dataset twice to generate the FP-growth tree. During the first scan, the dataset is scanned to obtain the 1-itemset.  The 1-itemset is then sorted in frequency-descending order. During the second scan, the items in each transaction are processed based on the sorted 1-itemset order and the FP-growth tree is then built during the second pass based on the 1-itemset and the transactions of the dataset Regardless of the type of algorithms, the generation of the 1-itemset frequency count is the building block of the respective algorithm.  Thus, it is of importance that this procedure be processed in the least amount of time III  CUDA AND GPU CUDA was introduced by NVIDIA as a programming model to abstract the GPU hardware [3 h e C  C++, Fortran and OpenCL to support parallel processing via GPU hardware.  In CUDA, the computational element of an algorithm that is to be executed in GPU is known as a kernel  Once compiled, a kernel is composed of many threads that execute the same program in parallel.  One can view a thread as an iteration in a loop.  For example, figure 1 illustrates a sequential C program construct to perform a vector addition of one million elements of array A with array B and the result is placed in array C  01 for \(int i=0; i<1000000; ++i    i] + B  i   Fig. 1.  Code illustrating sequential vector addition of one million elements   The equivalent CUDA kernel function for the vector addition is illustrated in figure 2.  Note that arrays A and B are source operands, array C is the destination operand, while integer variable n contains the number of elements to be added 1___global___void 2 vectorAdd \(const float *A, const float *B, float *C, int n 3 4 int i = blockDim.x * blockIdx.X + threadIdx.x 5 if \(i < n A  i] + B  i  6  Fig. 2.  CUDA kernel function equivalent to perform vector addition of  one million elements  Once compiled, the instruction  i] + B  i]é in lin e 5 of figure 2 can be viewed as an unrolled loop as shown in figure 3.  Each unrolled instruction is viewed as one thread and all threads are executed at the same time in parallel by the GPU C  0] + B 0 C  1] + B 1   C[99999 A  99999 99 B[99 999 99  Fig. 3 Unroll loop of C A  i    B i  Each thread has its own thread index as defined by the built-in variable threadIdx.x as shown in line 4 of figure 2.  It has its own private local memory and registers to store data Multiple threads are then grouped together into thread blocks each having its own block index as defined by the built-in variable blockIdx.x as shown in line 4 of figure 2.  Threads within the thread block can share information through its own block shared memory.  They can also synchronize among themselves either sequentially or concurrently using barrier synchronization.  At the top of the hierarchy, known as grid  is a group of thread blocks..   Each grid executes one kernel function.  Blocks within the grid can share information using global memory  Data from the host computer needs to be transferred to the GPU memory before the GPU can process it.  The GPU memory should first allocate the size of the data to be transferred using the cudaMalloc instruction  as illustrated in lines 3 and 5 of figure 4  Once the memory is allocated, the data is then transferred to the GPU memory using the cudaMemcpy instruction as illustrated in line 4 and 6 of figure 4.    Once the GPU has processed the data, it is transferred back to the CPU memory using the same cudaMemcpy  instruction as illustrated in line 10 of figure 4.  Note that the transferring of data between host memory and GPU memory presents a significant overhead in GPU processing  Once the data are transferred, the calling program should define the number of threads in a block before invoking the kernel function.  This is illustrated in line 7 of figure 4.  The typical value is 256 and the maximum value is 1024 as defined by the compute capability CC\ a device.  The compute capability  CC   of a device identifies the features supported by the GPU hardware and is used by applications during runtime to determine which hardware features and instructions are available on the cuurent GP e number of blocks per grid is then computed based from the 2016 IEEE Region 10 Conference TENCON  P roceedings of the International Conference 211 


number of threads per block.  This is illustrated in line 8 of figure 4.  The CUDA kernel is invoked liked a typical function but with the added parameters of number of blocks per grid and the number of threads per block inside the symbol.  This is illustrated in line 9 of figure 4 1int main \(void 2  3 cudaMalloc\(\(void **\&d_A, size 4 cudaMemcpy\(d_A, h_A, size, cudaMemcpyHostToDevice 5 cudaMalloc\(\(void **\&d_B, size 6 cudaMemcpy\(d_B, h_B, size, cudaMemcpyHostToDevice 7 int threadsPerBlock = 256 8 blocksPerGrid = \(n+threadsPerBlock-1\readsPerBlock 9 vectorAdd <<<blocksPerGrid, threadsPerBlock>>> \(d_A d_B, d_C, n 10 cudaMemcpy\(h_C, d_C, size cudaMemcpyDeviceToHost 11 Fig.4.  Main C program to invoke the CUDA kernel function   Inside a GPU, it is composed of at least two \(2\ming multiprocessor \(SM\ Within each SM, threads are dispatch for execution in a group of 32 threads known as warp Up to four warps can be be dispatched per execution during each clock cycle IV  S EQUENTIAL I MPLEMENTATION OF 1-I TEMSET F REQUENCY C OUNT  The sequential implementation of 1-itemset count is implemented using C++ on a Visual Studio 2013 platform The data in the dataset are structured as composed of having n  transactions \(i.e., row\ and each transaction having k items Since the purpose of the 1-itemset count is to count the frequency of each 1-itemset, there is no need to maintain the structure of the file.  Thus, the data is read in and store as linear array in the memory Once the data are in the memory, the sequential 1-itemset count is performed.   The 1-itemset count is stored in an Item  structure containing two members index containing the item while count contains the frequency of that item in the dataset The process loops through the linear data in the memory and performs the frequency count by updating the corresponding Item.count of Item.index  V  CUDA  I MPLEMENTATION OF 1-I TEM S ET F REQUENCY C OUNT  The CUDA implementation of 1-itemset count is implemented using C++ on a Visual Studio 2013 platform with CUDA extension toolkit 6.5 installed.   The system is developed on a machine having a GPU hardware of  NVIDIA Geforce GT 640M with 2GB of memory and compute capability of 3.0.  The CPU handles the transfer of data from file to memory while the GPU handles the 1-itemset frequency count.  As with the sequential implementation, the data is stored as linear array in the memory frequency count is stored as an Item structure containing two members.  In order to minimize the overhead involved in data movement, the structure is declared using CUDAês cudaMallocHost instead of using the new instruction of C++.  This is illustrated in line 3 of figure 5.  This is to page locked that part of the memory in order to prevent the virtual memory system from swapping it out of the memory.   The direct memory access \(DMA mode can now be used to transfer data between CPU and GPU and vice versa thereby speeding up the process.  Using this method, the average memory throughput between CPU to GPU memory is 8.763GB/s or an improvement factor of 23 as compared to the traditional method which has an average throughput of  only 378.993MB/S  1int main \(void 2  3 cudaMallocHost\(\(void **\&h_struc, tsize 4 cudaMalloc\(\(void **\&d_struc, tsize 5 6 int threadsPerBlock = 256 7 blocksPerGrid = \(n+threadsPerBlock-1\readsPerBlock 8 vectorOneItemCount <<<blocksPerGrid threadsPerBlock>>> \(d_array, d_struc, itemcount 9 cudaMemcpy\(h_struc, d_struc, tsize cudaMemcpyDeviceToHost 10 11 cudaFreeHost\(h_struc Fig.5.  Main calling program code snippet to illustrate hos page-locked memory is initialized The code snippet for CUDA kernel is illustrated in figure 6.  Array A which contains n  k item is used as indexed to the B.num structure.  For example, if A  th en B 5 num  which is the counter for item 5 is incremented. Atomic operation is used since it is possible for different threads to access the same B.num structure at the same time.  This prevents race hazard to happens but at the expense of serializing the parallel operation.   This has an affect especially with dense dataset since a particular item exists more often than others  1___global___void 2 vectorOneItemCount \(int *A, Item *B, int n 3 4 int i = blockDim.x * blockIdx.X + threadIdx.x 5 if \(i < n\ atomicAdd\(&\(B[A um   6  Fig. 6.  CUDA kernel code snippet to perform 1-itemset frequency count  VI  D ATA S ETS USED IN THE E XPERIMENT  To evaluate the sequential as well as the CUDA implementation of the 1-itemset frequency count, four real datasets and two synthetic datasets are used for the performance tests.  These datasets are often used in previous study of frequent itemset mining.  Both real and synthetic datasets are obtained from [12  T h e accid ent dataset co n t a i n s  traffic accident data of Flanders, Belgium.  The pumsb dataset 212 2016 IEEE Region 10 Conference TENCON  P roceedings of the International Conference 


contains census data.  The kosarak dataset contains clickstream data of a Hungarian on-line news portal.  The retail dataset contains the retail market basket data from an anonymous Belgian retail store.  The T10I4D100K and T40I10D100K synthetic dataset are used to simulate retail consumer basket data.  The former contains an average transaction length of 4 while the latter contains an average transaction length of 10.  Table I summarizes the characteristics of these datasets, where the average transaction length is denoted by Avg. length the number of items is denoted by items the number of transactions is denoted by  Trans and the total number of total items, which is obtained by multiplying the average length with the total number of transactions, is denoted by Total Items Note that both accident and pumsb datasets are dense datasets while the rest are sparse datasets  T ABLE I. S UMMARY OF DATASETS USED  Dataset Avg length Items Trans Total Items Accidents 33.8 468 340,183 11.500,870 Pumsb 74 2113 49,046 3,629,404 Retail 10.31 16470 88,162 908,576 Kosarak 7.02 36831 1,001,053 7,029,013 T10I4D100K 10.1 870 100,000 1,010,228 T40I10D100K 39.6 942 100,000 3,960,507  VII  R ESULTS  AND A NALYSIS  Table II shows the execution time, in milliseconds, of the sequential 1-itemset frequency count process.  Each dataset is executed 10 times in order to average out the result.  The result excludes initialization and transfer time from file to memory as they are common for both sequential and CUDA implementations.  The execution time is linear to the total number of items in the dataset.   Table III shows the execution for CUDA 1-itemset frequency count.  One can note that the overhead of transferring data from CPU to GPU accounts to half of the execution time.  The improvement factor of CUDA over the sequential implementation ranges from 2.90 times for dense dataset and up to 21.67 times for sparse datasets.   The reason sparse dataset has a better speedup is due to the fact that the items are much spread apart and thus less contention for the same index counter.  This leads to less atomic operation which will cause serialization of the process  T ABLE II. E XECUTION TIME OF S EQUENTIAL 1ITEM FREQUENCY C OUNT  Dataset Execution time \(in msecs Accidents 132.67 Pumsb 120.07 Retail 117.72 Kosarak 126.98 T10I4D100K 122.02 T40I10D100K 124.19  T ABLE III. E XECUTION TIME OF CUDA  1ITEM FREQUENCY C OUNT  Dataset Transfer time CPU to GPU \(in msecs GPU exec Time \(in msecs Transfer time GPU to C PU \(in msecs Total time in msecs Accidents 7.39 25.57 12.73 45.69 Pumsb 4.95 11.62 9.17 25.74 Retail 1.41 6.48 4.36 12.25 Kosarak 9.07 18.85 8.64 36.56 T10I4D100K 0.78 2.83 2.03 5.64 T40I10D100K 2.7 9.02 4.62 16.32  VIII  C ONCLUSION AND R ECOMMENDATIONS   From the result, it can be seen that CUDA implementation performs at least two times better than sequential implementation.  In the frequency itemset mining, the 1itemset frequency count is just the first step.  Thus overall system improvement can further be improved by developing other modules in CUDA kernel.  Thus the overhead involved in transferring data between CPU and GPU memory can be diluted.  In this information age, the volume of data has reached exa-scale level already. Thus, there is a need to take advantage of all available computing resources.  Improvement in hardware technology and its widespread availability has made parallel processing a viable option in improving FIM algorithm IX  B IBLIOGRAPHY  1  Roger Luis Uy, "Beyond multi-core: A survey of architectural innovations on microprocessor," in 2014 international conference on humanoid, nanotechnology, information technology, communication and control, environment and management \(HNICEM 2014 Palawan, 2014 pp. 1-6 2  Peter N. Glaskowsky, "NVIDIA's Fermi: The first complete GPU computing architecture," NVIDIA Corporation, White paper 2009 3  Gerassimos Barlas Multicore and GPU programming: An integrated approach Waltham, Ma, USA: Morgan Kaufmann, 2015 4  Florin Gorunescu Data mining: Concepts, Models and Techniques, ISRL 12 Berlin, Germany: Springer-Verlag, 2011 5  Zhihong Deng and Zhonghui Wang, "A new fast vertical method for mining frequent patterns International Journal of Computational Intelligence Systems vol. 3, no. 6, pp. 733-744, December 2010 6  Zhong-Hui Wang, JiaJian Jiang, and Zhi-Hong Deng, "A new algorithm for fast mining frequent itemsets using N lists Science China Information Services vol. 55, no. 9, pp. 2008-2030, 2012 7  Zhi-Hong Deng and Sheng-Long Lv, "Fast mining frequent itemsets using Nodesets Expert Systems with Applications vol. 41, pp. 45054512, 2014 8  Zhi-Hong Deng, "DiffNodesets: An efficient structure for fast mining frequent itemsets Applied Soft Computing vol. 41, pp. 21423, April 2016 9  Rakesh Agrawal and Ramakrishnan Srikant, "Fast algorithms for mining association rules," in Proceedings of the 1994 international conference on very large data bases \(VLDB'94 Santiago, 1994, pp. 487-499  Jiawie Han, Jian Pei, and Yiwen Yin, "Mining frequent patterns without candidate generation," in Proceedings of the 2000 ACMSIGMOD international conference on management of data \(SIGMOD '00 Dallas 2000, pp. 1-12  NVIDIA Corporation, "CUDA C programming guide," NVIDIA Corporation, Programming GUide PG-02829-001_v7.5, 2015  Bart Goethals. \(2004\ Frequent itemset mining dataset repository Online  h t t p    f i mi u a ac be d at a    2016 IEEE Region 10 Conference TENCON  P roceedings of the International Conference 213 


 International Conference on Computing, Communication and Automation \(ICCCA2016   91      local counts of the candidates received from all the nodes checks count against minimum support threshold and produces candidates and its count as key-value pairs We have executed the MapReduce based Apriori using data structure trie and hash table trie \(HTtrie\without filleted transactions and then with filtered transactions. Figures 1 and 2 show the execution times corresponding to trie hash table trie \(HTtrie\ trie on filtered transactions \(TrieOnFT\ and hash table trie on filtered transactions HTtrieOnFT for datasets BMS_WebView_1 and BMS_WebView_2 respectively Fig. 1  Execution time of four variations of Apriori on BMS_WebView_1 From both the Fig 1 and Fig  2 we can see that the two techniques hash table trie and filtered transactions drastically reduce the execution time when applied independently and adds more improvement when applied jointly Fig. 2  Execution time of four variations of Apriori on BMS_WebView_2 x-none B  Speculative Execution Speculative execution is a strategy of Hadoop that provides fault tolerance and reduces job execution time 1 When a TaskTracker a daemon process running on slave nodes that executes map and reduce tasks\performs poorly or crashes the JobTracker a demon process running on master node that accepts job and submit tasks to TaskTrackers launches another backup task on another nodes to accomplish the task faster and successfully This process of redundant execution is called speculative execution and the backup copy of task is called speculative task 25  Am on g the or i gin al  and speculative tasks which one completes first is kept while other  good at most of the time but it affects cluster efficiency by duplicating tasks So it should be disabled it is enabled by default when a cluster has limited resources Speculative execution is best suited in homogeneous environments but degrades the job performance in heterogeneous environments  One can dis able sp ec ul at ive execu ti on for M ap per s  and Reducers by setting the value of "mapreduce.map.speculative and "mapreduce.reduce.speculative" to false either in mapredsite.xml file of cluster configuration or in job configuration of MapReduce code   We have executed the MapReduce based Apriori with trie data structure when speculation is on and off. Fig   3 shows the observed execution time on dataset BMS_WebView_1 for v arying value of minimum support   Fig. 3  Execution time of trie based Apriori when speculative execution is enabled and disabled In Fig 3 it can be seen that speculative execution comes in action only for a job taking longer time to complete Speculative task is not launched for short jobs. It is launched when all the tasks of a job is assigned resources and then for a task running for a longer time or failed to complete   x-none C  Detection and Elimination of Slower Nodes Nodes in a heterogeneous cluster are of different capability Heterogeneity may arise due to differences i n hardware as well as using virtualization technology Virtualization facilitates efficient utilization of resources and environments for different operating systems. There are many benefits of VM-hosted \(virtual machine hosted\Hadoop such as lower cost of installation, on demand cluster setup, reuse of remaining physical infrastructure on demand expansion and contraction of cluster size   I n our lo ca l cl us te r Ta ble 1 and 2 NN NameNode DN3 DataNode3 and DN4 DataNode4 are virtual machines while DN1 DataNode1 and DN2 DataNode2 are physical machines However virtualization provides efficient re-use and management of resources but on the cost of performance. Virtual machines are slower in comparison to physical machines We have observed that eliminating slower node from cluster improves the performance. Detecting a slower node is not obvious always We have used the idea of measuring heterogeneity in terms of data processing speed proposed in    wh e re sa m e Ma pRedu c e job with  sam e am ou nt of da ta  is  separately executed on each node and running time of each node is recorded. We have not executed the job separately on each node but on whole cluster We have executed the job with 12 Mappers corresponding to frequent 2-itemsets generation for higher and lower value of minimum support on 


 International Conference on Computing, Communication and Automation \(ICCCA2016   92      BMS_WebView_1 dataset Fig 4 and Fig  5 show the sn apshot of execution time of 12 Mappers on 4 DNs of the cluster for two values of minimum support Fig. 4  Sn apshot of execution time of 12 Mappers on 4 DNs for higher value of minimum support Fig. 5  Sn apshot of execution time of 12 Mappers on 4 DNs for lower value of minimum support Fig. 6  Execution time of trie based Apriori on two different clusters In Fig 4 and 5 the DN on which the particular task is executed can be found by exploring  4 tasks showing elapsed time 28 29 and 30 sec are on VMs DataNodes DN3 and DN4 whereas tasks showing elapsed time 19 and 20 sec are on physical DataNodes DN1 and DN2 Similar difference can be seen in Fig 5 where task s showing around 15 minutes are on VMs DataNodes and around 8 minutes are on Physical DataNodes. Removing both DN3 and DN4 greatly slowdown the performance but removing only one of them significantly enhances the performance. Fig. 6 shows the relative influence A natural question may arise here that is it a better idea to remove a slower DN? Here we would like to mention the two  perspective As a Hadoop developer one can design and incorporate an efficient data distribution scheme that assigns data blocks to DNs as per ratio of their processing speed Similarly an efficient load balancing scheme can be incorporated to migrate tasks from busy or slower DNs to idle  perspective where we only focus of MapReduce based applications not on underlying  eliminate slower DN if that reduces the execution time x-none D  Data Locality and Data Block Distribution HDFS breaks a large file into smaller data blocks of 64 MB and stores as 3 replicated copies on DNs of cluster. Data blocks are the physical division of data Data file can be logically divided using input split. The number of Mappers to be run for a job is equal to the number of logical splits. If input split is not defined then the default block size is the split size In all the earlier cases discussed above, we have specified the split size which resulted into 12 Mappers and input file was not divided physically. In this case we have not used split size instead divide the input file into smaller blocks of 200 KB. So for the dataset BMS_WebView_1 there are 5 data blocks block0 block1 block2 block3 block4 This requires 5 Mappers to being executed corresponding to these 5 data blocks When an input file is put into HDFS it is automatically splitted into blocks and distributed to different  control Each time when putting the input file into HDFS  results a different distribution We set the replication factor RF\ to 1 so that a block resides on only one DN. We put the same input file into HDFS twice and get two different block distributions \(BD\ BD1 and BD2. Now we set the replication factor to 4 so that all blocks would be available on each DN Table III describes the three block distributions BD1, BD2 and BD3 The influence of three block distributions on the execution time of trie based Apriori for different value of minimum support is shown in Fig 7  TABLE III  T ABLE S TYLES  BD  RF  Blocks on DN1  Blocks on DN2  Blocks on DN3  Blocks on DN4  BD1  1  block1 block2  block0 block3  No Block  block4  BD2  1  block0 block3  block2  block4  block1  BD3  4  All 5 blocks  All 5 blocks  All 5 blocks  All 5 blocks   


 International Conference on Computing, Communication and Automation \(ICCCA2016   93     Fig. 7  Execution time of trie based Apriori on three different block distributions x-none In Fig   7, BD1 exhibits minimum execution time compared to BD2 and BD3 In BD1 blocks are located only on three DNs i.e two physical a nd one VMs DN So in this case Mappers are not running on both slower DNs tha t make the execution faster. Execution time for BD2 is poor than that o f BD1 due to using both VMs DNs  BD3 exhibits the worst performance since all the blocks are available on each DN MapReduce processes a data block locally on the DN where the block is present. F or block distribution BD3, all Mappers are running on the same DN since all blocks are locally available to each node In different attempt of running a job DN may be d ifferent each time but all the Mappers are being run on a same DN. All Mapper s running on the same DN does not make use of available resources which leads to increased execution time Here it can be seen that due to higher replication factor data locality may be a hurdle that slow down the execution  x-none E  Controlling Parallelism with Split Size x-none Hadoop is designed to process big datasets that does not mean one cannot be benefited for small datasets. Apriori is a CPU-intensive algorithm and consumes a significant time for smaller datasets. To reduce the execution time we need more than on e task running in parallel Split is used to control the number of map tasks for a MapReduce job A split may consist of multiple blocks and there may be multiple splits for a single block So without changing the block size user can control the number of Mappers to be run for a particular job We have used the method setNumLinesPerSplit\(Job job int numLines  of class NLineInputFormat  from MapReduce library to set the number of lines per split. In our earlier cases we were running multiple Mappers against different parts of the same block Here we set the split size 5K lines on block distribution BD3 which contains 5 data blocks This creates 12 splits i.e 12 Mappers  size for BD3 the n  blocks are considered as input s plits. Fig   8 shows the difference in execution time for these two cases  Here it can be seen that how the split size controls the parallelism Smaller split size launches more number of Mappers which consequently increase the parallelism. It does not mean that more number of Mappers always results into better performance. Increasing the number of Mappers beyond a particular point starts to degrade the performance due to unnecessary overheads and shortage of resources   To achieve the right level of parallelism it must be taken care that the map task is CPU-intensive or CPU-light as well as the size of dataset to be processed Fig. 8  Execution time of trie based Apriori with Input Split and without Input Split on BD3 x-none F  Issues Regarding MapReduce Implementation The efficiency of an algorithm running as a MapReduce job is extensively influenced by data structure used and algorithm itself A third factor that cannot be ignored is the implementation technique.  Implementation technique may be regarding to implementation of various modules of Apriori e.g candidate generation support counting of candidates against each transaction pruning of infrequent itemsets or regarding to MapReduce implementation of Apriori MapReduce implementation of Apriori is central to discussion here. A major issue in MapReduce based Apriori is to invoke candidate generation i.e apriori-gen  Algorithm 7 at appropriate place inside Mapper class. In our implementations we have invoked apriori-gen  inside customized method map of Mapper class. In Mapper class, two methods setup  and map  are customized and one method apriori-gen  is defined Method setup  is called once at the beginning of a task It is customized to read frequent itemsets of previous iteration from distributed cache and to initialize prefix tree Method apriori-gen  generates candidates using prefix tree containing frequent itemsets The map  is invoked for each line of input split of dataset If there are 100 lines of input assigned to a Mapper then map method will be invoked for 100 times Subsequently it invokes apriori-gen  repeatedly each time Since apriori-gen  method produces candidates which is independent of input instance, so need not to invoke repeatedly inside map  method The apriori-gen  metho d is  computation intensive and increases the execution time when invoked repeatedly. This repeated computation can be fixed if we invoke apriori-gen  outside of map  Theoretically it sounds good but did not work when invoked inside setup  method We have also tried another way in which apriorigen  is invoked inside overrided method run  of Mapper class but again could not achieve expected reduction in execution time VI  C ONCLUSIONS  In this paper we have investigated a number of factors affecting the performance of MapReduce based Apriori algorithm on homogeneous and heterogeneous Hadoop 


 International Conference on Computing, Communication and Automation \(ICCCA2016   94   cluster and presented strategies to improve the performance It has been shown that how hash table trie data structure and transaction filtering technique can significantly enhance the performance Factors like speculative execution physical  VMs DataNodes, data locality block distribution and split size are such that their proper tuning can directly enhance the performance of a MapReduce job even without making algorithmic optimization Approaches of MapReduce implementation of Apriori is another important factor that also influence the performance R EFERENCES  1  J. Han and M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann Publishers 2006  2  J S Ward a  b y d ata a survey of big d ata d  http://arxiv.org/abs/1309.5821v1 3  Apache Hadoop, http://hadoop.apache.org 4  Big data is useless without algorithms Gartner says http://www.zdnet.com/article/big-datais useless-withoutal gorithmsgartner-says/, Retrieved Nov. 2015 5   algorithms for mining association rules  Proceedings Twentieth International Conference on Very Large Databases, Santiago 1994 pp. 487 499 6   and distributed association mining a survey Concurrency, IEEE, vol 7, no. 4,pp. 14 25, 1999 7  K. Bhaduri, K. Das, K. Liu, H. Kargupta and J. Ryan, Distributed Data Mining Bibliography 2008  8  HDFS  Architecture Guide https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html Retrieved Sept 2015  9  MapReduce Tutorial http://hadoop.apache.org/docs/current/hadoopmapreduce-client/hadoop-mapreduce-clientcore/MapReduceTutorial.html, Retrieved Sept. 2015 10  Yahoo Hadoop Tutorial http://developer.yahoo.com/hadoop/tutorial/index.html 11   ACM SIGOPS Operating Systems Review vol 37 no 5 pp 29 43 2003  12    Commun., vol. 51, pp 107 113, 2008 13   mining using clouds an experimental implementation of apriori over mapreduce    14   Apriori: association rules algorithm based on mapreduce  IEEE 2014  15   as a programming model for association rules algorithm on hadoop  nternational Conference on Information Sciences and Interaction Sciences ICIS 2010 vol. 99, no. 102, pp. 23 25  16   implementation of apriori algorithm based on mapreduce  h ACIS International Conference on  Software Engineering Artificial Intelligence Networking and Parallel  Distributed Computing IEEE 2012 pp 236 241  17   Hadoop as a platform for distributed association rule mining   COMPUTING 2013 the Fifth International Conference on Future Computational Technologies and Applications, pp. 62 67  18  M-Y Lin P-Y Lee and S based frequent itemset mining algorithms on mapreduce in  Proceedings 6th International Conference on  Ubiquitous Information Management and  2012, Article 76 19   itemset mining on Hadoop  Proceedings IEEE 9th International Conference on Computational Cybernetics \(ICCC\Hungry, 2013, pp. 241 245  20   strategy of mining association rule based on cloud computing   Proceedings IEEE International Conference on Business Computing and Global Informatization BCGIN 2011 pp 29 31 21  Honglie Yu, Jun Wen, Hongmei Wang and Li Jun, "An improved apriori algorithm based on the boolean matrix and Hadoop Procedia Engineering 15 \(2011\1827-1831, Elsevier 22  Matteo Riondato, Justin A. DeBrabant, Rodrigo Fonseca and Eli Upfal PARMA: a parallel randomized algorithm for approximate association rules mining in mapreduce in Proceedings 21st ACM international conference on information and knowledge management 2012 pp 8594  23  Jiong Xie et al Improving mapreduce performance through data placement in heterogeneous hadoop clusters in IEEE International Symposium on Parallel & Distributed Processing,  Workshops and Phd Forum \(IPDPSW\ 2010, pp. 19  24  Matei Zaharia Andy Konwinski Anthony D Joseph Randy Katz and Ion Stoica Improving mapreduce performance in heterogeneous environments in 8th USENIX Symposium on Operating Systems Design and Implementation \(OSDI\ 2008, vol. 8, no. 4, pp. 2942  25  Hsin-Han You, Chun-Chung Yang and Jiun-Long Huang, "A load-aware scheduler for MapReduce framework in heterogeneous cloud environments in Proceedings of the ACM Symposium on Applied Computing, 2011, pp. 127-132 26  Faraz Ahmad Srimat Chakradhar Anand Raghunathan and T N Vijaykumar, "Tarazu optimizing MapReduce on heterogeneous clusters," ACM SIGARCH Computer Architecture News, vol. 40, no. 1 pp. 61-74, 2012 27  HADOOP PERFORMANCE TUNING white paper Impetus Technologies Inc October 2009 https://hadooptoolkit.googlecode.com/files/White%20paperHadoopPerformanceTuning.pdf 28  Apache Hadoop NextGen MapReduce YARN http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarnsite/YARN.html, Retrieved Sept. 2015 29  SPMF Datasets http://www.philippe-fournierviger.com/spmf/index.php?link=datasets.php 30   ata structure for data  in Mathematical and Computer Modelling, vol 38  no. 7, pp. 739-751, 2003 31  Ferenc Bodon A fast apriori implementation in Proceedings IEEE ICDM workshop on frequent itemset mining implementations  90, 2010 32  Sudhakar Singh, Rakhi Garg and P. K. Mishra  analysis of apriori algorithm with different data structures on hadoop cluster  International Journal of Computer Applications, vol. 128, no. 9, pp. 4551  2015  33  Christian Borgelt Efficient implementations of apriori and 351clat in Proceedings IEEE ICDM workshop on frequent itemset mining   34  Ferenc Bodon, "Surprising results of Trie-based fim algorithms," FIMI 2004  35  Ferenc Bodon A trie-based APRIORI implementation for mining frequent item sequences," in Proceedings 1st international workshop on open source data mining: frequent pattern mining  implementations ACM, 2005 36  Hadoop Wiki Virtual Hadoop https://wiki.apache.org/hadoop/Virtual%20Hadoop  


002 004\002 005\002 006\002 007\002 003\002 033\002 034\002 035\002 002\004\002\005\002 006\002 
002 005 007 033 035 004\002 004\005 004\007 004\005\006\007\003\033 
C Increased Rule Length 
002 005 007 033 035 004\002 004\005 004\007 004\033 004\035 005\002 020\020\030\027\011\025\021\013 026\032\025\025\011\020\021\030\032 025 011\021\012\030\017 023\004\002\002 023\004\002\004 
 
002\003\004\005\006\007\010\011\012\013\014\010\015\004\013'\013\022\015\(\023\011\\004\015\004\012\007\013 024\007\024!\004\007\013 002\003\004\005\006\007\010\011\012\013\014\010\015\004\013'\013\022\015\(\023\011\\004\015\004\012\007\013 024\007\024!\004\007\013 
Figure 6 Throughput for increasing length rule candidates The Y-axis is measured in billion evaluations per second The X-axis shows the corresponding rule length As before the gures include the theoretical trend-line computed from amplifying naive-sc on the GPU using Eq 4 reason the observed improvement is stable depending mostly on the item number Assigning multiple candidate rule collections to a single block resulted in Figure 7 Percentage improvement in execution time as compared to the default-tpsc kernel for the individually optimized kernels when using synthetic data   Figure 8 Percentage improvement in execution time as compared to the default-tpsc kernel for the individually optimized kernels when using real data For both kernels we observe a similar behaviour when we increase rule candidate length to a number larger than 32 After that point we require evaluating the preìx in two phases following a technique similar to parallel reduction although using warp vote functions This extra phase requires an additional synchronization step which increases the total execution time per iteration Additionally when we have prominent rules with item indices in sequence i.e accidents dataset as indicated by its sparsity pattern caching transactions does not provide any improvement However when there are many rules with out of sequence preìxes the cost of uncoalesced memory accesses matches the synchronization cost as indicated by experiments on dataset 1 VIII C ONCLUSION In this paper we studied the support count operation commonly used in association rule mining problems We proposed a work-efìcient parallel algorithm that is suitable for massively parallel architectures Furthermore we presented a data layout scheme used to enable low overhead coordination of the processing elements reduce the memory requirements and achieve high off-chip memory bandwidth utilization Furthermore we discussed in detail low level optimization strategies related to effective use of shared memory while presenting a simple strategy for resolving shared memory bank conîicts incurring minimal additional work However there is still some additional issues that we need to address Firstly we already considering resolving the issue of 
025 015\013\036\021\031\013\020 015\036\021\031\013\020 
improvement over the default-tpsc execution time A combination of loop unrolling and increase shared memory utilization from storing more candidate rules in the same block was the reason for the observed improvement In contrast enabling caching of transactions in shared memory with kernel mrs-tpsc presented less improvement in the relative execution time compared to mr-tpsc The culprit is this case is the additional synchronization step which is required after loading the data in shared memory Finally experiments performed on dataset 2 and 4 indicate similar behavior to our previous experiments where multiprocessor underutilization was limiting the maximum possible performance increase Even in the case where we increase the workload of participating blocks interleaved execution of warps is limited since as the block size is small In this section we discuss the effects of discovering rules with length larger than 32 Due to lack of space we present the results from the execution on synthetic data 1 and accidents which are good representatives of the observed behaviour We focus on the mrs-tpsc and mr-tpsc variations which we established to be highly optimized throughout our experiments   
025 015\013\036\021\031\013\020 015\036\021\031\013\020 
037\005\005\010 \004\012\007!\013   002 003 004\002 004\003 005\002 005\003 006\002 006\003 007\002 002\004\002\005\002 006\002 011\012\012\004\005\007\010\011\012\013   002 003 004\002 004\003 005\002 005\003 006\002 006\003 007\002 002\004\002\005\002\006\002 004\007\024\010$\013  014%\031&&\013 
18 
027\011$\012\014\017\021\036\021\031\013\020  025\012%&\011\036\013\020\022\036\022'\031\014  025\012%&\011\036\013\020\022\036\022\020\031\014 021\(\011\032\015\011\021\030\020    002 004 005 006 007 002\004\002\005\002 006\002 
1431 
1431 


 volume 22 pages 207Ö216 ACM 1993  R Agra w al R Srikant et al F ast algorithms for mining association rules In  volume 1215 pages 487Ö499 1994  E Ansari G Dastghaibif ard M K eshtkaran and H Kaabi Distrib uted frequent itemset mining using trie data structure  35\(3 2008  M Atzmueller and F  Puppe Sd-mapÖa f ast algorithm for e xhausti v e subgroup discovery In  pages 6Ö17 Springer 2006  C Creighton and S Hanash Mining gene e xpression databases for association rules  19\(1 2003  W  F ang M Lu X Xiao B He and Q Luo Frequent itemset mining on graphics processors In  pages 34Ö42 ACM 2009  K Geurts G W ets T  Brijs and K V anhoof Proìling of high-frequenc y accident locations by use of association rules  1840 2003  A Ghoting G Buehrer  S P arthasarathy  D Kim A Nguyen Y K Chen and P Dubey Cache-conscious frequent pattern mining on modern and emerging processors  16\(1 2007  G Grahne and J Zhu Ef ciently using preìx-trees in mining frequent itemsets In  volume 90 2003  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation In  volume 29 pages 1Ö12 ACM 2000  J Hipp U G  untzer and G Nakhaeizadeh Algorithms for association rule miningÑa general survey and comparison  2\(1 2000  R Jin and G Agra w al An algorithm for in-core frequent itemset mining on streaming data In  pages 8Öpp IEEE 2005  R Jin and G Agra w al Systematic approach for optimizing comple x mining tasks on multiple databases In  pages 17Ö17 IEEE 2006  E Lindholm J Nick olls S Oberman and J Montrym Nvidia tesla A uniìed graphics and computing architecture  2 2008  J Liu Y  P an K W ang and J Han Mining frequent item sets by opportunistic projection In  pages 229Ö238 ACM 2002  L Liu E Li Y  Zhang and Z T ang Optimization of frequent itemset mining on multiple-core processor In  pages 1275Ö1285 VLDB Endowment 2007  B Mobasher  R Coole y  and J Sri v asta v a Automatic personalization based on web usage mining  43\(8 151 2000  E  Ozkural B Ucar and C Aykanat Parallel frequent item set mining with selective item replication  22\(10 2011  J Pei J Han H Lu S Nishio S T ang and D Y ang H-mine Hyper structure mining of frequent patterns in large databases In  pages 441Ö448 IEEE 2001  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster  In  pages 467Ö473 Springer 2003  C Silv estri and S Orlando gpudci Exploiting gpus in frequent itemset mining In  pages 416Ö425 IEEE 2012  A T ajbakhsh M Rahmati and A Mirzaei Intrusion detection using fuzzy association rules  9\(2 2009  T  T assa Secure mining of association rules in horizontally distrib uted databases  26\(4 2014  K W ang M Stan and K Skadron Association rule mining with the micron automata processor In  2015  M J Zaki Scalable algorithms for association mining  12\(3 2000  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors In  pages 43Ö43 IEEE 1996  F  Zhang Y  Zhang and J D Bak os Accelerating frequent itemset mining on graphics processing units  66\(1 2013  Y  Zhang F  Zhang Z Jin and J D Bak os An fpga-based accelerator for frequent itemset mining  6\(1 2013 
002 004 005 006 007 002 004\005\035 005\003\033 006\035\007 003\004\005 002 033 004\005 004\035 005\007 002 004\005\035 005\003\033 006\035\007 003\004\005 
ACM SIGMOD Record Proc 20th int conf very large data bases VLDB IAENG International Journal of Computer Science Knowledge Discovery in Databases PKDD 2006 Bioinformatics Proceedings of the fth international workshop on data management on new hardware Transportation Research Record Journal of the Transportation Research Board The VLDB Journal FIMI ACM SIGMOD Record ACM sigkdd explorations newsletter Data Mining Fifth IEEE International Conference on Data Engineering 2006 ICDEê06 Proceedings of the 22nd International Conference on IEEE micro Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining Proceedings of the 33rd international conference on Very large data bases Communications of the ACM Parallel and Distributed Systems IEEE Transactions on Data Mining 2001 ICDM 2001 Proceedings IEEE International Conference on Advances in Knowledge Discovery and Data Mining Parallel Distributed and Network-Based Processing PDP 2012 20th Euromicro International Conference on Applied Soft Computing Knowledge and Data Engineering IEEE Transactions on Proceedings of the 2015 IEEE 29th International Parallel and Distributed Processing Symposium Knowledge and Data Engineering IEEE Transactions on Supercomputing 1996 Proceedings of the 1996 ACM/IEEE Conference on The Journal of Supercomputing ACM Transactions on Reconìgurable Technology and Systems TRETS 
Figure 9 Execution time measured for increasing rule size  the Xaxis indicates the rule length multiprocessor under-utilization For dataset with low number of items we can assign individual groups of threads in the same block to different rule collections effectively increasing the block size as well as utilization Secondly we would like to adapt our solution to an architecture consisting of multiple GPUs and address challenges related to partial result sharing A CKNOWLEDGMENT This work was supported by the U.S National Science Foundation under grant ACI-1339756 R EFERENCES  Frequent itemset mining dataset repository  2015 URL http://ìmi.ua.ac.be/data  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases In 
014\010\015\004\013\016!\004\005!\021\013 026\027\012\007\030\004\007\010\005\013\031\013 014\010\015\004\016!\004\005!\021\013 037\005\005\010 \004\012\007!\013 
015\036\021\031\013\020 015\013\036\021\031\013\020 
1432 
1432 


 Frequent pattern mining Current status and future directions  2007  R Agra w al and R Srikant F ast algorithms for mining association rules in  1994  M J Zaki Scalable algorithms for association mining  vol 12 no 3 pp 372Ö390 2000  J Han J Pei and Y  Y in Mining frequent patterns without candidate generation in  2000  Y  Zhang F  Zhang and J Bak os Frequent itemset mining on large-scale shared memory machines in  2011  F  Zhang Y  Zhang and J D Bak os  Accelerating frequent itemset mining on graphics processing units  vol 66 no 1 pp 94Ö117 2013  Y  Zhang  An fpga-based accelerator for frequent itemset mining  vol 6 no 1 pp 2:1Ö2:17 May 2013  P  Dlugosch  An efìcient and scalable semiconductor architecture for parallel automata processing  vol 25 no 12 2014  I Ro y and S Aluru Finding motifs in biological sequences using the micron automata processor in  2014  R Agra w al T  Imieli  nski and A Swami Mining association rules between sets of items in large databases in  1993  H No yes Micronês automata processor architecture Reconìgurable and massively parallel automata processing in  June 2014 keynote presentation  M J Zaki  Parallel data mining for association rules on shared-memory multi-processors in  1996  L Liu  Optimization of frequent itemset mining on multiple-core processor in  2007  I Pramudiono and M Kitsure ga w a P arallel fp-gro wth on pc cluster in  2003  E Ansari  Distributed frequent itemset mining using trie data structure  vol 35 no 3 p 377 2008  W  F ang  Frequent itemset mining on graphics processors in  2009  B Goethals Surv e y on frequent pattern mining  Univ of Helsinki Tech Rep 2003  C Bor gelt Ef cient implementations of apriori and eclat in  2003 p 90  Frequent itemset mining dataset repository   http mi.ua.ac.be/data  J Rabae y  A Chandrakasan and B Nik oli  c  2nd ed Pearson Education 2003 
100 1000 10000 5 50 500 5000 re_sup = 0.12 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails for re_sup = 0.08 re_sup = 0.08 
GPU fails re_sup = 0.42 re_sup = 0.42 Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm  Computation Time \(s Size of Dataset \(MB Eclat-6C \(32nm Eclat-1G \(28nm Apriori-AP 45nm Apriori-AP 32nm Apriori-AP 28nm GPU fails re_sup = 0.3 re_sup = 0.3 b T100 Figure 11 Performance prediction with technology normalization as a function of input size implementation In contrast the capability of our AP ARM solution scales nicely with the data size since the AP was designed for processing streaming data With the challenge of the big data era a number of other complex pattern mining tasks such as frequent sequential pattern mining and frequent episode mining have attracted great interests in both academia and industry We plan to extend the proposed CPU-AP infrastructure and automaton designs to address more complex pattern-mining problems A CKNOWLEDGMENT This work was supported in part by the Virginia CIT CRCF program under grant no MF14S-021-IT by C-FAR one of the six SRC STARnet Centers sponsored by MARCO and DARPA NSF grant EF-1124931 and a grant from Micron Technology R EFERENCES  J Han 
et al Data Min Knowl Discov Proc of VLDB 94 IEEE Trans on Knowl and Data Eng Proc of SIGMOD 00 Proc of CLUSTER 11 J Supercomput et al ACM Trans Reconìgurable Technol Syst et al IEEE TPDS Proc of IPDPSê14 Proc of SIGMOD 93 Proc of Fifth International Symposium on Highly-Efìcient Accelerators and Reconìgurable Technologies et al Proc of Supercomputing 96 et al Proc of VLDB 07 Proc of PAKDD 03 et al IAENG Intl J Comp Sci et al Proc of DaMoN 09 Proc FIMI 03 Digital Integrated Circuits 
1 10 100 1000 10000 0.1 1 10 100 1000 
a Webdocs 
699 


