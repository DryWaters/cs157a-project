IEEE lntemational workshop on ImlIigent Om Acquisition and Advanced Computing System Te~hn010g and Apptications 8-10 Septembe 2W3 Lviv Ukraine A C1,assification Method of Fuzzy Association Rules             Jianjiang Lu1J,3','Baowen.X~',3\Hongji Yang4 1 Department of Computer Science and Engineering Southeast University, Nanjing 2 10096 China 2 PLA University of Science and Technology Nanjing 210007, China 3 Jiangsu Institute of Software Quality Nanjing 210096 China 4 School of Computing 
De Montfort University, Leicester LE1 9BH England    Abstract Partition method of interval is adopted in current classification based on associations CBA but this method cannot reflect the actual distribution of data and exists the problem of sharp boundary Quantitative attributes are partitioned into several fuzzy sets by fuzzy c-means algorithm and search technology of Apriori algorithm is improved to discover interesting fuzzy association rules which are used to build classifcation system Becausefizzy c-means algorithm can embody the actual distribution of the data and fuzzy sets can soften partition boundary the classifcation 
system of the fuzzy association rules can obtain better classifcatiori accuracy than two popular classification methods C4.5 and CBA Keywords  classification; quantitative attributes fuzzy c means algorithm fuzzy association rules 1 INTRODUCTION Classification is an important topic in data mining technology The problem of classification is concemed with the mining of a set of production rules that can allow the values of an attribure in a database to be accurately predicted based on those of other attributes The decision tree are the most popular approaches to solve the classification Association rules can also be used for classification CBA The categorical and quantitative attrihutes 
are mainly dealt with in 1-3 In particular when dealing with quantitative attributes their domains are usually divided into equal-width or equal-frequency intervals But it introduces some problems The first problem is that equi width partitioning cannot embody the actual distribution of the data The second problem is caused by the sharp boundary In most cases the resulting intervals are not too meaningful and are hard to understand Ref 181 uses fuzzy set to soften partition boundary of the domains, and presents the concept of fuzzy association rules but it does not present partition algorithm that can embody the actual distribution of the data and does not present the mining algorithm for fuzzy association rules which fits for large databases 
Ref 9 uses the relational fuzzy c-means algorithm to partition the quantitative attributes into several linguistic terms, then the problem of mining association rules with linguistic terms is introduced by combining linguistic terms The relational fuzzy c-means algorithm can emhody the actual distribution of the data. Furthermore linguistic terms can soften partition boundary But combining linguistic terms can obtain excessive association rules with linguistic terms so the mining algorithm cannot fit for large database In this paper quantitative attributes are first partitioned into several fuzzy sets by fuzzy c-means algorithm and search technology of Apriori algorithm is improved to discover interesting fuzzy association rules which are used to build classification system The rest of this paper is 
organized as follows In section 2 Apriori algorithm is improved for mining fuzzy association rules In section 3 the classification system of fuzzy association rules is built In section 4 the classification system of fuzzy association rules is simplified The conclusions are briefed in section 5 2 AN ALGORITHM FOR MINING FUZZY ASSOCIATION RULES Let T r,,r,,..-,t be a relational database I represents the j-th record in T let I i,,i2;:.,i be the attribute set where ij denotes a Boolean categorical or quantitative attribute tj[i represents value of 
the j-th record in attribute it Values of the record in attribute need to be partitioned into several fuzzy sets for mining fuzzy association rules Let A and A he two values of the record in Boolean attribute then two values can be partitioned into two fuzzy sets A and A  Categorical attribute with fewer values can be partitioned with the same method Each quantitative atmbute is partitioned into several fuzzy sets by FCM     This work was supported in part by the National Natural Science Foundation of China \(NSFC 
60073012 National Grand Fundamental Research 973 Program of China 2M\National Research Foundation for the Doctoral Program of Higher Education of China Natural Science Foundation of Jiangsu Province China BK2001 W 0pening.Foundation of State Key Laboratoly of Soitware Engineering in Wuhan University and Opening Foundation of Jiangsu Key Laboratory of Computer Information Processing Technology in Soochow University c Coirupondence to Baowen Xu Department of Computer Science and Enginening Southeast University Nanjing 210096 China Email bwxu@seu.edn.cn 0-7803-8138-6/03/$17.00 82003 IEEE 248 


algorithm"01 After clustering with the FCM algorithm partition matrix U and c centers v are obtained Fuzzy sets are usually represented with triangular fuzzy numbers for classification The method is as follows Let pi\(x be the grade of membership of xj in the fuzzy set with center vk  let x   x  pt I 2 p xj E 1,2  c We first find the samples with the minimum grade of membership at both sides of the center vk in Xk u[v let the left sample with the minimum grade of membership he xi  its grade of membership be x  and let the right sample with the minimum grade of membership be xi  its grade of membership be pt\(x  then the expression of triangular fuzzy numbers f'\(.r or a,v,,b with center v1 is In order to mine fuzzy association rules we first build a new database through original database T In this new database attributes are fuzzy sets so attributes are called fuzzy attributes Values of the record in fuzzy attributes are obtained as follows Let i he a fuzzy set of attribute i  then ii is a fuzzy attribute. Value of the j-th record in fuzzy attribute it is ii\(fj[ikJ iL\(f,[i,J is the grade of membership of tj[ik with respecl to fuzzy set ii In this new database the set of all fuzzy attributes are still denoted I the value of the j-th record in fuzzy attribute y is still denoted lj\(yk  It is obvious that tj\(yk falls in O,I Let An association rule is an implication of the form X  Y  Because attributes in X and Y are fuzzy attributes X  Y is called fuzzy association rule The support and confidence of fuzzy association rule are defined as follows Definition 1 The support of X is defined as follows  249 Fuzzy attribute sets with at least a minimum support are called frequent fuzzy attribute sets Definition 2 The support of X follows Y is defined as Definition 3 The confidence of X  Y is defined as follows Because fj\(yr falls in O,11 we can know that all subsets of a kequent fuzzy attribute set must also be frequent according to definition 1 With the above finding we can directly modify Apriori algorithm to mine fuzzy association rules 3 CLASSIFICATION SYSTEM OF FUZZY ASSOCIATION RULES Let 1=[i,,i2,,..,im,i he the attribute set of classification databases Attribute i is a categorical attribute with values C,,C,;..,C  which are all class labels Let y=\(y,.y2,...,ym be a sample where y,,y2,'.',ym are the values taken by attributes il,i2;..,im In this section we will discuss how to use fuzzy association rules to classify the sample y  We use interesting fuzzy association rules to build classification system Interesting fuzzy association rules are the rules with at least a minimum support and a minimum confidence respectively Suppose we use the algorithm in section 2 to discover M interesting fuzzy association rules as following For GI 2   M R If X\(l,k is B\(l,k and  and XUk isB\(l,,k then i is C where X 1 k X\(2,k  X\(1  k E il i i I B\(I  k are fuzzy sets of attribute X\(l,,k C E C,,C,,-.,C,l  We use these association rules to build the rule base of classification system When a sample y is to he classified compute the discriminant function values of each class gh\(y according to the following formula where X\(j,k y is the value taken by attribute X\(j,k in the sample Y  B\(j,k j,k y is the grade of membership of X\(j,k y in fuzzy set B\(j,k  n;=,B\(j,k y is the activated degree of sample 


y to the fuzzy association rule R  We compare these discriminant function values and take the class label corresponding to the maximum value as the classification result of the sample y  This inference method considers the information provided by each rule for sample classification At the same time, because fuzzy association rules are easy to be understood the classification system built has a better interpretability In order to check the accuracy of our classification system this paper selects Wine dataset from UCI Machine Learning Repository which has 13 quantitative attributes 1 categorical attribute and 178 records Each quantitative attribute is partitioned into three fuzzy sets represented with triangular fuzzy numbers by FCM algorithm In the experiment ten-fold cross-validation method is applied to estimate the classification accuracy The dataset is randomly divided into ten disjoint subsets with each containing approximately the same number of records Sampling is stratified by the class labels to ensure that the subset class proportions are roughly.the same as those in the whole dataset For each subset a classifier is built using the records not in it The classifier is then tested on the withheld subset to obtain a cross-validation estimate of its accuracy Then ten cross-validation estimates are averaged to provide an estimate for the classifier built from all the data The cross-validation estimate in each subset is obtained as following 221A perfect classification system has a few rules and a good accuracy In order to control the complexity of the classification system we first mine 1000 interesting fuzzy association rules on the withheld subset and rank these rules by the their support Some rules that have high support are selected to evaluate the accuracy In order to save the computing time we select the number of rules at a multiple of 50 such as 50 100,150 et al We select 20 times and select the number of rules with the best accuracy Table 1 shows the experimental results with the classification system of fuzzy association rules CFAR Table 1 Experimental results We compare CFAR with classification methods C4Sr6\222 and CBA The algorithm of C4.5 is downloaded from http://www.cse.unsw.edu.au/-quanlian CBA is downloaded from http://www.comp.nus.edu.sg/-dm2 Where the minimum support is set to I the minimum  250 confidence is set to 50 and other parameters aTe unchanged The accuracy of C4.5 is 92.7 CBA is 91.6 CFAR is 97.15686 It is obvious that the accuracy of CFAR is better than CBA and C4.5 in the Wine dataset 4 SIMPLIFYING CLASSIFICATION SYSTEM There are two issues that must be addressed in the classification system of zzy association des The first is that a huge number of rules could contain noisy information The second is that a huge set of rules would extend the classification time This could be an important problem in applications where fast responses are required So fuzzy association rules should be pruned The pruning techniques that we employ are the following Definition 4 Let r X  C he a fuzzy association rule the lift of a rule r is defined as follows conf\(C is the expectation confidence of consequent C without arbitrary condition If IW is greater than 1 then rule r is positively correlated meaning X encourages C  If ifr\(r is less than 1 then rule r is negatively correlated meaning X discourages C  If lift is equal to 1 then X and C are independent Fuzzy association rules that their lifts are less than 1 or equal to 1 should be pruned It is insufficient to prune the fuzzy association rules with their lifts The difference of minimum confidence minconf-dif is introduced next to prune the fuzzy association rules farther Definition 5 Given two fuzzy association rules r  X a C and r2  X\222x C  We say that the rule r2 is sub-rule of the rule r1 if X\222c X  The improvement of a fuzzy association rule can be defined as the minimum difference between its confidence and the confidence of any proper suh-mle with the same consequent Definition 6 Given a fuzzy association rule X  C  the improvement of X  C imp\(X C is defined as follows  Given a minconf-dif if im&X  C is greater than minconf-dif then the rule X  C contains new information Otherwise we consider that the sub-rule of fuzzy association rule X SC have contained the information provided by X  C  So fuzzy association rule X  C should he pruned For example given two fuzzy association rules Rule I if blood pressure is high and dextrose is normal then he has an illness sup=lO conJc41 Rule 2 if hlood pressure is high then he has an illness sup=12 conf=40 Suppose minconf-dij is set to 5 It is obvious that Rule 2 is a sub-rule of Rule 1 The difference between the confidence of Rule I and the confidence of Rule 2 is I 


which is less than 5 So rule 2 cannot provide new information and will be pruned To Wine dataset figure 1 shows the average number of the fuzzy association rule pruned with different minconf-dif in the ten-fold cross-validation method In addition we can notice from figure 1 that fuzzy association rules can be pruned effectively with the minconf-dif increasing Figure 2 shows the average test accuracy with different minconf-dij In addition we can notice from figure 2 that some useful fuzzy association rules may be,pnhed when the minconf-dif increases, this will make the test accuracy descend A perfect classification system has a few rules and a good accuracy Let minconf-difbe 0.10 the average number of the fuzzy association rule is 52 the average test accuracy is 96.60 Comparing with the classification system in section 3, the average number of the fuzzy association rule descends 68 The average test accuracy only descends 0.55686 So we can claim that the simplified classification system is a perfect classification system 0 I 0 005 01 015 02 025 mmconf-dd Fig 1 Average rule numben 9 0.7  0.6  0.5  I 0 0.05 0.1 0.15 0.2 0.25 minconf-dif Fig 2  Average accuracy 5 CONCLUSIONS A classification system of the fuzzy association rules is presented In this classification system quantitative atrributes are partitioned into several fuzzy sets by fuzzy c-means algorithm and search technology of Apriori algorithm is improved to discover interesting fuzzy association rules which are used to build classification system. Because fuzzy c-means,algonthm can embody the actual distribution of the data and fuzzy sets can soften partition boundary the classification system of the fuzzy association rules can obtain better classification accuracy than two popular classification methods C4.5 and CBA 6 REFERENCES I B Liu W Hsu Y Ma Integrating classification and association de mining Proceedings of the Intemarional Conference on Knowledge Discovery and Data Mining USA New York 1998, pp 80-86 2 M Mehta J Rissanen R Agrawal SLIQ A fast scalable classifier for data mining Proc of the 9 bt'l Con on Extending Database Technology India Mumbai 1996, pp 544555 3 P Smyth R.M Goodman An information theoretic approach to rule induction from databases IEEE Transactions on Knowledge and Data Engineering 4 4 1992\pp 301-316 4 M.S Chen I Han P.S Yu Data mining An overview from a database perspective IEEE Transactions on Knowledge and Data Engineering 8 6 1996 pp 866-883 5 U.M Fayyad Mining databases Towards algorithms for knowledge discovery Bulletin of the Technical Commitree on Data Mining 21 I 1998 pp 335 341 6 J.R Quinlan C4.5 Program for Machine earning Morgan Kaufmann. San Mateo CA 1993 pp. 20-30 7 R Srikant R Agrawal. Mining quantitative association rules in large relational tables Proceedings of the ACM -SICMOD Conference on Management of Data Montreal Canada 1996, pp 1-12 8 M.K Chan F Ada H.W Man Mining fuzzy association rules in database Proceedings of rhe ACM Sixth Intemational Conference on Information and Knowledge Management Las Vegas Neveda 1997 pp 10-14 9 J J Lu Z L Song 2 P Qian Mining linguistic valued association rules Journal of Sofhvare 12 4 2001 pp. 607-611 in Chinese  Hathaway J.W Davenport J.C Bezdek Relational dual of the c-means algorithms Parrem Recognition 22 \(2 1989\pp. 205-212 I I R Agrawal R Srikant Fast algorithms for mining association rules Proceedings of the 1994 International Conference on Very Large Databases Santiago, Chile 1994, pp 487-499 25 1 


    Fig 6  Predict Flow and Real I\221low ofKey Link S2k I I_ ____ll__l____l--.l  Fig 7  Predict Flow and Real Flow of Key Link S\222k Fig 8  Predict Flow and Real Flow ofKey Link L V SIMULATIONS SUMMARY AND CONCLUSIONS Table 1V shows the performance comparison between Dynamic Assignment algorithm new algorithm of this paper and NPR AAE stands for average absolute error and APE is Average percent crror The results presented are for an Intel  Pentium IO 600 MHz computer with 64 MB RAM running Windows 98 in command prompt mode with no other system demands While individual results may vary the findings are indicative of general performance Not only AAE and APE decrease a lot than Dynamic Assignment Algorithm but also does Ex Time from 15s to 8s.Although it seems that Ex time is approximately the same for new algorithm and NPR with increasing size of the network new algorithm will show its superiority over NPR in time consuming However its predict accuracy isn\222t as good as NPR Simulation test result shows the new traffic network flow forecasting algorithm based on data mining above can exactly decrease executive time and increase predict accuracy at the same time comparable with dynamic assignment algorithm Tab.Ar: Performance Index Comparison Index 1 MA\222PE I ExTime\(m 1 Dynamic Assignment I 12 I 15 Proposed Algorithm I 9 1.44 NPR I 7.5 I I 56 But its predict accuracy isn\222t as good as NPR Some future research work is needed to solve this problem Maybe in one sub network two key links can be selected out to reason other links\222 traftic flow or some other deduction method can be used instead of fuzzy logic based on data mining Additionally since new traffic data is producing every day so the history database in this algorithm is also increasing at a quick speed then some data record must be deleted, but how to manage and update the history database Some research also should be done to try to assure a updated and concise database all the time Acknowledgement This work has been supported in part by the Outstanding Oversea Scientist Award by the SDPC the State Development Planning Commission\and the KIE Grant by the Institute of Automation Chinese Academy of Sciences all to Prof Fei-Yue Wang We are grateful to the helps from the Bureau of Beijing Traffic Administration And the authors are sole responsible for the arguments and conclusions in this paper References I Xiaoyan Gong 223Review of traffic network flow forecasting\224 ICSEC, CASIA Report December 2001 2 M Papageorgiou 223Dynamic Modeling assignment and route guidante in traffic networks\224 Transportation Research B 248\(6 pp 471-495.1990 Zhaosheng Yang 223Urban traftic flow routing theory and model\224 Chinese Communication Press Beijing China January 2000 4 G.A.Davis N.L.Nihan 223Nonparametric Regression and Short Term Freeway Traffic Forecasting\224 ASCE Journal of Transportation Engineering 1 l7\(2 pp 178-188,March 1991 B.L.Smith M.J.Demetsky 223Traffic Flow Forecasting Comparison of Modeling Approaches\224 Journal of Transportation Engineering 123\(4 pp 261 266.1997 6 Xiaoyan Gong Shuining Tang and Feiyue Wang 223Three Improvements on KNN-NPR for Short-time Traffic Flow Forecasting\224 Proceeding of the IEEE 5th International Conference on ITS Singapore September 2002 7 Agrawal A.N Swami 223Mining Association Rules between Sets of Items in Large Databases\224 SIGMOD Conference pp 207-216 1993 3 5 197 


I 1 highway network I association anaivsis key link S'k association rules traffic flow forecasting based flow prediction value of S'k Fig.1 Scheme of trafic network flow prediction algorithm based on data mining Fig.2  A Loop Highway Network 198 


not share an y itemsets with the b oundary of an y itemset X k 2X whic hisac hild of X  In other w ords for eac h c hild X k 2X of X w e remo v e from F  X c  all mem ber itemsets in F  X k c   Then these pruned b oundaries ma ybe used in order to generate the rules The resulting algorithm is illustrated in Figure 6 This algorithm uses as input the itemsets X whic h are generated in the 014rst phase of the algorithm at the appropriate lev el of minsupp ort  The algorithm FindBoundary of Figure 5 ma y b e used as a subroutine in order to generate all the b oundary itemsets These b oundary itemsets are then pruned and the rules are generated b y using eac h of the itemsets corresp onding to the b oundary in the an teceden t 4.1 Rules with constrain ts in the an teceden t and consequen t It is easy enough to adapt the ab o v e rule generation metho d so that particular items o ccur in the an teceden t and/or consequen t Consider for example the case when w e are generating rules from a large itemset X  Supp ose that w e desire the an teceden t to con tain the set of items P and the consequen t to con tain the set of items Q W e assume that P  Q 022 X  W e shall refer to P as the ante c e dent inclusion set  and Q as the c onse quen t inclusion set  In this case w e need to rede\014ne the notion of maximalit y and b oundary itemsets A v ertex v  Y  is de\014ned to b e a maximal ancestor of v  X  at con\014dence lev el c an teceden t inclusion set P  and consequen t inclusion set Q if and only if P 022 Y  Q 022 X 000 Y  S  Y  S  X  024 1 c  and no strict ancestor of Y satis\014es all of these constrain ts Equiv alen tl y  the b oundary set con tains all the itemsets corresp onding to maximal ancestors of X  It is easy to mo dify the algorithm discussed in Figure 5 so that it tak es the an teceden t and consequen t constrain ts in to accoun t The only di\013erence is that w e add an un visited v ertex v  T  to LIST if and only if S  T  024 S  X  c  and T 023 P  Also a v ertex v  R  is added to BoundaryList  only if it satis\014es the mo di\014ed de\014nition of maximalit y  5 Generation of the adjacency lattice In this section w e discuss the construction of the adjacency lattice The pro cess of constructing the adjacency lattice requires us to 014rst 014nd the primary itemsets There are t w o main constrain ts in v olv ed in c ho osing the n um ber of itemsets to prestore 1 Memory Limits In order to a v oid I/O one ma y wish to store the primary itemsets and corresp onding adjacency lattice in main memory  1 Recall that Theorem 2.1 c haracterizes the size required b y the adjacency lattice for this purp ose Assume that w e desire to 014nd N itemsets Note that b ecause of ties in the supp ort v alues of the primary itemsets supp ort v alues ma y not exist for whic h there are exactly N itemsets Th us w e assume that for some slac kv alue N s w e wish to 1 Storing the adjacency lattice on disk is not suc h a bad option after all The total I/O is still prop ortional to the size of the output rather than the n um b er of itemsets prestored Recall that the graph searc h algorithms used in order to 014nd the large itemsets and asso ciation rules visit only a small fraction of the v ertices in the adjacency lattice F unction NaiveFindThr eshold\(Numb er ofIt emset s N Slack N s  b egin High  max i f Supp ort of item i g Low 0 Gener ated 0 while  Gener ated 62  N 000 N s N  b egin Mid  High  Low   2 Gener ated  DH P  Mid  end return Mid  end Algorithm ConstructL attic e\(Numb er ofItem sets N Slack N s  b egin p  NaiveFindThr eshold\(N N s  F or eac h itemset X  f i 1 i r g with S  X  025 p do Add the v ertex v  X  to the adjacency lattice with lab el S  X  Add the edge E  X 000f i k g X  for eac h k 2f 1 r g end Figure 7 Constructing the adjacency lattice 014nd a primary threshold v alue for whic h the n um ber of itemsets is b et w een N 000 N s and N  2 Prepro cessing Time There ma y b e some practical limits as to ho wm uc h time one is willing to sp end in prepro cessing Consequen tly ev en if it is not p ossible to 014nd N itemsets within the prepro cessing time it ough t to b e able to terminate the algorithm with some v alue of the primary threshold for whic h all itemsets with supp ort ab o v e that v alue ha v e b een found A simple w a y of 014nding the primary itemsets is b y using a binary searc h algorithm on the v alue of the primary threshold using the DHP metho d discussed in Chen et al as a subroutine This metho d is somewhat naiv e and simplistic and is not necessarily e\016cien t since it requires m ultiple executions of the DHP metho d This metho d of 014nding the primary threshold is discussed in the algorithm NaiveFindThr eshold of Figure 7 The time complexit y of the pro cedure can b e impro v ed considerably b y utilizing a few simple ideas 1 It is not necessary to execute the DHP subroutine to completion in eac h and ev ery iteration F or estimates whic h are lo w er b ounds on the correct v alue\(s of the primary threshold it is su\016cien t to terminate the procedure as so on as N or more large itemsets ha v e b een generated at the lev el of supp ort b eing considered 2 It is not necessary to start the DHP pro cedure from scratc h in eac h iteration of the binary searc h pro cedure It is p ossible to reuse information b et w een iterations Let I  s  denote the itemsets whic hha v e supp ort at least s  It is p ossible to sp eed up the preprocessing algorithm b y reusing the information a v ailable in I  Low  Generating k itemsets in I  Mid  is only a matter of pic king those k itemsets in I  Low  whic h ha v e supp ort at least Low  This do es not mean that ev ery itemset in I  Mid  can b e immediately generated using this metho d Recall from 1 ab o v e that the DHP algorithm is often terminated b efore completion if more than N itemsets ha v e b een generated in that iteration Consequen tly  not all itemsets in I  Low  ma ybea v ailable but only those k itemsets for whic h k 024 k 0  for some k 0 are a v ailable Th us w eha v e all 


 0 1 2 3 4 5 6 7 8 9 x 10 4 0 0.002 0.004 0.006 0.008 0.01 0.012 0.014 0.016    Primary threshold Number of itemsets prestored T10.I4.D100K  T10.I6.D100K  T20.I6.D100K  Figure 8 Threshold v aration with itemsets prestored DataSet Conf Sup DHP Online T10.I4.D100K 90 0  3 100 sec instan taneous T10.I6.D100K 90 0  3 130 sec instan taneous T10.I6.D100K 90 0  2 240 sec 2 seconds T20.I6.D100K 90 0  5 100 sec instan taneous T able 3 Sample illustration s of the order of magnitude adv an tage of online pro cessing those k itemsets in I  Mid  v ailable for whic h k 024 k 0  These itemsets need not b e generated again 6 Empirical Results W e ran the sim ulation on an IBM RS/6000 530H w orkstation with a CPU clo c k rate of 33MHz 64 MB of main memory and running AIX 4.1.4 W e tested the algorithm empirically for the follo wing ob jectiv es 1 Prepro cessing sensitivit y The prepro cessing tec hnique is sensitiv e to the a v ailable storage space The larger the a v ailable space the lo w er the v alue of the primary threshold W e tested ho w the primary threshold v alue v aried with the storage space a v ailabili t y W e also tested ho w the running time of the prepro cessing algorithm scaled with the storage space 2 Online pro cessing time W e tested ho w the online pro cessing times scaled with the size of the output W e also made an order of magnitude comparison b et w een using an online approac h and a more direct approac h 3 Lev el of redundancy W e tested ho w the lev el of redundancy in the generated output set v aried with user sp eci\014ed lev els of supp ort and con\014dence W e sho w ed that the lev el of redundancy in the rules is quite high Th us redundancy elimination is an imp ortan t issue for an online user lo oking for compactness in represen tation of the rules 6.1 Generating the syn thetic data sets The syn thetic data sets w ere generated using a metho d similar to that discussed in Agra w al et al Generating the data sets w as a t w o stage pro cess 0 1 2 3 4 5 6 7 8 9 x 10 4 0 2 4 6 8 10 12 14 16 18 x 10 4    Number of itemsets prestored Relative Computational Effort for preprocessing T10.I4.D100K  T10.I6.D100K  T20.I6.D100K  Figure 9 Computation v ariation with itemsets prestored 0 5000 10000 15000 0 10 20 30 40 50 60    Number of rules generated Response Time in seconds T10.I4.D100K  T10.I6.D100K  T20.I6.D100K  Figure 10 Online resp onse time v ariation with rules generated 20 30 40 50 60 70 80 90 100 0 2 4 6 8 10 12   Support fixed at 0.15 Confidence Total Rules Generated Essential Rules  T10.I4.D100K  T10.I6.D100K   Figure 11 Redundancy lev el v ariation with con\014dence 


 0.1 0.15 0.2 0.25 0 10 20 30 40 50 60 70 80 90   Confidence fixed at 90 Support Total Rules Generated Essential Rules  T10.I4.D100K  T10.I6.D100K   Figure 12 Redundancy lev el v ariation with supp ort 1 Generating maximal p oten tially large itemsets The 014rst step w as to generate L  2000 maximal p oten tially large itemsets These p oten tially large itemsets capture the consumer tendencies of buying certain items together W e 014rst pic k ed the size of a maximal p oten tially large itemset as a random v ariable from a p oisson distribution with mean 026 L  Eac h successiv e itemset w as generated b y pic king half of its items from the curren t itemset and generating the other half randomly  This metho d ensures that large itemsets often ha v e common items Eac h itemset I hasaw eigh t w I asso ciated with it whic hisc hosen from an exp onen tial distribution with unit mean 2 Generating the transaction data The large itemsets w ere then used in order to generate the transaction data First the size S T of a transaction w as c hosen as a p oisson random v ariable with mean 026 T  Eac h transaction w as generated b y assigning maximal p oten tially large itemsets to it in succession The itemset to b e assigned to a transaction w as c hosen b y rolling an L sided w eigh ted die dep ending up on the w eigh t w I assigned to the corresp onding itemset I  If an itemset did not 014t exactly itw as assigned to the curren t transaction half the time and mo v ed to the next transaction the rest of the time In order to capture the fact that customers ma y not often buy all the items in a p oten tially large itemset together w e added some noise to the pro cess b y corrupting some of the added itemsets F or eac h itemset I w e decide a noise lev el n I 2 0  1 W e generated a geometric random v ariable G with parameter n I  While adding a p oten tially large itemset to a transaction w e dropp ed min f G j I jg random items from the transaction The noise lev el n I for eac h itemset I w as c hosen from a normal distribution with mean 0.5 and v ariance 0.1 W e shall also brie\015y describ e the sym b ols that w eha v e used in order to annotate the data The three primary factors whic hv ary are the a v erage transaction size 026 T  the size of an a v erage maximal p oten tially large itemset 026 L  and the n um b er of transactions b eing considered A data set ha ving 026 T  10 026 L  4 and 100 K transactions is denoted b y T10.I4.D100K W e tested ho w the primary threshold v aried with the n um b er of itemsets prestored This result is illustrated in Figure 8 The 014gure sho ws that the primary threshold initially drops considerably as the n um b er of primary itemsets increases but it b ottoms out after a while W e also illustrate the v ariation of the computational e\013ort required with the a v ailable storage space in Figure 9 W e note that for the itemset T10.I4.D100K the computational e\013ort required in order to 014nd additional large itemsets after 014nding 20000 itemsets increases considerably with the n um b er of itemsets prestored This is b ecause for this particular data set the a v erage size of a maximal p oten tially large itemset or bask et is only 4 Consequen tly  the total n um b er of p ossible large itemsets is relativ ely limited On the other hand the computational e\013ort for prepro cessing required b y the data sets T20.I6.D100K and T10.I6.D100K is relativ ely similar This sho ws that the computational e\013ort required to 014nd a sp eci\014c n um b er of primary itemsets is more sensitiv e to the size of a t ypical bask et in the data rather than to the size of a transaction W e also tested the v ariation in the online running time of the algorithm with the n um b er of rules generated W e ran the online queries for v arying lev els of input parameters in order to test the correlation b et w een the running time and the n um b er of rules generated This is illustrated in Figure 10 This result is signi\014can t in that it sho ws that the running time of the algorithm increases linearly with the n um b er of rules generated for all the data sets used The absolute magnitude of time required in order to generate the rules w as an order of magnitude smaller than the time required using a direct itemset generation approac h lik e DHP  A brief summary of some sample relativ e 014ndings is illustrated in T able 3 W e also discuss the lev el of redundancy presen t in the rule generation pro cedure Figures 11 and 12 illustrate that the n um b er of redundan t rules is often m uc h larger than the n um b er of essen tial rules The b enc hmark for measuring the lev el of redundancy is referred to as the redundancy ratio and is de\014ned as follo ws Redundancy Ratio  T otal Rules Generated Essen tial Rules 1 Th us when the redundancy ratio is K  then the n um ber of redundan t rules is K 000 1 times the n um b er of essen tial rules The redundancy ratio has b een plotted on the Y-axis in Figures 11 and 12 W e see that in most cases the n um ber of redundan t rules is signi\014can tl y larger than the n um ber of essen tial rules This illustrates the lev el to whic h useful rules often get buried in large n um b ers of redundan t rules Also the redundancy lev el is m uc h more sensitiv e to the supp ort rather than the con\014dence The lo w er the lev el of supp ort the higher the redundancy lev el 7 Conclusions and Summary In this pap er w ein v estigated the issue of online mining of asso ciation rules The t w o primary issues in v olv ed in online pro cessing are the running time and compactness in represen tation of the rules W e discussed an OLAP-lik e approac h for online mining asso ciation rules whic ha v oids redundancy  


Ac kno wledgemen ts W ew ould lik e to thank V S Ja yc handran and Jo el W olf for their extensiv e commen ts and suggestions References  Aggarw al C C and Y uP  S Online Generation of Asso ciation Rules IBM R ese ar ch R ep ort R C 20899  Agra w al R Imielinski T and Sw ami A Mining association rules b et w een sets of items in v ery large databases Pr o c e e dings of the A CM SIGMOD Confer enc e on Management of data pages 207-216 W ashington D C Ma y 1993  Agra w al R and Srik an tR.F ast Algorithms for Mining Asso ciation Rules in Large Databases Pr o c e e dings of the 20th International Confer enc eon V ery L ar ge Data Bases pages 478-499 Septem b er 1994  Agra w al R and Srik an t R Mining Sequen tial P atterns Pr o c e e dings of the 11th Internation al Confer enc e on Data Engine ering pages 3-14 Marc h 1995  Agra w al S Agra w al R Deshpande P  M Gupta A Naugh ton J F Ramakrishnan R and Sara w agi S On the Computation of Multidimensi on al Aggregates Pr o c e e dings of the 22nd International Confer enc eon V ery L ar ge Datab ases pages 506-521  Chen M S Han J and Y uP  S Data Mining An Ov erview from Database P ersp ectiv e IEEE T r ansactions on Know le dge and Data Engine ering V olume 8 Num b er 6 Decem b er 1996 pages 866-883  Dyreson C Information Retreiv al from an Incomplete Data Cub e Pr o c e e dings of the 22nd International Confer enc eon V ery L ar ge Datab ases pages 532-543 Mumbai India 1996  Gupta A Harinara y an V and Quass D Aggregatequery pro cessing in data w arehousing en vironmen ts Pr o c e e dings of the 21st Confer enc eon V ery L ar ge Datab ases Zuric h Switzerland Septem b er 1995  Han J and F u Y Disco v ery of Multiple-Lev el Assocaition Rules from Large Databases Pr o c e e dings of the 21st Internation al Confer enc eon V ery L ar ge Data Bases Zuric h Switzerland 1995 pages 420-431  Harinara y an V Ra jaraman A and Ullman J Implemen ting Data Cub es E\016cien tly  Pr o c e e dings of the 1996 A CM SIGMOD c onfer enc e on Management of Data Mon treal Canada June 1996 pages 205-227  Houtsma M and Sw ami A Set-orien ted Mining for Asso ciation Rules in Relational Databases Pr o c e e dings of the 11th Internation al Confer enc e on Data Engine ering Marc h 1995 pages 25-33  Kaufman L and Rousseeu wP J Finding Gr oups in Data A n Intr o duction to Cluster A nalysis Wiley Series in Probabilit y and Mathematical Statistics 1990  Klemen ttinen M Mannila H Ronk ainen P  T oiv onen H and V erk amo A I Finding in teresting rules from large sets of disco v ered asso ciation rules Pr o c e e dings of the Confer enc e on Information and Know le dge Managements Gaithersburg MD USA 28 No v 2 Dec 1994  Len t B Sw ami A and Widom J Clustering Asso ciation Rules Pr o c e e dings of the Thirte enth International Confer enc e on Data Engine ering pages 220-231 Birmingham UK April 1997  Mannila H T oiv onen H and V erk amo A I Ef\014cien t algorithms for disco v ering asso ciation rules AAAI Workshop on Know le dge Disc overy in Datab ases pages 181-192 Seattle W ashington July 1994  Ng R T and Han J E\016cien t and E\013ectiv e Clustering Metho ds for Spatial Data Mining Pr o c e e dings of the 20th Internation al Confer enc eon V ery L ar ge Data Bases San tiago Chile 1994 pages 144-155  P ark J S Chen M S and Y uP  S An E\013ectiv e Hash Based Algorithm for Mining Asso ciation Rules Pr o c e e dings of the 1995 A CM SIGMOD International Confer enc e on Management of Data pages 175-186 Ma y 1995  Piatetsky-Shapiro G Disco v ery  Analysis and Presentation of Strong Rules Know le dge Disc overy in Datab ases 1991  Sa v asere A Omiecinski E and Na v athe S An E\016cien t Algorithm for Mining Asso ciation Rules in Large Data Bases Pr o c e e dings of the 21st Internation al Confer enc eon V ery L ar ge Data Bases Zuric h Switzerland 1995 pages 432-444  Sh ukla A Deshpande P  M Naugh ton J F and Ramasam y K Storage Estimation for Multidimensi on al Aggregates in the Presence of Hierarc hies Pr o c e e dings of the 22nd Internationa l Confer enc eon V ery L ar ge Datab ases pages 522-531 Mum bai India 1996  Srik an t R and Agra w al R Mining Generalized Asso ciation Rules Pr o c e e dings of the 21st International Confer enc eon V ery L ar ge Data Bases  pages 407-419 Septem b er 1995  Srik an t R and Agra w al R Mining quan titativ e association rules in large relational tables Pr o c e e dings of the 1996 A CM SIGMOD Confer enc e on Management of Data Mon treal Canada June 1996  T oiv onen H Sampling Large Databases for Asso ciation Rules Pr o c e e dings of the 22nd International Conferenc eonV ery L ar ge Datab ases pages 134-145 Mumbai India 1996  Ziark o W The Disco v ery  Analysis and Represen tation of Data Dep endencies in Databases Know le dge Disc overy in Datab ases 1991 


CMP A Fast Decision Tree Classifier Using Multivariate Predictions  449 H Wang and C Zaniolo Mining Recurrent Items in Multimedia with Progressive Resolution Refinement  461 0 Zai'ane J Hun and H Zhu Panel Session 22 Is E-Commerce a New Wave for Database Research Moderator Anant Jhingran IBM T.J Watson Research Center USA Panelists Sesh Murthy IBM T.J Watson Research Center USA Sham Navathe, Georgia Institute of Technology USA Hamid Pirahesh IBM Almaden Research Center USA Krithi Ramamrithan University of Massachusetts-Amherst USA Industrial Session 23 Java and Databases Pure Java Databases for Deployed Applications  477 N Wyatt Database Technology for Internet Applications  700 A Nori Session 24 Association Rules and Correlations Finding Interesting Associations without Support Pruning  489 E Cohen M Datar S Fujiwara A Gionis P Indyk R Motwani J Ullman and C. Yang Dynamic Miss-Counting Algorithms Finding Implication and Similarity Rules with Confidence Pruning  501 S Fujiwara J Ullman and R Motwani Efficient Mining of Constrained Correlated Sets  512 G Grahne L Lakshmanan and X Wang Session 25 Spatial and Temporal Data Analyzing Range Queries on Spatial Data  525 J Jin N An and A Sivasubramaniam Data Redundancy and Duplicate Detection in Spatial Join Processing  535 J.-P Dittrich and B Seeger Query Plans for Conventional and Temporal Queries Involving Duplicates and Ordering  547 G Slivinskas C Jensen, and R Snodgrass xi 


Industrial Session 26 XML and Databases Oracle  The XML Enabled Data Management System  561 S Banerjee V Krishnamurthy M Krishnaprasad, and R Murthy XML and DB2  569 J Cheng and J Xu Session 27 High-Dimensional Data Independent Quantization An Index Compression Technique for High-Dimensional Data Spaces  577 S Berchtold, C Bohm H Jagadish H.-P. Kriegel and J Sander Deflating the Dimensionality Curse Using Multiple Fractal Dimensions  589 B.-U Pagel F Korn and C. Faloutsos Similarity Search for Multidimensional Data Sequences  599 S.-L Lee S.-J Chun D.-H Kim, J.-H Lee and C.-W Chung Session 28 Web-Based Systems WRAP An XML-Enabled Wrapper Construction System for Web Information Sources  611 L Liu C Pu and W. Hun Self-Adaptive User Profiles for Large-scale Data Delivery  622 U Cetintemel M Franklin and C. Giles Industrial Session 29 Main Memory and Small Footprint Databases In-Memory Data Management in the Application Tier  637 The TimesTen Team SQLServer for Windows CE -A Database Engine for Mobile and Embedded Platforms  642 P Seshadri and P. Garrett Join Enumeration in a Memory-Constrained Environment  645 I Bowman and G Paulley xii 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


