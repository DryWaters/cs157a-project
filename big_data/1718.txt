Proceedmgs of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 VARIABLE PRECISION ROUGH SET MODEL BASED DATASET PARTITION AND ASSOCIATION RULE MINING QUAN-DE WANG  XIAN-JIA WANG b XIAN-PE1 WANG a School of Electron and Information Wuhan University, WuHan, China 430070 Institute of system engmeering Wuhan University, WuHan, China 430070 E-MAIL qdwang@263.net Abstract Discovery association rule is one of the most important tasks in data mining Many efficient algorithms have 
heen proposed in literature. In this paper a method of dataset-partition using conceptual hierarchy and Variable Precision Rough Set Model is presented. Algorithm of mining association rule using this technique is designed and asynchronous algorithm is proposed too The efficiency of algorithm and the factors that affect the efficiency of algorithm are analyzed hy mining association rule in dataset artificial generated The result of the experiment proves efficiency of the algorithm Keywords Data mining; Association rule Conceptual hierarchy 1 Introduction At present, data mining has become the focus of research 
in the field such as Artificial Intelligence, Database and statistics The main techniques used in data mining include Classification Cluster Association Rule Bayes Net and Server Vector Machine. These techniques have been used in Web mining, failure diagnosis, invasion detection, and so on Among these techniques association rule mining attracts most interests of researches in data mining, because of its successful applications in the business of supermarkets 1'51 Rough set theory, introduced by Zdaislaw Pawlak in the early 1980s is a new mathematical tool to deal with vagueness and uncertainty Since then this theory has generated a 
great deal of interests among logicians as well as among researchers dealing with machine learning and data mining And this approach has been successfully applied in many areas The Variable Precision Rough Set Model is the extension of Pawlak's Rough set theory and it can be used to solve the problem of classification while there is no functional or uncertain relation among attributions describing objects I In this paper based on Variable Precision Rough Set Model and association rule mining we propose a method to partition the dataset of association rule mining Gith conceptual hierarchy among items of dataset And the algorithm 
of association rule mining using this technique DPARMA Dataset Partition Association Rule Mining Algorithm is presented The asynchronous algorithm of association rule mining using this technique is presented too From the results of the test we have analyzed the efficiency of the algorithm and the main factor that affects the efficiency of the algorithm 2 Variable Precision Rough Set Model based Dataset Partition Among the algorithms of association rule mining the algorithm that partitions the dataset PD algorithm is important and has high efficiency and it's the base of asynchronous algorithms and distributed algorithms 31 PD algorithm partitions the dataset into data-blocks, searches 
frequent itemsets in each data-block and finds frequent itemsets in the whole dataset Methods of dataset partition the most important factor that affects the efficiency of PD algorithm So designing new efficient algorithm of data partition is the first important problem of PD algorithms needed to be solved 2.1 Conceptual hierarchy Definition 1:The dataset of association rule mining is denoted by D D=[tl,tz  tk  t tt={il,iz  ii  I lG6n is a transaction in dataset. Element ij\(llj+p in 
tk IS an item The set of all item in D is denoted by I and I=\(ii,iz  i,J\(misthenumberof iteminD Definition 2:Classes used to classify items in D is a general-item. The set of all general-items are denoted by I and Ip=[il,iz  i mg is the number of general-items Definition 3:If isIuI i'el and i belongs to the class denoted by i then i is the generalization of i denoted with i'=g\(i and i is the specialization of i denoted with i=s\(i 
If there is no general-item i in I satisfy i'=g\(i and i'=g\(i then i is the direct generalization of I, denoted by i'=dg\(i and i is the direct specialization of i denoted by i=ds\(i Definition 4Conceptual hierarchy is directed nncycle graph L=<V,E and V=IuI,.If i i,\(lSr,s<\(m+mg ir,i& V i?dg\(iJ then there is an arc from i to i, in L Defmition 5:In L=<V,E the hierarchy of i lSS\(m+mg ir V denoted by CN\(iJ CN\(i if the 0-7803-7508-4/02/$17.00 ZOO2 IEEE 21 75 


Proceeding of the Fust International Conference on Machine Learning and Cybemetics,'Beijing 4-5 November 2002 number of arcs from i zero and if i,,i lL,s<\(m+mg i V satisfy ipdg\(i then CN\(iJ CN\(i Definition 6The hierarchy number of k<V,E is denoted by MCN\(L\and MCN\(L max CN\(i ie V Definition 7:In L=<V,E let L\(n  i I isIuI CN\(i be the n Conceptual Layer of L From definition 7 we can conclude that L\(I in L=<V,E 2.2 Definition 8  U is a set\(the universe XdJ\(X Y@\(Y a the error classification rate of X relative to Y is denoted by c\(X,Y\and Variable Precision Rough Set Model 1 I XnY I I I x I I x I O IxI=o I 0 c\(X,Y where I I denotes cardinality of a set  Definition 9  An approximation space is a pair U,R where R is an equivalence relation defined on U U  R={Elr En is a set of the equivalence classes of the relation R Let p be a real number within the range OSpd.5 and set X\(XrU the p lower approximation of X is Rp u[EEUIRIc\(E,X  U{EE U/Rlc\(E,X S p  p upper approximation of X is R is called the positive region of X too It's the set of objects in U that can be classified into X with error classification rate not greater than p 2.3 Partition dataset  In all kinds of applications of association rule mining there's usually concephlal hierarchy among items in dataset 161 and we can use it to partition transactions in dataset The conceptual hierarchy L<V E in dataset D reflects the process of generalization of items in D and different items in D can be generalized to different conceptual layers Usually, generalization g\(i of any item or general-item i in L is unique, namely we can classify all items and general items precisely Let conceptual layer L\(n  i l=amCN\(L and set set-i,=\(i lie L\(l i lGQ then set-il set-iz  set-i is a partition of I Let R be the equivalence relation that U  R={set-i setLiz  set-ir we can use des listed below to partition transactions in dataset D Definition lO\(ru1es of transaction partition Let L=<V E be the conceptual hierarchy in dataset D conceptual layer L\(n il,iz  i l<nSMCN\(L set-i,=\(i I ie L\(l s\(i lGQ and a real number p within the range O<p<O.S For any transaction T\(TE D  it can be pdtioned into one data-blockamongD1 Dz ___ D Df 1 If Lower approximation of T RB\(T u{set-i E D/Rlc\(set-i T p Satisfy  RB T  Iset-i  l\(1 5 s 5 r I I and c\(T,set-i 5 p then transaction Tis partitioned into data-block D 2 all set-il set-i2   set-i Rg 7 not satisfy  RB\(T lse-i 81=l\(l<s5r c\(T,set-i 8  p I I and then transaction T is partitioned into data-block Df Algorithm DP\(dataset partition\is listed below Algorithm 1  DP algorithm Input dataset D set-il setLi2  set_ir,n B Output Partition of dataset D D1 D2 _ Dr w I D2=0  h0 W=0 2 transanion TED 31 4 for\(each setis E set-il set~i2   set-ir 5 6 DFDSU\(TJ 7 Taodeachoneofset~iI,set_lZ  setirdon't satisfy 8 W=WuITl if\(T and setLis satisfy NI~S of transaction piition  Nks OfmaCtiO tiO 3 DPARMA Algorithm DPARMA algorithm mines all association rules in dataset D by mining data-blocks D1 Dz  D Df step by step Let data-block D.E  DI Dz  D Df and set of items in D is ID  then IDS  set-i,u ID  set-i Definition 11 rules of itemset partition Let any Xyet-is and any X'G ID  set-is If itemset XuX is in data-block D it's classified using rules below and I~I~1-B*\(max\(lset-i,I,lset_i,l P  Iset-i,I  then itemset XuX can only be included in data-blocks D and Df and it is classifid into itemset class I 2 Otherwise itemset XuX can be included in data blocks D1 Dz _ D and D and it is classified into itemset class n Theorem 1  Let 1 T I denote the average length of all transactions in data-block D n=p IT 1 n'=\(I-P 1 T 1 max-s g I Iset_ill,\(sCt-i2l _ l*et-i,l I-P 2176 


Proceedmgs of the First International Conference on Machine Learning and Cybernetics Beijing 45 November 2002 max i B  i is no-negative integer The number of itemsets in data-block D which belongs to itemset class I is not greater than nl 1-B li jlmar-z The number of itemsets in data-block D which belongs to itemset class U is not meater than nn I<i<W_S mar_i<jrn From definition 10 we get max-sxnax-i and from equation I and equation 2\in theorem 1 we get nl>nll That is the number of itemsets belonging to itemset I is greater than the number of itemsets belonging to itemset II in data-block D and the support of these itemsets can he counted by only scanning data-blocks D and Df DPARMA algorithm makes full use of it to reduce the number of candidate itemsets find all frequent itemset as fast as possible and improve the efficiency of finding frequent itemsets in dataset D 1 Scan data-block DI Partition all possible itemsets in data-block D into itemset class I and itemset class U denoted by C1 Cy respectively Scad data-block DI and connt the support of all itemsets in Clu Cl in this data block Scan data-block Df and count the support of all itemsets in CI in this data-block So we can find frequent itemset of whole dataset D in CI with definition 11 and the set of these frequent itemsets denoted by F1 2 Scan data-block D 293 Partition all possible itemsets in data-block D into itemset class I and itemset class E denoted by C C respectively. Scan data-block D and count the support of all itemsets in C,uC,,uC Iutl uC in this data-block. Scan data-block Df and connt the support of all itemsets in C in this data-block So we can find frequent itemset of whole dataset D in C with definition 11 and the set of these frequent itemsets denoted by F lDr*I ID1 3 Scan data-block Df i If  2 minsupport then count the support of all itemsets in Ciu  kl Scan each data-block once more to count the support of itemsets in Cfu uC1 During the scanning of this time, we can find some frequent itemsets when finishing scanning one data block For example when finishing scanning data-block Ds\(I<sG-I we can find frequent itemsets in C,,+Iy All frequent itemsets in Cfu MI denoted by Fi Now all frequent itemsets in dataset D have been found ii If  IDJ  minsupport then count the support of all itemsets ID1 in Cfu U\200 At the same time find all frequent itemsets of data-block D< but not in Cfu  uCI and these itemsets denoted by Cf Scan each data-block once more to count the support of itemsets in CpuCfu  uCI We can find frequent itemsets of whole dataset D like i All frequent itemsets in CpuGu uCI are denoted by Fp 4 Let F be the set of all frequent itemsets of whole dataset D, then F=FluF~u  uF or F=F,uF,u  uFfuFp DPARMA algorithm is listed in algorithm 2 as follows Algorithm 2 DPARMA algorithm D Di  D DA minsupport minconlidence 1 F=0 2 Scan dam-block DI to decide set CI CI Count suppon of itemsets in CI and Cy and decide set F 3 F=Fu Fl;s=2 4 While\(sSr 5 I 6 of iternsets in C,uC,-&,~tu  Ucr and decide set Fa 7 F=FuF,;s 8 I 9 Scan data-block DA count suppon of itemseIs in CW...Uc or ccvcrJ...Ucr 1O all dam-block once more to decide F or Fp 1 I 01 FuF 13 F 14 R Input dam-blocks generated by DP algorithm Output set of all association mles R Scan datahlock D to decide set C C Count suppon WI 4 Parallel Algorithm based DPARMA Algorithm DPARMA algorithm can be redesigned to parallel algorithm easily 1 Assign data-blocks DI,Dz  D to processor Pl,Pz  P respectively, and data-block Di to each processor 2 Processor Pa\(lSsG partitions all possible itemsets in data-block D into itemset class I and itemset class 11 denoted by C C respectively Find all frequent itemsets of data-block D but not in C,uC and these itemsets are denoted by Ci Scan data-block D and count the support of all itemsets in C,uC,uCf in this data-block Scan data block Df and count the support of all itemsets in C in this data-block So we can find frequent itemsets of whole dataset D in C with definition 11 and the set of these frequent itemsets of whole dataset D are denoted by F 3 Processor P3\(lSsSr broadcasts the support of itemsets in C,M,uC to other processors 4 Processor PS\(KsG gets the support of itemsets in Clu uC uC,,uC from other processors scans data block Di and counts the support of all itemsets in Cf in this data-block According to minsupport Processor P finds frequent itemsets in Clu  uC,uC.,uCf and these frequent itemsets are denoted by F F is the set of all frequent itemsets in the whole dataset D 5 Each processor generates rule separately 5 Performance To study the effectiveness and efficiency of DPARMA 


Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 45 November 2002 algorithm we implement it and other algorithms of association rule mining in C++ and test them in a PC with a CPU clock rate of 1.5'3 256 megabytes of main memory and under the Microsoft Windows2000 operating system 5.1 Synthetic Dataset Generation The dataset used in the test is generated using a randomized item set .generation algorithm similar to the algorithm described in and we set parameter N to 512 and 1=1000 Table 1. Generated dataset In order to study the effectiveness and efficiency of DPARMA algorithm, especially when p or Df are different we generate the same size data-block DI,D2 Dr according to p firstly then generate data-block D and combine these data-block to dataset D Generate data-block DI,D2  Dr partition set of items I into 11 Iz  I and these sets of items are the sets of items in data-block DI DZ   Dr respectively In order to generate a transaction T in data-block D,\(lSsQ a transaction T is generated from set of items I firstly. Then a\(0SaSpxlTI items from sets of items IluI~u  U 13.1u 18+,u  u1 are chosen randomly to form itemset X Finally let T=TuX Generate data-block Df transactions in data-block D+ are generated from sets of items I,uIzu  uI Generate conceptual hierarchy in dataset we generate the same conceptual hierarchy L for all dataset and MCN\(L I L\(1 I 512 1 L\(2 I 128 I L\(3 I 32 I L\(4 1=8 1 L\(5 I 2 each general-item in conceptual layer L\(2\L\(3 L\(4\and L\(5\have four direct specialization item Dataset in Table 2 is generated by the method provided above Table'2 Datasets used to test DPARMA algorithm daraset 5.2 Comparing DPARMA algorithm with other algorithms of association rule mining We use dataset in Table 1 to test AIS algorithm Apriori algorithm DIC algorithm\(partition dataset into 10 data block freely and DPARMA algorithm\(parti6on dataset into 10 data-block freely The result is given in Figure I We use dataset in Table 2 to test AIS algorithm Apriori algorithm DIC algorithm \(partition dataset into 10 data block freely and DPARMA algorithm partition dataset with DP algorithm p=0.2 choose conceptual layer L\(3 The result is given in Figure 2 Fig.1 Comparing with other algorithms Using dataset in Table 1 minsupport=l dataset Fig.2 Comparing with other algorithms Using dataset in Table I minsupport=l  5.3 Main factors affecting DPARMA In order to test the effect of p to DPARMA algorithm we generate dataset DlOkT1618 with different p The result of testing is in Figure 3\(choose conceptual layer L\(3 The result of the test that different conceptual layer is chosen is in Figure 4\(dataset is DIOkT1618, p=0.2 Fig.3 Effect of p to DPARMA algorithm Using dataset DlOkT1618 minsupport=l 0 0 I 0 2 0 3 0.4 0 5 9i7R 


Proceedings of the First International Conference on Machine Leaming and Cybernetics Beijllg 45 November 2002 Fig 4 Effect of conceptual hierarchy Using dataset 10kT1618 minsupport=l 6 Conclusions This paper has proposed a new algorithm of association rule mining From the result of the performance test we know that the efficiency of this algorithm is decided by several factors such as p conceptual layer chosen etc That is what we should pay more attention to when we using it to the real world References I R.Srikant and R.Agrawa1 Mining generalized association rules.Proceedings of the 21st VLDB Conference ,Zurich,Swizerland 1995 21 Keyun Hu, Yuefei Sui Ju Wang Yuchang Lu rough set theory under similarity relation The 5th World Multiconference on Systemics Cybernetics and Informatics SCI 2OO1 USA 2001.7 31 Jiawei Han and Yongjian Fu Discoveru of Multiple Level Association Rules from Large Database Proceedings of the 21st VLDB Conference, Zurich Swizerland 1995 41 R.Agrawal and R.Srikant.Fast algorithms for mining association ru1es.h b of the 20 ht'l ConfOn Very Large Databases,Santiago,Chile,Sep 1994 51 J.Han J.Pei and Y.Yin.Mining frequent patterns without candidate generation In Proc 2000 ACM SIGMOD Int Conf Management of Data SIGMOD'OO Dallas,TX, May 2000 Bing Liu Minqing Hu and Wynne hsu Multi-Level Organization and Summarization of the Discovered Rules Kdd 2000,2000 61 21 79 


Since the resolution requirement of the lenslet array is rather relaxing An array of 32x32 lenses with 2.5 mm in diameter can provide at least 10 times the resolution of a commercially the TV monitor which has a resolution of about 3 lines/mm However the alignment of the optical system is critical for the matrix-vector operations The submatricies on TV screen have to be precisely imaged onto the input SLM by the lenslet array in superimposing position Since the proposed optical neural network is essencially a close-loop feedback system the precise alignment can be corrected by adjusting the position of each submatrix on TV screen by computer intervention and the intensity of the TV screen can also be adjusted Thus the proposed optical system can indeed perform in adaptive mode In experiments Hopfield and IPA models are chosen to perform pattern recognition using noisy input patterns  B    P  and  R  are three letters stored in the weight matrix as shown in Fig 7\(a The positive and negative parts of IWMs for IPA model are shown in Figs 7\(b and c while those for Hopfield model are displayed in Figs 7\(d and e In comparison between these two sets of IWMs it can be seen that the IPA model has two major advantages over the Hopfield model, namely 1 less interconnections and 2 fewer gray levels The later is significant because the IPA model requires only 3 gray levels to represent the IWM whereas the Hopfield model needs 2M+1 gray levels where M denotes the number of stored reference patterns The experimental results of these two models are obtained based on an input pattern  B  embedded in 30 random noise SNR  7 dB as shown in Fig.7\(f The output patterns of the IPA model and the Hopfield model are shown in Figs.7\(g and h respectively Because of the curvature of the video monitor screen the output results are somewhat distorted Nevertheless the results obtained from the IPA model have been shown better than those obtained from the Hopfield model  V CONCLUSIONS We have illustrated IPA neural network model for pattern recognition By using a simple logical rule the common features and the special features of the reference patterns can be obtained and the positive negative or no interconnections can be assigned to each neuron The IWM can be easily formulated which requires merely 3 gray levels An adaptive optical neural network is used to carry out the parallel processing Computer simulations and experimental results have shown that the IPA model can perform more effectively in terms of pattern recognition among similar patterns than the Hopfield model. The basic reason is that the IPA model puts more emphasis on the inter-pattern relationships while the Hopfield model deals only the intra-pattern association ACKNOWLEDGEMENT We acknowledge the support of the U S Army Research office contract DAAL03-87-0147 References l].Rumelhart D E and D Zipser Feature Discovery by competitive Learning Rumelhart D E and McClelland J L eds Parallel Distributed Processing Explorations in the Microstructure of ition  Chapter 5 Vol 1 pp151-193 MIT Press  1986 2 Kohonen T Self-Or~~ion and Associative Memory Springer-Verlag 1984 3 Fukushima K A Neural Network for Visual Pattern Recognition Computer Vol 21 No 3 65  1988 41 Carpenter G A and Grossberg S A Massively Parallel Architecture for a Self-organizing Neural Pattern Recognition Machine Computer Vision Graphics and Image Processing Vol 37 54 1987 5 Hopfield J J Neural Network and Physical System with Emergent Collective Computational Abilities," 79, 2554 1982 6].McClelland J L and Rumelhart D E An Interactive Activation Model of Context Effects in Letter Perception Part I An Account of Basic Findings I Psychological Review Vol 88 No 5 375\( 1981 7 P Smolensky Information Processing in Dynamical Systems: Fundations of Harmony Theory Rumelhart D E and McClelland J L eds Parallel istributed Processine ExDiorations in the Microstructure of Cognitiqn   Chapter 6 Vol 1 pp194 281 MIT Press 1986 8 Lu T Wu S Xu X and Yu F T S A 2-D Programmable Optical Neural Network to appear in Applied Optics 9].Farhat N H and Psaltis D Optical Implementation of Associative Memory Based on Models of Neural Networks Optical S ienal procesu ed by Horner J L Academic Press pp lo Athale R A Szu H H and Friedlander C B Optical Implementation of Associative Memory with Controlled Nonlinearity in the Correlation Domain Opt Lett Vol 11 No 7 482 1986 ll Johnson K M Handschy M A and Pagano Stauffer L A Optical Computing and Image Processing with Ferroelectric Liquid Crystals Opt Eng Vol 26 No 5 385\(1987      129-162 1987 663 


 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 percent of rules generated above confidence threshold ave clique sparseness/density 220complex_colocations\220  220simple_colocations\220  Figure 8 comparison of ef\223ciency from sparse to dense data features and the presence of multiple features apply the maxPI algorithm to the transactions as described in section 4 automatically pruning trivial/nonsensical collocations such as 000 000 001 000  For an analysis of the ef\223ciency and application across different spatial data sets see 6 and return a set of colocations and their con\223dences and calculate the signi\223cance of the con\223dences of the mined relationships with respect to their signi\223cance as described in section 6.1 7.2 Test Sets Synthetic data sets were created similar to those described in 1 b u t w i t h t h e s peci 223 c propert i e s o f s pat i a l data such e occurrence of a single item in many cliques and the occurrence of many items representing a single feature in one clique Set constitu ency was varied according to sparseness the number of features and the number of items The mining of relationships was varied according to the participation and con\223dence thresholds A comprehensive set of tests corresponding was completed across approximately 100,000 different set/parameter combinations A summary of results is given below Testing was undertaken to compare the ef\223ciency of mining complex relationships to the mining of simple relationships with maxPI and to investigate the relative frequencies of the different relationship types 7.3 Results Ef\336ciency As Figure 8 shows the ratio of rules generated to con\223dent rules found is typically more ef\223cient for the mining of 0 10 20 30 40 50 60 70 0 10 20 30 40 50 60 70 80 90 100 ave no. of rules confidence threshold 220complex\220  220one-to-many\220   220self-excl_col\220   220mulit-excl\220   220positive\220 Figure 9 comparison of relationship type frequencies complex rules especially when the data is sparse Although it was never the case here we do not rule out the possibility of the existence of a set such that the mining of simple relationships is more ef\223cient than the mining of complex relationships The results in Figure 8 are the average ratios for approximately 10,000 randomly generated data sets which were varied according to sparseness/density the average probability of a feature occurring in a given clique The maxPI and con\223dence were held constant at 0.6 and 0.8 respectively Varying the maximal participation index had little impact on the respective ratios Varying the con\223dence threshold varied the scale of the ratio but did not affect the scale of the two distributions with respect to each other A constant maintained across the generation of all sets were the inclusion of skews in the data such as 217the probability of 001 appearing in a clique increases by 0.15 if 000 and 002 are present\220 These were originally generated randomly then maintained as averages about which all random sets were created It is the interaction of such skews with the various thresholds that cause the unevenness in distributions in Figure 8 7.4 Results Frequency of relationship types The results in Figure 9 are averages for approximately 1000 separate data sets each with 10 features The number of features is the most sensitive variable in the relative frequencies due to the fact that there is the possibility of exponentially more exclusive and therefore complex sets with respect to the number of features in a clique as discussed in section 5.3 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


Typically the number of complex relationships found was greater than but correlated with the number of other relationship types found As Figure 9 shows the number of complex relationships at a given con\223dence threshold was sensitive to the variance in the number of the other relationship types Self-exclusion and self-colocation were modeled together in Figure 9 emphasize the complementary relationship between the two as described in section 5.2 This is revealed in the corresponding steepness f gradient for self-exclusion/colocation at con\223dence 000 000 001 001 and con\223dence 002 000 001 002  7.5 Limitations/Strengths of the representation While there are representational issues with any type of data appropiate representation is particularly important in the spatial domain 9 Limitations In one-to-many relationships this model doesn\220t capture interesting ranges or distributions in the 217many\220 which is a task better suited for mixture modelling or the techniques described in As pointed out in 10 t h e cos t o f ful l y t r ans cri b i n g s pat i a l data into a transactional representation can in some cases be more expensive than the mining of the colocations but as a full representation is necessary to accurately add the features representing absent and multiple items a solution to this in the current representation may be problematic Strengths The most obvious strength of this representation is that currently it is the only model that allows the mining of complex relationships in spatial data A major strength of a transactional representation of spatial data not explored here is that it may be combined with non-spatial data and so the addition of nonspatial data to the representation described here would be uncomplicated 8 Conclusions  Future Work We have de\223ned the concept of complex relationships in spatial data We have described how even in transactional representations spatial data is undamentally erent from other forms of data making the need to mine complex relationships of inherent interest We have demonstrated that even when simple relationships are the goal of mining spatial data the mining of complex relationships is necessary for determining the signi\223cance of those relationships We have implemented and demonstrated a transactional representation of spatial data that allows the ef\223cient mining of complex relationships and discussed its limitations and strengths 8.1 Future Work Apart from investigating improvements to the representation to address the limitations mentioned in 7.5 there are several future directions evident such as the application to other types of data with a spatial component such as spatiotemporal data and to a lesser extent natural language and biological systems One important step would be the combination of spatial coordinate features with spa tial volume features this is especially important in Geographic Information Systems where a volume may represent the area of a lake valley etc As we have demonstrated that with a purely coordinate system 003 in 004 000 003 must be treated as a volume the inclusion of features that explicitly represent volumes should prove interesting References  R  A gra w al and R  S r i kant  F ast al gori t h ms for m i n i n g a ssociation rules In J B Bocca M Jarke and C Zaniolo editors Proc 20th Int Conf Very Large Data Bases VLDB  pages 487\205499 Morgan Kaufmann 12\20515 1994 2 T  C  B aile y a n d A T  Gatrell Interactive spatial data analysis  Longman Scienti\223c  Technical 1995 3 S  B rin  R  R asto g i  a n d K Sh im M i n i n g o p timized g a in rules for numeric attributes IEEE transactions on knowledge and data engineering  15 2003 4 G  P iatetsk y Sh ap iro  Discovery analysis and presentation of strong rules AAAI/MIT Press 1991 5 J  H an J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In W Chen J Naughton and P A Bernstein editors 2000 M SIGMOD Intl Conference on Management of Data  pages 1\20512 ACM Press 05 2000 6 Y  H uang H Xi ong and S  S hekhar  M i n i n g con\223 dent colocation rules without a support threshold In Proc 18th M Symposium on Applied Computing ACM SAC  2003  K K operski and J Han Di sco v e ry of spat i a l a ssoci at i o n rules in geographic information databases In M J Egenhofer and J R Herring editors Proc 4th Int Symp Advances in Spatial Databases SSD  volume 951 pages 47\205 66 Springer-Verlag 6\2059 1995 8 R  M unr o S  C h a w l a  a nd P  S un C o mpl e x spat i a l r el at i onships University of Sydney School of Information chnologies chnical Report 539  2003 9 D  J  P euquet  Representations of space and time  Guilford Press 2002  S  S h ekhar and S  C ha wl a Spatial Databases A Tour  2002  S  S h ekhar and Y  H uang Di sco v e r i ng spat i a l c ol o cat i o n patterns A summary of results Lecture Notes in Computer Science  2121 2001  X  W u  C  Z hang and S  Z hang Mi ni ng bot h posi t i v e and negative association rules In 19th International Conference on Machine Learning ICML-2002  2002 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


 A A A A A A A A B B B B B B B A B A B A B A B AB A B A A A A B B B A B A B A A B B B B A B A B A B A B A B A B A disjoint B A inside B A contains B A equals B A meets B A covered by B A covers B A overlaps B A B A B A B A B A B AB Figure 4 Topology and resolution increase with minimum bounding circles 64Mb of main memory Since the Apriori algorithm uses the number of transactions as support and we wanted to compare our algorithm with Apriori we have implemented MaxOccur and the na\250 021ve with transaction based support MaxOccur1 The second version of MaxOccur MaxOccur2 used the object-based support as presented in Algorithm 3.1 Table 9 shows the average execution times for the four algorithms with different image set sizes and 033 0 0  05 for Apriori 223Na\250 021ve\224 and MaxOccur1 and 0  0035 for MaxOccur2 The results are graphically illustrated in Figure 5 Clearly MaxOccur scales well with both versions treating one thousand images in 1.3 seconds on average regardless of the size of the data set The running time for 002ltering the frequent item-sets with 033 0  the maximum support threshold line 16 of Algorithm 3.1 is negligible since it is done in main memory once the frequent item-sets are determined Moreover the calculation of the total number of items line 4 of Algorithm 3.1 is done during the 002rst scan of the data set and has limited repercussion on the algorithms execution time The major difference between Apriori and MaxOccur is in ascertaining the candidate item-sets and counting their repeated occurrences in the images Obviously MaxOccur discovers more frequent item-sets The na\250 021ve algorithm also 002nds the same frequent item-sets but is visibly capable of less performance in execution time The left graphic in Figure 6 shows the average number of frequent item-sets discovered with the three algorithms Apriori found on average 109 different frequent k-item-sets while MaxOccur1 and Na\250 021ve found 148 on the same data sets and MaxOccur2 found 145 on average The discrepancy between MaxOccur1 and MaxOccur2 is basically due to the different de\002nition of support The price we pay in performance loss with MaxOccur is gained by more frequent item-sets and thus more potentially useful association rules with recurrent items discovered ofimages Apriori Na\250 021ve MaxOccur1 MaxOccur2 10K 6.43 70.91 13.62 13.68 25K 15.66 176.69 32.35 34.11 50K 30.54 359.38 66.07 67.44 75K 44.93 514.33 97.27 101.23 100K 60.75 716.01 130.12 137.81 Table 9 Average execution times in seconds with different number of images 0 100 200 300 400 500 600 700 800 10K 25K 50K 75K 100K Apriori MaxOccur1 MaxOccur2 Na\357ve time images Figure 5 Scale up of the algorithms 6 Discussion and conclusion We have introduced in this paper multimedia association rules based on image content and spatial relationships between visual features in images using coarse to 002ne resolution approach and we have demonstrated the preservation and changes in topological features during resolution re\002nement We have put forth a Progressive Resolution Re\002nement approach for mining visual media at different resolution levels and have presented two algorithms for the discovery of content-based multimedia association rules These rules would be meaningful only in a homogeneous image collection a collection of semantically similar images or received from the same source channel Many improvements could still be added to the multimedia mining process to speed up the discovery or to re\002ne or generalize the discovered results 017 One major enhancement in the performance of the multimedia association rule discovery algorithms is the addition of some restrictions on the rules to be discovered Such restrictions could be given in a metarule form Meta-rule guided mining consists of dis#ofimages 033 0 0  25 0  20 0  15 0  10 0  05 10K 1.43 2.20 2.70 5.06 13.51 25K 2.80 4.78 6.31 11.20 32.35 50K 6.27 9.28 11.59 22.74 66.07 75K 8.24 13.57 17.69 33.94 97.27 100K 11.32 17.63 23.13 46.74 130.12 Table 10 Average execution time in seconds of MaxOccur with different thresholds 


 0 20 40 60 80 100 120 140 160 MaxOccur2 MaxOccur1 Na\357ve Apriori Apriori MaxOccur1 MaxOccur2 Na\357ve F k  Figure 6 Frequent item\255sets found by the dif\255 ferent algorithms covering rules that not only are frequent and con\002dent but also comply with the meta-rule template For example with a meta-rule such as 223 H-Next-to X Y   Colour x red  Overlap Y Z   P  Y Z  224 one need only to 002nd frequent 3-item-sets of the form f HNext-to\(red Y  Overlap Y 003  P  Y 003  g where Y is an attribute value and P a visual descriptor or spatial relationship predicate Obviously such a 002lter would greatly reduce the complexity of the search problem A method for exploiting meta-rules for mining multilevel association rules is given in  017 We have approximated an object in an image to a locale which is an area with a consistent visual feature such as colour Objects in images and videos are obviously more complex In a recent paper 9 re gions and their signatures are used as objects in a similarity retrieval system A computationally ef\002cient way to identify distinct objects in images is however still to be proposed Automatically identifying real objects and using spatial relationships between real objects would reduce the number of rules discovered and make them more signi\002cant for some multimedia applications 017 Object recognition or identi\002cation in image processing and computer vision is a very active research 002eld Accurately identifying an object in a video for example as being an object in itself is a very dif\002cult task We believe that data mining techniques can help in this perspective Multimedia association rules with spatial relationships using the motion vector of locales as a conditional 002lter can be used to discover whether locales moving together in a video sequence are part of the same object with a high con\002dence 017 There are many application domains where multimedia association rules could be applied and should be tested such as global weather analysis and weather forecast medical imaging solar surface activity understanding etc We are investigating the application with Magnetic Resonance Imaging MRI to discover associations between lesioned structures in the brain or between lesions and pathological characteristics Further development and experiments with mining multimedia data will be reported in the future References 1 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules In Proc VLDB  pages 487\226499 1994 2 M  J  E genhof er  Spatial Query Languages  PhD thesis University of Maine 1989 3 M  J  E genhof er and J  S har ma T opol ogi cal r e l a t i ons between regions in r 2 and z 2 In Advances in Spatial Databases SSD'93  Singapore 1993 4 U  M  F ayyad S  G  D j or go vski  a nd N  W e i r  A ut omat i n g the analysis and cataloging of sky surveys In U Fayyad G Piatetsky-Shapiro P Smyth and R Uthurusamy editors Advances in Knowledge Discovery and Data Mining  pages 471\226493 AAAI/MIT Press 1996 5 Y  F u a n d J Han  M e ta-ru le-g u i d e d m in in g o f a sso ciatio n rules in relational databases In Proc 1st Int Workshop Integration of Knowledge Discovery with Deductive and ObjectOriented Databases  pages 39\22646 Singapore Dec 1995 6 J  H an an d Y  F u  Disco v e ry o f mu ltip le-le v el asso ciatio n r u l es from large databases In Proc VLDB  pages 420\226431 1995 7 Z  N  L i  O R Z a 250 021ane and Z Tauber Illumination invariance and object model in content-based image and video retrieval Journal of Visual Communication and Image Representation  10\(3\:219\226244 September 1999 8 R  M iller a n d Y  Y a n g  Asso ciatio n r u l es o v e r i n t erv a l d ata In Proc ACM-SIGMOD  pages 452\226461 Tucson 1997 9 A  N atse v  R Rasto g i  a n d K Sh im W ALR U S A s imilar ity retrieval algorithm for image databases In Proc ACMSIGMOD  pages 395\226406 Philadelphia 1999  R Ng L  V  S  L akshmanan J  H an a nd A Pang E x ploratory mining and pruning optimizations of constrained associations rules In Proc ACM-SIGMOD  Seattle 1998 11 R Srik an t a n d R Ag ra w a l M i n i n g q u a n titati v e asso ciatio n rules in large relational tables In Proc ACM-SIGMOD  pages 1\22612 Montreal 1996  P  S t ol or z H  N a kamur a  E  M esr obi an R  M unt z E  S h ek J Santos J Yi K Ng S Chien C Mechoso and J Farrara Fast spatio-temporal data mining of large geophysical datasets In Proc Int Conf on KDD  pages 300\226305 1995  O  R  Z a 250 021ane Resource and Knowledge Discovery from the Internet and Multimedia Repositories  PhD thesis School of Computing Science Simon Fraser University March 1999  O  R  Z a 250 021ane,J.Han,Z.-N.Li,J.Y.Chiang,andS.Chee MultiMediaMiner A system prototype for multimedia data mining In Proc ACM-SIGMOD  Seattle 1998  O  R  Z a 250 021ane J Han Z.-N Li and J Hou Mining multimedia data In CASCON'98 Meeting of Minds  Toronto 1998 


18001  balancing mechanism which requires further investi gation 4.5 Speedup Figure 12 shows the speedup ratio for pass 2 vary ing the number of processors used, 16 32 48 and 64 where the curve is normalized with the 16 processor execution time The minimum support value was set to 0.4 4.5 0.5 1 1 0 I 10 20 30 40 50 60 70 number of mxessors Figure 12 Speedup curve NPA HPA and HPA-ELD attain much higher lin earity than SPA HPA-ELD an extension of HPA for extremely large itemset decomposition further in creases the linearity HPA-ELD attains satisfactory speed up ratio This algorithm just focuses on the item distribution of the transaction file and picks up the extremely frequently occurring items Transferring such items could result in network hot spots HPA-ELD tries not to send such items but to process them locally. Such a small mod ification to the original HPA algorithm could improve the linearity substantially 4.6 Effect of increasing transaction Figure 13 shows the effect of increasing transac tion database sue as the number of transactions is increased from 256,000 to 2 million transactions We used the data set t15.14 The behavior of the results does not change with increased database size The minimum support value was set to 0.4 The num ber of processors is kept at 16 As shown each of the parallel algorithms attains linearity 5 Summary and related work In this paper we proposed four parallel algorithms for mining association rules A summary of the four database size Sizeup 0 I 0 500 loo0 1500 uxw amount of transaction thousands Figure 13 Sizeup curve algorithms is shown in Table 5 In NPA the candi date itemsets are just copied amongst all the proces sors Each processor works on the entire candidate itemsets NPA requires no data transfer when the supports are counted However in the case where the entire candidate itemsets do not fit within the mem ory of a single processor the candidate itemsets are divided and the supports are counted by scanning the transaction database repeatedly Thus Disk 1/0 cost of NPA is high PDM, proposed in 6 is the same as NPA which copies the candidate itemsets among all the processors Disk 1/0 for PDM should be also high The remaining three algorithms SPA HPA and HPA-ELD partition the candidate itemsets over the memory space of all the processors Because it better exploits the total system's memory, disk 1/0 cost is low SPA arbitrarily partitions the candidate itemsets equally among the processors Since each processor broadcasts its local transaction data to all other pro cessors the communication cost is high HPA and HPA-ELD partition the candidate itemsets using a hash function which eliminates the need for transac tion data broadcasting and can reduce the comparison workload significantly HPA-ELD detects frequently occurring itemsets and handles them separately which can reduce the influence of the workload skew 6 Conclusions Since mining association rules requires several scans of the transaction file its computational requirements are too large for a single processor to have a reasonable response time This motivates our research In this paper we proposed four different parallel algorithms for mining association rules on a shared nothing parallel machine and examined their viabil 29 


Table 5 characteristics of algorithms ity through implementation on a 64 node parallel ma chine the Fujitsu AP1000DDV If a single processor can hold all the candidate item sets parallelization is straightforward It is just suf ficient to partition the transaction over the proces sors and for each processor to process the allocated transaction data in parallel We named this algo rithm NPA However when we try to do large scale data mining against a very large transaction file the candidate itemsets become too large to fit within the main memory of a single processor In addition to the size of a transaction file a small minimum support also increases the size of the candidate itemsets As we decrease the minimum support computation time grows rapidly but in many cases we can discover more interesting association rules SPA HPA and HPA-ELD not only partition the transaction file but partition the candidate itemsets among all the processors We implemented these al gorithms on a shard-nothing parallel machine Per formance evaluations show that the best algorithm HPA-ELD attains good linearity on speedup by fully utilizing all the available memory space which is also effective for skew handling At present we are doing the parallelization of mining generalized association rules described in 9 which includes the taxonomy is-a hierarchy Each item belongs to its own class hierarchy In such mining associations between the higher class and the lower class are also examined Thus the candidate itemset space becomes much larger and its computation time also takes even longer than the naive single level association mining Parallel pro cessing is essential for such heavy mining processing Acknowledgments This research is partially supported as a priority research program by ministry of education We would like to thank the F\221ujitsu Parallel Computing Research Center for allowing us to use their APlOOODDV sys tems References l R.Agrawal T.Imielinski and ASwami 223Min ing Association Rules between Sets of Items in Large Databases\224 In Proc of the 1993 ACM SIGMOD International Conference on Manage ment of Data pp207-216 May 1993 2 R.Agrawal and RSrikant 223Fast Algorithms for Mining Association Rules\224 In Proc of the 20th International Conference on Very Large Data Bases pp.487-499 September 1994 3 J.S.Park M.-S.Chen and P.S.Yu 223An Effec tive Hash-Based Algorithm for Mining Associ ation Rules\224 In Proc of the 1995 ACM SIG MOD International Conference on the Manage ment of Data SIGMOD Record Vo1.24 pp.175 186 June 1995 4 H.Mannila H.Toivonen and A.I.Verkamo 223Ef ficient Algorithms for Discovering Association Rules\224 In KDD-94:AAAI Workshop on Knowl edge Discovery in Databases pp.181-192 July 1994 5 A.Savasere, E.Omiecinski and S.Navathe 223An Effective Algorithm for Mining Association Rules in Large Databases\224 In Proc of the 21th International Conference on Very Large Data Bases pp.432-444 September 1995 6 J.S.Park M.-S.Chen and P.S.Yu 223Efficient Parallel Data Mining for Association Rules\224 In Proc of the 4th International Conference on In formation and Knowledge Management pp.31 36 November 1995 7 T.Shintani and M.Kitsuregawa 223Considera tion on Parallelization of Database Mining\224 In Institute of Electronics Information and Com munication Engineering Japan SIG CPS Y95 88 Technical Report Vo1.95 No.47 pp.57-62 December 1995 8 T.Shimizu T.Horie and H.Ishihata 223Perfor mance Evaluation of the APlOOO Effects of message handling broadcast and barrier syn chronization on benchmark performance-\224  In S WO PP 22292 9.2 ARC 95 Information Processing Society of Japan Vo1.92 No.64 1992 9 R.Srikant and R.Agrawal 223Mining Generalized Association Rules\224 In Proc of the 21th Inter national Conference on Very Large Data Bases pp.407-419 September 1995 30 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


