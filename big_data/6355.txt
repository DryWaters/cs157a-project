International Conference on Computer, Communication and Electrical Technology – ICCCET 2011, 18 th 19 th March, 2011   978-1-4244-9394-4/11/$26.00 ©2011 IEEE 45  Rule Induction using Rough Set Theory – An Application in Agriculture  Sabu M K School of Computer Sciences Mahatma Gandhi University Kottayam, Kerala, India sabu.mes@rediffmail.com Raju G Department of Information Technology Kannur University Kerala, India kurupgraju@rediffmail.com   Abstract Rough Set Theory RST proposed by Z Pawlak, is a  new mathematical approach to vagueness and uncertai nty Tools based on RST are found to be useful in addres sing data mining tasks such as classification, clustering and rule mining. In RST all computations are performed directly on the supplied data and works by making use of the granularity str ucture of the data  Association rules which play an important r ole in data mining, provide associations among attributes and g enerally they are helpful for decision making.  A problem of usin g conventional association rule algorithms is that too many rules are generated by these algorithms and it is very difficult to ana lyze these rules This paper proposes a rough set based approach to g enerate rules from an inconsistent information system consi sting of the preprocessed data collected from coconut cultivator s of the Keezhur Chavassery Grama Panchayath using stratifie d random sampling method.  An existing algorithm, namely, Le arning from Examples Module version 2 LEM2 is modified to inc orporate some conditions, leading to the generation of signi ficant rules.  By applying the proposed algorithm a set of significa nt rules are generated.  These rules are expected to be helpful to the farmers of the state to design their farming plans, which w ill enable them to improve their coconut production Keywords-Association Rule Mining Indiscernibility relation Local covering Lower approximation Minimal comple x Rough set, Upper approximation I   INTRODUCTION Rough Set Theory \(RST\ proposed by Zdzislaw Pawlak is a mathematical approach to intelligent data analysis and data mining [1, 2, 3, 4    I t  i s  a n  e m e r g i n g  s o f t  c o m p u t ing tool with wide range of applications in many domains especial ly in the areas of machine learning knowledge acquisition d ecision analysis, knowledge discovery from databases, exper t systems inductive reasoning and pattern recognition [5, 6  Rough set is an approximation of a vague concept \(set\ by a pair of precise concepts called lower and upper approximations whi ch are classification of the domain of interest into disjo ined categories The lower approximation is the descrip tion of the domain objects which are positively classified as b elonging to the subset of interest whereas the upper approximat ion is a description of objects which possibly belongs to th e subset The rough set approach provides efficient algorithm s for finding out hidden patterns in data, minimal sets o f data \(data reduction evaluating significance of data and gen erating sets of decision rules from data 1    I n  R S T  a l l  c o m p u t ation are performed directly on data sets  It requires no ad ditional parameters to operate such as thresholds or a grade  of membership as in fuzzy set theory, other than the s upplied data 7    I t  w o r k s  b y  m a k i n g  u s e  o f  t h e  g r a n u l a r i t y  s t r ucture of the data.  One advantage of Rough Set theory is that, i t provides a well understood formal model which is very helpful  in generating several kinds of information such as rel evant features or association rules using minimal model a ssumptions Association rule mining is a process to search rela tionships among data items in a given data set.  An associati on rule is a rule of the form a®b where a and b represent itemsets which do not share common items  Association rule algori thms can be used to extract rules from a decision table 8    The main problem with association rule algorithm is that too many rules are generated and it is difficult to analyze these rules and discover which ones are more important and interest ing 14    Using Rough set approach number of rules generated can be reduced considerably and the rules will be as signi ficant as the rules generated without using rough set approach [9       To generate association rules using Rough Sets the  rule induction algorithm LEM2 is selected and which is m odified to incorporate some important conditions leading to th e generation of significant rules. The proposed algor ithm is then applied on a small data set which is prepared by co llecting data from coconut cultivators of the Keezhur Chavassery Grama Panchayath and rules are generated  At the time of  data collection a number of attributes are considered a nd data are collected based on these attributes  For rule gene ration three important attributes namely atmospheric temperature  amount of rainfall and the amount of fertilizers used  for  coconut cultivation are selected from the collected data as  conditional attributes and the amount of coconut production is considered as the decision attribute As a preprocessing these  attribute values are mapped on to domains consisting of value s low medium, high Then the lower and upper approximat ions of various concepts the set of objects with the same decision attribute value\ are computed  With the help of th e proposed algorithm various certain rules are generated by in putting the lower approximations and possible rules are generat ed by providing the upper approximations  In this way va rious association rules are generated from the data 


International Conference on Computer, Communication and Electrical Technology – ICCCET 2011, 18 th 19 th March, 2011   46  The remainder of the paper is organized as follows  An introduction to the concept of local covering is gi ven in Section II, a description of the data set used is given in Section III and the algorithm used is described in section IV. In s ection V the work is explained with the data set together with t he results obtained.  Finally, Section VI gives the conclusion  II  LOCAL  COVERING An information system I  U A  d where d   A is usually called a decision table.  The elements of A are called conditional attributes and d is called the decision attribute.  A decision table defines an information function f  U  A  V where V represents the set of all attribute values Let a  A and v   V and t  a v be an attribute-value pair  The n a block of t, denoted by [t   i s  a  s e t  o f  o b j e c t s  f r o m U for which attribute ‘a’ has value v [12     Let x  U and B  A  An elementary set of B containing x denoted by [x B and is defined by the following set   a, v  a  B, f\(x, a\ = v Elementary set are subsets of U consisting of all c ases from U that are indistinguishable from x while using all a ttributes from B [12    E l e m e n t a r y  s e t s  a r e  a l s o  k n o w n  a s  i n f ormation granules which represents the building blocks of k nowledge about U  When subset B is restricted to a single a ttribute elementary sets are blocks of attribute-value pairs  defined by that specific attribute.  Elementary sets can also be defined by using the idea of indiscernibility relation the ma thematical basis of rough set theory   There are two main approaches to data mining from complete data sets based on RST.  In both approache s decision tables are used.  In the first approach, the entire attributes are used for analysis and hence the approach is known a s global The second approach is known as local in which blo cks of attribute-value pairs are used for analysis.  In th is paper a rule induction process based on local covering is discus sed because local coverings are useful for rule induction 12    To define the concept of a local covering, the idea of a mini mal complex is introduced first  A minimal complex corresponds  to a single rule.  Let X is a concept.  Let t be an attr ibute-value pair a, v\, [t  b e  a  b l o c k  o f  t  a n d  T  b e  t h e  s e t  o f  a l l such attributevalue pairs.   Set X is said to depend on set T of attribute-value pairs if and only if        t    t  T  X  A set T is a minimal complex of X if and only if X  depends on T and no proper subset T   of T exist such that X depends on T  12      A local covering corresponds to a rule set describ ing a concept  Let L be a non-empty collection of non-em pty sets of attribute-value pairs.  Then L is a local coveri ng of X if and only if the following conditions are satisfied [12  1  Each member T of L is a minimal complex 2   T    T   L} = X and 3.   L is minimal, i.e., L has smallest possible nu mber of members  III  DESCRIPTION  OF  THE  DATA  SET Data set used for this experiment is collected from  80 coconut cultivators of the Keezhur Chavassery Grama  Panchayath of the Kerala state[15    S t r a t i f i e d  r a n dom sampling is the sampling method used taking wards in the panchayath as the strata.  Number of samples is cho sen in such as way that at least 5% of the total population is being included in the study  At the time of data collection a num ber of attributes are considered and data are collected ba sed on these attributes  To construct the decision table to app ly the rule induction process we randomly selected three import ant attributes namely atmospheric temperature amount o f rainfall and the amount of fertilizers used for coconut cult ivation as the conditional attributes and the amount of coconut pr oduction is considered as the decision attribute.  TABLE I give s a portion of the actual data collected from the farmers [15   As a preprocessing these attribute values are mappe d on to three domain values namely low normal and high  T he definitions of these values for various attributes are given in TABLE II After the preprocessing the data set is formatted a s a decision table required for the rule induction algo rithm TABLE III gives the decision table. Rows of the dec ision table represent various objects and columns represent the  set of conditional attributes Temperature Rain Fertiliz ers and the decision attribute {Production TABLE I  DATA COLLECTED FROM FARMERS Temperature Celsius  Rain cm   Fertilizers Kg/Acre  Production Per Acre  32 121 90 4000 32 121 70 5000 34 100 50 7000 32 136 110 5000 34 123 140 9000 28 113 90 8000 38 124 120 7000 28 128 50 3000 As a preprocessing these attribute values are mappe d on to three domain values namely low normal and high   The definitions of these values for various attributes are given in TABLE II DOMAIN VALUES OF THE ATTRIBUTES  Temper ature Celcius  Rain Cm  Fertilizers Kg/Acre  Production in 1000/Acre  Low Less than  31 Less than 111 Less than 119 Less than 6500 Normal 31-33 111-115 119-121 6500-7500 High Greater than 33 Greater than 115 Greater than 121 Greater than 7500  


International Conference on Computer, Communication and Electrical Technology – ICCCET 2011, 18 th 19 th March, 2011   47   TABLE III DECISION TABLE Objects Temperature Rain Fertilizers Production 1 High High High Normal 2 High High High Low 3 High High Low Normal 4 High High Normal High 5 High Low Low Low 6 High Low Low Normal 7 High Low Normal Normal 8 High Normal Low Low 9 Low High High Low 10 Low High Low Low 11 Low Low Normal Normal 12 Low Normal Low High 13 Normal High High Low 14 Normal High High Normal 15 Normal High Low Low 16 Normal Low Low Low 17 Normal Normal Low High IV  ALGORITHM  USED  FOR  RULE  INDUCTION In the original version of LEM2 the idea used for generating the local covering is based on the condi tion [T-{t    X  But the use of this condition alone will leads  to the elimination of some useful attribute-value pairs fr om the local covering  This will greatly affect the efficiency of the rules generated  So in the proposed work to overcome th is we introduce another condition, say \(T-{t    before using the already specified condition T-{t    X The algorithm used for computing a single local covering for each lowe r or upper approximation of various concepts from the decision  table is presented below Algorithm MODILEM2\(X X represents a set of objects representing lower upper approximation of the concept selected t represents an attribute-value pair The algorithm returns a single local covering L of X  G := X L   while \(G   do  T   T\(G\ := {t | [t   G     while \(\(T  or \(not\([T   X  Select a pair t  T\(G\ such that |[t   G| is maximum if a tie occurs arbitrarily select any one pair T := T  t if \([t   G      G := [t   G T\(G\:={t |[t  G    t  T\(G T\(G\ := T\(G T   for each t in T do  if \(\(T t    then if \([T t   X\ then  T := T t  L := L  T G := X    L T T    for each T  L do if  P  L-{T P    X   t h e n   L := L – {T  VI  DATA ANALYSIS AND RESULTS  In TABLE III there are three elementary sets 1 3 6, 7 11 14 2 5 8 9 10 13 15 16 and 4 12 1 7 of the decision attribute Production These sets respect ively represent classification of farmers with production  normal low’, and ‘high’.  Elementary sets of the decision attribute are called concepts  The equivalence class structure i nduced by the conditional attributes {Temperature, Rain, Fert ilizers} are 1 2 3 4 5 6 7 8 9 10 11  12 13, 14}, {15}, {16}, {17}}.  From TABLE III, it is clear that the decision production does not depend on attrib utes Temperature, Rain, Fertilizers} because neither of 1, 2} nor 13 14 are subsets of any concept  Hence TABLE I II is inconsistent because entries 1 and 2 are conflictin g.  Entries 13 and 14 are also conflicting  These inconsistencies can be handled by using RST The idea is that for each concept X lower and upper approximations are computed and based on these approximations rules are generated  In TABLE III for the concept 1 3 6 7 11 14 the farmers with prod uction normal the lower approximation is 3 7 11 and  upper approximation is 1, 2 3 5 6, 7 11 13 14 S imilarly the lower approximation for the concept 2 5 8 9 10  13 15 16} is {8, 9, 10, 15, 16} and upper approximation i s {1, 2, 3 5, 6, 7, 11, 13, 14}.  And for the concept {4, 12 17}, the lower and upper approximation are same and is given by {4 12, 17 This means that the concept 4 12, 17} representin g farmers with production high are definable with the attri butes Temperature, Rain, Fertilizers}.  The other two co ncepts are not definable by the given attributes and hence the y are roughly definable or rough sets  The idea of lower and upper approximations is used  for rule mining in the case of inconsistent data sets  For any concept rules induced from its lower approximation s are 


International Conference on Computer, Communication and Electrical Technology – ICCCET 2011, 18 th 19 th March, 2011   48  certainly valid and hence such rules are called cer tain rules 14    R u l e s  i n d u c e d  f r o m  u p p e r  a p p r o x i m a t i o n  o f  t h e concept are possibly valid are called possible rules  After computing the lower and upper approximations  select those attribute-value pairs \(a, v\ satisfyin g the condition a v    X    where X represents the lower or upper approximation of the concepts as the case may be  These attribute-value pairs are then used in the modified  LEM2 algorithm to generate a local covering for the set X  Each member of the local covering is a minimal complex which corresponds to a single rule  Hence the local cove ring represents a set of association rules generated fro m the decision table satisfying the set X.  By changing t he set X, all the decision rules satisfying the lower and upper approximations of the remaining concepts are also g enerated separately by using the same algorithm Various att ributevalue pairs of the selected attributes are {\(Temper ature, low Temperature normal Temperature high Rain low Rain normal Rain high Fertilizers low  Fertilizers normal\, \(Fertilizers, high\}.  Then elementary set s defined by these attribute-value pairs are Temperature, low     9   1 0   1 1   1 2   Temperature, normal     1 3   1 4   1 5   1 6   1 7   Temperature, high     1   2   3   4   5   6   7   8   Rain, low     5   6   7   1 1   1 6    Rain, normal     8   1 2   1 7   Rain, high     1   2   3   4   9   1 0   1 3   1 4   1 5   Fertilizers, low     3   5   6   8   1 0   1 2   1 5   1 6  17 Fertilizers, normal     4   7   1 1   Fertilizers, high     1   2   9   1 3   1 4     If the set X is taken as 3 7 11 i.e the low er approximation of the concept {1, 3, 6, 7, 11, 14} r epresenting the normal production\, the attribute-value pair bl ocks [\(a, v  which satisfies the condition [\(a, v   X    are Temperature, low     9   1 0   1 1   1 2   Temperature, high     1   2   3   4   5   6   7   8   Rain, low     5   6   7   1 1   1 6   Rain, high     1   2   3   4   9   1 0   1 3   1 4   1 5   Fertilizers, low     3   5   6   8   1 0   1 2   1 5   1 6  17 Fertilizers, normal     4   7   1 1    When the set X = {3, 7, 11} is given to the algorit hm, the local covering given by the algorithm is Rain L ow Fertilizers Normal Temperature High Rain  High Fertilizers Low  Based on the local covering the following certain rules are generated describing th e ‘Normal production.  The rules are presented in LERS format   2, 2, 2 Rain, Low\ & \(Fertilizers, Normal  Production, normal  3, 1, 1 Temperature, High\  & \(Rain, High\ & \(Fertilizers Low   Production, normal  If we select the set X as 8 9 10 15 16 the l ower approximation of the concept {2, 5, 8, 9, 10, 13, 1 5, 16}, that is for production  low    t h e  l o c a l  c o v e r i n g  g e nerated by the algorithm is Temperature low Rain high  Temperature, Normal\, \(Rain, Low\}, {\(Temperature High Rain Normal Temperature Normal Rain Hig h Fertilizers, Low\}}.  Hence the certain rules desc ribing ‘Low production are  2, 2, 2 Temperature, Low\ & \(Rain, High  Production, Low  2, 1, 1 Temperature, Normal\ & \(Rain, Low  Production, Low  2, 1, 1 Temperature, High\ & \(Rain, Normal  Production, Low  3, 1, 1 Temperature, Normal\ & \(Rain, High\ & \(Fertilizers Low  Production, Low  Certain rules describing the ‘High’ production \(i.e when X 4, 12, 17}, the lower approximation of the concept 4, 12 17}\ are  2, 1, 1 Temperature, Low\ & \(Rain, Normal  Production, High  2, 1, 1 Temperature, Normal\  & \(Rain, Normal  Production High  2, 1, 1 Rain, High\ & \(Fertilizers, Normal  Production, High  Possible rules describing the ‘Normal’ production i.e when X = {1, 2, 3, 5, 6, 7, 11, 13, 14}, the upper approximation of the concept {1, 3, 6, 7, 11, 14 are  2, 1, 2 Temperature, High\ & \(Fertilizer, High  Production Normal  2, 2, 3 Temperature, High\ & \(Rain, Low  Production, Normal  2, 1, 2 Temperature, Normal\ & \(Fertilizers, High  Production Normal  2, 1, 1 Temperature, Low\ & \(Rain, Low  Production, Normal  3, 1, 1 


International Conference on Computer, Communication and Electrical Technology – ICCCET 2011, 18 th 19 th March, 2011   49  Temperature, High\ & \(Rain, High\ & \(Fertilizers Low   Production, Normal   Possible rules describing the ‘Low’ production \(i e., when X  1 2 5 6 8 9 10 13 14 15 16 the upp er approximation of the concept {2, 5, 8, 9, 10, 13, 1 5, 16}\ are  2, 2, 3 Temperature, Normal\ &\( Rain, High  Production, Low  2, 2, 3 Rain, Low\ & \(Fertilizers, Low  Production, Low  1, 3, 5 Fertilizers, high  Production, Low  2, 1, 1 Temperature, High\ & \(Rain, Normal  Production, Low 2, 2, 2 Temperature, Low\ & \(Rain, High  Production, Low VI  CONCLUSION Rough Set Theory provides sound mathematical tools for mining association rules even from an inconsistent information system.  In this paper, we present a modified versi on of LEM2 algorithm which has been proved reliable to draw d ecision rules from a decision table in an efficient manner To prove the efficiency of the proposed method, we applied the a lgorithm on a small data set related to farming and rules are g enerated.  To form the decision table used for rule mining in th is work the actual attribute values are mapped on to some inter vals and all these intervals are crisp in nature  In order to o vercome this drawback we are focusing on designing a novel algo rithm by incorporating soft computing technique such as fuzz y logic which may lead to better results R EFERENCES  1  Zdzislaw Pawlak: Rough sets and intelligent data an alysis  Information Sciences 147 \(2002\ 1 – 12, Elsevier 2  S.K. Pal and A. Skowron \(Editors\:  Rough Fuzzy Hyb ridization – A New Trend in Decision Making.  Springer – Verlag, Singapore, \(19 99 3  T Y Lin Y Y Yao L A Zadeh Editors Data M ining Rough Sets and Granular Computing., Physica– Verlag, \(2002 4  Wojciech Ziarko :  Rough Sets as a Methodology for Data Mining, Rough Sets in Knowledge discovery 1 Methodology and Applications  554  576 Physica Verlag, \(1998 5  M. Magnani: Technical Report on Rough Set Theory fo r Knowledge Discovery in Data Bases.  from internet, \(2003 6  I D¨untsch G Gediga and H.S Nguyen Rough set data analysis in the KDD process. URL: citeseer.nj.nec.com/387773.html 7  Songbo Tan Yuefen Wang and Xueqi Cheng Text Featu re Ranking Based on Rough-set Theory  2007 IEEE/WIC/ACM International Conference on Web Intelligence 8  Jiawei Han and Micheline Kamber Data Mining  Conc epts and Techniques Elsevier 9  Qiang Shen, Alexios Chouchoulas: Rough Set – Based Dimensionality Reduction for Supervised and Unsupervised Learning Internati onal Journal of Applied Mathematics and Computer  Sciences \(2001\, Vol.11 No.3, 583-601 10  Richard Jensen Combining rough set and fuzzy sets for feature selection Ph.D thesis from Internet \(2005 11  Songbo Tan Yuefen Wang and Xueqi Cheng An Efficie nt Feature Ranking Measure for Text Categorization. Proceedings of SAC 08 published by ACM 12  Jerzy W. Grzymala-Busse: Rough Set Theory with Appl ications to Data Mining 13  Zdzislaw Pawlak: Theoretical Aspects of Reasoning a bout Data. Kluwer Academic Publishers 14  Jiye Li Nick Cercone Discovering and Ranking Imp ortant Rules KDM Workshop, Waterloo, Canada, Oct 30-31, \(2006 15  Rajeesh E, Anupama M and Shijo M Joseph: Use of Dat a Mining Techniques for Sustainable Growth in Coconut Farming  


threshold given by users. When an individual X meets the requirements of users, its fitness function value should be greater than 1. Otherwise, the fitness function value is less than 1, and this individual will be eliminated in the next generation of genetic algorithm C. Selection Strategy Based on Self-Adaptive Suppression and Promotion The diversity of individuals is maintained by the individuals self-regulation[10]. The individuals selfregulation depends on the individuals fitness and concentration. When the concentration is constant, the greater the fitness is, the greater the probability of the individual being selected is. When the individuals fitness is constant, the greater the concentration is, the smaller the probability of the individual being selected is. This algorithm Figure 1.  Three-segment chromosomes first segment second segment third segment 0110010 01110   1100110 10011 1448 eliminates the individuals whose fitness value is less than 1 first, and then puts the individuals whose fitness value is greater than 1 in a new population. In this population, the selection probability Pi of the individual Xi is defined as 1   M j dif Xii f Xj e     2 Where ? is an adjustment factor which is a constant greater than zero, generally ?=1. di is the concentration of the individual, which  is defined as the number of individuals whose N-1 i jsdi 


 3 Where N is the population size, ? is the similarity constant which takes 0.75 1?? ? in this paper. ,i js is the similarity which is the degree of similarity of any two individuals Xi and Xj, i.e  the number of gene bits in which X  is same as the length of the chromosome i j i Xj s =      \(4 i js ? [0,1], if ,i js equals to 1, it is indicated that the two individuals are the same. While ,i js is equal to 0, it is indicated the two individuals are entirely different There are several advantages of this selection strategy The larger the individuals fitness is, the greater the probability of the individual being selected is. It ensures that the individuals which have greater fitness can be retained The fitness plays a promotive role and accelerates the convergence of the algorithm. The higher individuals concentration is, the smaller the probability of the individual being selected is. It ensures the diversity of the population The concentration plays a suppressive role and avoid premature D. Genetic Operation Genetic algorithm uses genetic operators to carry out selection, crossover and mutation on the population, and thus get a new generation of population. The genetic operators determine the search capability and convergence of the algorithm 1 chromosome in the population to the corresponding rule and then calculates selection probability Pi for each rule based on formula \(2 2 crossover. In detail, it classifies the domain of each attribute into a group and classifies the cut-points of each continuous attribute into one group. And then the crossover is carried out between the corresponding groups of two individuals by a certain rate 3 


mutated by a certain rate, that is, changing 0 to 1, 1 to 0 E. Attributes Reduction After the algorithm is finished, if the genome of some attribute x in the chromosome are all 0, it indicates that the cut-points set of it is empty, that is, attribute x is redundant for the decision-making and can be removed. Reduction set of attributes can be obtained by scanning chromosomes in the optimal population F. Terminating Condition The algorithm finally extracts the rules that meet the confidence threshold given by users, so the final output is not one optimal rule, but rather a set of rules that meet the threshold. The termination conditions we set are: the termination generation is 400, or there is no one rule less than threshold for the continuous 10 generations G. Rules Extraction The frequent rules are generated according to the fitness function and genetic operators. In order to mine the strong association rules finally, these rules must be extracted again Extraction criteria are: output the rule which meets the minimum confidence given by users, otherwise abandon it  IV. DESCRIPTION OF THE ALGORITHM According to the thinking above, a new model of immune genetic algorithm for mining association rules MAR_IGA Step1: Initialization. Let evolution generation count t=0 Determine the population size N and minsup, minconf Randomly generate an initial population G\(t Step2: Calculate each individuals fitness in the population G\(t 1 individual. If each genome in the third segment of some chromosome is invalid or all 0, we assign such chromosome a smaller fitness Step3: Filter individual according to the fitness value. If the fitness is greater than 1, keep the individual into the next generation G*\(t number M of individuals retained Step4: If M<N, randomly generate \(N-M put them into G*\(t Step5: Genetic operation 


Calculate the concentration di of each individual in population  G*\(t 3 Calculate the selection probabilities in population G*\(t 2 individuals to get population G'\(t Carry out crossover and mutation on G'\(t next generation of population G\(t+1 Step6: Compare to the termination conditions, if it meets the termination conditions, jump to Step7, otherwise t=t+1 jump back to Step2 Step7: Rules extraction. Restore each chromosome in the final population to the rule, and calculate the confidence of each rule. If confidence ? minconf , output the rule 1449 V. EXPERIMENTS AND ANALYSIS A. Descriptions of Experimental Data Sets and the Environment The experiment uses Abalone dataset obtained from UCI machine learning repository. The data set has 4177 samples It is composed of a discrete attribute and 8 continuous attributes. In this paper, we only mined such association rules X Y?  that Y was age. The setting of parameters: The size of evolutionary population N=40, crossover rate=0.95 mutation rate=0.01, minsup=0.15, minconf=0.6. The ratio of training data and test data was 1:2. The experiment was executed on Celeron\(R was VC B. Results of the Experiments In order to test the validity of the algorithm MAR_IGA we compared MAR_IGA with the classical genetic algorithm SGA and Apriori. The process of using SGA is: firstly we extracted rules from the discrete data by SGA. Secondly we searched the optimal cut-points set of the continuous data by SGA, and then used SGA to extract rules from the continuous data based on cut-points. And finally we combined the above results of rule extraction as the final optimal population. The process of using Apriori is: firstly we discretized continuous attributes using the discretization method based on the importance of cut-points[11], and then extracted the rules by Apriori. These three methods were run on the above data set. The results are shown in Table I TABLE I.  COMPARISON OF THESE THREE METHODS 


 Number of cut-points Number of rules Number of reduction attributes Execution times\(s MAR_IGA 28 13 5 48 SGA 37 10 6 56 Apriori 30 13 8 70 It can be seen from Table I that the number of reduction attributes got from MAR_IGA is almost the same as SGA but in other areas obvious advantages can be seen compared with SGA. It indicates that MAR_IGA using the immune system is closer to the optimal solution of the problem than the simple genetic algorithm. In addition, the number of the cut-points got from MAR_IGA is almost the same as the number got from Apriori which uses discretization method based on the importance of cut-points. But the Apriori has almost not reduced attributes. It is shown that MAR_IGA has a good effect of the attributes reduction and can get the simple, effective rules while ensuring the importance of the cut-points. To sum up, MAR_IGA get less cut-points of the continuous attributes, take shorter experimental time and get more effective rules VI. CONCLUSION Data can be divided into discrete attributes data and continuous attributes data. A new immune genetic algorithm MAR_IGA is presented in this paper. This algorithm has the following characteristics It uses the chromosomes with three segments simultaneously searching the best cut-points sets of the continuous attributes and extracting association rules, and implying the attributes reduction The immune mechanism is introduced in. It improves the local optimization of genetic algorithm effectively, and ensures the population diversity Simple and effective association rules can be obtained, while not reducing the importance of cutpoints and maintaining the original indistinguishable 


relationships of the information systems But how to set the parameters of genetic algorithm and how to guide the direction of the algorithm in order to facilitate association rules mining is our next work  REFERENCES 1] J. Han and M. Kamber, Data Mining:Concepts and Techniques, San Francisco: Morgan Kaufmann Publishers,2001 2] C.Y.Jia and X.J.Ni, Association rule mining: A survey,Computer Science, vol. 30, 2003,pp.145-149 3] F.Y.Li, L.P.Zhao, and H.Y.Wang, Improved mining method for association rules based on genetic algorithm, Computer Engineering and Applications, vol. 44, 2008,pp.155-158 4] W.S.Yao, L.Shang, and Z.Q.Chen, A quantization of real-value attributes based on evolution algorithm, Computer Applications and Software, vol. 22, 2005, pp.37-39 5] Z.Tong, K.Luo, Mining of association rules with decision attributes based on rough set, Computer Engineering and Applications, vol 42,2006,pp.166-169 6] F.Q.Shi, S.Q.Sun, and J.Xu, Association rule mining of Dansei knowledge based on rough set, Computer Integrated Manufacturing Systems, vol.14,2008,pp.407-411 7] H.S.Nguyen and A.Skowron, Quantization of real value attributes rough set and boolean reasoning approach, Proc of 2th Joint Annual Conf. on Information Science, IEEE Press,1995,pp.34-37 8] X.P.Wang and L.M.Cao,Genetic Algorithm-Theory, Application and Software Realization, Xian: Xian Jiaotong University Press,  2001 9] Y.Zhu, H.Zhang and L.D.Kong, Research and application of multidimensional association rules minng based on artificial immune system, Computer Science, vol. 36,2009,pp.239-242 10] J.G.Zheng and X.Y.Wang, DNA-Immune-Genetic algorithm based on information entropy, Computer Simulation,vol.23,2006,pp.163165 11] W.Ning and M.Q.Zhao, More improved greedy algorithm for discretization of decision table, Computer Engineering and Applications, vol.43,2007,pp.173-178 


our model is able to detect new behavior patterns for next generation applications. As a future work, we are planning to analyze a bigger database of traces and define behavior patterns for a wider range of applications REFERENCES 1] F. Baker, B. Foster, and C. Sharp. Cisco architecture for lawful intercept in IP networks. Internet Engineering Task Force, RFC, 3924, 2004 2] C. Borgelt and R. Kruse. Induction of association rules: Apriori implementation. In Compstat: Proceedings in Computational Statistics 15th Symposium Held in Berlin, Germany, 2002, page 395. Physica Verlag, 2002 3] WAND Trace Catalogue. http://www.wand.net.nz/wits/catalogue.php 4] C. Dewes, A. Wichmann, and A. Feldmann. An analysis of Internet chat systems. In Proceedings of the 3rd ACM SIGCOMM conference on Internet measurement, pages 5164. ACM, 2003 5] J. Erman, A. Mahanti, and M. Arlitt. Internet traffic identification using machine learning. In Proceedings of IEEE GlobeCom. Citeseer, 2006 6] C. Fraley and A.E. Raftery. MCLUST version 3 for R: Normal mixture modeling and model-based clustering. Technical report, Citeseer, 2006 7] M. Iliofotou, H. Kim, M. Faloutsos, M. Mitzenmacher, P. Pappu, and G. Varghese. Graph-based P2P traffic classification at the internet backbone. In IEEE Global Internet Symposium. Citeseer, 2009 8] T. Karagiannis, A. Broido, and M. Faloutsos. Transport layer identification of P2P traffic. In Proceedings of the 4th ACM SIGCOMM conference on Internet measurement, pages 121134. ACM, 2004 9] David Kotz, Tristan Henderson, Ilya Abyzov and Jihwang Yeo. CRAWDAD trace dartmouth/campus/tcpdump/fall01 \(v. 2004-11-09 http://crawdad.cs.dartmouth.edu/dartmouth/campus/tcpdump/fall01 November 2004 10] A.W. Moore and K. Papagiannaki. Toward the accurate identification of network applications. Passive and Active Network Measurement, pages 4154, 2005 11] TTT Nguyen and G. Armitage. A survey of techniques for internet traffic classification using machine learning. IEEE Communications Surveys Tutorials, 10\(4 12] I. Papapanagiotou and M. Devetsikiotis. Aggregation Design Methodologies for Triple Play Services. In IEEE CCNC 2010, Las Vegas, USA pages 15, 2010 13] V. Paxson. Bro: A system for detecting network intruders in real-time Comput. Networks, 31\(23 14] J. Xu, J. Fan, M. Ammar, and S.B. Moon. On the design and 


performance of prefix-preserving IP traffic trace anonymization. In Proceedings of the 1st ACM SIGCOMM Workshop on Internet Measurement, page 266. ACM, 2001.80 


denote the input attribute with the minimum integrated cost after the split, and let the set of allocated sample sizes, computed by the sample allocation method explained later, be ASM . Then DM | children are generated for the node M . For each child CHi, i = 1, . . . , |DM |, the associated query is QN ?{M = mi sample size is asi ? ASM , and potential splitting attributes is PSN ? M . The process of splitting is then recursively applied to children of node N In the process of calculating costs during the strati?cation process, we need to perform sample allocation, i.e., divide the parent nodes sample size among the potential children nodes This is required for calculating the integrated cost for the potential split. This is based on our sample allocation method, which we 328 describe next in Section IV-B. Furthermore, for calculating the integrated cost, the variance of target value ?2i and probability of output attribute A = a, ?i, for each stratum is computed based on the pilot sample Initially, the strati?cation process on the query space begins by calling Stratification\(R, null, F IA, n, null root node. The process of strati?cation would stop if there is no leaf node with integrated cost larger than a prede?ned threshold Each leaf node in the tree is a ?nal stratum for sampling, and the associated sample size denotes the number of data records drawn from the subpopulation of the stratum B. An Optimized Sample Allocation Method Now, we introduce our optimized algorithm for sample allocation which integrates variance reduction and sampling cost As introduced in section IV-A, integrated cost is de?ned by taking into account of variance of estimation and sampling cost The goal of our sample allocation algorithm is to minimize the integrated cost by choosing the sample size, ni, for each stratum In our algorithm, we adjust the value of SampCost and ?2s so that their in?uences on the integrated cost are in the same unit SampCost SampCost SmpCost\(r where SmpCost\(r entire population, and ?r denotes the probability of A = a being true for the entire population 2 2s 


2r where ?2r 2 n denotes the variance of estimation of the target value on the entire population The key constraint on the values of the sample sizes for each strata is that their sum should be equal to the total sample size A vector n = {n1, n2, . . . , nH} is used to represent the sample sizes, where the ith element, ni, is the sample size for the ith stratum By including sampling cost and variance of estimation into the integrated cost, our sample size determination task leads to the following optimization problem Minimize Cost\(n  i\(?s ni i v N2i niN2 2 i subject to  i ni = n 6 where Ni denotes the population size of data records under the space of A = a in ith stratum. Note that this value may not be known if A is an output attribute. However, the total number of records in the ith stratum is typically known, and can be denoted as DNi. Then Ni can be estimated by ?iDNi, and the population size of A = a on the entire population is estimated by N  i ?i DNi For ?nding the minimum of integrated cost, we utilize Lagrange multipliers, a well know optimization method. Lagrange multipliers aims at ?nding the extrema of a function object to constraints. Using this approach, a new variable ? called a Lagrange multiplier is introduced and de?ned by n n  


 i ni ? n If n is a minimum solution for the original constrained problem then there exists a ? such that \(n Lagrange function. Stationary points are those points where the partial derivatives of ?\(n n,??\(n 7 In our problem, by conducting partial derivatives on Formula 7 a group of equations are yielded as follows s i v N 2 i n2iN2 2 i + ? = 0 i = 1, ..H i ni = n 8 where the solution n leads to the minimum value of integrated cost in Formula 5 However, it is dif?cult to solve the group of equations directly Thus, we use numerical analysis to approximate the real solution. Newtons method is utilized for ?nding successively better approximations to the zeroes \(or roots Given an equation f\(x x tive of function f\(x method iteratively provides, xt+1, a better approximation of the root, based on xt, the previous approximation of root according to the following formula xt+1 = xt ? f\(xt f ?\(xt The iteration is repeated until a suf?ciently accurate value is reached, i.g. |f\(xt In our problem of Formula 8, there are H + 1 equations F \(xt xt xt n1, .., nH , ?}, where the equations in F \(xt fi\(xt s i v N 


2 i n2iN2 2 i + ? i = 1, .., H fH+1\(xt  i ni ? n The Newtons method is also applied iteratively via the system of linear equations JF \(xt xt+1 ? xt xt 9 where JF \(xt H + 1 H + 1 equation system F \(xt vector xt. The entry JF \(i, j d\(fi\(x dxj where fi\(x xt x From the Expression 9, a better approximation xt+1 is obtained based on previous approximation xt. The iterative procedure would be stopped if ?i|fi\(xt threshold, and then the sample size ni is allocated for each stratum so that the integrated cost is minimized. In reality, we are required to pick an integral number of records from each stratum during the sampling step. Thus, we round down each ni to its nearest integer, ni + 0.5 In the example shown in Table I, suppose we set both weights v and ?s to be 0.5. Further, assume the variance of the entire population, ?2r , to be 80000. The probability of A = a over the entire population ?r is 0.242. By using the proposed optimized sample allocation method, the sample sizes for the three strata 329 are 162, 299, and 139, respectively. In this case, the variance of estimation according to the Expression 2 is 93.66, and the estimated cost is 1943.7. We can see that, compared with Neyman allocation, the sampling cost is decreased by 42.1%, but results some increase in variance. Overall, this example shows that we can achieve lower sampling cost by trading off some accuracy C. Overall Sampling Process Algorithm 2 DiffRuleSampling\(DW1, DW2, F IA, t, St 1: PS ? a pilot sample from DW1, DW2 


2: DR ? identi?ed rules from PS 3: OA ? output attributes of DW1, DW2 4: for all R : X ? DW1\(t t 5: if X ?OA = null then 6: Acquire St data records from the space of X 7: else 8: R ? root node 9: Lf ? null 10: Strati?cation\(R,null, F IA, St, Lf 11: for all N ? Lf do 12: s ? sample size of N 13: Draw s data records from the subpopulation of N 14: end for 15: end if 16: Update the mean value of DW1\(t t 17: end for Algorithm 2 shows the overall sampling process for differential rule mining on two deep web data sources, DW1 and DW2 and with differential attribute t. The inputs of the algorithm also contain the full set of input attributes FIA as well as the sample size St. The algorithm starts with a pilot sample PS, from which the differential rules are identi?ed. For the rule R : X ? DW1\(t t St data records are directly drawn from the space of X \(Lines 5-6 t t containing output attributes, query spaces of DW1 and DW2 are strati?ed and sample is recursively allocated to each stratum with corresponding query subspace \(Line 10 of the tree built by strati?cation, a sample is drawn according to its sample size \(Line 13 t t is updated by the further sample \(Line 16 for association rule mining is very similar and not shown here V. EVALUATION STUDY We evaluate our sampling methods for association mining and differential rule mining on the deep web using two datasets described below US Census data set: This is a 9-attribute real-life data set obtained from the 2008 US Census on the income of US households. This data set contains 40,000 data records with 7 categorical attributes about the race, age, and education level of the husband and wife of each household and 2 numerical attributes about the incomes of husband and wife 


Yahoo! data set: The Yahoo! data set, which consists of the data crawled from a subset of a real-world hidden database at http://autos.yahoo.com/. Particularly, we download the data on used cars located within 50 miles of a zipcode address. This yields 30,000 data records. The data consists of 7-attribute with 6 categorical attributes about the age, mileage, brand, etc, of the cars and one numerical attribute, which is the price of the car Variance of Estimation is estimated for the target value \(i.e mean value in differential rule mining, and con?dence in association rule mining is calculated according to the Expression 2. Since variance of estimation reveals the variation of the estimated value from the true value, smaller variance suggests better estimation Sampling Cost is estimated by the number of queries submitted to data sources in order to acquire a certain number of data records containing target output attributes, i.e. A = a. Larger sample size implies higher sampling costs in a deep web setting where the queries are executed over the internet Estimate accuracy is estimated by Absolute Error Rate \(AER Small AER value indicates higher accuracy. For an estimator on variable Y with true value y and estimated value y, the AER of the estimator is calculated by AER\(y A. Association Rule Mining In this section, we present the results of our method for association rule mining. Using our overall approach, we have created four different versions, which correspond to four different sets of weights assigned to variance of estimation and sampling costs. 1 the weight ?v = 1.0 and ?s = 0.0, 2 the weight ?v = 0.7 and ?s = 0.3, 3 v = 0.5 and ?s = 0.5, and 4 weights ?v = 0.3 and ?s = 0.7. In addition, we also compare these approaches with a simple random sampling method, which is denoted by Random We focus on the queries in the form of A = a ? B = b where A and B are output categorical attributes. Other categorical attributes in the data set are considered as input attributes Our goal is to estimate Supp\(A=a,B=b A=a association rules are randomly selected from the datasets. Each of the 50 association rules are re-processed 100 times using 100 different \(pilot sample, sample iterations result is the average result for 5000 executions 


In all charts reported in this section, the X-axis is k, which denotes the size of sample under the space of a target rule drawn from deep web. The sample size for each point on X-axis is k x, where x is a ?xed value for our experiment, and depends upon the dataset. At each time, queries are issued to obtain kx data records under the space of a target rule. Overall, all our experiments show the variance of estimation, sampling costs and sampling accuracy with varying sample size Figure 1 shows the result from our strati?ed sample methods on the US census data set. The size of pilot sample is 2000, from which all of the 50 initial rules are derived. In this experiment the ?xed value x is set to be 300, which means the smallest sample size at k = 1 is 300, and the largest sample size at k 10 is 3000. Figure 1 a the ?ve sampling procedures. Figure 1 b cost for the sampling procedures. In order to better illustrate the experiment result, in each execution of sampling, the variance of 330 6DPSOLQJ9DULDQFH            9D UL DQ FH R I V WL PD WL RQ  


9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW           6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF            5 


 9DU 9DU 9DU 5DQG c Fig. 1. Evaluation of Sampling Methods for Association Rule Mining on US Census Dataset estimation and sampling cost for the sampling procedures var7 var5, var3, and rand are normalized by the corresponding values of Full Var. Thus, in our experiment, the values of sampling cost and variance of estimation for sampling procedure Full Var are all 1. Furthermore, Figure 1 c sampling procedures From Figure 1 a pared with sampling procedures Var7, Var5 and Var3, Full Var has the lowest estimation variance and the highest sampling cost. From sampling procedures Var7, Var5, and Var3, we can see a pattern that the variance of estimation increases, and the sampling cost decreases consistently with the decrease of the weight for variance of estimation. At the largest sample size of k = 10, the estimation variance of sampling procedure Var3 is increased by 27% and the sampling cost is decreased by 40 compared with sampling procedure Full Var. The experiment shows that our method decreases the sampling cost ef?ciently by trading off a percent of variance of estimation. Similar to variance of estimation, the sampling accuracy of these procedures also decreases with the decrease of the weight on variance of estimation. For the largest sample size at k = 10, we can see that the AER of sampling procedure Var3 is increased by 20 compared with sampling procedure Full Var. However, for many users, increase of the AER will be acceptable, since the sampling cost is decreased by 40%. By setting the weights for sampling variance and sampling cost, users would be able to control the trade-off between the variance of estimation, sampling cost, and estimation accuracy In addition, compared with sampling procedure of Full Var Var7, Var5, and Var3, sampling procedure Random, has higher estimation of variance, sampling cost and lower estimation accuracy. Thus, our approach clearly results in more effective methods than using simple random sampling for data mining on the deep web Figure 2 shows the experiment result of our proposed strati?ed 


sampling methods on the Yahoo! data set. The size of pilot sample on this data set is 2,000, and the ?xed value x for sample size is 200. The results are similar to those from the US census dataset. We can still see the pattern of the variance of estimation increasing with the decrease of its weight. Besides, the sampling accuracy is also similar to the variance of estimation. However although the variance estimation of sampling procedure Random is 60% larger than sampling procedure Full Var, the sampling cost of Random is 2% smaller than Full Var. This is because Full Var does not consider sampling cost. It is possible that Full Var assigns a large sample to a stratum with low ?, which denotes the probability of containing data records under the space of A = a, resulting the larger sampling cost than that of simple random sampling. Sampling procedures Var7, Var5, Var3 consider sampling cost as well, and have smaller variance estimation and sampling cost, compared with Random. Furthermore, Random has smaller sampling accuracy than Full Var, Var7 and Var5, but has larger sampling accuracy than Var3. This is because Var3 assigns much more importance to the sampling cost, and loses accuracy to a large extent To summarize, our results shows that our proposed strati?ed sampling are clearly more effective than simple random sampling on the deep web. Moreover, our approach allows users to tradeoff variance of estimation and sampling accuracy to some extent while achieving a large reduction in sampling costs B. Differential Rule Mining In this section, we present results from experiments based on differential rule mining. Particularly, we look at the rules of the form A = a ? D1\(t t categorical attribute and t is an output numerical attribute, while other categorical attributes in the data set are considered as input attributes In this experiment, we also evaluate our proposed method with different weights assigned to variance of estimation and sampling cost. Five sampling procedures, Full Var, Var7, Var5,Var3 and Random, have same meanings with those in the experiments of association rule mining. Similarly, 50 rules are randomly selected from the datasets, and each of the 50 differential rules are reprocessed 100 times using 100 different \(pilot sample, sample iterations 5000 runs First, we evaluated the performance of these procedures on 


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


