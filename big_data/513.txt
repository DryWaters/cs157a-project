Mining Insightful Classification Rules Directly and Efficiently Hongyan Liu Jim Chen Guoqing Chen Beijing 100084 People's Republic of China School of Economics and Management Tsinghua University miy Chcni Cm 1 ABSTRACT Classification is one of the important problems in the field of data mining. Many algorithms have been proposed to solve this problem and each has its own drawback This paper discusses issues about mining classification rules directly and proposes two algorithms namely UARC and GARC 
These algorithms use a more suitable association rule mining technique to fmd insightful and a complete set of rules directly and accurately Unlike most other association rule mining algorithms the algorithms proposed in the paper can find both frequent k itenisct and rules at the same step After each scan of the database, only rule itemsets and excluded itemsets arc saved and used to exclude much more itemsets to generate larger-candidate itemsets, which will save much computation time and memory Upon the information gain criterion, many training cases which satisfy a special condition can be deleted from database, which 
will lead to fewer I/O times for every remaining scan of database Finally a criterion is defnied to terminate the whole mining process much earlier and at the same time-produce a meaningful Wle 1 INTRODUCTION Classification is one of the important problems in the emerging field of data mining also ao\\\\n as KDD Knowledge Discovery in Database\wluch is aiined at hiding a small set of rules from training data set with predetermined targets I 
Many algorithms such as decision tree, neural network and statistics methods have been proposed to solve this problem Compared with other methods decision tree classifier is relatively better in speed accuracy and understandibility Many algorithms proposed in the past are only designed for memory-resident data set Quinlan's C4.5 or its new version C5.0 is one of such well hiowi methods 2 hi order to deal with the limitation in size of 0-7803-5731-0/99/$10.00 01999 IEEE III-911 data set for decision tree based classification algorithms, many 
algoritluns have been put forward in recent years   which, however have some drawbacks as follows First in order to find best split at every node every attribute and every value group or interval\should be test With very large data set this may result in'more times of scan of data set and high cost in efficiency On the other hand using sampling or partition techniques may result in low accuracy. Second the classification rules correspond to decision trees 
without pruning and final generalizing are usually cumbersome complex and inscrutable While pruning and final generating rules from trees need extra times to scan the data set Lastly as for numeric attributes the test forni is Adv where A is a attnbute and v is a value as form is not always intuitive and sometimes leads to complicate rules Recently a new algorithm which use association rule mining technique to fmd classification rules from large amount of data is discussed 
by Bing Liu et al.[6 Association rule mining is another important technique in data mining, which aims to find associations among itemsets in transactional database. There are many algorithms proposed in recent years to solve this problem      among which Apriori given by Agrawal and Srikant 1 994 is usually regarded as a typical algorithm In this algorithm, association rule mining is separated into two steps first generating a complete set of frequent itemsets and then producing association 
niles from frequent itemsets. Using this algorithm, Being Liu et al proposed a algorithm named CBA Classification Based on Association to find classification rules in two steps which is first generating a complete set of association rules called CARS among training data set and then building a classifier from CARS As a training data set for classification usually contains a huge number of association rules so it costs much time to find all of these association rules among which many are not useful, and need a large memory to store them. Furthermore, selecting a subset of these rules to build 


a classifier is also a time consuming task Therefore in this paper two algorithms are proposed to deal with these problems The ideas are as follows Firstly in order to prevent from producing a huge number of candidate itemsets the traditional association rule algorithm is modified The reason why so many candidate itemsets could be produced is that the minimum support is set to be very low due to the consideration of a complete set of candidate itemsets However a classification rule is after all a special kind of a association rule During the classification nile mining process what is important is to fhd rules that are frequent itemsets and at the same time have a high confidence instead of frequent itemsets only with low support So we combine the two steps of mining association rules to one and focus on finding and storing rule itemsets and escluded itemsets \(defined in section 2 both of which are very small sets compared with frequent itemsets On the other hand Apriori algorithm reduces the number of candidate itemsets by excluding the itemsets that do not satisfy minimum support to forni larger itemsets while in our algorithms in addition to this, another constraint is also defined to rculuce the candidate itemsets further. That is every itemset that already produced a rule is excluded to make larger candidate i temsets Secondly in order to save time during the scan of data set, based on the notion of information gain 2 some cases whch have produced a rule and contain a value of the first split attribute can be deleted from the database Then the generating method of candidate itemsets changes as well As a result the database will become smaller as the times of scan increases Thirdly in order to improve the efficiency of this algorithm and at the same time producing a insightful classification rule a criterion is defined to terminate the mining process earlier than the usual association nile mining algorithm The rest of this paper is organized as follows Section 2 describes thc classilication problem using the mining association nile method and the definition of information gain Section 3 presents the two algorithms in detail Section 4 shows some preliminary experiment results Finally section 6 contains our concludions 2 PROBLEM STATEMENT 2.1 Association Rule Mining Classification rules can be regarded as a special kind of association rules within a data set he data set for classification usually is separated into two groups one is for training and the other is for testing. Both of them contain a large amount of cases tuples consisting of many attributes and one class label The task of the classi,fication is to find a set of rules to determine an unseen case's class correctly A classification rule is the form of implication like X-C where X is the combination of attribute values, and Ci is one of g class labels As for association rules the rule's form is X-Y where X and Y are all combination of attribute values \(corresponding to items in transactional database So the difference is that Ci is a special kind of Y According to this we can modify the typical association rule's mining algorithm to mining classification rules directly and efficiently Let D be the data set stored in the database, which contains M cases for training and N cases for testing. Each case is described by r distinct attributes and one of g class labels. Let A be the set of r attributes A a a  a   and G be the set of g class labels G C,,C2 _ C I a VI U 1 1 where 200A v;is a value of attnbute a i=l 2  r j=1 2 _ I Let I be the set of all items I=fI,j I i=l 2  r j=1 2  n  and Xc_I be a subset of items Then a candidate itemset is defined as X C with two numbers lcount and wcount where lcount is the number of cases contain X and wcount is the number of cases containing X and labeled Ci A case d containing X means Xcd Another concern of this itemset is its size which is defined as the number of items included in X So if X contains i attribute values it is called a k-itemset The support and confidence of this candidate itemset is defined as follows vwunt support  x Cl    confidence  X Ci    ID1 vcount lcount wliere ID1 is the number of cases in database D If its support is greater thrtn a iiiiiumuin support called minsup given by user then this candidate itemset is a frequent itemset. Furthermore if the confidence of tlus frequent itemset is greater than a minimum confidence \(called minconf given by user this frequent itemset is then called a rule itemset and can be output as a rule x-c m 912 


Otherwise if the support of a candidate itemset is less than minsup it is called an excluded itemset that can not be used to produce larger itemsets. Other candidate itemsets except for the excluded itemsets and rule itemsets can be used to produce larger itemsets The reason why rule itemsets are excluded to produce larger itemsets is that if a rule X-C holds in D then X U Y-C also holds, where Y is another subset of items So one such rule can produce many rules like XU Y-C since Y can be any subset of I However in fact the rule X-C is the most meaningful one among those rules 2.2 Information Gain The information gain criterion is one of those methods 12][13 used to select best split attributes in decision tree classifiers We use it in our algorithm to help decide which training cases can be deleted from the database Suppose a attribute A has n distinct values that partition the data set T of training cases into subsets T T2  T For a data set ScD freq\(C,,S stands for the number of cases hi S that belong to class C IS denotes tlie number of cases hi data set S Thai info\(S is defined as follows to measure the average amount of information needed to identify the class of a case in S  where, g is the number of classes After data set T is partitioned in accordance with n values of attribute A the expected information requirement can be defined as 223ITiI  info A\(T  c  x info Ti The infomiation gained by partitioning T according to attribute A is delilies as i=l IT1 gain\( A fo T fo T Among all attributes in data set T the best split attribute is the one that maximizes the information gain 3 CLASSIFICATION ALGORITHMS 3.1 UARC: Association Rule Based Classification UARC is a direct classification rule\221s minhig algorithm in which a new method is proposed to build candidate itemsets and to prevcnt both the escludd itemsets and rule itemsets G-om generating more meaningless candidate itemsets In addition a new criterion is defined to terminate the rule mining process earlier The detail is given as follows During rule mining phase after first scan of data set a record class list called recClass is created initially to contain the class label attached to each case The ith entry of the class list corresponds to the ith case in the training database In the next scan of the database, once a rule is produced, each entry for the cases containing this rule will be set to a value assume 0 different from every class label. Using this list we can define a criterion to end the mining process That is to say after each scan of database if each entry of recClass is the same \(or almost the same say 90 which can be given by users according to the accuracy required and noise contained in this dataset\then stop further scan of database If the value of each entry rather than 0 is one of tlie class labels then a default rule can be produced Additionally alter each scan of database only rule itemsets excluded itemsets 221and information about them such as Icount wcount and the corresponding record numbers will be saved Other candidate itemsets will be cleared from candidate itemsets list before the next scan of the database starts During the next scan of the database, new candidate itemsets that do not contain any of excluded itenisets and rule itemsets will be generated for every cases. Figitre 1 gives the training process of UAKC 1 initclass\(recClass 2 rule  1-itemset cI support\(c and confidence\(c minconf  3 excluded  I-itemset cI support\(c 4 adjust\(recC1ass 5 for\(k=2; finish\(recClass  k<=r k 6 hiitcand\(cand currentc=O 7 for\(t=l;t<=M ttt 8 C,=gencand\(nde,exclude 9 IO addtoand c,cand 11 end 12 end 13 for each itemsct c E C R={c E cand I support\(c\and confidence\(c minconf  14 rule=nile U R 15 16 adjust\(recClass 17 end 18 sort\(ru1e excluded  c E cand I support\(cFniinsup III 913 


19 default\(rule 20 output\(rule Figure 1 Training process of algorithm UARC Lines 1 to 4 present the first scan of the database It initializes the value of recClass to be the class label of each case and produce all 1-itemsets from which rule itemsets and excluded itemsets are selected. According to each rule its corresponding cases\222 class label in recclass will be set to zero line 4 Lines 5 to 17 are what should be done in subsequent scan of database Dilring each scan for each case in D, several itemsets will be generated according to rule itemsets and excluded itemsets line 8 Each of these itemsets will be added to candidate itemset line 10 and the corresponding lcount, wcount and each record No recno are saved in structure variable named cand Then from candidate itemsets cand nile itemsets and excluded itemsets are selected and recClass is adjusted After each pass, finish\(recC1ass\check if every entry or almost of recClass has the same value, if so nest scan of database stop If scan ternunated, all of itemsets contained in nile vi11 be sorted according to its confidence and support line 18 Then produce a default nile according to recClass line 19 and fuially all of rules are output line 20 3.2 GARC Gain Based Association Rule Classification When using algorithm UARC in each pass of the algorithm every case is read from the database while in fact some cases can be deleted from tlie database permanently. However, which cases can be deleted is a problem of concern that is dealt with upon the information gain criterion After tlie first scan of the database all of candidate itemsets is saved to variable named cand Using each candidate itemset\222s lcount and wcount infoniiation gain for each attribute 4 that can be used to partition thc database D into 11 clitasets can he calculatcd as follow 224  E support A  ji x f confidence it x log confidence ik i=l k=I where Ti corresponds to the data set whose attribute A\222s valve equals v g is the numbers of classes i stands for itemset A vi C With the infoniintiori gain criterion a best split attribute \(called bestattr\can be selected after frrst scan of database Then during the next scan of database, we can delete those cases that produce rules containing attribute specified by bestattr in the previous scan This is similar to the tree building process if a rule that contains attribute bestattr is produced there is no need to deal with its corresponding cases In addition, there is a little change in the candidate itemset generating method For each case we can not produce all of its candidate itemsets and can only produce itemsets include bestattr The reason is that for the itemsets\221without attnbute bestattr the lcount and wcount values can not be counted correctly due to the deletion of some cases vliich contribute to the count of lcount and wcount For itemsets including attribute bestattr the values of the attribute bestattr included in itemsets is different from the values contained in datasets that.have been deleted so we can still count these itemsets correctly This algorithm is shown in figure 2 1 2 3 4 5 6 7 8 9 iritclass\(recC1ass nile  1 itemset clsupport\(cp=minsup and confidence\(c minconf   excluded  1 itemset cI support\(c  adjust\( recClass bestattr=gain\(cand getdeleted\(delrec, bestattr for\(k=2; finish\(recClass  k=r k initcand\(cand\currentc=O for\(t=l;t<=M ti 10 11 else 12 C gencand\(rule, exclude, bestattr 13 14 addtocand\(c,cand 15 end 16 end 17 if  t E delrec delete t from database for each itemset c E C R c E cand 1 support\(c and confidence\(c iniiicoiif  18 nile=nile U R 19 20 adjust\(recC1ass 21 end 22 sort\(ni1e 23 default\(ru1e 24 outpiit\(ni1e excluded  c E cand I support\(c Figure 2 Training process of algorithm GARC In this algorithm lines 5 6 10 12 are different from previous algoritlun UARC gain\(cand is to select bestattr based 011 III-914 


information gain criterion getdeleted\(delrec bestattr is to get the record number will be saved in delrec that can be deleted according to attribute bestattr Line 10 is to delete cases included in delrec. Gencand\(rule exclude differs from that in UARC as stated above Data Set pred 1 pred2 pred3 pred4 pred5 4 EXPERIMENTAL RESULTS c5.0 UARC GARC 0.000 0.000 0.000 0.000 0.000 0.000 0.060 0.015 0.015 0.080 0.030 0.060 0.150 0.115 0.140 This section shows an empirical performance evaluation of UARC and GARC The experiment was divided into two parts The first part compares UARC, GARC with the typical decision tree classifier C4.5 in terms of accuracy and the number of rules The second part examines the execution time and scalability of algoritluns UARC and GARC In order to accomplish the two parts of evaluation the data set used is the synthetic database proposed in  Each tuple in database has 9 attributes. Ten classification functions were used to produce data distributions of varying complesitics in tllis paper We use first 5 functions to produce 5 data sets named predl -predj among which predj corresponding function 5 is one of the hardest to characterize mid results in the highest classification errors oredl I 14 4.1 Accuracy and Number of Rules We use a small data set 200 cases for every function to present the accuracy and number of rules of three algorithms. Since C5.0 is the new version of C4.5 and much more accurate and faster than C4.5 in the experiment we compared our algorithms with C5.0 Table 1 shows the classification accuracy of different algorithm while table 2 shows the number of rules produced by every algorithm Table1 Accuracy of difrerent algorithms 16 16 pred2 pred3 15 21 15 20 30 30 pred4 pred5 In the experiment for a categorical attribute all i,ts possible values are mapped to a set of consecutive positive integers For a continuous attribute its value range is discretized into intervals and each interval is also mapped to a set of consecutive positive integers It can be seen from table 1 that both UARC and GARC are more accurate than C5.0 for every data set Table 2 shows that the number of rules produced by UARC or GARC is more than that produced by C5.0 while those rules that are not included in the result of C5.0 are useful to improve classification accuracy and are just what 25.0 can not obtain For example, for dataset pred4 one of rules found by both UARC and GARC is IF commission~40000 and commission<50000 THEN group= 1 This rule is useful and accurate according to function 4 but C5.0 could not found it Table 2 also indicates that UARC generates more rules than GARC for some datasets The reason is that the set of candidate itemsets becomes small due to the deletion of some cases, wvl~ch also lead to the lower accuracy of GARC than UARC 23 30 26 29 76 38 4.2 Speed And Scalability hi this part we use different size of data set of function 3 to examine tlie speed and scalability of our two algorithms along the number of training cases Figure 3 illustrates the execution time used by UARC and GARC as tlie iiumber of training cases increased from 200 to 100000 As we can see from figure 3 compared to UARC algorithm GARC is faster and the difference behveen UARC and GARC becomes larger as the number of cases increases The result also indicates that both two algorithms achieve near-linear execution time on disk resident dah 180 I 1 LLl 3 20  0 0 7 14 21 28 35 42 49 56 63 70 77 84 91 98 Number of examples\(in thousand Figiue 3 Speed and scalability IU 915 


5 CONCLUSION Both classification rule mining and association rule mining are important problems in the field of data mining Many kinds of classification algorithms have been proposed in the past and in recent years But each of thein has its drawbacks In this paper two algorithms are designed to use the basic concept of association rules to find classification rules directly and efficiently From the empirical evaluation we can see that both two algoritluns achieve better classification accuracy and produce complete, useful sets of rules. It is also shown that these two algorilluns both have a good scalability and GARC has a betttT execution speed than UARC ACKNOWLEDGEMENT This work was partly supported by National Science Foundation of China grant No 69674037 and grant No 79825 102 and National Defense Foundation of China REFERENCES I G Piatetsky-Shapiro U Fayyad P Smyth Froni data nriniiig to kirowlcdge discovery Air oiwvieii Advairccs iir Kiiowledge Discovety aiid Data Miiziiig AAAMT press 1996,pp 1-35 2 J R Quinlan C3.5 Progranis for Machine Leamiiig Morgan Kaufmaiui 1993 3 Manish Mehta, Rakesh Agrawal and Jomia Rissanen SLIQ A Fast Scalable Classijer for Data Mining Extending database technology 1996 pp. 18-32 4 J Catlett hlegaindrictioii: Macliine 'Learxiiig on kiy Laige Databases PID thesis, University of Sydney, 1991 5 K llsaDti S Rolilia I Sirrgli CLOIJDS 4 dccisiori Tree classifier for Large Ilata sets hi Proc Of the Fourth hit Conference on IGionlcdge Discovery R Data Mining New York New York August 27-3 I 1998 pp 2-8 6 Biiig Liu Wyiuie HSU and Yiming Ma hitegglit Classij7catioir mid Associatiori Ride hfiniiig In Proc Of the Fo~irth Int Conference on Knowledge Discovery R Data Milling New York New York August 27-3 1 1998, page 80 86 7 R Agrawal T Imielinski aiid A Swami hfiiriiig association ides behveeii sets of itctns iii large databases In: Proceeding of 1993 ACM-SIGMOD hit Conf On Management of Data Washington D.C 1993, pp 207-216 8 R Agrawal R Snkant Fast algorithm for mining association rules In Proc Of the 20th VLDB Conference Santiago, Chilc 1994 pp 487-499 9 J S Park M S Chen and P.S Yu An Effective Hash-based Algorithnr for bfinitig Association rules In Proceedings of ACM-SIGMOD Int Conf On Management of Data Baltimore MD 1995, pp 175-186 10.H Toivonen Sanrplirig Large Databases for Associatioir Rirles hi 22th International Conference on VLDB Bombay India. September 1996, pp 134-145 11,s Brin R Motwani J Ulhnan et al iamic iteniset coroitiirg arid iniplicatioii niles for market basket data In Proc. Of 1997 ACM-SIGMOD Int Conf On Management of Data Tucson, Arizona 1997, pp 255-264 12 S M Weiss aid C A Kulikowski Computer System that Lcarx Classificatiori and prediction A,fetliods from Statistics A~errml Vetss A./clc/iine Leainiiig arid Expert Sjatenis Morgan Kaufman 199 I 13 L Breiman et al Class~ficatioir and Regrcssioir T~~.L?s Wxhorth Belmont 1984 14 Rakes11 Agrawal Tomasz Iniielinsb Arun Swami Darrihase l4iitiiig 4 Pctfomraiice Perspective IEEE transaction on knowledge and data engineering December 1993,5\(6 pp 914-925 III 916 


937 of Lecture Notes in Computer Science Berlin 1995 Springer J Gray A Bosworth A Layman and H Pira hesh Data Cube A relational aggregation op erator generalizing group-by cross-tab and sub totals In 12th International Conference on Data Engineering ICDE\22296 pages 152  159 New Orleans Louisiana Feb 1996 J Han and Y Fu Discovery of multiple-level as sociation rules from large databases In Proceed ings of the 21st International Conference on Very Large Data Bases VLDB\22295 pages 420  431 Zurich Swizerland 1995 K Hatonen M Klemettinen H Mannila P Ronkainen and H Toivonen Knowledge dis covery from telecommunication network alarm databases In 12th International Conference on Data Engineering ICDE 22296 pages 115  122 New Orleans Louisiana Feb 1996 M Holsheimer M Kersten H Mannila and H Toivonen A perspective on databases and data mining In Proceedings of the First Inter national Conference on Knowledge Discovery and Data Mining KDD\22295 pages 150  155 Mon treal Canada Aug 1995 M. Holsheimer M Kersten and A Siebes Data surveyor Searching the nuggets in parallel In U M Fayyad G Piatetsky-Shapiro P Smyth and R Uthurusamy, editors Advances in Knowl edge Discovery and Data Mining pages 447  467 AAAI Press Menlo Park, CA 1996 T Imielinski A database view on data mining Invited talk at the KDD\22295 conference M Jaeger H Mannila and E Weydert Data mining as selective theory extraction in proba bilistic logic In SIGMOD\22296 Data Mining Work shop 1996 To appear J.-U Kietz and S Wrobel Controlling the com plexity of learning in logic through syntactic and task-oriented models In S Muggleton editor Inductive Logic Programming pages 335  359 Academic Press London 1992 W Kloesgen statements in databases Infurmation Systems 4\(1  69 1995 H Mannila and K.-J Raiha Design by example An application of Armstrong relations Journal of Computer and System Sciences 33\(2  141 1986 Efficient discovery of interesting Journal of Intelligent H Mannila and K.-J Raiha Design of Relational Databases Addison-Wesley Publishing Company Wokingham UK 1992 22 H Mannila and H. Toivonen Discovering gener alized episodes using minimal occurrences Tech nical Report C-1996-12 University of Helsinki Department of Computer Science March 1996 23 H Mannila and H Toivonen. Multiple uses of fre quent sets and condensed representations Tech nical Report C-1996-13 University of Helsinki Department of Computer Science March 1996 24 H Mannila and H Toivonen On an algorithm for finding all interesting sentences In Cybernetics and Systems Research 22296 Vienna Austria Apr 1996 To appear 1251 H Mannila H Toivonen and A I Verkamo Discovering frequent episodes in sequences In Proceedings of the First International Confer ence on Knowledge Discovery and Data Mining KDD\22295 pages 210  215 Montreal Canada Aug 1995 26 C J Matheus G Piatetsky-Shapiro and D Mc Neill Selecting and reporting what is interesting In U M Fayyad G Piatetsky-Shapiro P Smyth and R Uthurusamy, editors Advances in Knowl edge Discovery and Data Mining pages 495  515 AAAI Press Menlo Park CA 1996 27 K Mulmuley Computational Geometry An Introduction Through Randomized Algorithms Prentice Hall New York 1993 28 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases In Proceedings of the tist Inter national Conference on Very Large Data Bases VLDB\222951 pages 432  444, Zurich Swizerland 1995 29 J W Tukey Exploratory Data Analysis Addison-Wesley Publishing Company Reading MA 1977 8 


Hei kki Mannila Biographical Summary Heikki Mannila is a professor of computer science at the University of Helsinki where he also obtained his Ph.D in 1985 Since then he has been an associate professor at the Universities of Tampere and Helsinki a visiting professor at the Technical University of Vienna, and a guest researcher at the Max Planck Institut fur Informatik in Saarbrucken He has also worked at the National Public Health Institution in Helsinki as well as being an industry consultant His research interests include data mining machine learning database design and text databases He is the co-author of the book Design of Relational Databases Addison Wesley\and he has been the author of numerous articles on algorithms, databases machine learning and data mining He is one of the editors-in-chief of the new scientific journal \223Data Mining and Knowledge Discovery.\224 9 


Figure 7 Correlations between query con straints and new indexed constraint the index attribute may have different ranges We iden tify all possible constraint introduction solutions as follows Algorithm For each constraint on the index attribute CO.Roj Step 1 find the least expensive association from a query constraint to i.e find the incoming link C~.R  CO.Roj with the fewest exceptions For exam ple if the cardinalities of El     E5 are nl   n5 and n4 5 nz _ n3 then the least expensive association for CO.R is c6.b  CO.R~,\(E  E4 Step 2 filter out all the exceptions that do not satisfy at least one of the query constraints C1.Rl    C12 We do not need to test the exceptions for the antecedent of the corresponding rule since they satisfy it by definition The constraint introduction solutions identified in our example are the following  Introduced Constraints Exceptions ele t El Cz.n,\(e    C12.nl2\(e el  E4 C1.h e    C5.nS e C~.R e     C12.nll e ele  E5 C1.nl e     C5.nS e C~.R e    C12.n12 e CO.Rol CO.RO2 CO.RO3 5 Optimizing OQL queries In the previous sections a series of algorithms were given to find a collection of constraint elimination or con straint introduction solutions In this section we show how the original query is transformed to its optimized form using the optimal solution. Consider the OQL query select x fromExtentX as x where C~.R and   and Cn 5.1 Heuristic H1 Let  Cil   Ci,},Ei  be the maintained con straints and the exceptions of a solution i Only the main tained constraints of the optimization solution should be tested on the objects of the whole extent however all the constraints should be tested on the exception cases and the objects that satisfy the maintained constraints but not the ones omitted should be removed from the result Hence the original query should be converted to the following one select x from Extent2 as x where Cil and   and Ci except Ei If we assume that tests on the query constraints take roughly the same time, the optimal solution is the one with fewest exceptions Ei 5.2 Heuristic H2 Let  Co.Roi Ei  be the index constraint and the corre sponding exceptions of a constraint introduction solution Instead of testing the query constraints C1.~l    C,.R on the entire extent Extent X we apply them only on the re sults of the subquery select x from Extent2 as x 40 where CO.R Since Co.Roi is a constraint on a cluster index attribute, the select operation is expected to be quite fast The original query is transformed to its more efficient form select x from 90 as x where Cl and    and C union Ei The exceptions Ei are merged to the result because they satisfy the query constraints but not the new index con straint Co.Roi Since the union operation is relatively cheap the optimal solution is the one that introduces the index con straint with the highest selectivity 6 Discussion We now look at two different scenarios and estimate the extent to which heuristics H1 and H2 speed up query exe cution The Jirst scenario concerns frequently executed queries Assume that the association rules which are used by algo rithm 3 are not modified We may optimize a query once at compilation time then execute its optimized form It is worth optimizing provided that the execution time of the optimized query is less than the execution time of the orig inal query For heuristic H1 this happens only if the time 132 


saved by omitting some constraints is greater than the time needed to remove the exceptions from the result except operation The more the eliminated constraints and the fewer the exceptions the better the optimization As one of the referees pointed out, the time saved by the elimina tion of constraints is CPU-related Since query execution is dominated by data access time this optimization is not expected to alter performance significantly It would help only in contexts rich in associations with few exceptions in which users express many constraints in their queries Heuristic H2 is expected to bring more significant ben efits Firstly, this optimization involves a union operation which is much cheaper than the except operation used in H1 Secondly instead of retrieving all the objects of an extent from the database we need only look at the subset retrieved through an indexed constraint Hence we save a considerable amount of data access time, spending a negli gible amount of CPU time in evaluating the additional con straint The second scenario concerns queries which are exe cuted only once In this case the time required for opti mization is significant. This time depends on the algorithm that finds associations between relaxed constraints and tight constraints \(see section 3 since this is the most expensive step in the optimization process This algorithm finds paths in a directed graph, and combines the exceptions associated with each edge of the path to derive the total exceptions for the path. Therefore its complexity is a function of i the av erage number of exceptions in the existing association rules and ii the number of different constraints found in the an tecedents and the consequents of the rules We have already implemented the algorithms for apply ing H1 the next step is to implement the corresponding al gorithms for H2 This should not be difficult since the main algorithm  finding associations between rule constraints  is common to the two heuristics We intend to set up an experimental model in order to evaluate H1 and H2 in the scenarios discussed above 7 Conclusion The use of association rules for query optimization is rel evant to both relational and object-oriented database sys tems There has been a lot of research on generating asso ciation rules and maintaining them in the presence of up dates Research has also focused on finding heuristics that take advantage of rules in order to optimize a query Most of this work 2 31 has considered integrity rules rather than association rules with exceptions Semantic optimiza tion heuristics were also applied without considering indi rect associations In this paper we implement algorithms that apply two optimization heuristics presented by Siegel et al taking account of both exceptions and indirect asso ciations We show how to use these heuristics to optimize an OQL query The complexity of the optimization process is closely related to the complexity of the constraint graph which represents the set of association rules in the data It also depends on the number of exceptions associated with each rule We have designed an experimental framework to evaluate the two optimization techniques both in the con text of queries repeated frequently over a period of time, and in the context of ad-hoc queries executed once only The re sults of this experimental work will be presented in a later paper 8 Acknowledgements We are grateful to the anonymous referees who read the paper carefully and critically and made many helpful suggestions Agathoniki Trigoni is supported by a scholar ship from the Greek Scholarships Foundation and is deeply obliged to the National Bank of Greece References I R Agrawal T Imielinski and A Swami Mining asso ciation rules between sets of items in large databases In Proceedings of the 1993 ACM SIGMOD Intl Conference on Management oj data pages 207-2 16 1993 2 U Chakravarthy J Grant and J Minker Logic-based ap proach to semantic query optimization ACM Transactions on Database Systems 15\(2 162-207 1990 3 J Grant, J.Gryz J Minker and L Raschid. Semantic query optimization for object databases In ICDE pages 444-453 1997 4 R Miller and Y Yang Association rules over interval data In ACM SIGMOD 1997 5 J Park An effective hash-based algorithm for mining asso ciation rules In ACM SIGMOD pages 175-186 1995 6 M Siegel E Sciore and S Salveter A method for auto matic rule derivation to support semantic query optimiza tion ACM Transactions on Database Systems 17\(4 600 December 1992 7 R Srikant and R Agrawal Mining quantitative association rules in large relational tables In ACM SIGMOD Intl Con ference on Management ojdata pages 1-12 1996 8 D Tsur J Ullman S Abiteboul C Clifton R Motwani S Nestorov and A Rosenthal Query flocks a general ization of association-rule mining In ACM SIGMOD Intl Conference on Management of data pages 1-12 1998 Intelligent query answering in deductive and object-oriented databases In Fourth ACM Intl Conference on Information and Knowledge Management pages 244 251 1994 IO S Yoon 1 Song and E Park Semantic query processing in object-oriented databases using deductive approach In Intl Conference on information and knowledge management pages 150-157 1995 9 S Yoon 133 


