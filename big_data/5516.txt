    AbstractThis paper describes a model that discovers association rules from a medical database to help doctors treat and diagnose a group of patients who show similar prehistoric medical symptoms. The proposed data mining procedure consists of two modules. The first is a clustering module that is based on a neural network, Adaptive Resonance Theory 2 ART2 amount of medical records. The other module employs fuzzy set theory to extract fuzzy association rules for each homogeneous cluster of data records. In addition, an example is given to illustrate this model. Simulation results show that the proposed algorithm can be used to obtain the desired results with a reduced processing time I. INTRODUCTION ATA mining is popularly referred to as knowledge discovery from data \(KDD extracting desirable knowledge or interesting patterns from existing databases for specific purposes. Many types of knowledge and technology have been proposed for data mining. Among them, finding association rules from transaction data is the most commonly studied whelm An association rule is represented by X?Y where X and Y are a set of items. The rule means that the transaction records in a database that contain X also tend to contain Y. Many effective algorithms for mining association rules from large databases have been proposed [1], [2 Over the last two decades, artificial neural networks ANNs classification problems and finding association rules [3]. In the medical domain, neural networks have been used as a diagnostic decision support system. For example, a supervised learning neural network was developed for leukemia diagnosis [4]. Other fault detection models based on abdicative network model, and combined fuzzy logic and neural network, were proposed in [5], [6]. In [7] a hybrid model of Adaptive Resonance Theory \(ART c-mean clustering for medical classification and diagnosis with missing features was developed 


 Manuscript received Jan. 31, 2010. This work was supported by National Science Council, Taiwan under Grant NSC97-2221-E-027-034-MY3 Y.-P. Huang is with the Department of Electrical Engineering, National Taipei University of Technology, Taipei, Taiwan 10608 R.O.C. \(Tel 886-2-27712171ext 1002 , e-mail: yphuang@ntut.edu.tw T. T. H. Vu is with the Department of Electrical Engineering, National Taipei University of Technology, Taipei, Taiwan 10608 R.O.C. \(e-mail t7318111@ ntut.edu.tw J.-S. Jau is with the Department of Electrical Engineering, National Taipei University of Technology, Taipei, Taiwan 10608 R.O.C. \(e-mail pikachu1982124@yahoo.com.tw F.E. Sandnes is with the Faculty of Engineering, Oslo University College Oslo, Norway. \(e-mail: frodes@hio.no The efficiency of combining fuzzy logic and neural network has been shown in many applications [8], [9], [10 but the problem of discovering association rules in data mining is an open research question. Thus, this study proposes a methodology using fuzzy ART2 to increase the efficiency when discovering association rules. The input patterns are first fuzzified using fuzzy logic. The fuzzified patterns are then clustered into groups by the ART2 neural network. The patterns in each group have similar properties that in turn allow us to find the association rules among them This will significantly reduce the computational cost in finding the interesting rules We choose ART2 for clustering the patterns, because it is computationally effective and allows the user to easily control the degree of similarity of patterns placed on the same cluster. The inter- and intra-cluster differences among data indicate that ART2 clusters data according to Euclidean distance approximately In this paper, one case study is presented involving the grouping of patients who have undergone surgery for breast cancer. Those with similar symptoms are used to discover group association rules to assist the doctors in treatment and diagnosis The remainder of this paper is organized as follows Section II gives an overview of related work, while the proposed approach is presented in Section III. Section IV summarizes the experimental results and discussions. Finally the conclusion remarks are made in Section V 


II. RELATED WORK A. Fuzzy Neural Networks The learning algorithms of ANNs can be divided into two categories: supervised and unsupervised. Supervised learning involves training instances with labeled outputs, which give feedback about how learning is progressing. This is akin to having a supervisor who can tell the agent whether or not it was correct. In unsupervised learning, the goal is unknown as there are no pre-determined categorizations ANN technology offers a decisive job in terms of summarizing, organizing, and classifying data. Requiring a few assumptions, it also helps identify patterns among input data and achieves a high degree of predictive accuracy [11 The learning and recall procedures allow ANN to mimic the thinking models of human beings: through memorization and recall. In general, an ANN includes the following features parallel processing, error tolerance, recall memory, and optimization solutions Both ANNs and fuzzy models have been applied in many areas [8], [9], [10], [12], [13]; each with its own advantages and disadvantages. Therefore, how to successfully combine A Fuzzy ART2 Model for Finding Association Rules in Medical Data Yo-Ping Huang, Vu Thi Thanh Hoa, Jung-Shian Jau and Frode Eika Sandnes D 978-1-4244-8126-2/10/$26.00 2010 IEEE    these two approaches, ANNs and fuzzy modeling, has become an active area of research. Due to its lack of explanation ability, ANNs are unable to offer easily understandable explanations while the outputs are being generated [15]. Nevertheless, fuzzy models with its ability to articulately express knowledge and technology could compensate for the shortcomings of ANNs. A fuzzy neural network \(FNN produce parameters. It then adapts these parameters for optimization B. Association Rules Mining Mining association rules from large databases is a core topic of data mining. It detects hidden linkages of otherwise seemingly unrelated data. These linkages are rules that 


overpass a preset threshold and are deemed interesting Interesting rules allow actions to be taken based upon data patterns. They can also help make and justify decisions Association rules are defined in the form {X1, X2, , Xn Y, in which Y may present in the transaction if  X1, X2 Xn are all in the transaction. Notice the use of may to imply that the rule is only probable, not deterministic. The probability of finding Y in transactions with all X1, X2,, Xn is called confidence. The threshold that a rule holds in all transactions is called support. The level of confidence that a rule must exceed is called interestingness There are different forms of association rules. The simplest type, Boolean association rules, only shows valid or invalid association. In our medical example, Patients who have old age and large number of positive auxiliary nodes detected will die within 5 years is a Boolean association rule The problem of discovering association rules can be generalized into two steps 1 frequent set of items that exceeds the minimum support 2 For step 1, the Apriori algorithm has been the mostly mentioned algorithm. Many modifications [16]-[17], e.g speeding up and scaling up, of step 1 are about improving the Apriori algorithm by addressing its fallacy of generating too many itemsets. There are also algorithms that are not based on Apriori [18]-[20] but aim at addressing the issues of data mining efficiency Step 2 is mostly characterized by confidence and interestingness. Research has addressed different ways of generating rules [21] and alternative measures to interestingness [22 III. METHODOLOGY A. Data Transformation The original raw data are collected from experiments and investigations. To use these data in each application, we have to transform them into a suitable form. Transformations are widely used in statistics to normalize data to standard form Some common methods of re-expressing data are centering standardizing and normalizing In this study the input patterns for ART2 are fuzzified and the clustered results are used to find the association rules 


among data. Assume that each input pattern contains n attributes \(X1, X2, , Xn each attribute value to a real number in interval [0,1   of set A. For convenience, we denote the fuzzy membership function by f? , where the subscript f indicates the corresponding fuzzy set. The total number of fuzzy sets is denoted by S \( [ ]Sf ,0 functions for each attribute belongs to the interval of the attribute and the distribution of these attribute values The input pattern for ART2 can be represented by new patterns in form of membership degrees 11 f? , 22 f? ,, nnf f1 from 0 to S1, f2 from 0 to S2, , fn from 0 to Sn for describing the input patterns of the algorithm to make efficient in discovering the association rules, we use linguistic terms instead of membership degrees For example, for attribute X1 in a pattern, X1 is expressed by 3 fuzzy sets with linguistic terms: Small, Medium, Large S, M, L X1 will be represented by Large \(L  Fig. 1. Linguistic terms of X1 B. Fuzzy ART2 Neural Network 1 extracted, the proposed fuzzy neural network \(FNN the fuzzy ATR2 is employed to automatically cluster the pattern data. The input and output relation of the proposed fuzzy ART2 can be described as follows Input layer: The input layer consists of units that are called short term memories \(STM Weight layer: The weight layer consists of two kind of weights, i.e., bottom-up and top-down weights \( ijb  and ijt that are called long term memories \(LTM Output layer: The output layer is used to express the clustering results for the given data 2 The fuzzy ART2 learning procedure contains four steps as follows Step 1: Input the fuzzy vector into input layer Step 2: Calculate the distance between the bottom-up weights and fuzzy inputs. Find the shortest distance 


Step 3: If the shortest distance fails to the vigilance test, a new node is created with its weights equal to the fuzzy inputs. If a cluster wins the vigilance test, the    centroid of the cluster is adjusted to adopt the new input Step 4: The process is repeated until all given data have been clustered into suitable clusters If all winners do not pass the vigilance parameter test, it is necessary to create a new cluster and add the corresponding weights. If the state is resonance, the current fuzzy input is assigned to this cluster by modifying the corresponding weights The purposes of using the fuzzy ART2 before discovering the association rules include Decentralize the volume of data from the originally large database to some subsets that contain the smaller number of patterns Because patterns clustered to the same cluster possess the similar characteristics, the time taken to discover the association rules will be shorter than to the originally large data C. Algorithm for Finding Association Rules After grouping all patterns of the original data into some clusters, the data mining algorithms are used to find the association rules from each cluster. As mentioned above, we need two 2 steps to find association rules. First, to improve the efficiency of the Apriori algorithm, a direct hashing and pruning \(DHP  candidate set by filtering any k-itemset out of the hash table if the hash entry does not reach a minimum support Then, rules are determined by interesting measures  Rules become association rules when they have confidence larger than or equal to the minimum confidence IV. EXPERIMENTAL RESULTS AND DISCUSSIONS In this section, we use one benchmark study, Haberman's Survival problem, to illustrate the effectiveness of the proposed model. The data set is available from the UCI machine learning repository [23]. The Harbemans Survival 


datasets consist of 306 samples. Each data sample constituted 4 attributes: age of patient, patients year of operation number of positive auxiliary nodes detected and survival status. Survival status attribute contains only two values that are one and two to indicate the patient survived 5 years or longer and the patient died within 5 years, respectively. Table I gives all linguistic terms for 4 attributes in each pattern for this study The vigilance value in ART2 model will dominate the clustering results that in turn affect the number of patterns clustered to each cluster. Depending on the database size and the properties of the data, it is hard to decide the number of suitable clusters before executing the clustering job. In this study, mid-size clusters, such as 3 to 7, are tested for the medical data. In case only few data were found in a cluster, a check is performed to see whether they are outliers or not. For example, when analyzing datasets we found that one pattern 83, 58, 2, 2 remaining data. Therefore, only 305 patterns are used in the simulations to avoid the overhead of processing this outlier pattern First, we carry out the methodology by fuzzifying the input patterns using the labels of membership functions of 3, 3, 3 and 1 for four attributes, respectively \(case 1, the vigilance value is set to 3.6 illustrated in Fig. 2 for details. The cover range and the shape of membership functions may affect the results of association rules. Although those parameters can be further optimized by the genetic algorithm framework [14] they are not the goal of this research. In this example, all membership functions are empirically determined  Fig. 2. Linguistic terms of four attributes for case 1 After performing the fuzzy ART2, the data can be grouped into 3 clusters, in which clusters 1 to 3 contain 131, 115 and 59 patterns, respectively. We use DHP and interesting measures to find the association rules for each cluster Because of the differences among the volumes of patterns in clusters, minimum support \(min_sup confidence \(min_conf So, we pro rata set minimum support and confidence in accordance with percentage of the number of each cluster 


Fig. 3 illustrates two cases of the data ratio in the clusters. For example, cluster 1 in case 1 contains 43% of the 305 patterns so we set both minimum support and confidence to 57  Fig. 3. Cluster results for case 1 and case 2 The result of this case is compared with the case that only uses DHP to mine all 305 patterns without using Fuzzy ART2. Table II compares the results. Note that, min_sup and min_conf for mining all 305 patterns will be set lower than Acontainingtuples BandAbothcontainingtuplesBAconfidence ___ ______     that of each cluster because each cluster after clustering has the similar properties From Table II, we can see that the results of association rules of combining 3 clusters \(after mining separately clusters without fuzzy ATR2 time to process The purpose of the other experiment is to find out the effect of the number of membership functions on the association rules. The number of membership functions for the patient age attribute and the number of positive auxiliary nodes attribute will be respectively changed to 5 membership functions as shown in Fig. 4 \(case 2, the vigilance value is set to 3.4 case 1  Fig. 4. Linguistic terms of attribute 1 and attribute 3 for case 2 The results are presented in Table III and Table IV. In this case, the dataset is clustered into 5 clusters \(larger than that of case 1 57, 85 and 19. And the same as the case 1, the association rules are similar with or without using fuzzy ART2 Two cases take full advantage of fuzzy ART2 in finding association rules. It separates the dataset into some smaller groups to reduce the computational cost V. CONCLUSIONS 


This study proposes a novel approach for finding association rules from medical data. The combination of fuzzy model and ART2 neural network is developed to cluster the fuzzified dataset into several groups with similar properties. The groups are then particularly exploited for finding the association rules. The approach shows that the more computationally efficiency can be obtained by reducing processing time. An application considered is to group the patients who had undergone surgery for breast cancer into groups that have the similar properties to use group association rules. The experimental results show that the discovered association rules are exactly the same with the conventional approach. The merit of this research is that the computational time is significantly reduced and the approach can be applied to a variety of domains REFERENCES 1] M. H. Margahny and A. A. Mitwaly, Fast algorithm for mining association rules, in Proc. of ICGST Int. Conf. on Artificial Intelligent and Machine Learning, Cairo, Egypt, Dec. 2005 2] A. Savasere, E. Omieccinski and S.B. Navathe, An efficient algorithm for mining association rules in large databases, in Proc. 21th Int. Conf on Very Large Data Bases, Zurich, Switzerland, pp.432-444, Sep 1995 3] R. Li and Z. Wang, Mining classification rules using rough sets and neural networks, European Journal of Operational Research, vol. 157 no. 2, pp.439-448, Sep. 2004 4] A.J.C. Sharkey, G.O. Chandorth and N.E. Sharkey, A multi-net system for the fault diagnosis of a diesel engine, Journal of Neural Computing and Application, vol. 9, no. 2, pp.152-160, 2000 5] D. S. S. Lee, B. J. Lithgow and R. E. Morrison, New fault diagnosis of circuit breakers, IEEE Trans. on Power Delivery, vol. 18, no. 2, pp 454-459, Apr. 2003 6] J.M. Benitez, J.L. Castro and I. Requena, Are artificial neural networks black boxes, IEEE Trans. on Neural Networks, vol. 8, no. 5 pp.1156-1164, Sep. 1997 7] E.  Kolman and M. Margaliot, Are artificial neural networks white boxes, IEEE Trans. on Neural Networks, vol. 16, no. 4, pp.844-852 July 2005 8] Y.-P. Huang, T.-W. Chang, L.-J. Kao and F.E. Sandnes, Using fuzzy SOM strategy for satellite image retrieval and information mining Journal of Systemics, Cybernetics and Informatics, vol. 6, no. 1, pp.1-6 Nov. 2008 


9] S.A. Mingoti and J.O. Lima, Comparing SOM neural network with fuzzy c-means, k-means and traditional hierarchical clustering algorithms, European Journal of Operational Research, vol. 16, no. 3 pp.1742-1759, Nov. 2006 10] R.J. Kuo and Y.T. Su, Integration of ART2 neural network and fuzzy sets theory for market segmentation, Int. Journal of Operations Research, vol. 1, no. 1, pp.67-68, 2004 11] B.S. Ahn, S.S. Cho and C.Y. Kim, The integrated methodology of rough set theory and artificial neural network for business failure prediction, Expert Systems with Application, pp. 6574, 2000 12] G. Tayfur and V.P. Singh, ANN and fuzzy logic models for simulating event-based rainfall-runoff, Journal of Hydraulic Engineering, vol 132, no. 12, pp.1321-1330, Dec. 2006 13] A. Majumdar, P.K. Majumdar and B. Sarkar, Application of an adaptive neuro-fuzzy system for the prediction of cotton yarn strength from HVI fibre properties, Journal of the Textile Institute, vol. 96 no.1, pp.55-60, Jan. 2005 14] T.-P. Hong, C.-H. Chen, Y.-C. Lee and Y.-L. Wu, Genetic-fuzzy data mining with divide-and-conquer strategy, Journal of Evolutional Computation, vol. 12, no.2, pp.252-265, April 2008 15] R.J. Schalkoff, Artificial Neural Networks, McGraw-Hill, New York USA, 1997 16] J. Gao, S. Li and F. Qian, A method of improvement and optimization on association rules apriori algorithm, in Proc. 6th World Congress on Intelligent Control and Automation, Dalian, China, vol. 2 pp.5901-5905, June 2006 17] L. Ji, B. Zhang and J. Li, A new improvement on apriori algorithm, in Proc. of Int. Conf. on Computational Intelligence and Security, vol. 1 pp.840-844, Nov. 2006 18] Y.-P. Huang, L.-J. Kao and F.E. Sandnes, Efficient mining of salinity and temperature association rules from ARGO data, Expert Systems with Applications, vol. 35, no. 1-2, pp.59-68, Aug. 2008 19] L.V.S. Lakshmanan, C.K.-S. Leung and R.T. Ng, The segment support map: scalable mining of frequent itemsets, in Proc. of the ACM SIGKDD Explorations Newsletter, vol. 2 no. 2, pp.21-27, Dec 2000 20] J.S. Park, M.-S. Chen and P.S. Yu, Using a hash-based method with transaction trimming for mining association rules, IEEE Trans. on Knowledge and Data Engineering, vol. 9, no. 5, pp.813-825, Oct. 1997 21] J. Mata, J.L. Alvarez and J.C. Riquelme, Evolutionary computing and optimization: An evolutionary algorithm to discover numeric association rules, in Proc. of the 2002 ACM symposium on Applied 


computing, Madrid, Spain, pp.590-594, Mar. 2002 22] R. Natarajan and B. Shekar, A relatedness-based data-driven approach to determination of interestingness of association rules, in Proc. of  the 20th Annual ACM Symposium on Applied Computing, Santa Fe, New Mexico, USA, pp.551-552, Mar. 2005 23] http://archive.ics.uci.edu/ml    TABLE I ABBREVIATIONS OF THE PATIENT PARAMETERS Attribute No. Parameter Linguistic Term Age of patient 1 Ly Little_Young 2 Yo Young 3 Ma Middle_Aged 4 Se Senior 5 Ls Little_Senior Patients year of operation 1 Ol Old 2 Av Average 3 Re Recent Number of positive auxiliary nodes detected 1 Vs Very_Small 2 Sm Small 3 Me Medium 4 La Large 5 Vl Very_Large Survival status 1 Hi High  TABLE II THE ASSOCIATION RULES RESULTS FOR CASE 1 Cluster 1 Cluster 2 Cluster 3 Combining Cluster 1, 2, 3 All Dataset Sup. \(57 57 62 62 81 81 70 70 Hi <- Av  \(97.7, 97.7 82.6, 96.8 94.9, 98.2 97.7, 97.7 73.5, 96.9 Av <- Hi  \(97.7, 97.7 95.7, 83.6 98.3, 94.8 97.7, 97.7 96.4, 73.9 Ma <- Av  \(97.7, 100.0 82.6, 100.0 94.9, 100.0 97.7, 100.0 Hi  \(73.5, 100.0 


Av <- Ma  \(100.0, 97.7 100.0, 82.6 100.0, 94.9 100.0, 97.7 Ma  \(99.3, 74.0 Me <- Av  \(97.7, 100.0 82.6, 100.0 98.3, 100.0 97.7, 100.0 Hi  \(73.5, 100.0 Av <- Me  \(100.0, 97.7 100.0, 82.6 100.0, 98.3 100.0, 97.7 Me  \(100.0, 73.5 Ma <- Hi  \(97.7, 100.0 95.7, 100.0 93.2, 100.0 97.7, 100.0 Ma <- Av  \(96.4, 99.3 Hi <- Ma  \(100.0, 97.7 100.0, 95.7 94.9, 98.2 100.0, 97.7 Av <- Ma  \(99.3, 96.4 Me <- Hi  \(97.7, 100.0 95.7, 100.0 98.3, 94.8 97.7, 100.0 Me <- Av  \(96.4, 100.0 Hi <- Me  \(100.0, 97.7 100.0, 95.7 94.9, 98.2 100.0, 97.7 Me  \(100.0, 96.4 Me <- Ma  \(100.0, 100.0 100.0, 100.0 98.3, 94.8 100.0 100.0 99.3, 100.0 Ma <- Me  \(100.0, 100.0 100.0, 100.0 94.9, 100.0 100.0 100.0 100.0, 99.3 Ma <- Av Hi  \(95.4, 100.0 80.0, 100.0 100.0, 94.9 95.4 100.0 71.2, 100.0 Hi <- Av Ma  \(97.7, 97.7 82.6, 96.8 98.3, 100.0 97.7 97.7 73.5, 96.9 Av <- Hi Ma  \(97.7, 97.7 95.7, 83.6 100.0, 98.3 97.7 97.7 95.8, 74.4 Me <- Av Hi  \(95.4, 100.0 80.0, 100.0 93.2, 100.0 95.4, 100.0 71.2, 100.0 Hi <- Av Me  \(97.7, 97.7 82.6, 96.8 97.7, 97.7 73.5 96.9 Av <- Hi Me  \(97.7, 97.7 95.7, 83.6 97.7, 97.7 96.4 73.9 Me <- Av Ma  \(97.7, 100.0 82.6, 100.0 97.7, 100.0 73.5, 100.0 Ma <- Av Me  \(97.7, 100.0 82.6, 100.0 97.7, 100.0 73.5, 100.0 Av <- Ma Me  \(100.0, 97.7 100.0, 82.6 100.0, 97.7 99.3, 74.0 Me <- Hi Ma  \(97.7, 100.0 95.7, 100.0 97.7, 100.0 95.8, 100.0 Ma <- Hi Me  \(97.7, 100.0 95.7, 100.0 97.7, 100.0 96.4, 99.3 Hi <- Ma Me  \(100.0, 97.7 100.0, 95.7 100.0, 97.7 99.3, 96.4 


   TABLE III THE ASSOCIATION RULES RESULTS FOR CASE 2 Cluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 Sup. \(74 74 79 79 81 81 72 72 94 94 Hi <- Ma  \(78.2, 95.1 93.9, 95.2 94.7, 94.4 92.9, 97.5 100.0, 100.0 Ma <- Hi  \(94.9, 78.4 95.5, 93.7 94.7, 94.4 96.5, 93.9 100.0, 100.0 Av <- Ma  \(78.2, 98.4 93.9, 95.2 94.7, 100.0 92.9, 97.5 100.0, 100.0 Ma <- Av  \(97.4, 78.9 95.5, 93.7 100.0, 94.7 97.6, 92.8 100.0, 100.0 Vl <- Ma  \(78.2, 98.4 95.5, 95.2 94.7, 100.0 92.9, 100.0 100.0, 100.0 Ma <- Vl  \(98.7, 77.9 95.5, 95.2 100.0, 94.7 100.0, 92.9 100.0, 100.0 Av <- Hi  \(94.9, 97.3 89.4, 94.9 89.5, 100.0 96.5, 97.6 Yo Av  \(100.0, 100.0 Hi <- Av  \(97.4, 94.7 89.4, 94.9 94.7, 94.4 97.6, 96.4 Yo Vl  \(100.0, 100.0 Vl <- Hi  \(94.9, 98.6 90.9, 93.3 94.7, 94.4 96.5, 100.0 Av Vl  \(100.0, 100.0 Hi <- Vl  \(98.7, 94.8 100.0, 96.5 Vl <- Av  \(97.4, 98.7 97.6, 100.0 Av <- Vl  \(98.7, 97.4 100.0, 97.6 Av <- Ma Hi  \(74.4, 98.3 90.6, 97.4 Hi <- Ma Av  \(76.9, 95.0 90.6, 97.4 Ma <- Hi Av  \(92.3, 79.2 94.1, 93.8 Vl <- Ma Hi  \(74.4, 98.3 90.6, 100.0 Hi <- Ma Vl  \(76.9, 95.0 92.9, 97.5 Ma <- Hi Vl  \(93.6, 78.1 96.5, 93.9 Vl <- Ma Av  \(76.9, 98.3 90.6, 100.0 Av <- Ma Vl  \(76.9, 98.3 92.9, 97.5 Ma <- Av Vl  \(96.2, 78.7 97.6, 92.8 Vl <- Hi Av  \(92.3, 98.6 94.1, 100.0 Av <- Hi Vl  \(93.6, 97.3 96.5, 97.6 Hi <- Av Vl  \(96.2, 94.7 97.6, 96.4  


TABLE IV THE ASSOCIATION RULES FROM COMBINING 5 CLUSTERS FOR CASE 2 Combining 5 clusters All Datasets Sup. \(30 30 Hi <- Ma  \(78.2, 95.1 92.3, 79.2 40.5, 75.0 71.2, 41.7 Ma <- Hi  \(94.9, 78.4 74.4, 98.3 73.5, 41.3 30.4, 98.9 Av <- Ma  \(78.2, 98.4 76.9, 95.0 40.5, 96.0 39.2, 76.7 Ma <- Av  \(97.4, 78.9 93.6, 78.1 96.4, 40.3 72.2, 41.6 Vl <- Ma  \(78.2, 98.4 76.9, 98.3 40.5, 96.8 38.9, 96.6 Ma <- Vl  \(98.7, 77.9 76.9, 98.3 96.7, 40.5 39.2, 95.8 Av <- Hi  \(94.9, 97.3 96.2, 78.7 59.5, 72.5 93.1, 40.4 Hi <- Av  \(97.4, 94.7 92.3, 98.6 73.5, 58.7 43.1, 96.2 Vl <- Hi  \(94.9, 98.6 93.6, 97.3 59.5, 96.7 57.5, 72.2 Hi <- Vl  \(98.7, 94.8 96.2, 94.7 96.4, 59.7 71.2, 58.3 Vl <- Av  \(97.4, 98.7 89.5, 100.0 59.5, 96.7 43.1, 97.7 Av <- Vl  \(98.7, 97.4 94.7, 94.4 96.7, 59.5 57.5, 73.3 Yo <- Av  \(94.7, 100.0 94.7, 94.4 73.5, 96.9 72.2, 58.4 Av <- Yo  \(100.0, 94.7 90.6, 100.0 96.4, 73.9 57.5, 96.6 Yo <- Vl  \(94.7, 100.0 92.9, 97.5 73.5, 98.2 57.5, 96.6 Vl <- Yo  \(100.0, 94.7 96.5, 93.9 96.7, 74.7 93.1, 59.6 Yo <- Hi  \(92.9, 100.0 90.6, 100.0 96.4, 96.6 71.2, 98.2 Hi <- Yo  \(100.0, 92.9 92.9, 97.5 96.7, 96.3 72.2, 96.8 Av <- Ma Hi  \(74.4, 98.3 97.6, 92.8 30.4, 97.8 93.1 75.1 Hi <- Ma Av  \(76.9, 95.0 38.9, 76.5  


to be a well-suited performance measure for MC tasks where the number of negative instances significantly exceeds the number of positive instances [6]. Another advantage of AUPRC is its global nature and independence of a certain threshold value. The closer the AUPRC value is to 1, the better the performance Since these measures are based on the comparison of multi-labels, they depend on a transformation from rankings to classes. As a contrast we also used four wellknown ranking-based performance measures: One-Error OE RL C cision \(AP  for all ranking-based performance measures except AP. OneError evaluates how many times the top-ranked label is not in the set of proper labels of the instance. Ranking Loss is defined as the average fraction of pairs of labels that are ordered incorrectly. Coverage evaluates how far we need, on average, to go down the list of labels in order to cover all the proper labels of the instance. Average Precision evaluates the average fraction of labels ranked above a particular label i ? mt which actually are in mt And finally, the special hierarchical loss function H-loss 2] were utilized. Following [3], normalized costs were calculated: ci := 1/|c\(p\(i i ? L i of all direct children of i. Hierarchical loss \(H-loss consider mistakes made in subtrees of incorrectly predicted labels and penalizes only the first mistake along the path from the root to a node. The smaller the H-loss value, the better the performance B. 20 Newsgroups Dataset We modified the popular single-label dataset 20 Newsgroups [18], [19] by considering eight additional labels corresponding to the intermediate levels of the hierarchy faith, recreation, recreation/sport, recreation/vehicles, politics, computer, computer/hardware, science. This dataset is a collection of almost 20,000 postings from 20 newsgroups sorted by date into training \(60 40 data were preprocessed by discarding all words appearing only in the test documents and all words found in the stop word list [20]. Afterwards, all but the 2%-most-frequent words were eliminated to reduce the dimensionality. Documents were represented using the well-known TF-IDF \(Term 


Frequency  Inverse Document Frequency scheme [19]. The TF-IDF weights were then normalized to the range of [0, 1]. Conversion to TF-IDF and normalization were performed separately for training and test data. This resulted in the 1,070-dimensional dataset with 11,256 training instances, 7,493 test ones and 28 labels To test the performance of two HE algorithms, we first extracted hierarchies from the True Test Multi-Labels \(TTML and calculated the corresponding proximity measures. Both algorithms successfully extracted the original hierarchy We studied the performance of the multi-label classifiers and their ability to infer the class hierarchies in the presence of only partly available hierarchical information. We performed a series of HE experiments with multi-labels having a decreasing number of inserted non-leaf labels describing the levels in the hierarchy. We randomly removed such labels from 20%, 30%, and 40% of the training instances leaving them single-labeled. The results for predicted test multilabels are shown in Table I where the bold face marks the best classifier, and the first column \(left result of HE by Voting and the second \(right Thresholding \(referred as GT Comparing classification performance, one can see that the ART-based networks are superior to both the other classifiers in terms of most performance measures and that ML-FAM slightly outperforms ML-ARAM. Taken together they win on at least 6 and at most 8 out of 9 evaluation measures. BoosTexter has the second best performance, but its predictive power degrades more quickly with the increase in the number of single-label instances. The poorest MC results were shown by ML-kNN, its performance decreased very fast with any reduction in the number of multi-labels For example, F1 decreased by 15% while removing 40% of labels instead of 30%. It is also interesting to note that when trained on the dataset with 40% removed labels, ML-FAM and ML-ARAM significantly outperformed ML-kNN trained on the original dataset with all labels The hierarchy proximity measures confirm the good quality of predictions produced by the ART-based networks: The hierarchies were correct extracted by both HE algorithms of Section II-B even with 40% removed labels. The predictions of ML-kNN were the worst: The Voting variant of the HE 


algorithm could not extract the correct hierarchy with 30 assigning five labels incorrectly to the root label. None of the HE algorithms could extract the correct hierarchy in the absence of 40% multi-labels. With 40% and Voting, the number of labels falsely assigned to the root was 13, while with GT it was only three. For BoosTexter, Voting assigned two labels wrongly to the root label in the experiment with TABLE I 20 NEWSGROUPS ALL, -20%, -30% AND -40% RESULTS Measure all 20% 30% 40%ARAM FAM kNN BoosT. ARAM FAM kNN BoosT. ARAM FAM kNN BoosT. ARAM FAM kNN BoosT A 0.635 0.638 0.429 0.549 0.613 0.633 0.383 0.456 0.596 0.619 0.322 0.412 0.563 0.591 0.255 0.387 F1 0.694 0.696 0.565 0.677 0.675 0.688 0.528 0.604 0.662 0.677 0.469 0.566 0.640 0.657 0.392 0.542 F 0.691 0.692 0.480 0.605 0.671 0.688 0.429 0.507 0.658 0.676 0.364 0.465 0.630 0.652 0.296 0.441 OE 0.221 0.220 0.336 0.222 0.259 0.236 0.387 0.275 0.273 0.259 0.415 0.301 0.301 0.291 0.434 0.316 RL 0.100 0.098 0.124 0.073 0.108 0.110 0.128 0.077 0.098 0.103 0.132 0.079 0.101 0.106 0.135 0.082 C 4.188 4.168 6.080 4.164 4.397 4.446 6.184 4.286 4.246 4.340 6.326 4.328 4.334 4.463 6.397 4.379 AP 0.789 0.790 0.677 0.778 0.774 0.782 0.657 0.758 0.769 0.774 0.645 0.747 0.759 0.764 0.638 0.740 AUPRC 0.775 0.772 0.618 0.749 0.743 0.727 0.581 0.691 0.733 0.722 0.555 0.671 0.715 0.708 0.535 0.660 H-loss 0.103 0.123 0.121 0.094 0.102 0.098 0.124 0.108 0.106 0.103 0.132 0.117 0.115 0.111 0.145 0.122 Wins 1 5 0 3 1 6 0 2 2 6 0 1 2 6 0 1 LCAPD 0 0 0 0 0 0 0 0 0 0 0.12 0 0.05 0 0 0 0.51 0.26 0.17 0 CTED 0 0 0 0 0 0 0 0 0 0 0.14 0 0.07 0 0 0 0.50 0.21 0.21 0 TO* 0 0 0 0 0 0 0 0 0 0 0.11 0 0.05 0 0 0 0.39 0.18 0.15 0 30% removed labels and and six labels in the experiment with 40% removed labels. GT resulted in zero distances in the both cases. Assigning more labels to the root creates more shallow and wider hierarchies \(trivial case as stated before The good hierarchy extraction with ART networks demonstrates the system robustness  even with strongly damaged data the system can rebuild the original hierarchy C. RCV1-v2 Dataset The next experiment was based on the tokenized version of 


the RCV1-v2 dataset introduced in [21]. Only the topics label set consisting of 103 labels arranged in a hierarchy of depth four is examined here. Documents of the original training set of 23,149 were converted to TF-IDF weights and normalized Afterwards the set was splitted in 15,000 randomly selected documents as training and the remaining as test samples In this case, the Voting variant of HE applied to the TTML resulted in the LCAPD, CTED and TO* values 0.12, 0.15 and 0.13, respectively. The corresponding values of the GT variant are 0.05, 0.07 and 0.05. The poor performance of the Voting method is due to the fact that for the TTML only very high threshold values succeed in removing enough noise The Voting results are thus dominated by bad hierarchies extracted for all but the highest thresholds The classification and HE results for this dataset are shown in Table II. ML-ARAM has better performance results on this data set in all points than ML-FAM except for RL being the best of all classifiers in terms of the multi-label performance measures. BoosTexter is the best in terms of all ranking measures For both HE algorithms the distances of BoosTexter are the best, those of ML-FAM second, followed ML-ARAM and ML-kNN. All three distance measures correlate. Interesting is also that for ML-kNN the distance values obtained by both HE methods are almost the same The hierarchy extracted by GT from the TTML has much lower distances values as compared with the hierarchies extracted by both methods from predicted multi-labels. This reflects a specific problem of HE, since only a small fraction of the incorrectly classified multi-labels can prevent building of a proper hierarchy. For example, 16.5% of misassigned labels in the extracted hierarchy are responsible for about 80% of LCAPD calculated from the predictions of MLARAM. This large part of the HE error is caused by only 4% of the test data. Under these circumstances the other distances behave analogically. Most labels were not assigned making them trivial edges, but six labels were assigned to a false branch. This can happen when labels have a strong correlation and in the step Hierarchy Construction of the basic algorithm the parent is not unique in the confidence matrix. BoosTexters results suffer less from this problem because it generally sets more labels for each test sample 


Both HE algorithms behaved similarly on the predictions of the ART networks. They constructed a deeper hierarchy than the original one and wrongly assigned the same 11 labels to the root node. The higher distances come from Voting assigning more labels to the wrong branch. For MLkNN both algorithms again create very similar hierarchy trees, both misassigned 28 labels to the root label. For BoosTexter it was seven with Voting and eight with GT Voting produced a deeper hierarchy here D. WIPO-alpha Dataset The WIPO-alpha dataset1 comprises patent documents collected by the World Intellectual Property Organization WIPO ments. Preprocessing was performed as follows: From each document, the title, abstract and claims texts were extracted stop words were removed using the list from [20] and words were stemmed using the Snowball stemmer [22]. All but the 1%-most-frequent stems were removed, the remaining stems were converted to TF-IDF weights and these were normalized to the range of [0, 1]. Again, TF-IDF conversion and normalization were done independently for the training and the test set. The original hierarchy consists, from top to bottom, of 8 sections, 120 classes, 630 subclasses and about 69,000 groups. In our experiment, only records from the sections A \(5802 training and 5169 test samples H \(5703 training and 5926 test samples 1http://www.wipo.int/classifications/ipc/en/ITsupport/Categorization dataset/wipo-alpha-readme.html August 2009 TABLE II RCV1-V2 RESULTS Measure ARAM FAM kNN BoosT A 0.748 0.731 0.651 0.695 F1 0.795 0.777 0.735 0.769 F 0.805 0.787 0.719 0.771 OE 0.077 0.089 0.104 0.063 RL 0.087 0.086 0.026 0.015 C 11.598 11.692 8.563 5.977 AP 0.868 0.860 0.839 0.873 AUPRC 0.830 0.794 0.807 0.838 H-loss 0.068 0.077 0.097 0.081 Wins 4 0 0 5 LCAPD 0.29 0.22 0.25 0.20 0.34 0.34 0.21 0.18 


CTED 0.32 0.23 0.28 0.22 0.38 0.37 0.24 0.20 TO* 0.27 0.18 0.22 0.17 0.31 0.30 0.21 0.17 document in the collection has one so-called main code and any number of secondary codes, where each code describes a group the document belongs to. Both main and secondary codes were used in the experiment, although codes pointing to groups outside of sections A and H were ignored. We also removed groups that did not contain at least 30 training and 30 test records \(and any documents that only belonged to such small groups 7,364 test records with 924 attributes each and a label set of size 131 In this case, the Voting variant of the HE algorithm applied to the TTML resulted in the LCAPD, CTED and TO* values of 0.13, 0.12 and 0, respectively. GT showed the same values. Remarkable are the TO* distances, which are equal to 0. This is due to the fact that the WIPO-alpha hierarchy contains 16 single-child labels that are not partitioned by the true multi-labels: whenever a single-child label j is contained in a multi-label, so is its child, and vice versa. It is therefore theoretically impossible to deduce from the multilabels which of them is the parent of the other. As a result the HE algorithms often choose the wrong parent, resulting in higher LCAPD and CTED values. TO*, as described above is invariant under such choices The results obtained on the WIPO-alpha dataset are shown in Table III. The classification performance of the ART-based networks on this dataset is slightly worse than that of BoosTexter. Mostly in the terms of OE, RL, C, AP, AUPRC, and H-loss measures BoosTexter is better because its rankings are better and it assigned more labels to each sample. But the ART networks have better HE results because their predicted labels are more consistent with the original hierarchy. MLkNN has the worst classification results and distance values again. The reason for the high relative difference between LCAPD as well as CTED and TO* obtained for the ART networks or BoosTexter as compared to the results of the other datasets is because most of the labels were assigned in the right branch but not exactly where they belong Both HE algorithms extracted the same hierarchy from the predictions of ML-ARAM and a very similar hierarchy for ML-FAM. About 5% labels were assigned wrongly to the 


root label in the hierarchies of the ART networks. For MLTABLE III WIPO-ALPHA\(AH Measure ARAM FAM kNN BoosT A 0.588 0.590 0.478 0.564 F1 0.694 0.691 0.614 0.693 F 0.682 0.682 0.593 0.679 OE 0.052 0.057 0.110 0.042 RL 0.135 0.136 0.056 0.025 C 25.135 25.269 22.380 11.742 AP 0.790 0.785 0.724 0.802 AUPRC 0.720 0.684 0.688 0.762 H-loss 0.090 0.093 0.149 0.079 Wins 1 2 0 6 LCAPD 0.16 0.16 0.17 0.17 0.32 0.38 0.21 0.21 CTED 0.18 0.18 0.19 0.19 0.38 0.53 0.27 0.27 TO* 0.05 0.05 0.07 0.07 0.24 0.32 0.08 0.08 kNN both HE methods wrongly assigned about the half of the labels and about 20% of total labels were assgined to the root label. Here, GT extracted a much worse hierarchy as shown by CTED being 0.15 higher for GT than for Voting For BoosTexter both HE methods built the same hierarchy and no label was wrongly assigned to the root. All extracted hierarchies were one level deeper than the original one Although Voting produced worse hierarchies than GT on two previous datasets, this time its distance values were comparable or even better. In comparison to Voting, GT has higher values for all distances on the multi-labels of MLkNN. Voting has the advantage of being a much simpler method and of being more dataset independent. Still the tree distances have the same ranking order for all classifiers for both HE methods VI. CONCLUSION In this paper we studied Hierarchical Multi-label Classification \(HMC tive was to derive hierarchical relationships between output classes from predicted multi-labels automatically. We have developed a data-mining-system based on two recently proposed multi-label extensions of the FAM and ARAM neural networks: ML-FAM and ML-ARAM as well as on a Hierarchy Extraction \(HE algorithm builds association rules from label co-occurrences 


and has two modifications. The presented approach is general enough to be used with any other multi-label classifier or HE algorithm. We have also developed a new tree distance measure for quantitative comparison of hierarchies In extensive experiments made on three text-mining realworld datasets, ML-FAM and ML-ARAM were compared against two state-of-the-art multi-label classifiers: ML-kNN and BoosTexter. The experimental results confirm that the proposed approach is suitable for extracting middle and large-scale class hierarchies from predicted multi-labels. In future work we intend to examine approaches for measuring the quality of hierarchical multi-label classifications REFERENCES 1] M. Ruiz and P. Srinivasan, Hierarchical text categorization using neural networks, Information Retrieval, vol. 5, no. 1, pp. 87118 2002 2] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni, Incremental algorithms for hierarchical classification, The Journal of Machine Learning Research, vol. 7, pp. 3154, 2006 3] , Hierarchical classification: combining Bayes with SVM, in Proceedings of the 23rd international conference on Machine learning ACM New York, NY, USA, 2006, pp. 177184 4] F. Wu, J. Zhang, and V. Honavar, Learning classifiers using hierarchically structured class taxonomies, in Proceedings of the 6th International Symposium on Abstraction, Reformulation And Approximation Springer, 2005, p. 313 5] L. Cai and T. Hofmann, Hierarchical document categorization with support vector machines, in Proceedings of the thirteenth ACM international conference on Information and knowledge management ACM New York, NY, USA, 2004, pp. 7887 6] C. Vens, J. Struyf, L. Schietgat, S. Dz?eroski, and H. Blockeel Decision trees for hierarchical multi-label classification, Machine Learning, vol. 73, no. 2, pp. 185214, 2008 7] E. P. Sapozhnikova, Art-based neural networks for multi-label classification, in IDA, ser. Lecture Notes in Computer Science, N. M Adams, C. Robardet, A. Siebes, and J.-F. Boulicaut, Eds., vol. 5772 Springer, 2009, pp. 167177 8] M. Zhang and Z. Zhou, ML-kNN: A lazy learning approach to multilabel learning, Pattern Recognition, vol. 40, no. 7, pp. 20382048 2007 9] R. Schapire and Y. Singer, BoosTexter: A boosting-based system for text categorization, Machine learning, vol. 39, no. 2, pp. 135168 


2000 10] K. Zhang, A constrained edit distance between unordered labeled trees, Algorithmica, vol. 15, no. 3, pp. 205222, 1996 11] A. Maedche and S. Staab, Measuring similarity between ontologies Lecture notes in computer science, pp. 251263, 2002 12] G. Carpenter, S. Martens, and O. Ogas, Self-organizing information fusion and hierarchical knowledge discovery: a new framework using ARTMAP neural networks, Neural Networks, vol. 18, no. 3, pp. 287 295, 2005 13] A.-H. Tan and H. Pan, Predictive neural networks for gene expression data analysis, Neural Networks, vol. 18, pp. 297306, April 2005 14] G. Carpenter, S. Grossberg, N. Markuzon, J. Reynolds, and D. Rosen Fuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps, IEEE Transactions on Neural Networks, vol. 3, no. 5, pp. 698713, 1992 15] Y. Freund and R. Schapire, A decision-theoretic generalization of online learning and an application to boosting, Journal of computer and system sciences, vol. 55, no. 1, pp. 119139, 1997 16] K. Zhang and T. Jiang, Some MAX SNP-hard results concerning unordered labeled trees, Information Processing Letters, vol. 49 no. 5, pp. 249254, 1994 17] G. Tsoumakas and I. Vlahavas, Random k-labelsets: An ensemble method for multilabel classification, Lecture Notes in Computer Science, vol. 4701, p. 406, 2007 18] K. Punera, S. Rajan, and J. Ghosh, Automatic construction of nary tree based taxonomies, in Proceedings of IEEE International Conference on Data Mining-Workshops. IEEE Computer Society 2006, pp. 7579 19] T. Joachims, A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization, in Proceedings of the Fourteenth International Conference on Machine Learning. Morgan Kaufmann Publishers Inc. San Francisco, CA, USA, 1997, pp. 143151 20] A. McCallum, Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering, 1996 http://www.cs.cmu.edu/ mccallum/bow 21] D. Lewis, Y. Yang, T. Rose, and F. Li, RCV1: A new benchmark collection for text categorization research, The Journal of Machine Learning Research, vol. 5, pp. 361397, 2004 22] M. Porter, Snowball: A language for stemming algorithms, 2001 http://snowball.tartarus.org/texts/introduction.html 


the US census data set. The size of pilot sample is 2000, and all 50 rules are derived from this pilot sample. In this experiment the ?xed value x for the sample size is set to be 300. The attribute income is considered as a differential attribute, and the difference of income of husband and wife is studied in this experiment. Figure 3 shows the performance of the 5 sampling 331 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW    


      6D PS OL QJ  RV W 9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset procedures on the problem of differential rule mining on the US census data set. The results are also similar to the experiment results for association rule mining: there is a consistent trade off between the estimation variance and sampling cost by setting their weights. Our proposed methods have better performance than simple random sampling method 


We also evaluated the performance of our methods on the Yahoo! dataset. The size of pilot sampling is 2000, and the xed value x for the sample size is 200. The attribute price is considered as the target attribute. Figure 4 shows the performance of the 5 sampling procedures on the problem of differential rule mining on the Yahoo! dataset. The results are very similar to those from the previous experiments VI. RELATED WORK We now compare our work with the existing work on sampling for association rule mining, sampling for database aggregation queries, and sampling for the deep web Sampling for Association Rule Mining: Sampling for frequent itemset mining and association rule mining has been studied by several researchers [23], [21], [11], [6]. Toivonen [23] proposed a random sampling method to identify the association rules which are then further veri?ed on the entire database. Progressive sampling [21], which is based on equivalence classes, involves determining the required sample size for association rule mining FAST [11], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.A randomized counting algorithm [6] has been developed based on the Markov chain Monte Carlo method for counting the number of frequent itemsets Our work is different from these sampling methods, since we consider the problem of association rule mining on the deep web. Because the data records are hidden under limited query interfaces in these systems, sampling involves very distinct challenges Sampling for Aggregation Queries: Sampling algorithms have also been studied in the context of aggregation queries on large data bases [18], [1], [19], [25]. Approximate Pre-Aggregation APA  categorical data utilizing precomputed statistics about the dataset Wu et al. [25] proposed a Bayesian method for guessing the extreme values in a dataset based on the learned query shape pattern and characteristics from previous workloads More closely to our work, Afrati et al. [1] proposed an adaptive sampling algorithm for answering aggregation queries on hierarchical structures. They focused on adaptively adjusting the sample size assigned to each group based on the estimation error in each group. Joshi et al.[19] considered the problem of 


estimating the result of an aggregate query with a very low selectivity. A principled Bayesian framework was constructed to learn the information obtained from pilot sampling for allocating samples to strata Our methods are clearly distinct for these approaches. First strata are built dynamically in our algorithm and the relations between input and output attributes are learned for sampling on output attributes. Second, the estimation accuracy and sampling cost are optimized in our sample allocation method Hidden Web Sampling: There is recent research work [3 13], [15] on sampling from deep web, which is hidden under simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database Bar-Yossef et al.[3] proposed algorithms for sampling suggestions using the public suggestion interface. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy with a low sampling cost for a speci?c task, instead of simple random sampling VII. CONCLUSIONS In this paper, we have proposed strati?cation based sampling methods for data mining on the deep web, particularly considering association rule mining and differential rule mining Components of our approach include: 1 the relation between input attributes and output attributes of the deep web data source, 2 maximally reduce an integrated cost metric that combines estimation variance and sampling cost, and 3 allocation method that takes into account both the estimation error and the sampling costs Our experiments show that compared with simple random sampling, our methods have higher sampling accuracy and lower sampling cost. Moreover, our approach allows user to reduce sampling costs by trading-off a fraction of estimation error 332 6DPSOLQJ9DULDQFH      


     V WL PD WL RQ R I 9D UL DQ FH  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W 9DU 9DU 


9DU 5DQG b 6DPSOLQJ$FFXUDF          5  9DU 9DU 9DU 5DQG c Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset 6DPSOLQJ9DULDQFH             9D UL DQ FH R I V WL 


PD WL RQ  9DU 9DU 9DU 5DQG a timation 6DPSOLQJ&RVW          6D PS OL QJ  RV W  9DU 9DU 9DU 5DQG b 6DPSOLQJ$FFXUDF         


    5  9DU 9DU 9DU 5DQG c Fig. 4. Evaluation of Sampling Methods for Differential Rule Mining on the Yahoo! Dataset REFERENCES 1] Foto N. Afrati, Paraskevas V. Lekeas, and Chen Li. Adaptive-sampling algorithms for answering aggregation queries on web sites. Data Knowl Eng., 64\(2 2] Rakesh Agrawal and Ramakrishnan Srikant. Fast algorithms for mining association rules. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487499, 1994 3] Ziv Bar-Yossef and Maxim Gurevich. Mining search engine query logs via suggestion sampling. Proc. VLDB Endow., 1\(1 4] Stephen D. Bay and Michael J. Pazzani. Detecting group differences Mining contrast sets. Data Mining and Knowledge Discovery, 5\(3 246, 2001 5] M. K. Bergman. The Deep Web: Surfacing Hidden Value. Journal of Electronic Publishing, 7, 2001 6] Mario Boley and Henrik Grosskreutz. A randomized approach for approximating the number of frequent sets. In ICDM 08: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pages 4352 Washington, DC, USA, 2008. IEEE Computer Society 7] D. Braga, S. Ceri, F. Daniel, and D. Martinenghi. Optimization of Multidomain Queries on the Web. VLDB Endowment, 1:562673, 2008 8] R. E. Ca?isch. Monte carlo and quasi-monte carlo methods. Acta Numerica 7:149, 1998 9] Andrea Cali and Davide Martinenghi. Querying Data under Access Limitations. In Proceedings of the 24th International Conference on Data Engineering, pages 5059, 2008 10] Bin Chen, Peter Haas, and Peter Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In KDD 02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 462468, New York, NY, USA, 2002 ACM 


11] W. Cochran. Sampling Techniques. Wiley and Sons, 1977 12] Arjun Dasgupta, Gautam Das, and Heikki Mannila. A random walk approach to sampling hidden databases. In SIGMOD 07: Proceedings of the 2007 ACM SIGMOD international conference on Management of data pages 629640, New York, NY, USA, 2007. ACM 13] Arjun Dasgupta, Xin Jin, Bradley Jewell, Nan Zhang, and Gautam Das Unbiased estimation of size and other aggregates over hidden web databases In SIGMOD 10: Proceedings of the 2010 international conference on Management of data, pages 855866, New York, NY, USA, 2010. ACM 14] Arjun Dasgupta, Nan Zhang, and Gautam Das. Leveraging count information in sampling hidden databases. In ICDE 09: Proceedings of the 2009 IEEE International Conference on Data Engineering, pages 329340 Washington, DC, USA, 2009. IEEE Computer Society 15] Loekito Elsa and Bailey James. Mining in?uential attributes that capture class and group contrast behaviour. In CIKM 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 971 980, New York, NY, USA, 2008. ACM 16] E.K. Foreman. Survey sampling principles. Marcel Dekker publishers, 1991 17] Ruoming Jin, Leonid Glimcher, Chris Jermaine, and Gagan Agrawal. New sampling-based estimators for olap queries. In ICDE, page 18, 2006 18] Shantanu Joshi and Christopher M. Jermaine. Robust strati?ed sampling plans for low selectivity queries. In ICDE, pages 199208, 2008 19] Bing Liu. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data \(Data-Centric Systems and Applications Inc., Secaucus, NJ, USA, 2006 20] Srinivasan Parthasarathy. Ef?cient progressive sampling for association rules. In ICDM 02: Proceedings of the 2002 IEEE International Conference on Data Mining, page 354, Washington, DC, USA, 2002. IEEE Computer Society 21] William H. Press and Glennys R. Farrar. Recursive strati?ed sampling for multidimensional monte carlo integration. Comput. Phys., 4\(2 1990 22] Hannu Toivonen. Sampling large databases for association rules. In The VLDB Journal, pages 134145. Morgan Kaufmann, 1996 23] Fan Wang, Gagan Agrawal, Ruoming Jin, and Helen Piontkivska. Snpminer A domain-speci?c deep web mining tool. In Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, pages 192 199, 2007 24] Mingxi Wu and Chris Jermaine. Guessing the extreme values in a data set a bayesian method and its applications. VLDB J., 18\(2 25] Mohammed J. Zaki. Scalable algorithms for association mining. IEEE Transactions on Knowledge and Data Engineering, 12:372390, 2000 


333 


