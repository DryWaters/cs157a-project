html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">SECOND IEEE  INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS. JUNE 2004 Multitarget Tracking in Clutter Two Algorithms for Data Association Using BPNN and LVQNN M. Daneva Absfract4n this paper two algorithms for data association in the context of multiple target tracking on non manoeuvring aircrafts using Back-Propagation Neural Network \(BPNN Network \(LVQNN algorithms are compared with those of the standard method for data association based to the nearest-neigbhnur rule by Monte Carlo experiment and by using real radar data records Index Term-neural networks, radar data processing I. INTRODUCTION Radar data processing in automatic systems for air traffic control \(ATC aircraft's spatial location, speed and acceleration, beading angle, etc. Multiple target tracking \(MTl the radar information obtained from several radar scans The MTT system includes the following element: sensor data processing and measurement formation; correlation and data association; track initiation, confirmation, and deletion; track filtering and prediction; gating procedure The algorithms for data association \(DA an impoltant place in automatic systems for radar data processing for ATC. The purpose of the DA algorithm is to classfy the received radar measurements to the available tracks in the presence of clutter during the MTT process using the estimated kinematic parameters of the aircrafts provided by the tracking filter [l], [2]. As a part of the total set of algorithms for MlT, the DA algorithms can be probabilistic and heuristic [2]. The first are based on statistical methods, and the second are based on the artificial intelligence approach and in particular on the neural networks \(NNs them are first, the heuristic algorithms can do their functions without a priori knowledge about the statistics of the input data, and second - they are "model-free". An important quantity to each DA algorithm is to ensure high percentage of correct decisions \(associations or c~assifications ATC and the flight safely too. The accuracy of the algorithm for DA and the accuracy of the estimate provided by the tracking filter reflect to a great degree to M. D. Author is a W.D. student with the Dept. Of Radiocornmi cations, Faculty of Communications and Communicational Technologies Technical University of Sofia, 8 KI. Ohridski sv., 1000 Sofia, Bulgaria telephone: +359 02 965 32 97, e-mail: mimidan@ttu-sofiabg 0-7803-8278- 11041$20.00 02004 IEEE 92 pCc PA  depends on the choice of the most appropriate neural architecture, the learning paradigm, the pattern formulation, the accuracy and convergence of the training algorithm and the NN's initialization method [3]-[5 The used in practice neural DA algorithms for MTT usually are realized on a classification principle to use the capacities and main advantages of the NNs more completely. Suitable for association problems are the recurrent NNs as the Hopfield network, the associative memories, recurrent autoassociative NNs, Elman network adaptive resonawe theory NNs, as well as some self organizing networks - Kohonen maps, SARDNET \(Self Organizing Feature Map for Sequences Multilayer Perceptrons \(MLPs to relatively good results, because the separation surfaces in this case are always closed surfaces. The Radial Basis Functions NNs can be used in this case too. The feedforward NNs, as well as the recurrent NNs can be 


feedforward NNs, as well as the recurrent NNs can be used for classification purposes. MLPs are the traditional NNs for classification. They have relatively good generalization performance, and are the closest NNs to the optimal Bayesian classifier, which ensures the probability of correct classification of approximately 81.51 %. The LVQNNs ensure Pcc \('?A case, and which generally appears a hit lower than this for MLPs, but in case of highly non-linear separation boundaries even can exceed it [3]-[SI In this paper two algorithms for DA in the context of MTT, one using BPNN and one using LVQNN are presented. They are designed for tracking on non manoeuvring targets in ATC systems using single secondary surveillance radar, and are realized i n M a r m 191 environment. Their performances are compared with these of the standard DA algorithm based on nearest neighbour rule \(Nearest-Neighbour DA, " D A position only radar data [l]. A Monte Carlo \(MC verification of the results is done. An illustrative example using real radar data is shown. The proposed algorithm perfom relatively high PCC NNDA 11. INPUT DATA, NN ARCHITECTURE AND BASIS STEPS OF THE ALGORITHMS Two versions of each proposed algorithm are presented The basic version use as input data the actual and predicted target positions in polar coordinates range p in nautical miles, azimuth angle 0 in rad, and altitude h in feet, target identification codes, and reflection coefficient IO]. The extended version uses in addition information about the amplitude of the received signal, measured from the output of the quadratic detector of the receiver in dB The algorithms are denoted by BPNNDA \(BPNN Data Association for the case with no amplitude information included, and by BPNNDAAM \(BPNNDA with Amplitude Measurements LVQNNDA with Amplitude Measurements amplitude information. The training set contains the predicted positions of all existing targets, identification codes, and reflection coefficients for BPNNDA and LVQNNDA versions, and, in addition, the predicted amplitudes for BPNNDA and LVQNNDAAM versions The training set in the both cases includes simulated data for false targets too. The predicted positions of the targets are obtained by BPNN tracking fdters with model prediction without [ll] and with additional amplitude information included. The testing set contains the measured values of all the targets  parameters listed above and false measurements too. The tme and false measurements of one and the same target of interest have identical target identification codes, so that the DA algorithm must recognize them and to pass for next processing by the tracking filter and for track update only the measurements which can be used with the highest convenience. In this context the reflection coefficients and the amplitude information are used as additional features to distinguish the true measurements of a target from the false. Also, the target identification codes are used to distinguish the aircrafts each to other The neural architecture for BPNNDA and BPNNDAAh4 includes fully coMected multilayer feedfonvard neural network with single hidden layer with hyperbolic tangent activation functions in the input and hidden layer, and linear output layer. The numbers of neurons in the layers are 5-12-5 and 6-156 for the first and the second case, respectively. The Marquardt Levenberg training algorithm [4], [9] is used. It ensures the highest training convergence and minimum number of 


the highest training convergence and minimum number of epochs with relatively good approximation of the error  s minimum than the other training algorithms for BPNN The numbers of input and output neurons for LVQNNDA and LVQNNDAAM versions of the algorithm are the same as in BPNNDA and BPNNDAAM cases. The number of competitive neurons in the both of versions is equal to the number of existing tracks of targets, chosen as Q = 6 by reason of available hardware to perform the computations  The basic steps of the algorithms are as follow A. BPNNDA and BPNNDAAM algorithm Step 1. NN Initialization: Random initialization of the weights with minimum bound m&lt;bvl [I21 foI BPNNDA, and by the Shimodaira  s method, variant c [ 121 for BPNNDAAM case. These methods are chosen as the ClOSeSt to the oplimal case among seven weight initialization methods, included the usual random weight initialization in two variants - in[-0.05,0.05] and in 1,1], the Russo  s metbod, the Statistically Controlled Activation Weight Initialization \(SCAWI Nguyen-Widrow method, and the Nguyen-Widrow-Russo algorithm [4], [5 ]  by separated experiment Step 2. Input Data Preprocessing: The normalization coding \(NC 4 .9,0.9]  to avoid the high nonlinearly zones of the neurons  activation functions IS] for BPNNDA, and the same NC after Discrete Cosine Transform @CT the form DCT-IV [9] for BPNNDAAM Step 3. Training data set presentation, set k = 0, where k is the epoch  s number Step4. k = k + l Step 5. Forward computations: Compute the net internal activity levels and the output signals of all the neurons in the layers according the Mquardt-Levenberg algorithm Sfep 6. Error back-propagation: Compute the weight adjustments and the vectors of local error gradients for the output and the hidden units by the same training Sfep 7. Go to steps 4 to 6 to perform the iterations till the error minimum is found or the maximum number of epochs or the maximum learning rate is reached algorithm Step 8. Compute the local and global error function Step 9. Testing set presentation Step 10. Actual classification @A errors for scan k Sfep 11. Data postprocessing: Inverse normalition coding \(INC BPNNDAAM Step 12. k = k + 1: Repeat cyclically the steps 2 to 11 till the track deletion criterion, the missed data for a given track for six consecutive radar scans [I31 is satisfied B. LVQNNDA and LVQNNDAAM algorithm Step 1. NN Initialization - The usual initialization for LVQNN, which sets of the competitive neurons  weights equal to the center-vectors of the classes, and sets the weights of the linear neurons as small random numbers Step 2. Input Data Preprocessing: DCT and NC in Sfep 3. Training data set presentation, set k = 0 Step 4. Compute the distances between the pattern vector and the Voronoi  s vectors 141, [9] for a training examule a bv 41 4 .9 ,0 .9]  for both of LVQNNDA and LVQNNDAAM  Rg = R    1 Step 5. Determine the closest Voronoi  s vector V   the input vector by the maximum output signal y, =P J 2 Step 6. Adaptation of V 6.1 


Step 6. Adaptation of V 6.1 for correct classificatioc and 93 V   k+l    k Pk k    k 4 for incorrect classification [9] till the maximum number of epochs is reached. The learning rate is denoted by p Step 7. Testing set presentation Step 8. Actual classification @A errors for scan k Step 9. Data postprocessing: INC and inverse DCT for both of LVQNNDA and LVQNNDAAM Step IO. k = k + 1 : Repeat cyclically the Step 2 to Step 9 till the same track deletion criterion is satisfied The maximum number of epochs is k,,  = 100 for the both of versions; the learning rate is 0.07 for LVQNNDA and 0.1 for LVQNNDAAM. These values are chosen as the closest to the optimal case by separated e.xperiment The input data preprocessing and postprocessing techniques for all the versions of the algorithm are chosen as near optimal among six variants, included NC in [-1,1 NC with zero mean and unity variance [9], and the same methods with DCT before NC by separated experiment The used track deletion criterion is standard for seconday surveillance radar systems [13 The proposed algorithm for tracking on multiple targets may easily to be realized in software implementation on Windows or on Unix platforms using high-level algorithmic language, e.g. C, Ctc ,  Java, or other object-oriented computer languages, as the critic with respect to the time procedures can be realized using digital signal processors, or by hardware too. The main advantages of the hardware realization are simplicity and shorter codes, faster operations, much more accurate results, and possibilities to achieve high level of parallelism 111. EXPERIMENTAL RESULTS Simulated and live data for six tracks of non manoeuvring targets, chosen in random way are used to form the training set for the computer modelling. The live data from neighbour targets are recorded from the plot extractor  s output of Monopnlse Secondary Surveillance Radar CMSSR-401 [lo] with sampling time T=10 s, i. e the radar data are extracted and recorded before their processing by the tracking algorithm in the radar system The Live tracks with length of 90 consecutive scans are shown in Fig. 1. The recorded radar measurements are from tme and false targets. In this case false measurements are received only in scans k = 30 and k = 50. The live tracks are used as prototypes to model the tracks for the MC simulations, as follow. After polar to Cartesian transform to equalize the dimensions of the coordinates the measurements from the first and the last radar scan, are connected with straight line and spaced with scan time T to form the trajectory of non-manoeuvring target. The track TI is modelled using the first and the last point of the section with constant altitude and the first and the last point of the section with varying altitude. Next each track is corrupted with Gaussian noises, added directly to each coordinate to model the measurement errors according to the target kinematic model. The cumulative distribution _ e  i   I im Fig. I .  Live tracks used in the experiments function of the noises is verificated by chi-square test with significant level a = 0,05. Next the tracks are transformed back in polar coordinates and then they are processed by the algorithm under the test. By this way only true radar measurements are modelled: Next a number of false measurements for each track for a scan are modelled for 


measurements for each track for a scan are modelled for each MC run. Their locations are modelled as uniformly distributed in the validation regions \(the gates tracks. These validation regions are next used only for the standard  D A  algorithm. In this case the expected number of false returns within the gate of a track is 0.03 Next using a gating coefficient of three, the probability of at least one return falling within the gate volume is approximately 0.24. Any number of false measurements false plots   The parameters for comparison by MC runs are the averaged number of epochs k , the averaged fmal training error E/in, the averaged values of PCC training and DA stages, denoted by F p F g FMisde   theoretical number of arithmetic operations addition Z and multiplication x ,  and the total number of arithmetic operations required for data processing of one track for one scan by each proposed algorithm with included number of epochs, are presented in Table 1. The results from 50 MC runs and for one run using real radar data for the same tracks are presented in Table 2 and Table 3 respectively. The computational costs during the MC runs are estimated by the averaged CPU time r,pu and number TABLE I NUMBER OF ADDIllUNS AND MULllPLlCATlUNS FUR EACH ALGORITHM A t g o r i h  k z x Total BPNNDA 1 372 723 1095 BPNNDAAM 1 485 1123 1608 LVQNNDA 100 4% 1n 510 LVQNNDAAWr lm 379 387 784 94 of FLoat Opelatiom Per Second \(FLOPS data processing of a l l  tracks for scan k. The performances of all the algorithms for the MC runs are shown in Fig. 2 and Fig. 3, respectively. The resdts show that better quality of DA's performance appears by BPNNDA and BPNNDAAM versions of the algorithm. The percentage of correct classifications during the training and testing phases of the DA algorithm using BPNN is 100 % in all considered cases, while with LVQNN it is a bit smaller The pattern vector extended by the amplitude slightly decreases FE case of close spaced tracks of targets, it is possible to obtain nomro  value of ~ m i 8 that no one of the received plots \(true or false given target cannot be used for track update, but it occurs rarely - in less than 0.01 % of the cases. It is due to the facts first that the LVQNN performs the nearest-neighbour rule in neural variant, and second, the amplitude signals of close spaced targets have similar values. The last does not confuse the nehvork of the BPNNDAAM version. By the different kind of LVQNN's initialization, it performs smaller initial training ermrs than BPNN. The training error's final values \(at the end of training LVQNNDA and LVQNNDAAM algorithm are smaller  99 985 Fig. 2. MC simulations results: percentages of correct decisions during the training and testing phases i\\ 0.1 D J 1 IO k 100 Fig. 3. MC simulations results: averaged values ofthe fmal trainingerr0r and the number of epochs than these with BPNNDA and BPNNDAAM are, due to the longer training \(slower training convergence and more iterations required 


algorithm allows to stop the training when the early stopping criterion is met, which approximates the error minimum more roughly than using LVQNN, so that the training e m x  is higher. So, despite the higher computational complexity of BPNN and BPNNDAAM they need less time to perform the DA than LVQNNDA and LVQNNDAAM. The averaged CPU time in MTUE for data processing in BPNN case is near than this of D A .  Because of the lower clutter density in the case when real data are used, the LVQNNDA and LVQNNDAAM achieve 100 % of correct classifications for training and testing phase \(Table 3 and Fig. 4 Since in the first few scans there are no false plots, and due to the greater KF's tracking errors the tme radar measurements fall out of the track gates, than zero values of Ptest for the " D A  algorithm are obtained. All proposed algorithms outperform the classical NNDA, and due to the greater KF's tracking errors the true radar measurements fall out of the track gates, than zero values of for the " D A  algorithm are obtained. All proposed algorithm outperform the classical " D A approach with standard recursive KF to form the 95 TABLE I1 MONTE CARLO SIMULATIONS RESULTS Algorithm 5 F&amp; P%~A LVQNNDA 100 0.obos 99.99 99.97 0.00 0.001 1 .58 25855 LVQNNDAAM 100 0,0010 99.98 99.93 0.00 0.006 1.61 27023 BPNNDA o o.ooa4 i0o.m 1oo.m 0.00 0.000 0.37 1802900 BPNNDAAM 0 0.0029 100.03 1oo.m 0.00 0.000 0.50 312XiOO NNDA - - - 94.36 1.19 0.000 0.35 22801 TABLE 111 RESULTS USING REAL DATA Algorithm E E&amp; @TA BPNNDA 0 0.0025 100.03 loom 0.00 0.000 0.37 18M900 BPNNDAAM 0 0.0027 100.03 1oo.m 0.00 0.000 0.50 312sM LVQNNDA 100 0.0004 100.03 100.03 OBI 0.000 1.58 ' 25855 LVQNNDAAM 100 0.0002 1oo.m 1oo.m 0.00 0.000 1.61 27023 NNDA - - - 91.85 7.78 0.000 0.31 8906 PERFORMANCE COMPARISON FTHE DA ALOORITHMS DEPPND ON THE Usm TRACKING FILTER TABLE IV Filtsr DAalgolithm E.%i @Y BFNNDA 0.0024 lOOa0 1M LVQNNDA 013035 999 99.m 0.00 . 0.001 1 .S8 25855 BPNNDA 0.0154 100 99.97 0.00 0.m 0.40 2198800 Kalmanfikr LVQNNDA .0.243 99.9 98.63 0.00 0.061 1.67 25741 NNDA - - 94.35 1.19 0.m 0.35 22801 B F "  filer extrapolated positions of the targets. Because of the worse KF's tracking accuracy during the first few scans before the region of convergence, the true plots from the targets appears out of the gates of the tracks, so that false plots can be used for track formation. In this case nomro values of Pm,sde, occur. By the BPNN filter \( B P W I31 the moment when the filter enters in convergence almost does not affect to the quality of the proposed DA algorithms An experiment of 50 MC runs is provided for the BPNNDA and LVQNNDA algorithms using the extrapolated by KF position of the targets instead these  Fig. 4 data Percentages of correct decisions for data association using real obtained by the BPNNF Thus the performances of the neural algorithms for data association with respect to the used tracking accuracy of the filter are investigated and compared. The results are presented in Table 4 and Fig. 5 The results show that the tracking accuracy of the filter effects onto the quality of training and the quality of data association too. This effect is expressed more strongly in the LVQNNDA version of the algorithm. The values of 


the LVQNNDA version of the algorithm. The values of F z  using the extrapolated by KF positions \(LVQNNDA-KF are lower than these for the BPNNDA version in the same case \(BPNNDA-KF  increased, which is undesilable effect. Despite this, the tracking accuracy of the algorithm obtained during the data association process is better than this of the standard NNDA algorithm. The improvement in FE averagely of 4.31 % for LVQNNDA-KF and 5.61 % for BPNNDA-KF. The increasing of the DA's error is expressed more hardly during the first few radar scans where the KF's tracking error has the greatest values. It is seen in Fig. 5 ,  that the functions of F s LVQNNDA-KF version are similar to these obtained by the " D A  algorithm during the time. This is completely natural phenomena, because the LVQNN appears as nearest neighbour's implementation in neural variant. The extrapolated positions are one and the same for both the NNDA and the neural algorithms. And yet, due to the N"s ability to be trained, better results are obtained 96 1 10 k I00 25 Fig 5 the used tracking filter Percentages of corned decisions for data association depend on during the DA process. The values of the training error at the end of training for LVQNNDA-KF are increased of about two orders and these of BPNNDA-KF - of about one order greater than these of the cases using BPNNF The results from this experiment shows clearly the importance of achieving high tracking accnracy of the BPNNF in MTT, and relatively not a little improvement of the percentages of correct classifications using NN in the developed tracking algorithms, compared with the standard  D A  approach. And yet, due to the NN  s ability to be trained, better results are obtained during the data association. The values of the final training error for LVQNNDA-KF are increased of about two orders, and these of BPNNDA-KF - of about one order greater than these of the cases using B P N N F .  A l s o ,  using KF the neural algorithms for data association need more CPU time and FLOPS to perform the classification. The results from this experiment shows clearly the importance of achieving high tracking accuracy of the BPNNF in MTT and relatively not a little improvement of the percentages of correct classifications using NN in the developed tracking algorithms. The results from the experiments to investigate the performances of B P N N D A ,  B P N N D A A M LVQNNDA and LVQNNDAAM algorithms show that the BPNN and LVQNN can effectively to be used in such algorithms, because they ensure high quality of training and averagely more than 97 % of correct classifications during the testing, e.g. during the tme data association This percentage of correct classifications exceeds the percentage achieved by the standard NNDA algorithm with average CPU time commensurable with it  s of the  D A  algorithm IV. CONCLUSIONS In this paper two algorithms for data association in the context of MTT using BPNN and LVQNN are designed evaluated and compared with the classical  D A approach. Two versions of each algorithm are presented without and with additional amplitude information included. The experimental results show that these algorithms work efficiently in different environment and produce average more than 97 % of correct decisions which are more than these of the standard NNDA approach produce. The improved quality of MTT process will affect positively onto the level of safety and tmstwoahiness in ATC. One future research will extend 


tmstwoahiness in ATC. One future research will extend the DA logic of the algorithms for manoeuvring case V. ACKNOWLEDGMENT The author thanks to Mr. Balarev and Mr. Bojkov with ATSA Sofia Airport for kindly placed real radar database used for the experiments REFERENCES I ]  S. Blackman, Multiple Torget Tracking with Rodar Applications Nonvood, Arttech House, 1986 2] Y. Bar-Shalom \(editor vol. II, 1992, D e d k  Artech House 131 B. Kosko, Neurol Nehvorkr and Fuzzy Systems, Prentice Hill International, 1992 141 Haykin, S., Neural Networks: A Comprehensive Foundohon, Yd EditioR New Jersey, Rentice Hall, 1999 5 ]  C. G. Looney, Panern Recognition Using Neural Nerworkr, New Yn*, Oxford University Press, 1997 161 Konar, A., Artificiol Intelligence and soft Computing: Behoviorol and Cognifive Modeling of The Humon Brain, CRC Ress LLC Boca Raton, Florida, 2000 7] Micheli-Tzanakou, E., Supervised and \(insuperwed Poftern Recognition: Feorure fitroction ond Compufation, CRC Ress LLC, Boea Raton, Florida 2000 8] Jain, L, Fanelly, A.M., Recent Advances in Artljicrol Neurol Networkr: Design ond Applrcotions, CRC Press LLC, Boca Raton Florida, ZOM 9] Neural Networks Toolbox User  s Guide Version 3.0, ?he Math Works, Inc.. 1992-1998 IO] Manopulse Secondary Surveillance Radar Syst~m Description Technical Repmi, Cardion Inc., Repoll No. 131-162A Ill Daneva, M  A Comparative Analysis of Some Data Preprocessing Techniques for BPNN Tracking Filter  WWIII Internotional Scientljic Conference on Information, Communication ond Energv Systems ond Technologier ICEST 2W3, Sofia, Bulgaria, pp. 144 147 1121 Hernindez-Espinosa C., M. Femindez-Redondo  Multilayer Feedfonvard Weight Initialization  In:. Joint ConJ on Neural Networks, Washington DC, July 15-19 2001, vol. 1, pp. 166170 1131 International Standards and Recommended Practices: Aeronautical Telecommunications, Annex IO to L e  Convention of International Civil Aviation, vol. IV \(Surveillance Radar and Collision Avoidance Syxtems ICA0,pp. 185-186 97 pre></body></html 


children \(and S-extension children gether. During the candidate counting, we frequently need to locate the items in the customer sequences along these links Obviously, making the tree smaller or reducing the number of search operations can enhance the counting process In MSPS, by performing supersequence frequency based pruning in Phase 3, only a part of the candidate set needs to be processed. Thus, our pre?x tree is usually much smaller than PSP  s pre?x tree at each pass. Moreover we also reduce the search operations by trimming the customer sequences. In PSP, the items not in the pre?x tree are not trimmed from the customer sequence. Thus, when these items are processed, they are searched along the corresponding links exhaustively, even though they are not in those links. This unnecessary search cost is not trivial when the number of customer sequences is large. MSPS can avoid this problem. As the mining process makes progress, fewer and fewer items would remain in the longer candidate patterns, and the customer sequence trimming can save a lot of time 4.4 Analysis of the Sampling Here, we discuss some important issues in sampling. For both frequent itemset mining and sequence mining, if a pattern is found frequent in db but turns out to be infrequent in DB, it is an overestimate. On the other hand, if a pattern is infrequent in db but actually frequent in DB, it is a miss Both our research and [9] try to mine the exact result with the help of sampling. While we focused on how to maximize the performance improvement, more attention was given in [9] on how to reduce the probability of misses To achieve that goal, two methods were suggested in [9]: 1 mine a large sample, and 2 sup for mining the sample. These two methods can reduce the misses but also potentially degrade the overall performance. Mining a large sample cuts the merit of sampling while lowering the user-speci?ed minsup may generate a large number of overestimates. Obviously, a complete sample result without misses does not necessarily mean the best overall performance. In MSPS, the cost related to sampling includes all the overhead of mining the sample and verifying the sample results, whereas the performance gain is from the supersequence frequency based pruning. The effectiveness of this pruning is determined by how many long frequent patterns can be found from the sample. As different settings of sample size and the adjusted minsup for mining the sample are used, the overall performance varies accordingly. Thus, we pay our attention to the sample size and the adjusted minsup in the following discussion 4.4.1 Sample Size In [9, 10], the minimum sample size that guarantees a small chance of misses with certain con?dence is given by the Chernoff boundary. Unfortunately, this theoretic guideline is not quite practical because it is too conservative. In MSPS, a large sample can improve the quality of sample results with fewer misses and overestimates. Consequently verifying the sample result can be done quickly and the supersequence frequency based pruning can be very effective But the overhead of mining a large sample is high. On the other hand, with a small sample, the overhead of mining sample is low, but MFSdb may be in bad quality. Then, the cost of verifying the sample result containing many overestimates would be high. If the small sample size makes the minimum support count for mining the sample \(i.e minsup ? |db| or lowered minsup ? |db ing the sample itself may take a long time. Thus, a small sample does not necessarily mean a lower cost. Furthermore, if only few long frequent sequences are found under the border formed by MFSdb, then the supersequence frequency based pruning will not be effective, either. That  s why the sample should not be too large or too small Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  


Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE In general, we do not know the distribution characteristic of the database to be mined, so it is hard to determine the best sample size. MSPS allows users to choose a plausible sample size empirically. In our experiments, we set the sample size as 10% of the original database size. By using a default sample size, how to balance the cost related to the sampling and the quality of sample result mainly depends on the adjusted minsup for mining the sample. Even though the default sample size may not be the best one all the time with the method of adjusting the minsup, it works very well in practice according to our extensive experiments 4.4.2 Adjusting the User-speci?ed Minimum Support for Mining the Sample In the sample mining result, a certain rate of misses is tolerable. Our tests show that, for a missing k-sequence, if most of its long subsequences, such as subsequences with length k ? 1 or k ? 2, are found, then the supersequence frequency pruning is not affected much. In practice, as long as the sample size is not too small, the probability that most of these subsequences are also missed is quite low. Compared to misses, overestimates could be a bigger problem Once an infrequent k-sequence is identi?ed frequent in db at pass k, then it may be joined with many other k-sequences to generate a large number of false candidates in mining the sample. Most importantly, the situation may become even worse when the minimum support count for mining the sample is very small. We found this is more serious for sequence mining than for frequent itemset mining, because the search space is much bigger. For MSPS, it not only degrades the ef?ciency of mining the sample, but also causes a high cost to identify the overestimates In [9], they proposed using the lowered minsup for the sample, however they did not consider the case that the userspeci?ed minsup is very small. In that case, it is dangerous to lower the minsup further. In this research, we investigated how to avoid the overestimates in the case of small user-speci?ed minsup, because such mining task is more time-consuming There are three different cases that can happen when MSPS is used: 1 ply using it or even a lowered one to mine the sample works ne. Only a small number of misses and overestimates occur in our tests. This is usually safe because our default sample size is not very small. 2 sup is small, the sampling technique is challenged. Using a lowered minsup or even the original user-speci?ed minsup for mining the sample often causes many overestimates because lowered minsup?|db| or minsup?|db| is too small Even though increasing the sample size could be a solution for this case, it limits the merit of sampling. Thus, we consider increasing the minsup a little to mine the sample, hoping it will limit the overestimates to a reasonable level. In that case, more misses may occur. However, even though there is a missing pattern, as long as most of its long subsequences are still contained in the sample result, the supersequence frequency based pruning is not affected much. 3 In some rare cases, the user-speci?ed minsup is extremely small. Then, just increasing the minsup for mining the sample cannot solve the problem. We must consider increasing the sample size too. Actually, both 2 the same technical question: when user-speci?ed minsup is small, how to increase the minsup for mining the sample of a certain size? We must keep in mind that if the increase in the minsup for mining the sample is not enough, the problem of overestimates cannot be solved. On the other hand, if it is increased too much, we may not ?nd any long patterns from the sample Consider the original database DB and an arbitrary se 


quence X . If the support of X in DB is PX , then the probability that a customer sequence randomly selected from DB contains X is also PX . Let  s consider a random sample S with m customer sequences that are independently drawn from DB with replacement. The random variable TX which represents the total number of customer sequences containing X in S, has a binomial distribution of m trials with the probability of success PX . In general, if m is greater than 30, TX can be approximated by a normal distribution whose mean is m ? PX and the standard deviation is  m ? PX ? \(1? PX In MSPS, suppose that we draw a sample S with m customer sequences from DB, and then try to use the point estimator P ?X = TX/m to estimate the support of X in the population of DB. Then, P ?X is an unbiased estimator with mean m ? PX/m = PX and standard deviation m ? PX ? \(1? PX  PX ? \(1 ? PX If we assume that the support of X in DB, PX , is the user-speci?ed minsup that we want to estimate, then P ?X which is observed from a sample S, should be around PX with a normal distribution as described above. If the adjusted minsup is denoted as P ??X , we can assume P ??X &gt; PX because our goal is to ?nd out how much we should increase the minsup for mining the sample. If we use P ??X to mine the sample, the probability that the sequence X can be found as a local frequent sequence in db is 1?PZ , where Z P ??X ? PX  PX ? \(1? PX the z-score Let  s consider the standard deviation of P ?X PX ? \(1? PX 1? PX PX ? 1/2 0, 1/2]. Since minsup is usually smaller than 50%, we can assume the value of PX ? \(1? PX interval of [0,minsup]; that means, the standard deviation of P ?X is increasing in this PX interval. If the support of another sequence Y in DB is lower than the minsup PX Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE then both the mean and the standard deviation of observed P ?Y should be smaller than those of P ?X , respectively Therefore, compared with P ?X , the distribution curve of P ?Y is shifted left and is shaper. Thus, if we set the adjusted minsup for mining the sample as P ??X , the probability that an infrequent sequence Y is identi?ed as frequent in S should be lower than 1 ? PZ . This guarantees the probability that any infrequent sequence is identi?ed as frequent in the sample is lower than 1 ? PZ . In our experiments the critical value of Z is set to 1.28, where PZ = 0.90 such that the probability of the overestimate is at most 10%. From Z = \(P ??X ? PX  PX ? \(1? PX can drive P ??X = PX + Z  PX\(1? PX PX = minsup and m = |db This formula provides a theoretic guideline for adjusting the user-speci?ed minsup to mine the sample. Even though this adjusted minsup value may not be the best one all the time, it worked well in most of our experiments 5 Performance Analysis To compare MSPS with other algorithms, we implemented GSP and obtained the source codes of SPAM and 


SPADE from their authors  web sites. All the experiments were performed on a SuSE Linux PC with a 2.6 GHz Pentium processor and 1 Gbytes main memory MSPS was compared with others on various databases and we evaluated the scalability of these algorithms in terms of the number of items and the number of customer sequences. We also investigated how the sample size and the adjusted minsup for mining the sample affect the performance of MSPS. Since the sampling technique is probabilistic, we ran MSPS 100 times for each test. The average execution time of the 100 runs was reported as the performance result. The default sample size was ?xed as 10% of the test database for all experiments. The databases used in our experiments are synthetically generated as in [2]. The database generation parameters are described in Table 1 For all databases, NS = 5000 and NI = 25, 000; and the names of these databases re?ect other parameter values used to generate them 5.1 Performance Comparison We ran MSPS, GSP, SPADE and SPAM on three databases with medium sizes of about 100 Mbytes. The number of items in these databases is 10,000. In our tests SPAM could not mine these databases, and its run was terminated by the operating system. Our machine is a 32-bit system, but the user address space is limited to 2 Gbytes. In all these tests, SPAM always required more than 2 Gbytes memory, hence caused the termination Table 1. Parameters Used in Database Generation D Number of customers in the database C Average number of transactions per customer T Average number of items per transaction S Average length of maximal potentially frequent sequences I Average length of maximal potentially frequent itemsets N Number of distinct items in the database NS Number of maximal potentially frequent sequences NI Number of maximal potentially frequent itemsets As discussed before, when the user-speci?ed minsup is small, simply using it or a lowered one to mine the sample may cost too much due to so many overestimates. In practice, we may not know the data distribution characteristics of the database to be mined. Thus, we conservatively assumed that all user-speci?ed minsups in our tests are small and simply increased them a little bit for mining the sample. The adjusted minsup for each test is computed using the formula given before. The probability that an overestimate occurs is set to 10% at most, i.e., Z = 1.28 The test results are shown in Figure 2. With the optimization components integrated, MSPS performs much better than GSP because it processes fewer candidates in a much more ef?cient way. The advantage of SPADE is the ef?cient counting of the candidates by intersecting the idlists. However, when mining a medium size database with 400,000 customers, the counting for LDB2 in SPADE is inef?cient and degrades the overall performance very much Considering both factors, we can say that if there are not enough number of candidates to be counted, SPADE cannot show its ef?ciency. That is why SPADE is even worse than GSP when the minsup is big, as shown in some of the gures When the minsup is decreased, more and more candidates appear during the mining. In that case, the overhead of GSP in candidate generation, pruning, and especially the counting using a huge hash tree increases drastically For MSPS, this situation is considerably improved by using the supersequence frequency based pruning, the pre?x tree structure, and the customer sequence trimming. When many passes are required for the mining, most candidates usually appear after pass 2, hence MSPS can outperform GSP further when the minsup is decreased. This improvement also makes MSPS better than SPADE in most tests on 


ment also makes MSPS better than SPADE in most tests on the medium size databases. Only when the minsup is very small, SPADE can beat MSPS 5.2 Scalability Evaluation Both SPADE and SPAM need to store a huge amount of intermediate data to save their computation cost. When Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 0200 400 600 800 1000 1200 1400 0.3 0.2 0.1 0.08 0.06 Minimum Support Ex ecu tion Ti me se c GSP SPADE MSPS a 0 500 1000 1500 2000 2500 3000 0.3 0.25 0.2 0.15 0.12 Minimum Support Ex ecu tion Ti me se c GSP SPADE MSPS b 0 1000 2000 3000 4000 5000 0.3 0.25 0.2 0.15 0.12 Minimum Support Ex ecu tion Ti me se c GSP SPADE MSPS c Figure 2. Performance Comparison on Medium Size Databases the memory space requirement is over the memory size available, CPU utilization drops quickly due to the frequent 


available, CPU utilization drops quickly due to the frequent swapping. Compared with them, MSPS and GSP process the customer sequences one by one, hence only a small memory space is needed to buffer the customer sequences being processed. MSPS can also handle the situation that LDBk or C DB k can not be totally loaded into memory by using the signatures as explained in Section 4. Therefore MSPS does not require the memory space as much as GSP SPADE and SPAM Many real-life customer market-basket databases have tens of thousands of items and millions of customers, so we evaluated the scalability of the mining algorithms in these two aspects. First, we started with a very small database D1K-C10-T5-S10-I2.5 and changed the number of items from 500 to 10,000. The user-speci?ed minsup was 0.5 To run MSPS on such a small database with only 1000 customers, we selected the whole database as the sample and keep the user-speci?ed minsup unchanged to mine it Since MSPS does not apply the sampling on such a small database, supersequence frequency based pruning is not performed in mining. Thus, in this case, SPADE and SPAM performed better than MSPS and GSP as long as their memory requirement is satis?ed As the number of items is increased, SPAM shows its scalability problem. Theoretically, the memory space required to store the whole database into bitmaps in SPAM is D ? C ? N/8 bytes. For the id-lists in SPADE, it is about D ?C ? T ? 4 bytes. But we found these values are usually far less from their peak memory space requirement during the mining, because the amount of intermediate data in both algorithms is quite huge Compared with SPAM, SPADE divides search space into small pieces so that only the id-lists being processed need to be loaded into memory. Another advantage of SPADE is that the id-lists become shorter and shorter with the progress in mining, whereas the length of the bitmaps does not change in SPAM. These two differences make SPADE much more space-ef?cient than SPAM Second, we investigated how they perform on C10-T5S10-I2.5-N10K when the user-speci?ed minsup is 0.18 We ?xed the number of items as 10,000 and increased the number of customers from 400,000 to 2,000,000. SPAM cannot perform the mining due to the memory problem For SPADE, we partitioned the test database into multiple chunks for better performance when its size was increased Otherwise, the counting of CDB2 for a large database could be extremely time-consuming. We made each chunk contain 400,000 customers so that it is only about 100 Mbytes which is one tenth of our main memory size. Figure 3 shows that the scalability of MSPS and GSP are quite linear. As the database size is increased, MSPS performs much better than the others When database is relatively small with only 400,000 customers, SPADE performed the best, about 20% faster than MSPS. But SPADE cannot maintain a reasonable scalability as the database becomes larger, and MSPS starts outperforming SPADE. When the database size is increased from 1600K customers to 2000K customers, there is a sharp performance drop in SPADE, such that it is even slower than GSP. In that case, MSPS is faster than SPADE by a factor of about 8. As discussed before, counting CDB2 is a performance bottleneck for SPADE, because the transformation of a large database from the vertical format to the horizontal format takes too much time. When the database is very large, the transformation also requires a large amount of memory and frequent swapping, hence the performance drops drastically. Partitioning the database can relieve this problem to some extent but does not solve it completely. In addition, for the database with a large number of items and customers, SPADE needs more time to intersect more and 


customers, SPADE needs more time to intersect more and longer id-lists Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE 100 1000 10000 100000 400 800 1200 1600 2000 Number of Customers \('000s Ex ecu tion Ti me se c GSP SPADE MSPS Figure 3. Scalability: Number of Customers on C10-T5-S10-I2.5-N10K, minsup=0.18 Finally, we mined a large database D2000K-C10-T5S10-I2.5-N10K, which takes about 500 Mbytes, for various minsups. This database is partitioned into 5 chunks for SPADE, and the results are shown in Figure 4 Based on our tests, we found SPADE performs best for small size databases. For medium size databases, MSPS performs better for relatively big minsups while SPADE is faster for small minsups. When database is large SPADE  s performance drops drastically and MSPS outperforms SPADE very much. If the user-speci?ed minsup is big and there are very few long patterns, GSP may perform as well as, or even better than, others due to its simplicity and effective subsequence infrequency based pruning 100 1000 10000 100000 0.33 0.3 0.25 0.2 0.18 Minimum Support Ex ec uti on Ti me se c GSP SPADE MSPS Figure 4. Performance on a Large Database D2000K-C10-T5-S10-I2.5-N10K 6 Conclusions In this paper, we proposed a new algorithm MSPS for mining maximal frequent sequences using sampling. MSPS combined the subsequence infrequency based pruning and the supersequence frequency based pruning together to reduce the search space. In MSPS, a sampling technique is used to identify potential long frequent patterns early. When the user-speci?ed minsup is small, we proposed how to adjust it to a little bigger value for mining the sample to avoid many overestimates. This method makes the sampling technique more ef?cient in practice for sequence mining. Both the supersequence frequency based pruning and the customer sequence trimming used in MSPS improve the candidate counting process on the new pre?x tree structure developed. Our extensive experiments proved that MSPS is a practical and ef?cient algorithm. Its excellent scalability 


makes it a very good candidate for mining customer marketbasket databases which usually have tens of thousands of items and millions of customer sequences References 1] R. Agrawal and R. Srikant  Fast Algorithms for Mining Association Rules  Proc. of the 20th VLDB Conf., 1994, pp 487  499 2] R. Agrawal and R. Srikant  Mining Sequential Patterns  Proc. of Int  l Conf. on Data Engineering, 1995, pp. 3  14 3] J. Ayres, J. Gehrke, T. Yiu, and J. Flannick  Sequential Pattern Mining Using a Bitmap Representation  Proc. of ACM SIGKDD Conf. on Knowledge Discovery and Data Mining 2002, pp. 429  435 4] B. Chen, P. Haas, and P. Scheuermann  A New TwoPhase Sampling Based Algorithm for Discovering Association Rules  Proc. of ACM SIGKDD Conf. on Knowledge Discovery and Data Mining, 2002, pp. 462  468 5] S. M. Chung and C. Luo  Ef?cient Mining of Maximal Frequent Itemsets from Databases on a Cluster of Workstations  to appear in IEEE Transactions on Parallel and Distributed Systems 6] F. Masseglia, F. Cathala, and P. Poncelet  The PSP Approach for Mining Sequential Patterns  Proc. of European Symp. on Principle of Data Mining and Knowledge Discovery, 1998, pp. 176  184 7] J. Pei, J. Han, B. Mortazavi-Asl, H. Pinto, Q. Chen, U Dayal, and M. C. Hsu  Pre?xSpan: Mining Sequential Patterns Ef?ciently by Pre?x-Projected Pattern Growth  Proc of Int  l. Conf. on Data Engineering, 2001, pp. 215  224 8] R. Srikant and R. Agrawal  Mining Sequential Patterns Generalizations and Performance Improvements  Proc. of the 5th Int  l Conf. on Extending Database Technology, 1996 pp. 3  17 9] H. Toivonen  Sampling Large Databases for Association Rules  Proc. of the 22nd VLDB Conf., 1996, pp. 134  145 10] M. J. Zaki, S. Parthasarathy, W. Li, and M. Ogihara  Evaluation of Sampling for Data Mining of Association Rules  Proc. of the 7th Int  l Workshop on Research Issues in Data Engineering, 1997 11] M. J. Zaki  SPADE: An Ef?cient Algorithm for Mining Frequent Sequences  Machine Learning, 42\(1  60 Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence  ICTAI 2004 1082-3409/04 $20.00  2004 IEEE pre></body></html 


NIMBUS NOAA 8 NOAA 15 NPOESS Preparatory Project Orbital Maneuvering Vehicle Orbcomm Orbiting Carbon Observatory Orbiting Solar Observatory-8 Orbview 2 Orion 1 P78 Pas 4 Phoenix Pioneer P-30 Pioneer Venus Bus/Orbiter Small Probe and Large Probe Pioneer-I 0 Polar QuikSCAT Radarsat Reuven High Energy Solar Spectroscopic Imager REX-II Rosetta Instruments Sage III SAMPEX Satcom C3 Satcom C4 SBS 5 SCATHA Seastar SMART-1 SNOE Solar Dynamics Observatory Solar Maximum Mission Solar Mesosphere Explorer Solar Radiation and Climate Experiment Solar Terrestrial Relations Observatory Space Interferometry Mission Space Test Program Small Secondary Satellite 3 Spitzer Space Telescope SPOT 5A Stardust  Sample Return Capsule STEPO STEPI STEP3 STEP4 STRV ID Submillimeter Wave Astronomy Satellite Surfsat Surveyor Swift Gamma Ray Burst Exporer Synchronous Meterological Satellite-I TACSAT TACSAT 2 TDRS F7 Tellura Terra Terriers Tethered Satellite System Thermosphere Ionosphere Mesosphere Energetics and Dynamics TIMED TIROS-M TIROS-N TOMS-EP Total Ozone Mapping Spectrometer TOPEX TRACE Triana Tropical Rain Measuring Mission TSX-5 UFO I UFO 4 UFO 9 Ulysses Upper Atmosphere Research Satellite Vegetation Canopy Lidar VELA-IV Viking Viking Lander Viking Orbiter Voyager Wilkinson Microwave Anisotropy Probe WIND WIRE XMM X-Ray Timing Explorer XTE XSS-iO XSS-ii APPENDIX B FIELDS COLLECTED Mission Mission Scenario Mission Type Launch Year Launch Vehicle Bus Model Bus Config Bus Diameter Location Expected Life Flight Profile Flight Focus Technical Orbit Currency Total Cost Including Launch Unit Sat Costs Average Sat Cost Production Costs Launch Costs Operations Cost Orbit Weight Wet Weight Dry Weight Max Power EOL Power BOL Power  of Batts 17 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


