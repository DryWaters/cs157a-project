Ensuring Cost Efficient and Secure Software through Student Case Studies in Risk and Requirements Prioritization   Nancy R. Mead Software Engineering Institute nrm@sei.cmu.edu Dan Shoemaker University of Detroit Mercy dshoemaker1@twmi.rr.com Jeffrey Ingalsbe Ford Motor Company jingalsb@ford.com   Abstract  This paper presents a discussion of educational case studies used in security requirements assessment 
and requirements prioritization. Related to this, it introduces risk understanding as an added dimension to the requirements prioritization process. It should be self-evident that the final product should incorporate the requirements with the greatest value Nevertheless, in a time when security is a preeminent concern it should also be cl ear that risk elements should also be considered. As such, activities to reconcile risk with value are always essential However, since risk and value considerations are different, and sometimes opposed to each other, this paper presents a new process that will help decision makers reconcile these two factors within a single 
approach. This new process may also be incorporated into security requirements education and prioritization   1. Identifying the right requirements  It is well documented that requirements engineering saves money. That is because, according to a number of studies [3, 5, 12, 17, 18 to n a m e a few\ requirements concerns are the primary reason for projects to be significantly over budget, past schedule, or cancelled. Other studies have shown that 
reworking requirements defects on most software development projects costs 40 to 50 percent of total project effort, and the percentage of defects originating during requirements engineering is estimated at more than 50 percent [18, 20  So it is important to build a sound requirements set. However, besides the classic cost and feasibility issues, it is also critical to understand the potential risks associated with implementing each requirement That is because of the absolute need to ensure that the 
software that underpins our infrastructure is secure Yet up to this point, it is estimated that the exploitation of unsecured software costs the U.S economy an average of $60 billion dollars per year  portan tl y perh a p s  th e ex ploitatio n of a  flaw in the software that underlies basic services like power and communication could cause a significant disaster [7    Notwithstanding the national security concerns there are also sound business reasons for developing secure software. An earlier study found that the 
return on investment when security analysis and secure engineering practices are introduced early in the development cycle ranges from 12 to 21 percent with the highest rate of return occurring when the analysis is performed during application design Thus, the costs of poor security requirements argue that even a small improvement in this area would provide a high value  2. The special case of security requirements  The concern comes from the fact that security requirements are typically developed independent of 
functional requirements in the requirements engineering process. Much of requirements engineering research and practice has addressed the capabilities that the system will provide. So, from the user s perspective, a lot of attention is paid to the functionality of the system. But little attention is given to what the system should not do As a result, security requirements that are specific to the system and that provide for protection of essential services and assets are either not 
implemented, or are strapped on in a piecemeal and disaggregate fashion at the end of the process. Worse requirements that might add functional value but which represent added risk are never considered from that perspective. For instance, in the requirements prioritization process, ease of use is typically assigned a much higher priority, or is often traded off Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 1 978-0-7695-3450-3/09 $25.00 © 2009 IEEE 
 


against, rigorous access control. This means that any adversary can more easily attack the target system Therefore, in addition to employing applicable software engineering methods, the organization also has to understand how risk considerations impact its development processes. This is essential because if risk considerations are not a formal part of the requirements elicitation and prioritization process then security tends to be neglected during the build That reason alone should justify the need to embed security considerations into the initial requirements elicitation and prioritization process  3. Understanding requirements by understanding threats  A proper understanding of the threat picture allows the development team to do a better job of prioritizing requirements. Logically, the place to start when attempting to assess the security status of a given requirement is to understand all threats associated with it. That is because, without an adequate practical understanding of every conceivable way that a given requirement might be compromised, it is difficult to make a decision about its priority in the eventual product Threat understanding, is the role of threat modeling. Most threat-modeling methodologies share common themes. They emphasize systematic repeatable processes with work products that enhance communication and highlight risk in a clear, easy to understand way for all stakeholders. They differ on several planes \(scope, perspective, formality, and automation\he most popular threat modeling technique comes from Microsoft. It focuses on data flow and trust boundaries  T h e T r ik e methodology m ph asi zes an application s perspective while encouraging automation. Myagmar  o cu s e s on an attac k er s perspective but heavily emphasizes the role threat modeling plays in developing security requirements Threat models identify threats, but risk analysis is needed in order to understand the practical risk that each threat represents. At a minimum, risk analysis involves assessing the likelihood and impacts of each identified threat. That is because, without an adequate understanding of these two factors, it is hard to make a realistic decision about the consequences of any identified threat. An adequate risk analysis will ensure that all consequences of threat are factored into the requirements prioritization process  4. Assessing security risks through educational case studies  There are many techniques for assessing software risk o w e v e r, n o t all of th e m are us efu l  f o r assessing security risks. In previous security requirements engineering student team case studies 42 n u m b er of ri sk ass e s s m e nt t e chn i qu e s  w e re identified as potentially useful for identifying security risks after completing a literature review This review by student teams examined the usefulness and applicability of eight risk assessment techniques 1  General Accounting Office Model [3  2  National Institute of Standards and Technology NIST\ Model  3  NSA s INFOSEC Assessment Methodology   4  Shawn Butler s Security Attribute Evaluation Meth  5  Carnegie Mellon s Vendor Risk Assessment and Threat Evaluation 6  Yacov Haimes s Risk Filtering, Ranking, and Management Model [36  7  Carnegie Mellon s Survivable Systems Analysis Meth  8  Martin Feather s Defect Detection and Prevention  Each method was ranked in four categories 1  Suitability for small companies 2  Feasibility of completion in the time allotted 3  Lack of dependence on historical threat data 4  Suitability in addressing requirements The results of the ranking are shown in Table 1 After averaging scores from the four categories NIST s and Haimes s models were selected as useful techniques for security risk assessment Brainstorming, attack tree, and misuse case documentation were used to identify potential threat scenarios. The two independent risk assessment analyses produced a useful risk profile for the educational case study using quantitative methods In this particular case study, the student team also identified a set of essential services and assets. This activity is not part of the risk techniques used, but nevertheless can be a beneficial exercise if enough architectural information already exists to support it All findings from the risk assessment, along with the findings from the essential services and asset identification process, were used to help determine the priority level associated with each of the requirements  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 2 
 


5. Aligning and prioritizing value and risk: a prototype approach  AHP is a popular method for supporting decision making where multiple objectives are present [28, 15  T h is m e th od us es a pair-wise comparison matrix to calculate the relative value and costs of security requirements. By using AHP, the requirements engineer can also confirm the consistency of the result. There are five steps in the AHP method 1  Review candidate requirements for completeness 2  Apply a pair-wise comparison method to assess the relative value of requirements 3  Estimate the relative cost of implementing each candidate requirement 4  Calculate each requirement s relative value and implementation cost, and plot each on a costvalue diagram 5  Use the cost-value diagram as a map for analyzing the candidate requirement  In prior student team case studies, stakeholders implemented the pair-wise comparison method of AH T h e s t ak e h olders  w e re prov ided brief  instructions for using the AHP methodology. Because there were 9 security requirements, the method should produce a matrix with 81 \(9 x 9 However, the participants needed to fill the upper half of the matrix only, and each requirement had a value of 1 when compared to itself. Consequently each participant had to respond to 36 cells. \(In general, a matrix with n requirements must have responses in n * \(n-1\s. This rule reduced the participants workload in this step.\volvement of a number of stakeholders in this process leads to a structured discussion of the results and helps to build consensus on the validity of the cost/value assessments. The requirements engineer highlighted the cells that required feedback from the stakeholders. A sample of the feedback is shown in Table 2  Table 1. Ranking of assessment techniques Methodologies  Suitable for Small Companies Feasible to Complete within Time Frame Does Not Require Additional Data Collection Suitable for Requirements Average Score GAO 2 4 2 2 2.50 NIST 2 2 1 1 1.50 NSA/IAM 3 3 2 2 2.50 SAEM 4 4 4 4 4.00 V-Rate 3 4 4 4 3.75 Haimes 2 2 2 2 2.00 SSA 2 2 2 4 2.50 DDP/Feather 3 4 2 4 3.25  Table 2 Prioritization Feedback of Acme  A B C D E F G H I J 1  SR-1 SR-2 SR-3 SR-4 SR-5 SR-6 SR-7 SR-8 SR-9 2 SR-1 1 8 1/5 3 1 2 2 3 1 3 SR-2 1/8 1 1/5 1/7 1/7 1/7 1/7 1/9 1/9 4 SR-3 5 5 1 1 2 1 3 1 1 5 SR-4 1/3 7 1 1 1/2 1/2 3 1/2 1 6 SR-5 1 7 1/2 2 1 3 3 1 1/3 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 3 
 


7 SR-6 1/2 7 1 2 1/3 1 1/3 1 1 8 SR-7 1/2 7 1/3 1/3 1/3 3 1 3 2 9 SR-8 1/3 9 1 2 1 1 1/3 1 1/6 10 SR-9 1 9 1 1 3 1 1/2 6 1  AHP uses a pair-wise comparison matrix to determine the relative value and cost between security requirements. An arbitrary entry in row i and column j of the matrix, labeled a ij indicates how much higher \(or lower\e value/cost for requirement i is than that for requirement j. The value/cost is measured on an integer scale from 1 to 9 To illustrate this, consider the nine security requirements in our case study \(the requirements  final value is shown in Figure 1, and the requirements final cost is shown in Figure 2\he value of each requirement is relative. That is, if the value of a requirement is 20%, this requirement is twice as important as the 10% value of another requirement. The sum of the scores of the requirements should always be 100 According to Figure 1, the three most valuable requirements are SR-3, SR-5, and SR-6. Together they constitute 47% of the requirements total value The three least valuable requirements are SR-1, SR-4 and SR-8, which constitute 23% of the requirements  total value. Figure 2 shows that requirements SR-4 SR-7, and SR-9 are the three most expensive Together, they constitute 72% of the requirements  total cost. The three least expensive requirements are SR-1, SR-2, and SR-3 that constitute 7% of the requirements total cost The next step calculates the cost-value ratios for each requirement. The aim is to pinpoint the requirements that are most valuable and least expensive to implement based on   high value-to-cost ratio of requirement \(larger than 2.0   medium value-to-cost ratio of requirement between 2.0 and 0.5   low value-to-cost ratio of requirement \(less than 0.5 Using this, SR-1, SR-2, SR-3, SR-5, and SR-6 are identified as high priority and SR-4 and SR-7 are identified as low priority   0 2 4 6 8 10 12 14 16 18 Added Value SR-1 SR-2 SR-3 SR-4 SR-5 SR-6 SR-7 SR-8 SR-9 Requirement Identifier  Figure 1. Value distribution of requirements  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 4 
 


  0 5 10 15 20 25 30 35 Added Cost SR-1 SR-2 SR-3 SR-4 SR-5 SR-6 SR-7 SR-8 SR-9 Requirement Identifier  Figure 2. Cost distribution of requirements   6. Adding threat assessment to the prioritization process  AHP provides a good quantitative basis for making a decision about the relative priority of a given set of requirements. However, it does not factor in the additional dimension of risk. In order to do that all risks associated with each requirement have to be identified and assessed for likelihood and impact. As noted before, there are many ways of modeling risk These days, threat modeling is also a popular alternative, which we illustrate in conjunction with AHP prioritization, by subjecting our sample set to a threat modeling exercise.  A sample risk assessment form along these lines can be found at http://www.securedigitalsolutions.com/articles/Gener ic%20Risk%20Assessment%20Form.pdf Each threat associated with each requirement is identified through a threat model. That might produce such hazards as subject to cross site scripting attacks or inadequate access controls Each of these threats is then ranked on two factors, likelihood and impact. The ranking is expressed on a seven level Likert scale ranging from highest-to-lowest. The result would produce two outcomes likelihood \(1-7 and impact \(1-7\ce these factors have equal weight, in order to normalize them these two scores could then be multiplied together in order to obtain a single risk factor score \(\(L\kelihood * \(I\mpact R\k\or each threat. Then all of the individual threat scores are multiplied to obtain a single threat index. The aim is to rank the requirement set by the relative level of threat associated with each requirement. In our example that might produce the ranking shown in Figure 4 These are then ranked as very high risk \(>200 high risk \(>100\oderate risk \(>50\ow risk \(>25 and no risk \(<25\. In this case, SR-7 is a very highrisk requirement. SR-6 and SR-4 are high-risk requirements. SR-1, SR-2, and SR-3 are moderate risk requirements and SR-5, SR-8 and SR-9 represent negligible risks Then referring to the results of the AHP ranking it is possible to think about these requirements in a different way. SR-6, which was identified as a high priority, is also a high risk. Whereas, SR-1, SR-2 and SR-3, which are high priority items, are shown to only represent a moderate risk. Better still SR-5 which is a high priority requirement, is a low risk And to make a further case against SR-4 and SR-7 which were identified as low priority requirements in the AHP analysis, these two requirements also represent the two greatest risks in the requirements set Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 5 
 


 Figure 3. Cost-value diagram of requirements   Comparison of Threat Levels 84 60 56 112 27 150 240 24 5 0 50 100 150 200 250 300 R1 R2 R3 R4 R5 R6 R7 R8 R9 Requ irement Value  Figure 4. Comparative threat levels per requirement  That outcome changes our picture somewhat, in the sense that decision makers might want to revisit or perhaps re-prioritize based on potential risk. This is particularly true in the case of SR-6. It might also suggest that the status of SR-1, which is approaching high-risk status, be revisited. At the same time, it is easy to justify the priority of SR-2, SR-3 and SR-5 based on their relative threat index. Finally, it is also easy to consider dropping SR-4 and SR-7 out of the set if resource constraints arise This modified approach has not yet been applied in our student case studies. One of the challenges we face is that students have an expectation for crisp results. They are not used to the idea that when they are doing research case studies, the results may be unpredictable. In our view, this makes these kinds of case studies more interesting and more challenging for the students, as well as helping our research  7. Conclusions  The aim of this article was to introduce the potential additional value of threat modeling/ risk understanding to the requirements prioritization process. The information that these additional analysis processes provide permits decision makers Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 
 


to undertake the construction process in such a way that the most critical functional and security requirements are always included in the final build Exposing students to these processes can help them understand the sophisticated decisions that must be made in real development settings Moreover, that information can then be used by decision makers to make intelligent decisions about the amount of investment that they wish to make in their software. That will allow them to maximize their investment and thereby ensure both their profitability the as well as the overall safety and security of their organization. That will allow every member of the organization to be confident that their software assets provide the greatest value for the investment as well as the greatest security possible From the educational perspective, if we introduce students to sophisticated decision making processes in the real world, they will be better equipped to make such decisions when the time comes  8. References   I A l exan d e r  Misuse cases: Use cases with hostile intent  IEEE Software vol. 20, no. 1, pp. 58-66, 2003   T  Ben zel   Integrating security requirements and software development standards in Proc. 12th National Computer Security Conf Fort Meade, MD: National Computer Security Center, 1989, pp. 435-458  3 B. B o e h m a nd V  Ba sili  Software defect reduction  Top 10 list  IEEE Computer vol. 34, no. 1, pp. 135-137 Jan. 2001  4 J B a i l e y  A  Dro m m i  J I n g al sb e N Mead  an d D  Shoemaker Models for Assessing the Cost and Value of Software Assurance  Build Security In  https://buildsecurityin.us-cert.gov/daisy/bsi/articles knowledge/business/684.html  5 J.J. Carr Requirements engineering and management The key to designing quality complex systems  The TQM Magazine vol. 12, no. 6, pp. 400-407, 2000  6 R.A Clark   Breakpoint New York: G.P Putnam and Sons, 2007  7 R.A  Cla r k a nd H  A  Sc hm idt  A national strategy to secure cyberspace The President s Critical Infrastructure Protection Board, Washington, DC, 2002   E  C o l b ert an d D Yu  Costing Secure Systems Workshop Report 21 st Annual Forum on CoCoMo and Software Cost Modeling, University of Southern California, Center for Software Engineering, 2006  9 R   de L a ndts h e e r a n d A  v a n L a m s w e e r de  Reasoning about confidentiality at requirements engineering time in Proc. 10th European Software Engineering Conf held jointly with 13th ACM SIGSOFT International Symposium on Foundations of Software Engineering. New York, NY ACM, 2005, pp. 41-49  10 K G a rg a nd V   V a rm a   Security: Bridging the academia-industry gap using a case study in XIII Asia Pacific Software Engineering Conf. Proc New York, NY IEEE Computer Society Press, 2006, pp. 485-492  11  P. G i org i ni, H  M oura t i d is a nd N  Za nn one   Modeling Security and Trust with Secure Tropos  Integrating Security and Software Engineering: Advances and Future Visions Hershey, PA: IGI Global, 2007, pp 160-189  1 H Hech t and M  Hech t  How reliable are requirements for reliable software  Software Tech News  vol. 3, no. 4, 2000. Retrieved May 31, 2007 from http://www.softwaretechnews.com  1  S  Hon i d e n  Y T a h a ra N Yo sh io ka K T a gu ch i and  H. Washizaki Top SE: Educating superarchitects who can apply software engineering tools to practical development in Japan in Proc. 29th Int. Conf. on Software Engineering ICSE'07 New York, NY: IEEE Computer Society, 2007 pp. 708-718  1  M  Ho w a rd an d D L e Bl an c  Writing Secure Code  with CD-ROM\rosoft Press, 2001  15 J Ka rlsson Software Requirements Prioritizing in Proc. Second Int. Conf. on Requirements Engineering ICRE'96 Colorado Springs, CO, April 15-18, 1996. Los Alamitos, CA: IEEE Computer Society, 1996, pp. 110 116  16 J  K a rls s on, a nd K  R y a n  A Cost-Value Approach for Prioritizing Requirements  IEEE Software vol. 14, no. 5 Sept./Oct. 1997, pp. 67-74  17  S. L a ue s e n a nd O   Vinte r   Preventing requirement defects: An experiment in process improvement  Requirements Engineering, Volume 6, Number 1 February, 2001, pp. 37-50  18 T  Mc G i bbo n A business case for software process improvement revised DoD Data Analysis Center for Software \(DACS\ashington, DC, 1999  1 N R  M ead   Requirements Engineering for Survivable Systems Software Engineering Institute, Carnegie Mellon University, Pittsburgh, PA, CMU/SEI-2003-TN-013, 2003 http://www.sei.cmu.edu/publications/documents/03.reports 03tn013.html  20 N.R  Me a d a nd T  R Ste h ne y  II Security quality requirements engineering \(SQUARE\ethodology paper presented at the meeting of the Software Engineering for Secure Systems \(SESS05\, ICSE 2005 Int. Workshop on Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 
 


Requirements for High Assurance Systems, St. Louis, MO 2005  21 N  R  Me a d a nd E  D  H o ug h Security requirements engineering for software systems: Case studies in support of software engineering education in Proc. 19th Conf Software Engineering Education and Training   Los Alamitos, CA: IEEE Computer Society Press, 2006, pp 149-158  2  S  M y ag mar A  J Lee an d W  Yu rcik  Threat Modeling as a Basis for Security Requirements  Symposium on Requirements Engineering for Information Security \(SREIS 2005 http://www.projects.ncassr.org/mu lticast/papers/sreis05.pdf  23 N a ti ona l I n f r a s truc ture A d v i s o ry Counc il \(N IA C National strategy to secure cyberspace U.S. Department of Homeland Security, Washington, DC, 2003  2 M   Newm an   Software errors cost U.S. economy 59.5 billion annually National Inst. of Standards and Technology \(NIST\, Gaithersburg, MD, 2002  2 B   P a l y ag ar  Measuring and influencing requirements engineering process quality in Proc. AWRE 04, 9th Australian Workshop on Requirements Engineering 2004 Retrieved May 31, 2007 from http://awre2004.cis.unisa.edu.au  2 B   P a l y ag ar  A framework for validating process improvements in requirements engineering, in RE 04-IEEE Joint International 2004, pp. 33-36  2  S  T  Red w i n e \(E d     Software assurance: A guide to the common body of knowledge to produce, acquire and sustain secure software, version 1.1 U.S. Department of Homeland Security, Washington, DC, 2006  2 T L S a a t y   The Analytic Hierarchy Process New York, NY: McGraw-Hill, 1980  29  P. Sa it ta B. L a rc om a nd M Eddi ng ton   Trike v.1 Methodology Document [Draft  2005  30 D  Sh oe m a k e r, N R. Me a d  A  D r o m m i J  Ba ile y  a nd J. Ingalsbe SWABOK s fit to common curricular standard in Proc. 20th Conf. on Software Engineering Education and Training Los Alamitos, CA: IEEE Computer Society Press, 2007  31 G  Si ndre a n d A  Opda h l  Eliciting security requirements by misuse cases in Proc. TOOLS Pacific 2000 Los Alamitos, CA: IEEE Computer Society Press 2000, pp. 120-130  32 K  So o H o o, A  W  Su db ury  a nd A  R. J a quit h  Tangible ROI through Secure Software Engineering  Secure Business Quarterly vol. 1, no. 2, Fourth Quarter 2001 http://www.sbq.com/sbq/rosi/sbq_rosi_software_engineerin g.pdf  3  F  S w id erski an d W  S n y d er  Threat Modeling  Microsoft Press, 2004  34 S B u tle r Security Attribute Evaluation Method: A Cost-Benefit Approach in Proc. 24th Int. Conf. on Software Engineering Orlando, FL, May 19-25, 2002. New York, NY: ACM Press, 2002, pp. 232-240  35  S.L  Co rn f o r d M  S Feath er an d K.A  Hick s DDP  A Tool for Life-Cycle Risk Management 2004 http://ddptool.jpl.nasa.gov/docs/f344d-slc.pdf  3 Y Y Haim es  Risk Modeling, Assessment, and Management 2nd ed. Hoboken, NJ: John Wiley and Sons Inc., 2004  37 H  F.L i ps o n N R Me a d a nd A  P  M oore  A RiskManagement Approach to the Design of Survivable COTSBased Systems 2001 http://www.cert.org/research/isw/isw2001/papers/Lipson29-08-a.pdf  38 U  S. G e ne ra l A c c ounti n g O f f i c e   Information Security Risk Assessment: Practices of Leading Organizations, A Supplement to GAO s May 1998 Executive Guide on Information Security Management  U.S. General Accounting Office, Washington, DC., 1999  3 G  S t o n e bu rn er A  G o gu en  an d A  F e ri n g a  Risk Management Guide for Information Technology Systems  National Institute of Standards and Technology Gaithersburg, MD, Special Publication 800-30, 2002 http://csrc.nist.gov/publications/nistpubs/800-30/sp80030.pdf  4 N R  M ead   Survivable Systems Analysis Method  2002. http://www.cert.org/archive/html/analysismethod.html  41 N a ti ona l Sec u r ity  A g enc y  INFOSEC Assessment Methodology 2004. http://www.iatrp.com/iam.cfm  42  N.R. Mead, E. Hough, and T. Stehney Security Quality Requirements Engineering \(SQUARE Methodology Software Engineering Institute, Carnegie Mellon University, Pittsburgh, PA, CMU/SEI-2005-TR009, ADA452453, 2005 http://www.sei.cmu.edu/publications/documents/05.reports 05tr009.html  43 J A lle n, S. Ba rn um R. Ellis on, G  Mc G r a w a nd N.R Mead Software Security Engineering: A Guide for Project Managers  Upper Saddle River, NJ: Addison-Wesley ISBN -13:978-0-321-50917-8, 2008  44 N.R  Mead   Requirements Prioritization Case Study Using AHP  Build Security In September 2006 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 
 


https://buildsecurityin.us-cert.gov/daisy/bsi/articles/bestpractices/requirements/534-BSI.html  45 B  W  Bo eh m   Software risk management Piscataway NJ: IEEE Press, 1989 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 
 


23.5 kHz, was used for all ImpactEnergy feature extraction 7  Figure 6. ABS Feature Results In this case, the fault was clearly identified as a spall located on the inner race of the bearing. Therefore, the inner race fault frequency would be expected to be a key piece of evidence for fault detection. Figure 7 provides the inner race fault feature trends obtained for the conventional and ImpactEnergy spectra  Figure 7. Effect of IE Processing on Detection Again, all feature values are normalized on a 0-1 scale While the conventional feature provides good indication of the progressed fault condition, the resulting detection threshold results in several cases of false alarms and missed detection. The feature value trend obtained after the signal processing process provides clear separation of the baseline incipient fault and progressed fault classes. As illustrated in the trend plot, thresholds can be created for this feature that provide fault detection without error as well as a fault severity assessment To further improve the performance of the inner race feature, feature fusion was used to combine the three best performing features into a single health index. Due to the small number of data samples available to refine the fused feature, a simple linear projection was created using principal component analysis \(PCA linear algebra reduces data dimensionality while retaining the most critical information. The resulting fused feature Figure 8 classes than is observed in the inner race feature alone  Figure 8. Fused Feature Trend Plot Finally, sensor fusion was attempted to further enhance the fault response of the inner race feature. The bearing test rig collects sensor data in three locations. Two of these measurements are taken in close proximity to the test specimen. The third accelerometer is located far from the test specimen in close proximity to another bearing. It is not expected that this distant accelerometer would provide useful information about subtle faults present in the test bearing. All features presented so far were derived from the radial sensor near the specimen. To demonstrate sensor fusion, three techniques: beamforming, principal component analysis and the SUMPLE algorithm were applied to the time domain data from the two closest accelerometers [8 The resulting feature trends obtained from data derived from each fusion technique are presented in Figure 9. The inner race feature extracted from the radial sensor only is provided for reference purposes   Figure 9. Inner Race Feature Sensor Fusion Results The results show that there was no observed benefit from sensor fusion using any of the three techniques. In this case the information gained from the axial sensor does not provide additional evidence of the fault condition. Note that the beamforming and SUMPLE results display the undesirable effect of a slight increase in feature variance These results demonstrate that the benefit of sensor fusion is 8 highly dependent on the quality of the information obtained from each individual sensor, and if not done carefully can actually decrease the usefulness of the extracted features 6. AN ARCHITECTURE FOR THE INTEGRATION OF DIAGNOSTICS AND PROGNOSTICS Figure 10 depicts an architecture for advanced detection and prognosis. In this architecture, sensor measurements and operational parameters are input in real time. Data is preprocessed to reduce the effect of noise, before computing condition indicators or features indicative of a component  s 


condition indicators or features indicative of a component  s health Using the features and a model describing the component  s degrading state, fault detection and failure prognostic algorithms based on particle filtering are applied [7 Statistical analysis is implemented to evaluate the probability of a fault being present. When the fault is detected with a given confidence level, the prognostic algorithm is activated to predict the remaining useful life RUL only a convenient compromise between data-driven and model-based techniques, but also the means to evaluate performance with statistical indices. Moreover, the particle filtering based algorithm provides a means to deal efficiently with nonlinear and non-Gaussian noise. The nonlinear dynamic state model described by \(1     1 ,1 2 ,2 2 1 ,2 1   1  1       1 0 , if 1 0 0 1  0 1 , else 0 0 0 d d b d d c c d c T T T b T d d c x t x t f n t x t x t x t x t x t t y t x t v t x x f x x x x          1 where fb is a non-linear mapping, xd,1 and xd,2 are Boolean states that indicate normal and faulty conditions respectively. The paramter xc is the continuous-valued state that represents the fault dimension, w\(t t mean Gaussian noise signals, n\(t identically distributed \(i.i.d time-varying model parameter that describes the propagation of the fault dimension under a fatigue stress dependent on the loading profile that is being applied to the specimen under consideration This approach provides a recursively updated estimate of the probability for each fault condition considered in the analysis. These estimated probabilities may activate alarm 


analysis. These estimated probabilities may activate alarm indicators if they exceed appropriate thresholds for the probability of detection \(typically 90% or 95 particularly useful approach when the normal operation of a system is defined through a dynamic state-space model. It is  Sensor Data 0 5 1 1 5 2 Preprocessing Fault Detection 50 100 150 200 250 3 3.5 4 4.5 PF Detection Routine: GAG =246 50 100 150 200 250 0 0.5 1 Probability of Failure 2.5 3 3.5 4 4.5 0 2 4 x 10-3 Type I Error = 5%. Type II Error =1.9577 Fisher Discriminant Ratio =12.1834 Operating conditions and inputs Diagnostic Model  1   1    Features    d b d c t d c t d c x t f x t n t x t f x t x t t t h x t x t v t     Features &amp performance Feature Extraction 0 1 00 2 00 30 0 400 500 6 00 70 0 80 0 9 00 100  In terp o la t io n  o f fe a tu re  va lue  w ith  no is e In terp o la t io n  o f fe a tu re  vla ue s nap s h ot  w ith  gro un d t ru th  da ta Prognostic Model 1  Failure Prognosis Figure 10. Proposed architecture for the integration of diagnostics and prognostics 9 also important to note that this diagnostic framework allows for estimation of each of the system  s continuous-valued states as a probability density function \(pdf computed at the moment of fault detection, and provided as initial conditions to the prognostic routines, thus giving a suitable insight to the inherent uncertainty in the prediction problem When a fault is detected, the prognostic algorithm is activated. Prognosis may be understood as the result of the procedure where long-term \(multi-step  


procedure where long-term \(multi-step  describing the evolution in time of a fault indicator  are generated with the purpose of estimating the remaining useful life \(RUL approaches related to prognosis may be found in the literature. Few of them, however, offer appropriate tools for real-time estimation of the RUL as a continuous function of time A two-level procedure has been developed to address the failure prognosis problem. This procedure intends to reduce the uncertainty associated with long-term predictions by using the current state pdf estimate and a nonlinear dynamic state-space model. In the first level, p-step ahead predictions are generated based on an a priori estimate, adjusting their associated probabilities according to the noise model structure. A second level uses these predictions and the definition of critical thresholds to estimate the RUL pdf also referenced to as the time-to-failure pdf When the prognosis algorithm is implemented, the pdf of fault detection is used as the initial pdf to run the prognostic algorithm. The model for prognosis is developed using a Paris Law realtionship. For the bearings in focus, a fault is modeled as a quantified surface area of defect, denoted as D, such that nDC dt d D D    0          2   which states that the rate of defect growth is related to the instantaneous defect area D under a steady operating condition. C0 and n are material dependent coefficients. In discrete time form, we have     3 Results with Surrogate Bearing Data The following results are applied to the surrogate bearing data set only, since the fault size \(ground truth for this data set. As indicated earlier, 32 vibration snapshots are present in this data set, with 16 baseline \(healthy points and eight points from each of two \(spall Hence, 32 feature values are generated. To run failure prognosis, artificial data in-between these data points must be generated. The available vibration data correspond to different service hours. From these points, we interpolate the data according to service time in terms of minutes. This way, enough data points can be generated to run the failure prognosis algorithms. Figure 11 shows the ground truth data and times of available vibration snapshots. Interpolation of the ground truth points is used to assign spall sizes to the vibration snapshots. Figure 12 shows the time-based interpolation of feature values. From these two interpolations, we can estimate the expected feature values corresponding to different fault sizes and generate a feature progression curve as shown in Figure 13 Results of Failure Prognosis Since the data is interpolated in terms of minutes, the RUL expectation and 95% confidence interval are also given in minutes. Long-term predictions are provided after fault is detected and using the current estimate for the state pdf as initial condition. Results are depicted in Figure 14 through Figure 16 Figure 14 shows the minute at which the fault is detected Before this time instant, the prognostic routines are disabled. As soon as the fault is detected, the pdf estimates at that time are used as the initial conditions for the prognostic routines, as shown in Figure 15. When a new measurement comes in, the prognostic algorithms will provide an estimate of the remaining useful life. Figure 16 shows the result at the 350th minute The comparison between actual time-to-failure and estimated time-to-failure is shown in Figure 17. The red vertical line is the time at which the system fails, while the black line is the actual time-to-failure. The magenta line is 


black line is the actual time-to-failure. The magenta line is the expected value of the estimated pdf and the blue line is defined as the lower bound of the 95% \(just-in-time window around the expected value [9   Figure 11. Timeline showing ground truth points and interpolated spall sizes for available vibration snapshots 0 100 200 300 400 500 600 700 800 900 1000 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 Time \(min Sp a ll S iz e  m m 2  Interpolation of spall size groundtruth spall size snapshot with data available 10  Figure 12. Interpolation of fault dimension and feature vector  Figure 13. Interpolation of fault dimension according to feature values  Figure 14. Diagnostic result when a fault is detected  Figure 15. Initial prognostic estimation right after the fault is first detected  Figure 16. Failure prognosis at the 350th minute of bearing operation 0 100 200 300 400 500 600 700 800 0 100 200 300 400 500 600 700 800 R e m a in in g Li fe  m in  


 Current Time \(min  Healthy Failure Expected Just-in-time line Actual Remaining Life  Figure 17. Results of failure prognosis 0 100 200 300 400 500 600 700 800 900 1000 0 0.02 0.04 0.06 0.08 0.1 0.12 Time \(min Sp al l S iz e  m m 2 Interpolation of spall growth according to feature values 0 100 200 300 400 500 600 700 800 900 1000 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Time \(min Fe a tu re V a lu e    Interpolation of feature value with noise Interpolation of feature vlaue snapshot with ground truth data 11 CONCLUSIONS This paper shows that enhancements to diagnostic techniques are desirable as well as attainable additions to Health and Usage Monitoring Systems \(HUMS particularly in the case of rotorcraft component monitoring Enhancements like those presented support CBM efforts primarily in two ways: reduce the sensitivity of diagnostic processes to both signal noise and variations in environmental and operating conditions, and improve the performance of detection systems as well as the task of fault identification \(e.g., severity quantification instantiation of reliable prognostics Representative examples, motivated by the interest of the U.S. Army in transitioning from time-based \(using TBO definitions drive train bearing, illustrates the potential benefits of 


pursuing an integrated approach to diagnostics and prognostics, combining technologies for enhanced data preprocessing, advanced diagnostic-support algorithms, fusion at the sensor/feature levels, and an adequate framework for false alarm mitigation and uncertainty management. An architecture for achieving such integration is presented, with emphasis on supporting a robust performance of diagnostics operations, even in the presence of such kinds of disturbances as those observed in data acquired by HUMS vibration sensors. The present study also gives relevance to seeded fault because the technologies discussed can integrate knowledge about damage mechanism interactions or physics-of-failure models, as well as make use of multiple-sensor and multiple-feature data sets representative of known fault conditions For this reason, the team behind this project is evaluating a potential opportunity to perform a series of tests on rotorcraft drive train bearings with varying fault severities and under multiple, though realistic, operating conditions Such tests are being planned to provide algorithm/model validations, as well as diagnostic/prognostic performance assessments, in support of providing the U.S. Army with technologies that make detection systems more robust allow for the implementation of prognostics, and extend the useful life of drive train components. Component degradation testing thus remains as future, follow-up work to the research reported in this document ACKNOWLEDGMENTS This work has been partially supported with a cooperative agreement by the Army Research Laboratory under contract number W911NF-07-2-0075. In addition to the primary authors, we would like to thank government and contractor representatives from organizations supporting the Army Utility \(Blackhawk Estes, Mr. Carlos Rivera, and Dr. Jon Keller. This work has also benefitted greatly from consultations with other Army Research Laboratory and NASA Glenn researchers such as Dr. Timothy Krantz, Dr. David Lewicki, Dr. Harry Decker Dr. Hiralal Khatri, Mr. Ken Ranney and Mr. Kwok Tom REFERENCES 1] Branhof, R.W., Grabill, P., Grant, L., and Keller, J.A  Application of Automated Rotor Smoothing Using Continuous Vibration Measurements  American Helicopter Society 61st annual forum, Grapevine, Texas June 1  3, 2005 2] Dora, R., Wright, J., Hess, R., and Boydstun, B  Utility of the IMD HUMS in an Operational Setting on the UH60L Blackhawk  American Helicopter Society 60th annual forum, Baltimore, Maryland, May 7  10, 2004 3] Zakrajsek, J.J., Dempsey, P.J., et al  Rotorcraft Health Management Issues and Challenges  NASA report TM  2006-214022. February, 2006 4] Suggs, D.T., and Wade, D.R  Vibration Based Maintenance Credits for the UH-60 Oil Cooler Fan Assembly  American Helicopter Society, CBM Specialists Meeting, Huntsville, Alabama, February 13 2008 5] Baker, C., Marble, S., Morton, B.P., and Smith, B.J  Failure Modes and Prognostic Techniques for H-60 Tail Rotor Drive System Bearings  IEEEAC paper #1122 IEEE, 2007 6] Keller, J.A., Branhof, R., Dunaway, D., and Grabill, P  Examples of Condition Based Maintenance with the Vibration Management Enhancement Program   American Helicopter Society 61st Annual Forum Grapevine, Texas, June 1  3, 2005 7] Zhang, B., Sconyers, C., Byington, C.S., Patrick, R Orchard, M.E., and Vachtsevanos, G.J  Anomaly Detection: A Robust Approach to Detection of 


Detection: A Robust Approach to Detection of Unanticipated Faults  International Conference on Prognostics and Health Management, Denver, Colorado October 6-9, 2008 8] Byington, C.S., Watson, M., Lee, H., and Hollins, M  Sensor-level Fusion to Enhance Health and Usage Monitoring Systems  American Helicopter Society, 64th Annual Forum, Montreal, Canada, April 29-May 1, 2008 9] Engel, S.J., Gilmartin, B.J., Bongort, K., and Hess, A  Prognostics, the Real Issues Involved With Predicting Life Remaining  Proceedings of the IEEE Aerospace Conference, Big Sky, Montana, March 18-25, 2000 12 BIOGRAPHY Romano Patrick is a Project Manager at Impact Technologies. He received a Ph.D. in Electrical Engineering from the Georgia Institute of Technology specializing in model-based machine health diagnostics and prognostics. He also holds an MBA from Georgia Tech and degrees from U Texas, Arlington and U. Panamericana, Mexico. With career focus on interdisciplinary integration of technologies, his recent work involves practicable diagnostics/prognostics design for complex systems, such as rotorcraft drive trains Past experience includes automation and design for a variety of industrial and government sponsors \(DARPA, Lockheed Martin, Northrop Grumman, etc and program coordination at U. Panamericana, and some entrepreneurial R&amp;D Matthew J. Smith is a Senior Project Engineer at Impact Technologies. During his tenure with Impact, Matthew has performed multiple efforts pertaining to bearing vibration analysis, diagnostic and prognostic system development, and experimental study of faulted system reponse and fault progression. Previously, as a research assistant at Penn State and the NASA Glenn Research Center, Matthew performed experimental and analytical oil-free bearing analyses Matthew received his B.S. and M.S. degrees in Mechanical Engineering from The Pennsylvania State University. His research interests include: prognostic health assessment for bearing and actuator systems, grease degradation modeling and fault classifier development Bin Zhang received his Ph.D. degree from Nanyang Technological University, Singapore in 2007. He received his BE and MSE degrees from Nanjing University of Science and Technology, China, in 1993 and 1999, respectively. He is a senior member of IEEE. From 2005 to present, he has been a Post-Doc with the School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta GA His current research interests are fault diagnosis and failure prognosis, systems and control, digital signal processing learning control, intelligent systems and their applications to robotics, power electronics and various mechanical systems Carl S. Byington is a Professional Engineer and the Director of Systems Engineering at Impact Technologies. He directs R&amp;D in pursuit of advanced, automated systems health management for land-based, shipboard, and airborne machinery for military and commercial customers. He is Chairman of the Machinery Diagnostics &amp; Prognostics Committee of ASME and a member of IEEE, AIAA, SAE and AHS. He has a BS degree in Mechanical Engineering from the University of Pennsylvania and an MS in Aeronautical Engineering from George Washington University, and has published over 60 papers, book chapters, magazine and journal articles related to diagnostics and prognostics technologies George Vachtsevanos is Professor Emeritus at the Georgia Institute of Technology and also serves as the Chief Scientist at Impact Technologies, LLC. He directed the Intelligent Control Systems laboratory at Georgia Tech for the past 28 years where faculty and students are conducting research in fault diagnosis/prognosis and fault-tolerant control of engineering systems, intelligent control of industrial 


engineering systems, intelligent control of industrial processes, neurotechnology and cardiotechnology, and unmanned systems. His research work has been sponsored by government and industry and has published over 250 technical papers in his area of expertise. He is the lead author of a book on "Intelligent Fault Diagnosis and Prognosis of Engineering Systems" published by Wiley in 2006. He is the recipient of the Georgia Tech Interdisciplinary Activities award and the ECE Distinguished Professor award Romeo de la Cruz del Rosario, Jr. is the Chief of the Electronics Technology Branch at the U.S. Army Research Laboratory. He also serves as the Army Technology Objective ATO P&amp;D Operational Readiness and Condition Based Maintenance He received the B.E.E. degree from the Catholic University of America, Washington, D.C., and the M.S.E. and Ph.D degrees in Electrical and Computer Engineering from the Johns Hopkins University, Baltimore, MD. Since 1991 he has been an engineer at the Harry Diamond Laboratory then U.S Army Research Laboratory working in several areas including high power microwave technology characterization &amp; modeling of heterostructure RF devices and fabrication and failure analysis of electron devices and circuits  pre></body></html 


movies. In the case of the volume of critical reviews however, there was no big difference between mainstream and non-mainstream movies. WOM and critical reviews were usually positive H1 and H2 tested the relationship between WOM and weekly box office revenue, and the results supported the hypotheses. The volume of WOM was positively related to weekly box office revenue, while the valence of WOM had no significant effect. H3 H4a, and H4b tested the impact of critical reviews, and the results also supported the hypotheses except H4b The volume and valence of critical reviews had no consistent significances to weekly box office revenue H3 H4b. Table 7 showed that the number of critical reviews was statistically significant to aggregate box office revenue \(H4a for the attitude of critical reviews \(H4b the result more detail, an additional test was performed using only those factors related to critical reviews as independent variables. The result of the additional test supported H4, but the signs were reversed, i.e. positive critical reviews had minus signs, and negative critical reviews had plus signs. This reversed signs imply that the preference of critical reviewers is very similar to that of normal moviegoers. H5s and H6s tested the different effects of WOM and critical reviews on mainstream and non-mainstream movies. The result failed to determine that WOM give different impact on mainstream and non-mainstream movies, so H5a and H5b were rejected. H6, however, was supported, i.e the effects of critical reviews were different for mainstream and non-mainstream movies. There were no significant relationships between critical reviews and aggregate box office revenue in mainstream movies. For non-mainstream movies, however, the volume of critical reviews and the percentage of negative critical reviews were significant. Nonmainstream movies have fewer sources from which consumers can get information, and this might explain the results The above findings lead to several managerial implications. First, producers and distributors of movies could forecast weekly box office revenue by looking at previous weeks? volume of WOM. It does not matter what attitude people have when they spread WOM, the important factor is its volume. Therefore producers and distributors need to develop an appropriate strategy to manage WOM for their movies For example, the terms related to WOM marketing such as buzz and viral marketing are easily found Second, for the distributors who usually distribute less commercial and more artistic movies, and consequently have a smaller market compared to the major distributors, critical reviews can impact their movies box office revenues in a significant way. There are usually fewer sources for information for nonmainstream movies than mainstream movies, and so small efforts could leverage the outcomes. Finally, for those who are dealing with mainstream movies, the finding that the valence of WOM and critical reviews do not have significant relationship with box office revenue can have certain implications. Particularly, the attitude of critical reviews showed reversed effects Therefore, they may need to concentrate on other features rather than attitude of moviegoers or critical reviews, such as encouraging moviegoers to spread WOM This study contributes to the understanding of the motion picture industry, especially the relationship between box office revenue and WOM including critical reviews. There are existing studies that already 


critical reviews. There are existing studies that already dealt with similar issues, but this study has some differentiated features compare to prior studies. First the data used in this study was collected from South Korea, while most of the relevant studies usually focus on the North American market. This helps to provide the opportunity to understand the international market especially the Asian market, even though South Korea is a small part of it in terms of the motion picture industry. Second, movies were categorized to two groups, i.e. mainstream and non-mainstream and this study attempted to determine how WOM impacts these categories differently by testing several hypotheses In this study, there are also several limitations that could be dealt with in future research. First, using box office revenue as a dependent variable is more meaningful for distributers rather than producers. Due to there is close correlation between box office revenue and number of screens, one of producers? main concerns is how many screens their movies can be played on. Moreover, DVD sales are also important measurement for success of movies these days, and so it also could be a dependent variable. Therefore, it could be possible to give more fruitful managerial implications to various players in the motion picture industry by taking some other dependent variables Second, in this study, movies were categorized simply as mainstream and non-mainstream movies, but there could be further studies with diverse techniques of movie categorizations. For example, it would be possible to study the varying influence of WOM or critical reviews on different genres or movie budgets Third, an interesting finding of this study is that positive critical reviews could have negative relationship with box office revenue while negative critical reviews could have positive relationship. This study tried to provide a reasonable discussion on the issue, but more studies could be elaborate on it  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 References  1] Dellarocas, C., The Digitization of Word of Mouth Promise and Challenges of Online Feedback Mechanisms Management Science, 2003. 49\(10 2] Bone, P.F., Word-of-mouth effects on short-term and long-term product judgments. Journal of Business Research 1995. 32\(3 3] Swanson, S.R. and S.W. Kelley, Service recovery attributions and word-of-mouth intentions. European Journal of Marketing, 2001. 35\(1 4] Hennig-Thurau, F., et al., Electronic word-of-mouth via consumer-opinion platforms: What motivates consumers to articulate themselves on the internet? Journal of Interactive Marketing, 2004. 18\(1 5] Fong, J. and S. Burton, Electronic Word-of-Mouth: A Comparison of Stated and Revealed Behavior on Electronic Discussion Boards. Journal of Interactive Advertising, 2006 6\(2 6] Gruen, T.W., T. Osmonbekov, and A.J. Czaplewski eWOM: The impact of customer-to-customer online knowhow exchange on customer value and loyalty. Journal of Business Research, 2006. 59\(4 7] Garbarino, E. and M. Strahilevitz, Gender differences in the perceived risk of buying online and the effects of receiving a site recommendation. Journal of Business Research, 2004. 57\(7 8] Ward, J.C. and A.L. Ostrom, The Internet as information minefield: An analysis of the source and content of brand information yielded by net searches. Journal of Business Research, 2003. 56\(11 


9] Goldsmith, R.E. and D. Horowitz, Measuring Motivations for Online Opinion Seeking. Journal of Interactive Advertising, 2006. 6\(2 10] Eliashberg, J., A. Elberse, and M. Leenders, The motion picture industry: critical issues in practice, current research amp; new research directions. HBS Working Paper, 2005 11] S&amp;P, Industry surveys: Movies and home entertainment 2004 12] KNSO, Revenue of Motion Picture Industry 2004 Ministry of Culture, Sports, and Tourism, 2004 13] Duan, W., B. Gu, and A.B. Whinston, Do Online Reviews Matter? - An Empirical Investigation of Panel Data 2005, UT Austin 14] Zhang, X., C. Dellarocas, and N.F. Awad, Estimating word-of-mouth for movies: The impact of online movie reviews on box office performance, in Workshop on Information Systems and Economics \(WISE Park, MD 15] Mahajan, V., E. Muller, and R.A. Kerin, Introduction Strategy For New Products With Positive And Negative Word-Of-Mouth. Management Science, 1984. 30\(12 1389-1404 16] Moul, C.C., Measuring Word of Mouth's Impact on Theatrical Movie Admissions. Journal of Economics &amp Management Strategy, 2007. 16\(4 17] Liu, Y., Word of Mouth for Movies: Its Dynamics and Impact on Box Office Revenue. Journal of Marketing, 2006 70\(3 18] Austin, B.A., Immediate Seating: A Look at Movie Audiences. 1989, Wadsworth Publishing Company 19] Bayus, B.L., Word of Mouth: The Indirect Effects of Marketing Efforts. Journal of Advertising Research, 1985 25\(3 20] Faber, R.J., Effect of Media Advertising and Other Sources on Movie Selection. Journalism Quarterly, 1984 61\(2 21] Eliashberg, J. and S.M. Shugan, Film critics: Influencers or predictors? Journal of Marketing, 1997. 61\(2 22] Reinstein, D.A. and C.M. Snyder, The Influence Of Expert Reviews On Consumer Demand For Experience Goods: A Case Study Of Movie Critics. Journal of Industrial Economics, 2005. 53\(1 23] Gemser, G., M. Van Oostrum, and M. Leenders, The impact of film reviews on the box office performance of art house versus mainstream motion pictures. Journal of Cultural Economics, 2007. 31\(1 24] Wijnberg, N.M. and G. Gemser, Adding Value to Innovation: Impressionism and the Transformation of the Selection System in Visual Arts. Organization Science, 2000 11\(3 25] De Vany, A. and W.D. Walls, Bose-Einstein Dynamics and Adaptive Contracting in the Motion Picture Industry Economic Journal, 1996. 106\(439 26] Bagella, M. and L. Becchetti, The Determinants of Motion Picture Box Office Performance: Evidence from Movies Produced in Italy. Journal of Cultural Economics 1999. 23\(4 27] Basuroy, S., K.K. Desai, and D. Talukdar, An Empirical Investigation of Signaling in the Motion Picture Industry Journal of Marketing Research \(JMR 2 295 28] Neelamegham, R. and D. Jain, Consumer Choice Process for Experience Goods: An Econometric Model and Analysis. Journal of Marketing Research \(JMR 3 p. 373-386 29] Lovell, G., Movies and manipulation: How studios punish critics. Columbia Journalism Review, 1997. 35\(5 30] Thompson, K., Film Art: An Introduction. 2001 McGraw Hill, New York 31] Zuckerman, E.W. and T.Y. Kim, The critical trade-off identity assignment and box-office success in the feature film industry. Industrial and Corporate Change, 2003. 12\(1 


industry. Industrial and Corporate Change, 2003. 12\(1 27-67 32] KOFIC, Annual Report of Film Industry in Korea 2006 Korean Film Council, 2006 33] Sutton, S., Predicting and Explaining Intentions and Behavior: How Well Are We Doing? Journal of Applied Social Psychology, 1998. 28\(15 34] Basuroy, S., S. Chatterjee, and S.A. Ravid, How Critical Are Critical Reviews? The Box Office Effects of Film Critics Star Power, and Budgets. Journal of Marketing, 2003. 67\(4 p. 103-117  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 pre></body></html 





