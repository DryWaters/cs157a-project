 ARE THERE CONTAGION EFFECTS IN THE DIFFUSION OF IT OUTSOURCING  Kunsoo Han  Robert J. Kauffman                             Arti Mann  Barrie R. Nault McGill University                Arizona State University             Arizona State University              University of Calga ry kunsoo.han@mcgill.ca  rkauffman@asu.edu  amann3@asu.edu  nault@ucalgary.ca   Abstract Growth in the IT outsourcing industry during the past ten years has been exceptional. In this study, we theorize about and analyze the growth patterns of IT outsourcing at the level of the industry and at the firm level. We analyze 
a data set of firms using a lognormal model as a means to gauge the presence of over-dispersed diffusion and contagion effects that seem to drive outsourcing. A critical aspect of our research methodology is to study instances of outsourcing events especially mega-deals  that are likely to have affected decision-making by other firms in an industry. Our results suggest that during the periods where a rapid increase in the number of announcements following mega-deal announcements occurred, there is a hierarchical contagion effect The diffusion of IT outsourcing is not distributed lognormally, which suggests the presence of 
other underlying dynamics. Our analysis shows the presence of other underlying drivers that do not permit the diffusion pattern to be lognormal. The diffusion pattern for smaller firms appears to depend on the extent of outsourcing diffusion among the larger firms Keywords Contagion theory, diffusion, empirical research, industry analysis, IT services, outsourcing ________________________________________________ 1. INTRODUCTION Outsourcing has been around for a long time in different sectors of industry, but what is surprising about IT services outsourcing is how fast it has grown and how widely it has spread. Now any firm in any country with the requisite infrastructure and the 
right personnel can be a supplier of IT services. As a result, onshore and offshore IT outsourcing in the last decade has shown exceptional growth. The authoritative industry statistics firm, IDC, has reported that the business process outsourcing \(BPO\ market is expected to reach US$ 641 billion by 2009, with a cumulative annual growth rate \(CAGR\ of 10.9% from 2005 on T h e Gartn e r Grou p [8] predicts th at  worldwide spending on IT outsourcing will rise from US$408 billion in 2007 to US$441 billion in 2008, an increase of 8.1%.  Gartner also expects worldwide end-user spending on IT services to grow by 7.3 annually through 2011 to reach US$958 billion As researchers interested in the development of new theoretical perspectives that explain a variety of business phenomena, we wish to establish the extent 
to which the spread of IT outsourcing is analogous to the spread of a contagious disease Contagion effects  are present in the spread of diseases, which have precipitating events that prompt diffusion across a population. They also constitute an economic phenomenon  at m a y occ u r at ran d o m in depen d ent of each other and across locations Our premise in this research is that there is more structure and a theoretical basis in the rationale that underlies the observed empirical regularities of IT outsourcing s diffusion.  So our exploration involves looking for evidence that runs counter to the most basic intuition that IT outsourcing follows a linear growth curve. This is much the same as we might expect for the diffusion of a disease, where predis 
posing situational factors, characteristics of the population, or aspects of a specific disease might lead to the spread of an epidemic in a manner that is not altogether random. We leverage this analogy and seek to refine our understanding of how the diffusion of IT outsourcing occurred at the firm and industry levels For over a decade academicians have been conducting research on outsourcing. Even though numerous studies have been conducted, most have focused on a particular aspect of outsourcing, for instance, the risks involv e bes t practices 16 the reasons for outsourcing d w h y  f i r m s  h a v e  tended to form focal outsourcing relationships with their suppliers [15   T o o u r  kno w l e d ge no ne o f t h e  studies looked at the empirical regularities of the dif 
fusion of outsourcing as a whole, nor have there been attempts to explain the observed patterns We will focus on several research questions: How can the development of new theory help to shed light on the diffusion patterns of IT outsourcing? What should be the conceptual bases for such theory? What role have IT outsourcing mega-deals contracts with amounts greater than US$1 billion, played in the observed empirical patterns of diffusion In addition to studying the diffusion of IT outsourcing from a contagion theory perspective, we also evaluate a new empirical methodology which has been applied in medical epidemiology, political science and economics, but not yet in the information 
systems \(IS\iscipline. Our model involves the lognormal distribution which is appropriate when the data are overdispersed i.e., not occurring at regular intervals in time or across space\d the underlying change or growth is multiplicative rather than additive and separable.  Such models are intended to capture proportionate effects in diffusi o a gi v e n amount of diffusion, based on whatever drivers that explain it, is likely to produce a correlated amount of Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 1 978-0-7695-3450-3/09 $25.00 © 2009 IEEE 


 growth in diffusion in a subsequent period The paper is laid out as follows. §2 gives background and theory that explain the diffusion process for outsourcing.  It also discusses growth models that have been used in prior studies, and offers primary statement of our theory. In §3 we present the hypotheses and in §4 we introduce our two data sets. Then in §5 we discuss the empirical regularities for IT outsourcing that we have observed and introduce the methodology that we apply to test our proposed theory. In §6, we provide the results of our analysis and extends our basic model to address the unexplained variance in the estimation of our base model and presents the results. In §7 we discuss the results and broader theoretical implications. §8 concludes 2. THEORY A number of theoretical perspectives support our proposal for contagion-theoretic view of the diffusion of IT outsourcing. We consider diffusion of innovation theory and contagion effects theory. They offer an explanatory perspective on the diffusion of IT outsourcing at an industry and firm level 2.1. Outsourcing IT outsourcing is the significant contributions by external vendors e phy s i cal an d/or h u m a n resources associated with the entire or specific components of the IT infrastructure in the user organization  Business process outsourcing often involves IT outsourcing, and is the contracting of a specific business process or service to a third-party cont e rally  BP O in clu des t h e s o f t ware, the process management, and the people to operate the service that is outsourced  We consider IT and IT-based BPO, since it accounts for a large share of overall outsourcing activities related to IT  The growth of IT outsourcing has been exponential in the last two decades. One of the main reasons for the exponential growth of both onshore and offshore outsourcing is the technological advances that have occurred, dramatically affecting the way business is conducted. Cairncross [5  r e m i nd s u s tha t t h is  death of distance has led to developments in the field of IT outsourcing that are pervasive as compared to outsourcing in other fields such as manufacturing For twenty years, IT outsourcing has spread across different industries and moved from routine back office work to strategic business processes 2.2. Diffusion Diffusion is the process by which information about a focal technological innovation is communicated through certain channels over time among members of a social system, leading to the technology s adoption e ch n o l ogy di ff us i o n  g e n e ral l y  follows an s shaped  curve with five phases. Each is a proportion of the total number of adopters up to some time: innovators, early adopters, early majority, late majority, and laggards. To understand diffusion, one must understand the concept of network externalities  This is the change in utility a subscriber derives from a communication service as others join ith  interdependent demand, multiple equilibria can coexist at any price. The observed equilibrium depends on the static model, on the disequilibrium conditions and on the disequilibrium adjustment process A key characteristic of the related diffusion of innovations theory is its emphasis on communication channels  Social systems e.g., industry associations developers meetings, annual shareholder meetings etc.\ the sources of influence on adoption decision T h e Bas s 3 m odel   w i del y  u s ed i n di f f usion studies, argues that potential adopters are influenced by mass media and word of mouth I S  research, the diffusion of innovations theory has been used to study how technological innovations are embraced at different levels of analysis. In the present research, the focal innovation is IT outsourcing 1 The related social system consists of firms that may be located across different geographic regions, are in different industries, and share similar characteristics that may make them competitors or business partners 2.3. Contagion Effects A contagion effect is defined as the spread of a particular type of behavior through time and space as a result of a prototype or model performing the behavior and either facilitating that behavior in the observer or reducing the observer s inhibitions against performing that same behavior 22, p. 1006 Contagion effects theory offers a refined expression of the diffusion of innovations theory, and provides a starting point for understanding the underlying processes of diffusion t does s o th rou gh ev a luating diffusion behavior on the basis of a different point of view.  This view posits the possibility of the connectedness of outsourcing events over time.  Contagion effects may arise in two ways. The first is spillovers due to normal ties and interdependencies among different activities in a market.  These include aligned senior management interests, business activities in an industry, in a region, across firms with similar interests or operating characteristics, etc Another explanation is external to business, industry and geographical, based on macroeconomic drivers    1 Loh and Venkatraman [19  r e f e r to IT outsourcing as an administrative innovation Another way to think of it is as a technological innovation since outsourcing has been made possible through the communication capabilities of information and communications technologies Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 2 


 Contagion effects theory has been used to explain the adoption patterns of successive technology generations of analog and digital wireless phones   The adoption patterns can be partly explained on the basis of hierarchical c ontagion effects They are defined as the effects of variables that influence the extent of the connection between the behavior of the units of analysis of interest over time. So we might see hierarchical effects across industries, geography firms, strategic business units, senior managers, and so on.  Other studies use contagion effects theory and the related notion of co-movement to study how statelevel shocks on the growth of networks propagate to the national level, and vice ve T h e th eor y h a s  also been used to study the spread of financial turbulence, riots, civil disobedience, and other phenomena Another stream of research in which contagion effects theory occurs is political science. The theory has been used to explain adoption of innovations  and social security policy by states r o w th of terrorism [23  a n d o t he r no nt e c h no l o gi c a l p h e n o mena. One study on the spread of urban disorders in 1960s is especially interesting. Midlarsky pirically tests whether the spread of civil disorders in small cities in the United States can be attributed to baseline diffusion effects as well as hierarchical contagion effects In Midlarsky s work, the relevant hierarchical contagion effect that is proposed is the association between civil unrest in smaller cities and what has occurred in the larger cities a large city to small city effect.  He notes that this may occur even though there are differences in the critical mass  of the minority populations in the smaller cities An important finding from the prior research is that contagion effects theory appears to be helpful in explaining fast-spreading, over-dispersed phenomena.  We believe that it applies well to our efforts to explain the diffusion of IT outsourcing because the IT outsourcing data are over-dispersed 3. HYPOTHESES A US$625 million BPO deal signed between Nortel Networks and PricewaterhouseCoopers in 2000 was an important deal that had a major impact on the development of BPO outsourcing o m e  consider it as the deal that started the business process outsourcing wave [1 i m ilarl y  t h e outsourcing deal between Eastman Kodak and IBM in 1989 had a significant impact on IT outsourcing and is believed to have established IT outsourcing as a strategic choice for successful businesses and resulted in diffusion of IT outsourcing. Loh and Venkatraman a v e called t h is t h e Kodak effect  Another way to think about these kinds of problems is in modeling terms that describe the connectedness of the events that precede later events, and the extent to which the events themselves are random. In our study, we find that the precipitating events, the mega-deals, are random and occur across unrelated industries and do not follow any fixed pattern. We can view randomly occurring independent precipitating events as potentially influencing the later diffusion of IT and BPO outsourcing 2 This occurs on a proportional basis, so the number of new outsourcing deal announcements is proportional to the number of deals that have been concluded   Hypothesis 1 \(The Outsourcing Diffusion Hypothesis  The outsourcing diffusion process will be randomly proportionate to the set of responses to prior precipitating events  Evidence of a contagion effect might include a statistically significant increase in the number of IT outsourcing contract announcements in a period immediately following an earlier period which had IT outsourcing announcements from leading firms. Indicating presence of a contagion effect where the leading firms act as prototype or model for other firms to follow. An alternative explanation is the hierarchical contagion effect that we mentioned earlier, which involves managerially interesting stratifiers and theory-based observations of their relevance   Hypothesis 2 \(The Hierarchical Outsourcing Adoption Hypothesis With contagion effects the diffusion of outsourcing will depend on the influence of one or several contagion variables whose effects act in a hierarchy  The Outsourcing Diffusion Hypothesis \(H1\aracterizes patterns of IT outsourcing. The Hierarchical Outsourcing Adoption Hypothesis \(H2\ this at a more fine-grained level. The hierarchical effect could be from larger to smaller firms, or from firms in close by to those that are farther away from each other.  We only consider hierarchical effects that occur due to firm size. This is a good start for analysis and is defensible. It is done in studies of firm performance and strategic management 4. DATA We used two data sets in this study. The first captures the IT outsourcing information at the indus  2  A related technical requirement of the estimation models that we will use is the independence of the precipitating events.  Similar to Midlarsky [22 w e c onduc t our a n a l y s is  under the premise that this assumption is appropriate.  In fact, however, this is an empirical issue that requires additional exploration and analysis on our part to nail down We can nail this down when we have more data from more industries and more diverse geography, for example, outsourcing across different countries  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 3 


 try level.  The second relates to IT and BPO outsourcing information at the firm level. The first set is annual data for 60 non-farm industries in the U.S private sector provided by the Bureau of Labor Statistics \(BLS\d the Bureau of Economic Analysis BEA\ for 1998 to 2006. The industries are defined by the four digits 1997 North American Industry Classification System \(NAICS Appendix 1 provides details on the sources, construction procedure, and deflators used for the industry data. We used chain-type quantity indices as deflators 3 which show the growth of output \(or other variables\ over time holding prices cons  The BEA introduced these indices in 1996 to improve the accuracy of its estimates of the growth in real GDP by eliminating the bias present in fixedweight indices that had been used The second data set was collected from a full text search of company announcements related to IT and BPO outsourcing between April 1, 1999 and March 31, 2008. We used two leading news sources PR Newswire and Business Wire See Appendices 2 and 3.  We also used the online Lexis/Nexis database to search the news wires for announcements containing the words deal or contract or launch or announcement in the  same sentence as the words BPO and IT and outsourcing or offshoring  The search yielded 583 announcements in total, of which 210 announcements were relevant Not all of the relevant details for example, dollar amounts for the contracts required for analysis  were found in the company announcements, however. To collect these data, we searched other secondary sources, including trade journals, company websites magazine articles, and newspaper articles. We took extra care to differentiate between independent announcements and announcements that were a part of ongoing deals. We only include independent announcements. We collected announcement data at the firm level which involved either clients or vendors, or both that were located in the U.S.  We did this to   3 The BEA introduced the chain-type Fisher index into its measures of real output and prices to address the problem of choosing the base period with which all other periods are compared. This index is the geometric mean of the conventional fixed-weighted Laspeyres index which uses the weights of the first period in a two-period setting, and the Paasche index which relies on the weights of the second period. Changes in the Fisher index are calculated using the weights of adjacent years. These annual changes are chained or multiplied together to form a time-series that allows for the effects of changes in relative prices and in the composition of output over time.  See Landefeld and Parker [17 f o r a ddi tio na l de ta ils  maintain consistency across the industry and firmlevel data. Our final data set consisted of 70 announcements with complete information 5. INDUSTRY-LEVEL EMPIRICAL ANALYSIS We look at the industry level IT outsourcing data and compare the patterns observed with the announcement data that we collected. We expect to see similar kinds of empirical regularities that are specific to the diffusion of IT outsourcing across firms.  We focus on 1999 to 2006 across a few industries where there is high usage of IT. We called these ITintensive industries We selected these industries because these industries have the largest share of the worldwide BPO mark h ey i n cl u d e broadcas ting and telecommunication \(NAICS 5130\king and finance \(NAICS 5210, 5220 and 5230\, computer systems design and related services \(NAICS 5415 and information and data processing services NAICS 5140\  Figures 1, 2, and 3 present IT outsourcing trends over time within and across these NAICS industries over the same time period Fig. 1. Total Outsourcing in the IT-Intensive Industries by Year in US$, 1999 -2006  Fig. 2. IT Outsourcing Deal Announcements in the U.S., 1999-2006  We observe the following patterns across ITintensive industries. Based on industry-level data there is sharp growth from 1999-2000 and then a sharp decline from 2000-2002, after this period we observe a steady growth. For our announcement data Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 4 


 for the same industries, there are similar patterns except for the period 2001-2002. Our data show an increase in the number of announcements during this period, except for this period. These observations suggest that: IT outsourcing has been growing consistently across IT-intensive industries from 2003 onwards; and the industry outsourcing announcement data that we have collected reflect the industry trends except for the period of 2001-2002  Fig. 3. IT Services by NAICS Industry in US 1999-2006  For the industry level of analysis, we see somewhat different patterns. Different industries followed similar patterns from 1999 to 2002, but from 2002 onwards somewhat different patterns occurred. When we compare the outsourcing patterns within different industries with that at industry level we find some marked differences. We observe a consistent growth pattern in the broadcasting and telecom industries 5.1. Modeling Background We examine the overall patterns of outsourcing at the firm level over time with a methodology based on the estimation of a lognormal diffusion model 1  T h e u n d erl y i n g reas on f o r th e application o f t h is  model to the study of outsourcing is its emphasis on the proportionate effect of the diffusion process. Our conjecture is that outsourcing diffusion develops over time via a mechanism in which each additional increment of outsourcing-related events is proportional to the existing size of the process in other words the current installed base of outsourcing contracts We next provide empirical results to justify the application of this model to IT outsourcing 5.2. Empirical Model Development The lognormal distribution model has been used to describe different growth processes \(e.g., personal income, gross national income, etc.\corporates an assumption of independence regarding observations of diffusion. It incorporates a high degree of skewness and leptokurtosis compared to other distributions e ap ply t h is to test th e diff usion pat terns and gauge the proportionate effect for diffusion We next specify the lognormal diffusion model that we will apply to our data. Let  i be the set of mutually independent random drivers for IT outsourcing growth, and x i be the dollar amounts associated with the set of i 1 to n outsourcing announcements.  The variation among the announcements is expressed by the different dollar amounts of the underlying contracts. Based on the reasoning that we discussed involving a proportionate effect, the incremental change x i   x i1 should be a random proportion of the existing value  x i   x i1   i x i1  i 1, 2  n 1 Equation 1 can be interpreted as the change in the dollar amount of IT outsourcing as the result of an additional announcement and is proportional to the size of the dollar amount of the IT outsourcing contract. The equation can be stated as the fundamental equation underlying a lognormal distribution  log x n log x 0   1    2       n 2 Log x n is normally distributed in the limit by the additive form of the central limit theorem. So x n also is lognormally-distributed as  f  x   1 x  2   exp  1 2 log x  m        2          x  0  3 where m is the mean of the logarithm of x with standard deviation The parameters m and can be estimated by two additional equations   m  log 2 x i i  1 n  n 4   2  log 2 x i  m  2 i  1 n  n  1 5 This model gives us an opportunity to look at whether the data exhibit proportional behavior and fits the lognormal distribution. If the model does not fit, then we might have to extend it by including other variables, a different functional form or different underlying assumptions to capture the true behavior 5.3. Statistical Tests of the Main Hypothesis The main purpose of this part of our analysis is to see if the lognormal distribution fits our data. If the distribution fits then we can conclude that the IT diffusion follows a normal growth pattern. To accomplish this, we use the 2 goodness of fit test The null hypothesis is whether the frequency distribution of the relevant events observed in our sample is consistent with a lognormal distribution The variable x i is the dollar amount for each outProceedings of the 42nd Hawaii International Conference on System Sciences - 2009 5 


 sourcing announcement as before. We use logarithmic \(base 10\alues for the dollar amounts to define announcement categories. The different categories are defined as follows: 10 5 for $100K-$1M, 10 6 for 1M-$10M, and so on. All outsourcing contract announcements had value more than $100K thus we selected our start value as 10 5 For analysis purposes we selected the beginning points of each range rather than the mid-points to represent the logarithmic values. This does not change our conclusion, since it applies to all categories From Equations 2 and 3, we can see that log x i is normally distributed, and this form is necessary for us to implement the 2 test. Table 1 reports the values that we obtained with k 3 degrees of freedom, with k  representing the number of categories   Table 1.Predicted Distributions in US$, 1999-2008 D EAL R ANGE  L OG 10 US F RE  QUENCY  E XPECTED  F REQ UENCY 10 5 US$1001,000K 5 5 2.3 10 6 US$1-10M 6 13 16.4 10 7 US$10-100M 7 24 35.5 10 8 US$100MIB 8 31 23.0 10 9 US$1-10B 9 9 4.5 Notes Lognormal distribution of dollar amounts for outsourcing deals; 82 announcements total; mean by deal range of 7.14; std dev. = 0.91. Also  2 11.90 with 1 d.f p 05\. The values of the first two categories were combined for the  2 analysis 5.4. Results for the Base Modeling Approach Our null hypothesis is that our data follows a lognormal distribution. Our analysis shows that the observed distribution does not fit the lognormal distribution very well though. This is reflected in the  2 value of 11.90 \(1 d.f.\ost of the observed values demonstrate positive skewness. For the US$ categories 10 5 10 8 and 10 9 the observed values are higher than the expected values. Also, the observed values in the upper range of US$1 to 100 million of the announcement data are under-represented, whereas those in the lower range of US$100 million to US$10 billion are over-represented The under-representation of US$1 billion-plus deals can be explained on the basis of industry trends In the IT outsourcing industry, not many mega-deals have been signed. Those that have been signed encompass services contracts for multiple locations across multiple nations and sometimes even multiple business functions. The number of vendors that can provide such large-scale services based in the U.S. is small, as suggested by our data. A plausible explanation for the over-representation of the lower dollar range deals may be that higher dollar outsourcing deals get more press.  Clearly, we have some additional details to understand in this context Our results suggest that the lognormal distribution may not be capable of characterizing the diffusion patterns of IT outsourcing for the data and time period that we used. There are two possible explanations. First, the model may not be suitable, even though the lognormal model is generally appropriate for representing over-dispersed data. Our data are over-dispersed beyond the range of lognormal distribution. Another possibility is that some other process is at work here, such that the combination of two processes makes a single pattern representation of the lognormal model ineffective 6. EXTENSION AND RESULTS To capture the patterns of diffusion beyond the lognormal model, we need to adjust our approach 6.1. Background on the Modeling Extension We evaluate our data longitudinally as the sum of the dollar amounts of announcements.  These should approximate a straight line if the announcements occur randomly.  We also test for the presence of a hierarchical contagion effect in diffusion for IT outsourcing. Prior diffusion contagion studies have shown that adoption follows a hierarchical pattern. We represent the hierarchical adoption effect in term of large and small firms. Diffusion patterns in prior studies suggest that a hierarchical contagion effect may come into play when we see a rapid increase in the outcome variable of interest.  In this case, it is the number of outsourcing announcements following news of a mega-deal. To test for a hierarchical contagion, we analyze outsourcing announcement frequency by firm size, for different time periods. We use the number of employees for firm size See Appendix 4.\e check for over or underrepresentation of mid-to-small firms 6.2. Variables and Empirical Model Pattern analysis  To represent the data in longitudinal form, we adopt the transformation equations from Midlarsky ou r con t ext. We s u m t h e dollar amounts of announcements and apply log transformations to them. We represent the dollar amount associated with announcements i 1  n as x i  with  i representing the random precipitating event  the mega-deals The related expression is x 1  k 1 e 1  x 2  k 2 e 2   x n  k n e n 6 where k i is the constant of proportionality between the dollar amount and the exponential function Multiplying Eq. 6 s terms by one another gives 11 1  nn n ii ii i ii i xke ke         Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 6 


   12 12    n n kk k e     7 The log of Equation 7 is log x 1 x 2   x n  k  1   2   n   i  1 n  log x i  k  1   2   n 8 Precipitating incidents occur in time-order, so n  occurs after n1 Thus 1 log  i n i xkt    9  It follows that the sum of the logarithms of the dollar amounts of the outsourcing deals is proportional to time. To understand the overall pattern, we plotted the logarithms of the cumulative outsourcing dollar amounts for 1999 to 2007 in Figure 4 Fig. 4. Outsourcing Events by Year, 1999 to 2007  We see marked differences in the pattern between years.  Three years 2000, 2003 and 2006  are of interest to explore the contagion effect phenomena because of the rapid increase we see in the announcement data. To get an understanding of how the dollar amounts are distributed, we plotted the log of total outsourcing deal values for selected quarters from 2002 to 2007 in Figure 5 We observe a rapid growth in the log of the dollar amounts of outsourcing deals from 1999 to 2000 from 2002 to 2003, and from 2005 to 2006. These periods of rapid increases coincide with the periods in which we observed either mega-deal announcements or multiple announcements that are above US$100 million. Although we have not tested this for significance, we observed an increase in the number of smaller dollar amount deals that appear to follow these higher-value deals also To test the hypothesis of hierarchical contagion effects, we tabulated the number of announcements by firm size for this time period  We did this for 2003, 2006 and 2007, and for all the other years where we did not see a rapid increase in the cumulative logarithmic dollar amounts 4 See Table 2 Fig. 5. Log of Total Outsourcing in Selected Quarters in US$, 1999-2007  We test for whether the proportion of smaller sized firms is greater in the contagion periods as compared to that in the non-contagion periods, as suggested by the contagion theory. We find that the proportion of smaller firms is greater in the contagion period. Thus, there seems to be a hierarchical contagion effect: smaller firms adopt based on the external influence of large outsourcing-adopting firms adoption.  More analysis is needed to confirm the finding Table 2. Announcement Frequency by Firm Size P ERIOD  OF T IME  F IRMS WITH    50,000   EMPLOYEES  F IRMS WITH    50,000   EMPLOYEES  Contagion \(2003 2006-2007\ 16 8 Non-contagion 1999-2002 and 2004-2005\ 15 14 Notes There are 16 announcements in the data set for which no client information was available, and so we did not include them An alternative explanation, but one that still represents a hierarchical contagion is based on deal size For example, it may be the case that observations of larger deals by different firms drive their willingness to do larger deals themselves.  Similarly smaller deals may beget other smaller deals that cut   4 It is important to ask: What are the alternative explanations for these patterns? How can they be excluded? For example, the years 2002 and 2005 had improving economic conditions. These may have enabled firms to do more IT outsourcing.  Smaller deals may have increased more than bigger ones because they required more financial resources We have not tested this assertion directly, but it is possible to conceptualize this as a hierarchical diffusion effect.  It would be led by the financial condition of the firms.  We will explore number of alternative explanations like this in more data work as we further develop this research Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 7 


 across other hierarchical stratifiers.  A related possibility is that the contagion effects are neither driven by firm size or deal size.  Instead, these  may be driven by the deal vendors, so that mega-deals or smaller deals with a particular vendor may lead to other deals that are observed in the market with the same vendor.  We are exploring these alternative explanations so we can present more refined results in the future 7. DISCUSSION  The topic of diffusion patterns of IT has been of long-standing interest to researchers in IS, marketing and economics. Our focus has been on understanding IT outsourcing and business process outsourcing from the perspective of contagion effects theory 7.1. Contagion Effects of IT Outsourcing In the first part of our study we asserted that the diffusion pattern of IT and BPO outsourcing will be randomly proportionate to the set of announcements which are responses to prior precipitating events. We also asserted that in the presence of contagion effect the diffusion of IT outsourcing to small firms depends on the extent of outsourcing penetration in large firms, a hierarchical contagion effect We obtained evidence that showed that the diffusion of IT outsourcing is not distributed lognormally which does not match our initial modeling assertion Our analysis showed the presence of other underlying drivers, like hierarchical contagion effects, that do not allow the diffusion pattern to be lognormal. By analyzing the frequency of outsourcing announcements based on the firm size we were able to obtain evidence for the hierarchical contagion effect.  This indicated that the diffusion pattern of outsourcing for smaller firms depends on the extent of outsourcing diffusion among the larger firms 7.2. Broader Theoretical Implications Contagion effect theory provides a way to assess diffusion patterns from a number of different perspectives based on the choice variable for stratification. It is appropriate to try out several hierarchical diffusion stratifiers to identify a significant effect Another implication of this research is that the analytical and theoretical techniques we applied may be useful in studying diffusion patterns in analogous settings, where the analysis hypothesizes the presence of some kind of hierarchical contagion effect. For example, contagion-theoretic diffusion patterns may be observed for adoption of mobile payment systems electronic billing systems, or social networking sites 8. CONCLUSION We conclude with the primary contributions of our research, and then we consider limitations 8.1. Contributions The results of this study have theoretical and practical implications. We apply diffusion contagion effects theory in a new context: to study the diffusion patterns of IT outsourcing. This helps us to understand hierarchical adoption patterns in this context Our perspective is to look at aspects of the growth pattern for IT outsourcing that does not fit the expectations of a simple baseline model. We also demonstrate a new methodology for empirical research on IS diffusion: the lognormal model A problem with this type of field research on outsourcing is the difficulty in obtaining meaningful data. We created a new data set, which provides a different vantage point for the analysis of contagion effect-driven IT outsourcing diffusion. We also demonstrated the analogical development of an empirical model from medical epidemiology and political science, something which offers an interesting perspective on the broader development of new methodology for the IS field. Our results also will help managers understand the diffusion patterns of IT outsourcing, at the industry and firm level. This will enable them to make decisions related to IT outsourcing with an awareness of the macro and micro levels Our results are specifically helpful for vendors, who stand to gain a lot by making the information about their contract wins public. Especially if their clients are either high profile clients or the contract deals are high value 8.2. Limitations We note several limitations of the present research that we will address in future work. First, our results are based on outsourcing contract announcements and contract details from two well-accepted news sources.  So they may be biased to the extent that not all outsourcing contract announcements and details are covered by these sources. Also, the announcement rate and time are dependent on corporate guidelines and because of that there might be a time lag, between the time when actual contract is signed and the announcement is released. Still they represent industry trends, so excluding contracts not covered by these sources probably has not adversely affected our results. Also, our results are similar to Loh and Venkatraman s w h o s h o w ed org a n izations mimic the behavior of other organizations in terms of the use of their communication channels Second, we have focused on firm size as our main stratifier for observing contagion effects. There are other possible criteria that can be used to analyze  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 8 


 the contagion effect, for instance, geographic location, industry, international linkages, IT-intensity and managerial structure, etc.  The variable that we chose, firm size, is measurable and valid based on our field study observation, and consistent with control variables used in other research on firm strategy [7  i a s cale econ o m i es is i m portan t beca u s e it  represents how firms can produce internally, if they make the decision to outsource. Third, the trend of IT and BPO outsourcing is very recent in companies for the smallest two size categories of firms This study is exploratory.  We focused on the overall pattern of IT outsourcing.  We have given no consideration to external f actors, like regulatory and non-business considerations that may influence the diffusion of IT outsourcing within an industry or even across different geographic locations.  These are issues that we will explore in continuing research REFERENCES 1  Aitchison, J., and Brown, J. A. C Lognormal Distribution: With Special Reference to Its Use in Economics Cambridge Univ. Press, Cambridge, UK, 1957 2  Aubert, B.A., Patry, M., and Rivard, S. Assessing the risk of IT outsourcing. In R. Sprague \(ed Proc. 31st Hawaii Intl. Conf. Sys. Sci Maui, HI, January 1998 IEEE Comp. Soc. Press, Los Alamitos, CA, 1998 3  Bass, F. M. A new product growth for model consumer durables Mgmt. Sci 15, 5, 1969, 215-227 4  Business Wire IDC study identifies BPO adoption trends across top five vertical industries. August 25 2004. Available at findarticles.com/p/articles mi_m0EIN/is_2004_August_25/ai_n6168308 Last accessed on September 15, 2008 5  Cairncross, F The Death of Distance: How the Communications Revolution Is Changing Our Lives Harvard Bus. School Press, Boston, MA, 2001 6  Collier, D., and Messick, R. E. Prerequisites versus diffusion: testing alternative explanations of social security adoption Amer. Pol. Sci. Rev 69, 4, 1975 1299-1315 7  DeLone, W. H. Firm size and the characteristics of computer use MIS Quarterly 5, 4, 1981, 65-77 8  De Souza, R., Hale, K., Adachi, Y., and Ng, F. Dataquest insight: IT services forecast, worldwide, 20042011 Gartner Dataquest August 20, 2007 www.gartner.com/DisplayDocument?id=512914&ref g_sitelink Last accessed on September 15, 2008 9  Dornbusch, R., Park, Y. C., and Claessens, S. Contagion: understanding how it spreads World Bank Research Observer 15, 2, 2000, 177-197 10  Frauenheim, E.  IT firms expand from PCs to payroll CNETNews.com January 14, 2003.  Available at news.cnet.com/2100-1001-980495.html Last accessed on September 15, 2008 11  Gray, V. Innovation in the states: a diffusion study Amer. Pol. Sci. Rev 67, 4, 1973, 1174-1185 12  Harris, P. Outsourced learning: a new market emerges Learning Circuits June 5, 2003. Available at www learningcircuits.org/NR/exeres/6B519990-F1D6-4D0 E-B181-7979137DB9FC.htm Last accessed at September 15, 2008     13  Kauffman, R. J., and Kumar, A.  Understanding state and national growth co-movement: a study of shared ATM networks in the United States Electronic Commerce Research and Applications 7, 1, 2008, 2143 14  Kauffman, R. J., and Techatassanasoontorn, A. A. The global diffusion patterns of successive technology generations: the case of analog and digital wireless phone growth. Intl. Conf. Info. Comm. Tech. and Dev., UC Berkeley, CA, May 2006 15  Kern, T., and Willcocks L. Exploring relationships in IT outsourcing: the interaction approach Eur. J. Info Sys 11, 1, 2002, 3-19 16  Lacity, M.C., and Willcocks, L.P  An empirical investigation of IT sourcing practices: lessons from experience MIS Qtrly 22, 3, 1998, 363-408 17  Landefeld, J.S., and Parker, R.P. BEA s chain indexes time series, and measure of long-term economic growth Survey of Current Business U.S Department of Commerce, Washington, DC, May 1997 18  Loh, L., and Venkatraman, N. Determinants of IT outsourcing: a cross-sectional analysis J. Mgmt. Info Sys 9, 1, 1992a, 7-24 19  Loh, L., and Venkatraman, N. Diffusion of IT outsourcing: influence sources and the Kodak effect Info Sys. Res 3, 4, 1992b, 334-358 20  Mahajan, V., Muller, E., and Bass, F.M. New product diffusion models in marketing: a review and directions for research J. Mktg 54, 1, 1990, 1-26 21  Mahajan, V., and Peterson, R Models for Innovation Diffusion Sage Publications, Beverly Hills, CA, 1985 22  Midlarsky, M. I. Analyzing diffusion and contagion effects: the urban disorders of 1960s Amer. Pol. Sci Rev 72, 3, 1978, 996-1008 23  Midlarsky, M. I., Crenshaw, M., and Yoshida, F. Why violence spreads: the contagion of international terrorism Intl. Studies Quarterly 24, 2, 1980, 276-303 24  Ono, Y. and Stango, V. Outsourcing, firm size, and product complexity: evidence from credit unions Economic Perspective 1Q, Federal Reserve Bank of Chicago, IL, 2005, 1-10 25  Rogers, E Diffusion of Innovations  5 th Edition Free Press, New York, NY 2003 26  Rohlfs, J. A theory of interdependent demand for a communications service The Bell Journal of Economics and Management Science 5, 1, 1974, 16-37 27  Rossiter, R.D. Fisher ideal indexes in the national income and product accounts J. Econ. Educ 31, 4 2000, 363-373 28  Trolley, E. Training outsourcing: a bit of history TrainingOutsourcing.com January 2004. Available at www.trainingoutsourcing.com/exec_history.asp Last accessed on September 15, 2008 29  ZDNet Research Worldwide BPO market to grow at 10.9% a year ZDNet.com October 24, 2005. Available at blogs.zdnet.com/ITFacts/?p=9278 Last accessed on September 15, 2008 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 9 


 Appendix 1. Data Sources and Construction Procedure  V ARIABLE  BEA  S OURCE  C ONSTRUCTION P ROCEDURE  D EFLATOR  Output Industry accounts Gross output by industry in 2000 US Chain-type quantity index for outputs IT capital Fixed asset data Net stock of info. proc. equip. and software \(comp. and peripheral equip., comm.., instruments, photocopy equip., office/ acct equipment, software\, by industry, 2000 US Chain-type quantity index for fixed assets by type Non-IT capital Fixed asset data Net stock of private fixed assets, excluding information processing equipment and software by industry, 2000 US Labor Industry accounts BLS empl. statistics Total full-time equivalent employees by industry multiplied by average annual work hours of 2,080 hours None IT outsourcing KLEMS intermediate use estimates Sum of an industry s intermediate inputs purchased from NAICS 5142 \(Data Proc. Serv.\nd NAICS 5415 \(Comp. Sys. Design and Related Serv.\ in 2000 US Chain-type quantity index for intermediate inputs Non-IT services intermed. Inputs BEA industry use tables Industry s total intermediate inputs, excluding purchased IT services, in 2000 US Chain-type quantity index for intermediate inputs Appendix 2. Sample of Representative Announcements Two announcements from BusinessWire and PRWire  are shown below. When US$ were not mentioned, we used secondary sources \(e.g., trade magazine, newspaper articles, and company websites\for the relevant details   Hewitt Associates to Provide HR BPO Services to PepsiCo; Firm Continues Growth of HR BPO Business, Signing Eighth Deal Since Close of Hewitt and Exult Merger    Dateline: Lincolnshire, IL. April 12, 2005  Hewitt Associates \(NYSE:HEW\, a global human resources services firm, announced today that it will provide comprehensive HR BPO services to PepsiCo NYSE:PEP\, a world leader in convenient foods and beverages Under a ten-year agreement, Hewitt will provide HR BPO services in the U.S. and HR application development and hosting for the U.S. plus 67 additional countries globally The HR application development and hosting will support PepsiCo's approximately 64,000 employees in the U.S., and approximately 38,000 of PepsiCo's employees in 67 countries globally   CSC Signs $20 Million Outsourcing Contract with Wilton Re; CSC's Life Insurance BPO Services to Support Reinsurer's Acquisition Strategy Dateline  El Segundo, Calif. Sept. 13, 2007 Computer Sciences Corporation \(NYSE:CSC\day  announced that it has signed a 10-year, $20 millionbusiness process outsourcing \(BPO\ontract with Wilton Re to support the privately owned life reinsurance group's growth strategy of acquiring large blocks of insurance policies. Under the agreement, CSC will transition approximately 80,000 policies acquired by Wilton Re to CSC's insurance BPO operations and provide full back-office administration services, including policy and claims administration, customer service, billing and premium processing  Appendix 3. Coding Sample Announcements We implemented the following process to code the announcements. Only announcements for IT and IT-based BPO were included. All announcements for a contract amount greater than US$1 billion were classified as megadeals All others based on the dollar amount were put into different groups, for example, deals from US$100K-1M Five such groups were formed Appendix 4. Size Classification of Firms Based on the number of employees the firms were classified into following categories extra small with less than 1,000 employees small with greater than 1,000 but less than 10,000 employees medium with more than 10 000 but less than 50,000 employees large with greater than 50,000 but less than 100,000 employees; and extra large with greater than 100,000 employees   Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


Automated Windowed Coefficient Trending Earlier we attempted to find explanatory models whose coefficients changed over time in a predictable way This would then let us predict future coefficient values for these models and apply these trended models towards future estimates The problem was that we were not able to find such stable models at least manually We automated our search for stable models by building a system we refer to as the Automated Coefficient Trend Search ACTS tool A patent is currently pending for the method implemented in this software Against the Far Out data set ACTS is designed to automatically permute fields run chronologically-windowed regressions with the chosen fields establish linear and nonlinear trends for the resulting coefficients and then use these trended coefficients to estimate a future set of data ACTS also automatically tries up to seven different transformations on any given dependant or independent variable The length of calibration windows and the increment between them are automatically varied ACTS establishes most promising coefficient trends and also rates the performance of predictions done using trended-coefficient models With so many parameters transformations windows length and intervals to evaluate ACTS needed to search a huge problem space Despite assembling a cluster of 8 computers to run ACTS if necessary for months the problem still required pruning Each run of ACTS was therefore restricted to producing models with a different number of variables between 3 and 15 We quickly found that the best models contained very few parameters due to over-fitting on wider models permitting us to pare down the problem Neural Netv 1.00 0.75 0.50 0.25 I  M The system produced literally thousands of forecasts for each project If the distribution of project costs and project costs estimates were known a confidence interval could be computed using the cost estimates and a method such as Bayesian model averaging used to integrate them 5 Not having a priori knowledge of this distribution we chose instead to use the inter-quartile range of the estimates that is we looked at whether the estimate fell between the first and third quartile of the estimates The top and bottom 25 were disregarded because there are always outliers and it was presumed the system could work well even when many of the individual estimates were not very good Given the very large number of estimates produced by the system literally thousands or tens of thousands depending on the settings we could afford to squander a few Results were best with only three variables per equation In eight out of 31 projects 26 of the projects the project's actual costs fell within the inter-quartile range of the forecasts of costs for that project produced by the ACTS models For 30 out of 31 projects the inter-quartile range of estimates was made up of over a thousand estimates while for the 3 1st project only 446 estimates were part of the inter-quartile range The significance of these numbers is revealed in the next paragraph When equations with four variables were used only one of the true project costs fell within the inter-quartile range of forecasts produced by ACTS When more than 4 variables were used none of the true project costs fell within the interquartile range From this we concluded that ACTS performed best when only 3 variables were included in each model Using more variables appears to produce over-fitted models Aork MRE By Lauch Month Chronologic 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Launch Month Since 1960 Figure 10 Neural network magnitude of relative error sorted chronologically 11 space 


incapable of accurate forecasts Of course the wider the inter-quartile range of forecasts the more likely it is that the true forecast will fit within that range But how parsimonious is the range As figure 11 shows in about 55\2600 of the inter-quartile ranges created by ACTS the 3rd quartile of estimates is 50%0 larger than the Ist quartile of estimates For about 90\2600 of the ranges created the difference between the I't and 3rd quartile is less than 75\2600 and for 97\2600 of the projects the top of the range is 100 or less than the bottom of the range 45\2600 of the true project costs falling into the new range When the top and bottom of the range were changed by 50\2600 the range estimates encompassed 77\2600 of the true project costs In the end however we felt that the adjusted ranges were too wide for practical use Adaboosting using binomial logit classifiers Short for adaptive boosting the Adaboost algorithm 6 is a type of machine learning algorithm where a set of classifiers are adjusted to favor previous misclassifications These classifiers are actually other estimating algorithms Percentage of Time 3rd Quartile no more than XO/o Greater than 1st Quartile 120 100 80 60 40 20 0 25 50 75 100 Figure 11 Variation in ACTS-produced estimates measured between 3rd and 1st quartile Percentage of time True Cost Falls in Range Produced by ACTS 3 variable ersion 90 8070 60 5040 30 20 10 l 0 Interquartile  or10  or25  or50  or75 Figure 12 The effect of widening the estimating range measured by  of estimates now lying within Of the eight projects that fell within the average percentage difference between the 1st and 3rd quartile was 55 with median of 52 This indicated that the inter-quartile range might be too restrictive We therefore proceeded to widen the range of cost estimates for each project multiplying the ISt quartile estimate by 1 X%0 and the 3rd quartile estimate by 1  X using various Xs In each instance we compared actual project costs to estimates produced and determined how frequently the actual fell into the range of estimates Results are summarized in figure 12 When the top and bottom of the range were changed by 10 32 of the actual estimates fell within the new range Extending each of the range boundaries by 25 resulted in Adaboost works by iteratively weighting observations so that poorly classified ones are given more weight during the next iteration With the algorithm focused on improving estimates for exceptional cases it is somewhat more sensitive to noisy data and outliers though less likely to over fit We chose to test this approach due to the latter property The boosting algorithm seemed structured particularly for binary decisions though we are seeking to estimate a continuous variable cost We therefore staged the method by estimating cost bands so that the estimate for each band reverted to a binary decision The algorithm would separately estimate whether an observation was 12 


our evaluation method tested potential models using either ordinary least squares least absolute above or below X dollars then Y dollars then Z such that instead namely that we were examining a proportion rather X  Y  Z than a binary outcome but this was impractical to use and we judged that the problem was not strongly biased towards The boosting algorithm still required a classifier and model either context For a classifier we implemented binary logit78 a discrete Adaboost MRE By COST Lu was similar to ACTS but with 0 1.00 0.75 0.50 0.25 0.00 0.25 0 50 0 75 1 00 1 25 1 50 1 75 2 00 Cost Figure 13 Adaboost magnitude of relative error sorted by actual cost Adaboost MRE CHRONOLOGICALLY Year Figure 14 Adaboost magnitude of relative error sorted chronologically choice model specifically intended for a binary dependant variable is the cost above or below this point There may have been some support for using the probit model 7 This was done by a statistical programmer with output checked against the statistics program STATA 8 we obtained from this first stage system We also attempted to use OLS as a classifier but our experiments showed no significant effect from adding relatively orthogonal i.e unrelated models Since using more than one model did not make a difference boosting using OLS was little more than a single OLS model with estimates divvied up in bands no dynamic re-weighting of classifiers and thus no boosting effect to speak of To obtain models some changes and improvements and again put were discarded and resolved by the latter 13 Lu 1.00 0.75 0.50 0.25 0.00 0 0.50 0.75 1 00 1.25 1 50 1 75 2 00 error which better discounts outliers and weighted least squares The models which we built another windowed test system that our computing cluster to use trying to find good ones from were then used to bootstrap the binary logit method although coefficients a sea of potential transformations This time 


This exercise required a number of decisions about how many classifiers to use how many bands to estimate and how many variables should be used in the models All these choices were made empirically Models with 3 variables were marginally more accurate than those with 4 More models seemed generally better to a point so we used 99 models We also used 22 cost bands checking whether an estimate was above or below 12M 35M 44M and so on through 519M It is interesting to note that there was no data scarcity-induced limit on the number of bands we could have used since each was estimated using all observations lying above and below and not just between bands As with the neural network training was carried out using 52 missions from the 1960s through 1996 and testing occurred on 58 missions from 1996 through 2007 The trend in estimate deviation would again permit us to gauge whether predictive accuracy decayed over time Results are shown sorted by cost in figure 13 and chronologically in figure 14 Figure 13 shows that MREs are large and negative for missions at 56M actual mission cost and below Above this point results are mixed although not as poor as at the very low range There seems to be no degradation in predictive accuracy as projects increase in scale The correlation between estimates and actual values is approximately 0.66 Sorted chronologically in figure 14 no trend can be seen as estimates go farther out up to 11 years as was seen with the neural network Again this may have been due to the normalization process in which we mapped each variable to its percentage ranking based on future and past minimum and maximum values basically embedding the effect of time within the factors being input to the model It also may be due to a relatively stable era Weighted combinations of simple models A persistent problem in multivariate estimation methods which has been mentioned is that a model may be produced that over fits the observations used for calibration It would thus not be adequately generalized to estimate out of that sample Techniques such as jack-knifing and outlier removal can mitigate this We chose another approach developing models that were little more than simple rules of thumb and then combining these in a weighted manner so each would have a say in the estimate The weightings were formulated using Excel's Solver to minimize estimation error on a set observations lying 15 to 25 years before the test set We refer to this method as continuous boosting because as with the Adaboost method it too involves a mix of classifiers that are combined according to their performance against a set of test observations Unlike Adaboost which solves a binary problem the algorithms are here estimating on a continuous scale Also unlike Adaboost the models are re-weighted rather than the observations to maximize prediction Before presenting results it is worth noting that we ran a control test where all variables from the simple rules of thumb were combined into a single model and run on the same windows performance was nowhere near as good We chose the models by hand guided by the criteria that they should be relatively orthogonal rely on differing parameters so they could each contribute a unique influence to the combined result Figure 15 shows that MREs are mostly positive though for lower cost missions errors are regularly negative and large Continuous Boost MRE By COST M M 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Cost Figure 15 Continuous boosting magnitude of relative error sorted by actual cost 14 


CONCLUSIONS Given that this project was intended to estimate missions lying 10-15 years out we structured it differently than one intended to estimate contemporary projects A variety of conventional techniques were not used as we felt they would over fit training observations and thus not be suitable for prediction We also were not sure which approach would work so we tried many We heavily favored ensemble methods where models are combined because we surmised that any one model could not be guaranteed to have the best view of the future However a single neural network ultimately yielded the most competitive results Another finding was that methods built upon simple models such as with three variables generally did work best not surprisingly because they were less likely to over fit calibration data A graph of the three best methods is shown in figure 18 with estimates for the same missions sorted by cost For the neural network calibration occurred until just before the results shown for continuous boosting calibration occurred no more recently than 15 years prior to the results and for Adaboost calibration also occurred up until the results shown A graph sorted by year is not shown we think it instructive that no results seemed to degrade over time As mentioned but obvious from figure 17 the neural network performs best followed by Adaboost and then continuous boosting It is interesting to note that the three cases in which the neural network goes haywire and predicts too low also correspond to worst performances for the Adaboost method pointing to exceptional data points We will be further investigating these regularly errant results and other outliers which may result in estimating improvements ACKNOWLEDGEMENTS This work was carried out under Small Business Innovation Research contract FS9453-05-C-0023 with the Air Force Research Lab The authors wish to gratefully acknowledge Judy Fennelly and later Ross Wainwright our technical points of contact at AFRL for their continuous support and encouragement Our contract officer Timothy Provencio also provided invaluable assistance Our critical seed stock of data was provided by Joseph Hamaker who previously was Director of NASA Headquarters Cost Analysis Division and now is Senior Cost Analyst with SAIC Ainsley Chong and Dale Martin USAF Ret also lent considerable assistance with data gathering APPENDIX A MISSIONS COLLECTED Active Cavity Radiometer Irradiance Monitor Satellite Active Magnetospheric Particle Tracer Explorer Adeos Advanced Communications Technology Satellite Advanced Composition Explorer Alexis Amos-I AMSC-1 Anik El Anik E2 Applications Technology Satellite-I Applications Technology Satellite-2 Applications Technology Satellite-5 All MREs Lu 1.00 0.75 l 0.50l 0.25 l 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75-2.00oi v 1   Cost o N X  X~~~~~Cl Figure 16 Comparison of estimating error for three best methods 15 


Applications Technology Satellite-6 Aqua Argos Atmospheric Explorer AURA Aurora 2 Calipso Cassini Cassini Spacecraft  Huygens Probe Chandra X Ray Observatory CHIPSat Clark Clementine CloudSat COBE Columbia 5 Contour CRRES DART Dawn DBS-1 Deep Impact Flyby Spacecraft  Impactor Deep Space 1 Deep Space 2 Defense Meteorological Satellite Program-5D Defense Meteorological Satellite Program-5D3 Defense Support Program DSCS 3 FIO DSCS 3 F7 DSCS I DSCS-II DSCS-IIIA DSCS-IIIB Dynamics Explorer-I Dynamics Explorer-2 Earth Observing Satellite 1 Earth Radiation Budget Experiment EchoStar 5 Extreme Ultraviolet Explorer Far Ultraviolet Spectroscopic Explorer FAST FLTSATCOM 6 Galaxy 5 Galaxy 11 Galaxy Evolution Explorer Galileo Orbiter  Probe Gamma Ray Large Area Space Telescope GE 1 GE 5 Genesis GFO 1 Globalstar 8 Glomr GOES 3 GOES 9 GOES N GPS-1 GPS-IIR GPSMYP GRACE Gravity Probe-B GRO/Compton Gamma Ray Observatory GStar4 Hayabusa HEAO-1 HEAO-2 HEAO-3 HESSI-II High Energy Transient Explorer-II HETE HST ICESat Ikonos IMAGE IMP-H Inmarsat 3-F5 Intelsat K INTELSAT-II INTELSAT-IV International Ultraviolet Explorer Iridium James Webb Space Telescope Jason 1 JAWSAT KEPLER KOMPSAT LANDSAT1 LANDSAT-4 LANDSAT-7 Lewis Lunar Orbiter Lunar Prospector Magellan Magsat Mariner-4 Mariner-6 Mariner-8 Mariner1 0 MARISAT Mars Exploration Rover Mars Express/Beagle 2 Mars Global Surveyor Mars Observer Mars Odyssey Mars Surveyor 2001 Orbiter Mars Pathfinder  Sojourner Rovers Mars Polar Lander Mars Reconnaissance Orbiter Mars Telecommunication Orbiter Mars Climate Orbiter Messenger Meteor Mid-course Space Experiment MightySat Milstar 3  Adv EHF Model-35 Morelos NATO III Near Earth Asteroid Rendezvous NEAR Shoemaker New Horizons 16 


NIMBUS NOAA 8 NOAA 15 NPOESS Preparatory Project Orbital Maneuvering Vehicle Orbcomm Orbiting Carbon Observatory Orbiting Solar Observatory-8 Orbview 2 Orion 1 P78 Pas 4 Phoenix Pioneer P-30 Pioneer Venus Bus/Orbiter Small Probe and Large Probe Pioneer-I 0 Polar QuikSCAT Radarsat Reuven High Energy Solar Spectroscopic Imager REX-II Rosetta Instruments Sage III SAMPEX Satcom C3 Satcom C4 SBS 5 SCATHA Seastar SMART-1 SNOE Solar Dynamics Observatory Solar Maximum Mission Solar Mesosphere Explorer Solar Radiation and Climate Experiment Solar Terrestrial Relations Observatory Space Interferometry Mission Space Test Program Small Secondary Satellite 3 Spitzer Space Telescope SPOT 5A Stardust  Sample Return Capsule STEPO STEPI STEP3 STEP4 STRV ID Submillimeter Wave Astronomy Satellite Surfsat Surveyor Swift Gamma Ray Burst Exporer Synchronous Meterological Satellite-I TACSAT TACSAT 2 TDRS F7 Tellura Terra Terriers Tethered Satellite System Thermosphere Ionosphere Mesosphere Energetics and Dynamics TIMED TIROS-M TIROS-N TOMS-EP Total Ozone Mapping Spectrometer TOPEX TRACE Triana Tropical Rain Measuring Mission TSX-5 UFO I UFO 4 UFO 9 Ulysses Upper Atmosphere Research Satellite Vegetation Canopy Lidar VELA-IV Viking Viking Lander Viking Orbiter Voyager Wilkinson Microwave Anisotropy Probe WIND WIRE XMM X-Ray Timing Explorer XTE XSS-iO XSS-ii APPENDIX B FIELDS COLLECTED Mission Mission Scenario Mission Type Launch Year Launch Vehicle Bus Model Bus Config Bus Diameter Location Expected Life Flight Profile Flight Focus Technical Orbit Currency Total Cost Including Launch Unit Sat Costs Average Sat Cost Production Costs Launch Costs Operations Cost Orbit Weight Wet Weight Dry Weight Max Power EOL Power BOL Power  of Batts 17 


Batt Power Batt Type Stabilization Type Propulsion Mechanism  of Solar Panels  of Solar Cells Manufacturing Qty Satellites in Constellation On-Orbit Spares Channels Number of Bands Data Storage Processing Power Source type Block Name Thermal Control Material Type Level of Technology Known Inheritance Propulsion station keeping Number of Axes Ground Based Spares Pointing Accuracy APPENDIX C EXISTING MODELS Numerous models are today in use for estimating spacecraft cost Two of the most common are the NASA/Air Force Cost Model and the Aerospace Small Satellite Cost Model Here is a description of the NAFCOM model The NASA/Air Force Cost Model NAFCOM is a parametric estimating toolfor space hardware It is based on historical NASA and Air Force space projects and is primarily used in the very early phases of a development project NAFCOM can be used at the subsystem or component levels The database currently includes 122 missions including 76 unmanned earth orbiting 24 unmanned planetary 11 launch vehicles 8 manned 3 engines It uses parametric relationships to estimate subsystem or component level costs for any aerospace hardware including earth orbital spacecraft manned spacecraft launch vehicle upper stages liquid rocket engines scientific instruments or planetary spacecraft 7 And for the Aerospace Small Satellite Cost Model SSCM employs a parametric methodology for estimation of program cost and is best suited to the early conceptual development phase of a spacecraft program during which time the design is likely to be less mature and when cost and performance trades can be easily performed SSCM consists of a collection of cost-estimating relationships or CERs which estimate the costs of developing andproducing a spacecraft system with the following subsystems  Attitude Determination and Control Subsystem ADCS  Propulsion  Power  Telemetry Tracking  Command TT&C  Command  Data Handling C&DH  Structure  Thermal CERs were also developed for integration assembly and test IA&T program management PM and systems engineering SE and launch and orbital operations support LOOS Individual subsystem cost estimates are statistically rolled up to yield a cost-risk distribution which provides the estimator with a range of cost estimates andpercentiles 8 The SSCM was calibrated from over 100 post-1990 Earth-orbiting andplanetary missions REFERENCES 1 Lack of Disciplined Cost-Estimating Processes Hinders Effective Program Management GAO study 04-642 2 Jilla Cyrus D and Miller David W Satellite Design Past Present and Future International Journal of Small Satellite Engineering 12 February 1997 3 Bearden David A A Complexity Based Risk Assessment of Low-Cost Planetary Missions:When Is A Mission Too Fast and Too Cheap Fourth IAA International Conference On Low-Cost Planetary Missions JHU/APL MAY 2-5 2000 4 Kellogg Mahr and Lobbia An Analogy-based Method for Estimating the Costs of Spacecraft IEEEAC paper 1371 Version 4 5 Hoeting Jennifer A Methodology for Bayesian Model Averaging An Update f 6]btp/ewiieiao iiAaos 7 Keith Smith NASA/Air Force Cost Model Science Applications International Corporation 8 18 


BIOGRAPHIES Lee Fischman served as Principle Investigator for this project Lee is Senior Director of Development at Galorath Incorporated where he directs much of the new product development and research at the firm He developed SEER for Software  Hardware Integrations with Microsoft Project the Comparison Sizing tool COTS Software model in addition to various data mining information extraction and expert systems Previously he was a software designerlprogrammer in the New York financial industry Lee earned a BA from the University of Chicago and an MA from UCLA both in economics Mike Kimel carried out statistical work on the project Mike is an Economist for Galorath Inc in addition to maintaining his own quantitative consulting practice He has also taught Economics and Advanced Statistics at the Graziadio School of Business and Management at Pepperdine University run the Competitive Strategy group for a Fortune 500 Telecom Company and worked as a Consultant at PriceWaterhouse LLC now PriceWaterhouse-Coopers He earned a Ph.D in Economicsfrom UCLA Troy Masters programmed analytic methods and is integrating the Far Out model into its parent product SEER for Hardware previously SEER-H Troy is a Software Engineer with Galorath Incorporated where he has been the primary developer for a range ofproducts He earned a BS in computer science from UCLA David J Pine was our subject matter expert helping us assemble data and gain insight into technical trends Dave is retired after a 34-year career with the National Aeronautics and Space Administration NASA currently is a consultant to various government and industry entities While at NASA his organizations in the Office of the Chief Financial Officer and later at the IPAO at Langley Research Center were responsible for the conduct of major NASA program analyses and evaluations for the NASA Administrator and Deputy Administrator From early 1988 through the end of 1990 he was the Deputy Program Manager for the Hubble Space Telescope Program specifically responsible for the telescope operations and science support aspects of the program He earned a BS in Aerospace Engineering from the Polytechnic Institute of Brooklyn and a Masters of Engineering Administration from the George Washington University 19 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


