  1 An Introspection Framework for Fault Tolerance in Support of Autonomous Space Systems  Mark L.  James Mark.James@JPL.NASA.GOV  Hans P.  Zima Zima@JPL.NASA.GOV  Jet Propulsion Laboratory California Institute of Technology 4800 Oak Grove Drive Pasadena, CA  91109   Abstract This paper describes a software system designed for the support of future autonomous space missions by providing an infrastructure for runtime monitoring, analysis and feedback. The objective of 
this research is to make mission software executing on parallel on-board architectures fault tolerant through an introspection mechanism that provides automatic recovery minimizing the loss of function and data.  Such architectures are essential for future JPL missions because of their increased need for autonomy along with enhanced on-board computational capabilities  while in deep space or time-critical situations The standard framework for introspection described in this paper integrates well with existing flight software architectures and can serve as an enabling technology for 
the support of such systems. Furthermore, it separates the introspection capability from applications and the underlying system, providing a generic framework that can be also applied to a broad range of problems beyond fault tolerance, such as behavior analysis, intrusion detection performance tuning, and power management 12  T ABLE OF C ONTENTS  I NTRODUCTION 1  C 
ORE C APABILITY FOR I NTROSPECTION 2  N O L OSS C OMPUTATION C APABILITY NLC\.........3  SHINE.......................................................................4  FRONTIER 5  CONCLUSIONS......................................................6  R EFERENCES AND C 
ITATIONS 6  B IOGRAPHY 7  1  1 1-4244-1488-1/08/$25.00  2008 IEEE 2 IEEEAC paper #1394, Version 1 Updated  December 14, 2007  I NTRODUCTION  NASA’s mission of deep space exploration poses one of the most difficult challenges facing computer science research—that of designing, building, and operating progressively more capable autonomous spacecraft and 
planetary rovers \(as well as aircraft and perhaps even submarines\ Given the communication latencies for such missions, the need for enhanced autonomy becomes obvious: Earth-based mission cont rollers will be unable to directly control distant sp acecraft and robots to ensure timely precision and safety, and to capture rapidly changing events, such as dust devils on Mars This will lead to dramatic changes in the way missions will be conducted in the not-too-dist ant future: they will require more intelligent software, automatic recovery from faults and high-capability low-power on-board parallel systems 
based on emerging multi-core and/or FPGA technology with a potentially large number of CPUs on a chip.  Even in the near future, space-based ra dar algorithms are projected to require computing power on the order of 50 GigaFlops sustained – orders of magnit ude beyond current capabilities Existing technology will not be able to cope with these requirements.  The state-of-the-art usually addresses this problem via hardware redundancy.  In contrast, the objective of the research described in this paper is to provide adaptive fault tolerance for mission software executing on such architectures through an introspection 
mechanism that supports automa tic recovery in deep space or time-critical situations, minimizing the loss of function or data, and reducing the ove rhead of fixed redundancy schemes such as Triple Modular Redundancy \(TMR\or NModular Redundancy \(NMR Achieving this goal requires a major leap in technology We propose the design and development of a standard framework for introspection that can be integrated with and 


  2 deployed on existing flight software architectures as an enabling technology for the support of high-reliability systems Introspection enables a softwa re system to become selfaware by monitoring its execution behavior, reasoning about its internal state, making decisions or recommendations about appropriate changes of the system or system state when necessary, and supporting recovery from faults.  A Core Capability for Introspection  CCI  be reusable across multiple application domains and hardware platforms, providing system and application independence As a proof of concept for the CCI, a No-Loss Computation   NLC capability will plug in to the CCI software infrastructure.  The NLC will detect, analyze, and isolate the loss of function or data resulting from a fault by using a distributed class of no-loss computation agents in combination with automatically generated directives for the continuation of the computation to provide a seamless autonomous recovery The Spacecraft Health Inference Engine \(SHINE\a reasoning inference engine developed at JPL, will be used to respond to complex combinations of real-time events to perform detection, isolation, and recovery from certain classes of anomalies induced from hardware or software corruption. We plan for a first step in the validation of our approach by using an existing flight software application from the Mars Exploration Rover \(MER\ as well as the Synthetic Aperture Radar \(SAR\image processing code both of which represent prototypical codes that contain key elements relevant for many future missions In imaging radar as the radar moves, a pulse is transmitted at each position; the return echoes pass through the receiver and are recorded in an 'echo store.' Because the radar is moving relative to the ground, the returned echoes are Doppler-shifted \(negatively as the radar approaches a target positively as it moves away\paring the Dopplershifted frequencies to a reference frequency allows many returned signals to be "focused" on a single point effectively increasing the lengt h of the antenna that is imaging that particular point. This focusing operation commonly known as SAR processing, is now done digitally on fast computer systems. The trick in SAR processing is to correctly match the variation in Doppler frequency for each point in the image: this requires very precise knowledge of the relative motion between th e platform and the imaged objects \(which is the cause of the Doppler variation in the first place In summary, our approach pursu es research in strategic areas that are important to NASA and JPL. Introspection is a fundamentally new capability providing a low-overhead and cost-effective approach that will increase mission safety, improve reliability, and simplify software development, especially as systems scale in size and performance. This work doe s not replace redundancy-based solutions to fault tolerance but will rather significantly improve  existing capabilities by adding a novel softwarebased technology.  Our approach relieves the programmer from the highly complex and error-prone process of anticipating the impact of a hardware fault on an executing application and provides automatic recovery in the event of a loss of function, or in corruption or loss of data Compared with traditional expensive and power-hungry fixed-redundancy schemes, the fault tolerance strategy proposed here can improve performance as well as power consumption by a factor of at least 3, easily reaching an order of magnitude improvement depending on the application and the environment.  Similar considerations apply to software development C ORE C APABILITY FOR I NTROSPECTION  A major component in our design is a core capability for introspection \(CCI\ich clearly separates between the introspection capabilities and the application program and the underlying system.  This supports system- and application-independence of the core introspection mechanisms, and its reusability in the context of different systems CCI can be thought of conceptually similar to a hardware backplane with plug-and-play functionality for seamlessly adding and removing capabilities, such as performance tuning, diagnostics, and fault recovery.  This approach is standards-based: the implementation of particular modules builds on the protocols already in place, and a common set of interfaces for accessing devi ces and services is being provided.  This idea reflects the Mission Data System MDS\common set of interfaces for core functionalities.  Like MDS our design is independent of the specific software application and functions at the CCI abstraction level, thus minimizing the amount of detailed implementation knowledge required by the programmer CCI includes a high-speed inference engine \(SHINE providing several functions: \(1\sed as an intelligent router to distribute collected data to the embedded CCI components; \(2\ is programmed to respond to complex combinations of events based upon real-time and archived data to provide advanced monitoring; and \(3\ provides the needed functionality for detection, isolation, and recovery from anomalies.  The inference engine is based on a version of SHINE The CCI contains a long-term database \(LDB\hat is used as a repository of knowledge about the architecture, the operating and runtime system, key components of the software environment, and the application itself 


  3 Furthermore, the LDB provides storage for information about the execution of the application, which is nontransient in nature.  This information will be indexed by a predefined set of built-in cr iteria along with user-supplied keys that define the data flows and type of data being exchanged between component s in addition to other information.  The LDB will be implemented using an existing tool, which has been successfully delivered to NASA's Deep Space Network for storing real-time monitor data.  Because of the potentia l time-critical nature of the problem, great attention is being paid to guaranteeing realtime response for both the acquisition of data from their sources as well as their rapid distribution to the components that use it A high-speed short-term cache component based on an existing tool called Quick Cache directly buffers the incoming results from high-speed hardware or software probes.  The purpose of this component is to capture and sufficiently slow down the data so it can be indexed in the LDB A unifying umbrella component called FRONTIER is being used to integrate the LDB and the reasoning components so they can be interfaced as a single subsystem.  FRONTIER is a general-purpose front end system for real-time reasoning systems, responsible for interfacing to all external data sources, converting the raw data to higher-level data storing it in the LDB, and providing a general information exchange between all reasoning components FRONTIER is responsible for interfacing to all external data sources, converting the raw data to higher-level data e.g., engineering units, normalization\oring it in a database and providing a gene ral information exchange between all reasoning components.  It is a component of a new class of integrated reasoning systems for fault detection, isolation, diagnostic s, recovery and prognostics Because of the potential time-cr itical nature of the problem we are addressing, great attention has been paid to providing real-time response fo r both the acquisition of the data from the real-time sources and rapid distribution of it to the components that use it The CCI operates asynchronously with respect to the application, and will be implemented as a system of asynchronously operating agents, which are autonomous and mobile computational entities with internal state N O L OSS C OMPUTATION C APABILITY NLC Driven by the desire of scientists for higher levels of detail and accuracy in their applica tions, the size and complexity of computations is growing faster than the improvement in processor technology. With conventional CMOS chip technology approaching physical limits in the areas of power consumption, memory latency, and instruction-levelparallelism, parallel processing offers the most promising path to continuing the trend towards higher performance Actual hardware developments – all the way from embedded systems to supercomputers – provide systems with a growing number of processors, which are able to generate a large number of parallel threads.  One of the problems resulting from this approach is the increasing probability of hardware failu res, coupled with operating system problems.  This issue is addressed by the No-Loss Computation Capability \(NLC\rovided in our system Failures can be broadly categorized as soft and hard.  Soft failures occur when the program continues to execute but either incorrect results are generated or resources are directly or indirectly compromised.  These kinds of failures are insidious because they ubiqu itously manifest themselves in apparently unrelated contex ts.  Hard failures are those that generate an exception.  Introspection can equally be applied to detection and remedi ation of both kinds of faults however here we will be focusing on hard failures The NLC module is being implemented as a plug-in to the introspection backplane.  By plug-in we mean that additional software capabilities can be added and removed into the infrastructure using a standard set of Application Program Interfaces \(API\hen an algorithm is distributed across multiple threads executing on distinct processors, a loss of one of those threads or processors can potentially result in the loss of all the incremental results up to that point.  The probability of a hardware failure during the course of a long execution can be exceptionally high Traditionally, this problem has been addressed by establishing checkpoints where a part of the current state of the execution is saved.  Then in the event of a failure, this state information can be used to restore the execution state and resume the computation from that point Checkpoints are a reasonable approach when the combined total of the state information is not overwhelming in size or limited by the available bandwidth for the transport to the checkpoint repository.  An additional potential problem with checkpoints is the further burden they put on the user to implement them and in limited resource situations, such as space, they critically burden the overall computation They need to address impor tant issues such as: \(1 reformulating the algorithm for checkpointing, \(2\deciding the frequency of checkpointing, \(3\ecting the anomaly 4\ding a repository for th e state information and \(5 building the infrastructure to tie it all together.  The problem becomes even more complex b ecause often the scientist is not the one who implements the algorithm but it additionally requires a programmer who is an expert in parallel programming and the parallel target architecture Our approach is a multi-tier method that involves the hierarchical detection of the anomaly by using a distributed class of no-loss computation agents coupled with 


  4 enhancements to the compiler to generate NLC state vectors for continuation of the computation in the event of a failure An NLC is broken down into detection, state generation and recovery.  We are relying on an event being triggered whenever a loss of computation on a thread occurs.  Events will be described in the form of SHINE rules that connect hardware and software triggers with the running application and whose antecedents are the actions to take place when that scenario is detected Such an event can be signaled by a cooperating thread, the operating system, or as a result of the failure of an invariant or assertion monitored by the NLC.  State generation is accomplished by extensions to the open source GNU C/C compiler that generates the necessary state information when the code is compiled.  Additional program analysis optionally supported by user directives\dentifies the variables that need to be repr esented in the state vector and decides when these states need to be recorded.  In addition to the state information, a si de-effect analysis is being performed to detect other affect ed threads, and to generate the appropriate information to have those computations restarted The recovery component is activated on a loss of computation signal from one of the NLC agents.  When the signal is detected, it queries the database recovering the most recent recorded state fo r the thread, restores it and restarts any dependent out of thread computations SHINE The Spacecraft Health Inference Engine \(SHINE reusable inference engine for the monitoring, analysis and diagnosis of real-time and non-real-time systems.  It is a system developed at NASA to meet many of the demanding and rigorous AI goals for current and future needs.  SHINE was designed to be efficient enough to operate in a real-time environment and to be utilized by applications written in conventional programming langua ges such as ADA, C/C JAVA, etc.  These applications can be executing in a distributed computing environment on remote computers or on a system that supports multiple programming languages Knowledge-based systems for automated task planning network management, monitoring, diagnosis and other applications require a sophisticated software infrastructure based on artificial intelligence concepts and advanced programming techniques. Software development tools that can accelerate the research and development of new artificial intelligence applications are highly desirable.  The SHINE system was developed for that purpose.  Included in the system are facilities for developing advanced reasoning processes, memory-data structures and knowledge bases blackboard systems, and spontaneous computation triggers SHINE is a reusable knowledge-based software tool originally designed for real-time monitoring, analysis, and diagnosis of spacecraft a nd ground telecommunication systems through using advanced reasoning processes but it since has been extended to provide solutions for many new areas It advances the state-of-the art in automated systems by enabling solutions to a broad class of problems that previously were considered in tractable because of real-time system requirements, application delivery size constraints portability, and availability on flight or specialized hardware SHINE introduces a novel paradigm for knowledge visualization and ultra-fast inference that goes well beyond traditional forward and backward chaining methodology.  A sophisticated mathematical transformation based on graphtheoretic Data Flow analysis is introduced that reduces the complexity of conflict-resolution during the match cycle from O\(n 2 o O\(n\any kinds of inference operations see Figure 1\ation executes compiled SHINE knowledge bases at an excess of 16 million rules per second on flight hardware \(200 MHz PPC million to 1 billion rules pe r second on a standard GHz desktop system.  This is several orders of magnitude faster than the performance provided by commercially available systems and delivers applicati ons to resource-restricted platforms and embedded hardware Figure 1: SHINE Dataflow Representation The development of SHINE is based upon the long-term experience at JPL in developing expert systems for the diagnosis of spacecraft health Computational efficiency and high performance are especia lly critical in any kind of automated reasoning kind of software.  That consideration 


  5 has been an important objectiv e for the SHINE system and has led to its design as a t oolbox of unique capabilities that may be used independently or collectively in the development of knowledge-based systems SHINE’s unique approach to knowledge representation knowledge-visualization and reasoning processes enables system designers to build high-performance expert systems previously only within the r each of “artific ial intelligence experts”. It is intended for those areas of reasoning where speed, portability, and reuse are of critical importance Such areas would include sp acecraft monitoring, control and health, telecommunication analysis, Policy Based Network Management \(PBNM\, network diagnosis information reduction and summarization, medical diagnosis, robotics or basically any area where rapid and immediate response to high-speed and dynamically changing data is required It comprises a collection of high-level software tools that revolutionizes the creation of efficient, reusable knowledgebased software systems \(see Figure 2\ike existing expert systems it generates only application-critical code thereby eliminating the need to deploy the complete environment.  Its unique approach to the creation of knowledge-based systems is based on sophisticated compiler technology, one of the fastest inference engines in the industry, and a large library of AI problem-solving techniques Figure 2: SHINE Development and Delivery Environment SHINE provides ultra computing performance on desktops computers that were previously not even available on large mainframe systems.  For ease of use and broad-spectrum applicability it directly interf aces with applications written in C, C++, ADA, JAVA, and LISP and generates native code for C, C++, and LISP With targets planned for ADA BASIC, FORTRAN, and JAVA SHINE has contributed to reduce operations cost and provides real-time response to any instrumented system.  In particular, it has been a valuable aid in monitoring and maintaining NASA satellite communication systems \(Deep Space Network spacecraft/military systems, improved reliability and safety in eight NASA deep space missions that include Voyager Galileo, Magellan, Cassini and Extreme Ultraviolet Explorer \(EUVE\ In summary, SHINE is a well tenured system and has been successfully delivered to many military and non-military projects and is licensed commercially FRONTIER Every conventional or intelligen t real-time data processing system requires a front-end co mponent to interface to the external world to acquire data, preprocess it and make it available for analysis.  This capability has been replicated a countless number of times for each new system that is built In an attempt to avoid this duplication of effort and to provide a plug-and-play com ponent functionality, we have developed FRONTIER.  It is a general-purpose front end for real-time analysis and reasoning systems.  FRONTIER is not limited to only advanced processing but is also directly applicable to any pr oblem where large volumes of data need to be collected, pr ocessed and recorded in realtime FRONTIER is responsible for interfacing to all external data sources, converting the raw data to higher-level data e.g., engineering units, normalization\oring it in a database and providing a gene ral information exchange between all reasoning components.  It is a component of a new class of integrated reasoning systems for fault detection, isolation, diagnostic s, recovery and prognostics Because of the potential time-cr itical nature of the problem we are addressing, great attention has been paid to providing real-time response fo r both the acquisition of the data from the real-time sources and rapid distribution of it to the components that use it It is designed as a reusable system, enabling it to be rapidly tailored to a broad class of reasoning and non-reasoning systems with a minimum amount of effort.  Each component of the system is programmable, executing in an embedded virtual machine.  Th is approach enables on-the-fly  reconfiguration of each elemen t, and the virtual machine implementation makes the system completely independent from the particular piece of hardware on which it runs.  The top-level elements of FRONTIER are written in C++ for real-time performance, however, configuration setups preprocessing and conversion algorithms, heuristic analysis and noise source definitions ar e written in a virtual language 


  6 that can be downloaded to each of the its components while they are running to provide hot swaps of software FRONTIER is composed of a hardware-specific interface to the external world and seven reusable components \(see Figure 3 Figure 3: FRONTIER Architecture Diagram  The eight components of the system, when unified, form a dynamically programmable gene ral purpose front end data processor that provides the following functionality   Source of all external data for all reasoning components via a consistent external interfaces   A uniform and ubiquitous exchange of data and information between reasoning components   A subscription-based relational database to store real-time data from extern al sources and distribute real-time and historic numeric and symbolic information between reasoning components   Filtering of real-time measurements to eliminate irrelevant data   Up- and down conversion of the sampling of measurements through programmable rate converters   Automatic correlation and alignment of timeskewed measurements so measurements properly arrive on actual or pseudo frame boundaries   Programmable numeric preprocessing of data \(e.g engineering units conversion, feature extraction   Programmable event detection based upon complex relational expressions through a rulebased language to provide inference annotation of measurements   All components of the FRONTIER are commanded and programmed through the subscription-based database   Optional socket-based interfaces between reasoning components so components can be remotely located CONCLUSIONS Verification and validation of software systems \(V&V\has been addressed by a broad range of methods, including static analysis, model checking, testing, runtime monitoring and formal proofs. In this paper, we describe the design of an introspection infrastructure for applications in an environment with a large number of parallel threads.  Our approach to introspection integrates the monitoring of executions, analysis of system state, and changes to the system state.  Compared to th e current state-of-the-art, the innovative features of our work include the following  We have designed a generic software framework for introspection providing a “backplane” capability with plug-in functionality, and realized by a set of asynchronous agents  The objects of the introspection capability are applications characterized by explicit and implicit concurrency, and executing on multi-processing hardware systems providing a large number of threads  Our design focuses on the requirements of future spaceborne systems that need to support an enhanced degree of autonomy and  provide application-specific adaptive fault tolerance  The combination of monitoring, analysis, and feedback-oriented modifica tion of system state has been described in the literature and partially implemented in a number of systems, but no design based on massively parallel multi-core architectures has yet been implemented Current approaches to V&V usually deal with individual methods in isolation.  While we do not at this time present a unified approach to V&V, we plan to combine elements of static analysis, formal proofs, and monitoring in our future work R EFERENCES AND C ITATIONS  Afjeh,A., Homer,P., Lewandowski,H., Reed,J.  and Schlichting,R.  Development of an intelligent monitoring and control system for a heterogeneous numerical propulsion system simulation In Proc 28th Annual Simulation Symposium Phoenix, AZ, April 1995 Brockmeyer,M.,  Jahanian F., Heitmeyer, C.,and Labaw B.An approach to monitoring and assertion-checking of real time specifications in Modechart In Workshop on Parallel and Distributed Real-Time Systems April 1996 Gu,W., Eisenhauer,G., Schwan,K and Vetter,J.  Falcon: Online monitoring for steering parallel programs 


  7 Concurrency: Practice and Experience 10\(9\699–736 Aug 1998 Fijany, F.  Vatan, A.  Barrett, M.  James, C.  Williams and R Mackey, “A Novel Model-Based Diagnosis Engine Theory and Applications”, 2003, IEEE Aerospace Conference, 2003 Fijany, F.  Vatan, A.  Barrett, M.  James, and R.  Mackey An advanced model-based diagnosis engine, The 7th International”, Symposium on Artificial Intelligence Robotics and Automation in Space, 2003 James, M., “NMC, A Data Exchange Server, Reference Manual”, Jet Propulsion Laboratory, California Institute of Technology, 2003 James, M., “Quick Cache.  A Multi-Tiered Real-Time, Cache System for Monitoring Hardware Systems, Reference Manual”, Jet Propulsion Laboratory, California Institute of Technology, 2002 James, M., “SHINE,a High-Speed Inference for the Diagnosis of Complex Software and Hard ware  Systems, Reference Manual”, Jet Propulsion Laboratory, California Institute of Technology, 2003 James, M., “FRONTIER, a General Purpose Front-End for Real-Time Reasoning Systems”, Jet Propulsion Laboratory, California Ins titute of  Technology, 2004 Mehrotra, P.  and Zima, H.P High Performance Fortran for Aerospace Applications Parallel Computing Special Issue on Parallel Computing in Aerospace, Vol.27 No.4,pp.477--501 \(2001 Miller,B., Callaghan, M.,Cargille,J., Hollingsworth,J., Irvin B.,Karavanic, K., Kunchithap adam,K.  and Newhall,T Paradyn parallel performance measurement tools IEEE Computer 28, November 1995 Schroeder,B., Aggarwal, S.,and Schwan,K.  Software analysis of safety constraints In Proceedings 16th Symposium on Reliable and Distributed Systems pages 80–87 IEEE Computer Society, October 1997 Snodgrass,R.  A relational approach to monitoring complex systems IEEE Transactions on Computers 6\(2\156–196 May 1988 Sterling, T.L.  and Zima, H.P.  Gilgamesh: A Multithreaded Processor-In-Memory Architecture for Petaflops Computing, Proceedings of SC2002 -- High Performance Networking and Computing, November 2002 Widom,J.  and Ceri,S.\(Editors Active Database Systems  Morgan Kaufmann, 1996 Zima, H.P., Chapman, B.M.  Supercompilers for Parallel and Vector Computers. ACM Press Frontier Series/AddisonWesley \(1990 Zima, H.P., Chapman, B.M.  Compiling for DistributedMemory Systems. Invited Paper, Proceedings of the IEEE.  Special Section on Languages and Compilers for Parallel Machines, pp.  264-287 \(February 1993 B IOGRAPHY  Mark L.  James is a senior R & D researcher of the Advanced Computing Algorithms and ISHM Technologies Group in the Flight Software and Data Systems Section within NASA’s Jet Propulsion Laboratory Pasadena, CA, California Institute of Technology.  At JPL Mark is Principal Investigator and Task Manager in realtime inference and knowledge-based systems.  His primary research focus is on high-speed inference systems and their application to planetary and d eep spacecraft systems, and medical applications.  His expertise includes core artificial intelligence technology, high-sp eed real-time inference systems, flight system software architectures and software design and implementation for those systems Mark has received a number of NASA awards which include: Monetary NASA Board Awards, NASA Software of the Year Award Nominee, JPL Team Excellence Award JPL JET Productivity Award, Major NASA Monetary Award, NASA Certificate of Recognition, NASA Achievement Award, NASA E xceptional Service Medal and NASA Exceptional Service Plaque for 1989.  He is author of two inventions, 36 scientific and technical papers, 10 technical manuals, 3 patents, and 73 NASA recognized new technologies Hans P. Zima is a Principal Scientist in the Information Systems Directorate of the Jet Propulsion Laboratory in Pasadena and a Professor Emeritus of the University of Vienna, Austria. He received his Ph.D. degree in Mathematics and Astronomy from the University of Vienna in 1964  His major research interests ha ve been in the fields of highlevel programming languages compilers, operating systems, and advanced software tools.  In the early 1970s while working in industry, he designed and implemented one of the first high-level real-time languages for the German Air Traffic Control Agency.  During his tenure as a Professor of Computer Science at the University of Bonn Germany, he contributed to the German supercomputer project “SUPRENUM”, leading the design of the first Fortran-based compilation system for distributed-memory architectures \(1989\.  After his move to the University of Vienna, he became the chief designer of the Vienna Fortran language \(1992\ that provided a major input for the High  


  8 Performance Fortran de-facto standard. His research over the past four years focused on the design of the “Chapel programming language in the framework of the DARPAsponsored High Productivity Computing Systems \(HPCS program. During the past year, Dr. Zima has become involved in a design effort targeting space-borne faulttolerant high capability computi ng systems. He is the author or co-author of about 200 publications, including 4 books 


that is based on stereo range information Taken together all of the stereo range maps derived from stereo image pairs form a cloud of 3D points all around the rover that conform to the terrain surface We can derive a height map or elevation map from the stereo point cloud by creating an artificial grid in X northing and Y easting and letting the Z axis be elevation We can arbitrarily select the spacing horizontal and vertical grid to achieve the desired mosaic image resolution Early work in this area suggests that 0.01 collisions between multiple points that occupy the same grid cell the maximum elevation should be selected In undulating terrain it is important to preserve relatively large grid regions where no points are available such regions are often the invisible rock and terrain features from the rover's point of view that are the failing of the current state of practice Very small regions of a few contiguous cells or even single cells in the grid will sometimes occur due to failures in stereo correlation due to lack of sufficient Figure 6-Overhead mosaics of Opportunity Navcam images Above a mosaic based on a ground plane projection Below a mosaic based on an elevation map meter per pixel spacing is effective for Navcam elevation maps The overall size of the mosaic can be selected arbitrarily as well Empirical use of Navcam stereo data for targeting and navigation indicates a distance of 15 meters from the rover in any direction is the maximum beyond which the data becomes too sparse to derive an effective elevation map To create the elevation map the point cloud can be inspected one point at a time and the elevation value assigned to the X-Y grid cell in which it falls To resolve texture or other factors To fill in these very small regions with no data a median filter can be applied over the elevation map to assign the median elevation value of those in the local neighborhood to the cell Once assembled the elevation map is now usable as a mosaic projection surface A mosaic image the same size as the elevation map may be constructed by projecting the 3D point formed by the X-Y cell coordinates and the Z 9 


elevation value through the camera model into the original Navcam or Pancam image The corresponding color value at the pixel coordinates ideally an interpolated value is assigned to the mosaic image Figure 6 shows an example of such a mosaic When overlaid onto a HiRISE image this type of mosaic adds useful context including surface features observed by the rover as well as enabling the correlation of targets where observations made by other instruments in the rover science payload are located Further development and integration of this mosaicking methodology is currently underway at the time of this writing and will be further matured in the coming years leading up to Mars Science Laboratory operations 5 CONCLUSIONS Adaptive level-of-detail tile based delivery of images is now supporting operations of the MER and Phoenix missions The performance of this strategy has proven to be superior by orders of magnitude to the previous state of practice The original Science Activity Planner system required entire image collections to be transferred to remote users in order to browse mosaics The transfer time for Pancam image collections was typically on the order of 15 minutes or longer and the delay was compounded by Internet connections over very long distances such as across the United States or to other countries The coast-to-coast transfer time is now only a few seconds or less for each screen of tiles for the initial transfer Tiles are only transferred once and then stored in a local browser cache to prevent redundant transfers JPEG2000 image compression is being used to compress the tiled image data that is served to the science investigators The quality of this image compression has proven to be very suitable for Mars rover images and the performance superior to other formats such as JPEG and PNG The JJ2000 reference implementation for Java applications 5 has proven in practice to be a capable implementation for encoding and decoding JPEG2000 image data 6 FUTURE WORK Microsoft's HD Photo compression algorithm offers reduction in image size similar to JPEG2000 with a lower computational cost This technique may be an improvement over JPEG2000 encoding of Mars rover images although at the time of this writing it still lacks the multiplatform support that is required to serve a diverse community of science investigators If this limitation is overcome it would be worthwhile to experiment with HD Photo and quantify its performance when compared with JPEG2000 in quality size reduction and computational cost On July 31 2007 the Joint Photographic Experts Group and Microsoft announced that this compression technique was under consideration for a new JPEG standard tentatively titled JPEG XR 6 This will hopefully lead to a multiplatform implementation of this compression algorithm that could be applied to many domains such as Mars rover imaging REFERENCES 1 Justin N Maki Todd Litwin Mark Schwochert Ken Herkenhoff Operation and Performance of the Mars Exploration Rover Imaging System on the Martian Surface 2005 IEEE International Conference on Systems Man and Cybernetics October 10-12 2005 2 Y Yakimovsky and R Cunningham A system for extracting three-dimensional measurements from a stereo pair of tv cameras Computer Graphics and Image Processing vol 7 pp 195-210 1978 3 Jeffrey S Norris Mark W Powell Marsette A Vona Paul G Backes Justin V Wick Mars Exploration Rover Operations with the Science Activity Planner IEEE International Conference on Robotics and Automation April 2005 4 J S Norris M W Powell J M Fox K J Rabe I Shu Science Operations Interfaces for Mars Surface Exploration 2005 IEEE Conference on Systems Man and Cybernetics October 15-17 Big Island HI October 15 2005 5 JJ2000 JPEG2000 reference implementation of JPEG2000 http://jj2000.epfl.ch 6 Microsoft HD Photo press release ACKNOWLEDGEMENTS The research described in this publication was carried out at the Jet Propulsion Laboratory California Institute of Technology under a contract with the National Aeronautics and Space Administration BIOGRAPHY Mark Powell is a Senior Member of Technical Staff at the Jet Propulsion Laboratory Pasadena CA since 2001 He received his Ph.D in Computer Science and Engineering in 2000 from the University of South Florida Tampa His dissertation work was 10 


in the area of advanced illumination modeling color and range image processing applied to robotics and medical imaging and received the award for Outstanding Dissertation from the University of South Florida At JPL his area offocus is science data visualization and science planning for telerobotics He supported the 2004 Mars Exploration Rover MER mission operations as a Science Downlink Coordinator facilitating the timely downlink and analysis of science data from the rovers He received the NASA Software of the Year Award for his work on the Science Activity Planner science visualization and activity planning software used for MER operations He also received the Imager of the Year award from Advanced Imaging Magazine for his work on Maestro the publicly available version of the Science Activity Planner for MER Mark has been programming in Java and loving every minute of it since it was first used in web browsers in 1995 He his wife Nina and daughters Gwendolyn and Jacquelyn live in Tujunga CA Thomas Crockett started working at JPL in 2005 after completing his Bachelor of Science degree in Computer Science at the University of Arizona where he discovered an interest in graphics He joined the Maestro team in 2006 and began developing advanced image browsing capabilities that would be used by the MER Phoenix and MSL missions to view and smoothly navigate around very large images Recently he has been extending this work to tackle the problem of mapping and spatial browsing of the science data collected by a mission Jason Fox received his Masters of Science degree in Computer Science from Purdue University in the spring of 2003 At that time he began working for the NASA Jet Propulsion Laboratory in Pasadena California in the Planning Software Systems group Jason first worked on the Mars Exploration Rover MER mission performing verification and validation of the Activity Planning and Sequencing Subsystem's modeling infrastructure After the rovers  successful landing his role shifted to that of Tactical Activity Planner on the Integrated Sequencing Team and was responsible for the daily construction of the integrated rover activity plan After leaving MER Jason joined the Maestro team and began development on the science operations tool Maestro the successor to the Science Activity Planner co-winner of the NASA Software of the Year in 2004 In addition to Maestro he is also the JPL leadfor the Phoenix Science Interface tool that will support scientific operations of the Phoenix lander 2007 At JPL his areas of focus include collaborative distributed operations for Mars rovers and landers and science planning for telerobotics In his spare time Jason is in training for the Coeur d'Alene Ironman competition to be held in June 2008 Joseph Joswig received his Masters of Science degree in Computer Science from the University t o California Los Angeles in the Spring of 2005 He is a Software Engineer in the Planning Software Systems various~Gou flth NAssiAn Jeta ER heix n S and tchnolgy prjec Propul s inc Laboratiory in or wa ouedo eeorked fo mutherpast twoun softwaresysteyears asL aTLT moembe NS-ofnthe Cener K10 ove lso h Ma estrog thea Masupproting wrhafoue ondeveloping o a multi-robot gomnadcn roundste to support White Sands Missile Range Jeffporruisy isath Exportin ovrsorwhc supy ervisorwne of the 2004NASASofwareof te Plannn Awr.H softwarenl leadng te deelopentf operatlions system s for ah20 Mar SieneabratryRoerndaMars et Sofut r Cassni Satunian Orbter Mar Reco naisanc Orbter and.the.pirit an pportunityM....M ars Explratin.Roers-.fo.whih.thy.wre.c-winer.o.th 2004 NASA Software of the...Year...Award.....He..is..currently leading the develoment.of.the.uplinksystem.for.the.200 MarScencLaoraoryRovroandalvaiety Laofrlunry and Martian operations technology projects Jeff is a strong 11 


advocate for the application of agile development to Cora and has a lovely daughter Sara In his spare time methodologies and open source software in mission critical he likes to run marathons and continually bugs his friends applications He received Bachelor's and Masters degrees to join him in Computer Science from MIT and lives with his wife and two children near Pasadena CA Khawaja Shams joined the Planning Software Systems group at the NASA Jet Propulsion Laboratory in 2005 and he has since been focused on development of OSGI-based web services to enable Maestro's rich client applications His prior work experience includes employment at Malin Space Science Systems and the Internet Protocol Team at Nokia Mobile Phones Khawaja earned a Master's degree in Computer Sciencetfom Cornell University and a Bachelors degree in Computer Science from University of California San Diego Khawajaes current research interests include browser-based telemetry monitoring systems for robotics peer-to-peer systems and PESTful web based services memobern of Tathe a Uplaning edrsosbefrpann Sotwae systems itgropatigwtthsceetam an theadn Jth Pnieropliongemtruhtedyt a Laboratiory.o HER recer ived rton uis,h oe fomt bigaprofthe Ca Monaetoeabinthlasnfr woredME ponetiherueo the Sciencelanin sQfta sotae develsopingeoe frte asScec Lactivit PMLane and Seqene InHLTEgRatono Engineer constructing validating..and.veri.ying..and.bundling sequences used to command the MER rovers....He.moved onto being a Tactical Uplink Lead responsible for planning the sequences integratingwith the science team an softare.o Hey iorsalso a developer fortheMar.Scenc Lander MSL and the ATHLETE Robot project    investigating robots to be sent to the Moon Jay is married 12 


11 Xiao Yang L Haizhon S Choi 2004 Protection and Guarantee for Video and Voice Traffic in IEEE 802.1 le Wireless LANs INFOCOM 2004 Twenty-third Annual Joint Conference of the IEEE Computer and Communication Societies Volume 3 Issue 7-11 21522162 12 W Spearman J Martin A Distributed Adaptive Algorithm for QoS in 802.1 le Wireless Networks Proceedings of the 2007 International Symposium on Performance Evaluation of Computer and Telecommunication Systems SPECTS'07 San Diego CA July 2007 pp 379-386 13 Lim L.W Malik R Tan P.Y Apichaichalermwongse C Ando K Harada Y Panasonic Singapore Labs A QoS Scheduler for IEEE 802.1l e WLANs Consumer Communications and Networking Conference 2004 pp 199-204 14 V Vleeschauwer J Janssen G Petit and F Poppe Quality bounds for packetized voice transport Alcatel Tech Rep 1st Quarter 2000 15 ITU Series H Audiovisual and Multimedia Systems Infrastructure of audiovisual services Coding of moving video H.264 03/2005 International Telecommunication Union 12 BIOGRAPHY cooperative signal received his B.S Engineering from respectively processing and sensor networks He M.S and Ph.D degree in Electrical UCLA in 1993 1995 and 2000 Will Spearman is a Master's Candidate at Clemson University's School of Computing His work focuses on QoS in 802.cle and wireless networks His background includes a B.S in Psychology with a minor focus in Computer Science He currently is employed at Network Appliance Inc Dr Jim Martin is an Assistant Professor in the School of Computing at Clemson University His research interests include broadband access autonomic computing Internet protocols and network performance analysis He has received funding from NASA the Department of Justice BMW IBM and Cisco Dr Martin received his Ph.D from North Carolina State University Prior to joining Clemson Dr Martin was a consultant for Gartner and prior to that a software engineer for IBM Jay Gao joined the Jet Propulsion Laboratory in 2001 and is currently a senior research staff in the Communications Networks Group in the Telecommunication Research and Architecture section His research is primarily focused on space-based wireless communications and networking with emphasis on applications for the Mars Network He is currently conducting research for developing quality-of-service QoS protocols for the envisioned Interplanetary Network IPN and study optimization and protocols for deep space Ka-band communications He also supports requirements definition and interface design activities for the Department of Defense's Transformational Communications MilSatcom project and system engineering effort for NASA's Exploration System and Mission Directorate ESMD supporting the Constellation Program for return of human to the Moon and Mars Other research interests include optical-based sensorweb discrete event simulation of distributed communication/sensor systems energy efficient routing and self-organization algorithm for 13 


  14  Figure 5:  Site B1 Terrain horizon ma sk with 1 degree azimuth spacing  Figure 6:  Site B1 Terrain horizon mask with 1 de gree azimuth spacing, in e quatorial coordinates 


  15  Figure 7: Lunar South Pole Solar Illumination Yearly Average  Figure 8:  Lunar South Pole DTE Visibility Yearly Average 


  16  Figure 9: Lunar North Pole Sola r Illumination Yearly Average  Figure 10:  Lunar North Pole D TE Visibility Yearly Average 


  17  Figure 11: Site A1 Elevation Topography  Figure 12: Site A1 Yearly Average Solar Illumination and DTE visibility, Medium Resolution 


  18   Figure 13:  Site LB Te rrain Horizon Mask  Figure 14:  Theory and Computed values of Average Yearly Solar Illumination 


  19  Figure 15:  Theory and Computed values of Average Yearly DTE Communication  Figure 16:  Heliostat Mirror Design to Eliminate Cable Wrap 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


