Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 MINING CONSTRAINED CUBE GRADIENT USING CONDENSED CUBE YU-BA0 LIU YU-CAI FENG JIAN-LIN FENG School of Computer Science, Huazhong University of Science and Technology, Wuhan, China 430074 E-MAIL yubliu0263.net  Abstract Constrained cube gradient mining is an important mining task and has broad applications. The goal of constrained cube gradient mining is to extract the interesting pairs of gradient probe cell from a data cube The constrained cube gradient mining 
faces the obstacle of large requirements in time and space for the generating of combination of gradient cells and probe cells In this paper we explore the condensed cube approach that is a novel and effEcient data organization technique to the mining of constrained cube gradient A new algorithm based on the condensed cube approach is developed through the extension of the existing efficient mining algorithm LiveSet-driven The results of experiments show our algorithm is more effective than the existing algorithm on the performance of mining constrained 
cube gradient Key words Data cube; Constrained cube gradient Condensed cube Cube computation 1 Introduction Many interesting applications may need to analyze the changes of measures in multidimensional space 16\222 The problem of mining changes of measures in a multidimensional space was first proposed by Imielinski et al as a cubegrade problem 31 which can be viewed as a generalization of association rules 221I and data cubes 51 It studies how changes of measures aggregates of interest are associated with the changes in the underlying characteristics of sectors where the changes in sector characteristics are expressed in terms of dimensions of the cube and are limited 
to specialization drill-down generalization \(roll-up\and mutation a change in one of the cube\222s dimensions\Cubegrades are significantly more expressive than association rules in capturing trends and patterns in data because they use arbitrary aggregate measures not just COUNT as association rules do Cubegrades can also support sophisticated 223what if\222 analysis tasks dealing with behavior of arbitrary aggregates over different database segments As such, cubegrades can be useful in marketing, sales analysis, and other typical data mining applications in business However it poses serious challenges on both understandability of results and on computational efficiency and scalability The constrained cube gradient mining r61 represents a confined but interesting version of the cubegrade problem The goal of constrained cube gradient mining is to extract the 0-7803-7508-4/02/$17.00 02002 IEEE interesting 
pairs of gradient-probe cell satisfying the specifying constraints from a data cube Though the constraints can restrict the search space of cells the constrained cube gradient mining still faces the obstacle of Iarge requirements in time and space for the generating of combination of gradient cells and probe cells since the data cube is always very large. Motivated by the condensed cube technique 41 that can reduce dramatically the size of cube itself and the cube computation time we explore this technique to the mining of constrained cube gradient in this paper A new algorithm named as eLiveSet based on the condensed cube approach is developed through the extension of the existing LiveSet algorithm In our algorithm we focus on the issue on how to derive the live set of gradient cells that 
are stored in a condensed format since the derivation of live set of gradient cells is a key issue for the LiveSet-style algorithm There are other related works including the cube computation r1*21 and the mining of constrained association rules 222I Recentla f investigated environment named as CubeExplorer online exploration of data cube is developed 2 An overview of condensed cube approach The semantics of the CUBE BY operator is to partition a relation into groups based on the values of the attributes specified in the CUBE BY operator and then apply aggregations functions to each of such groups, the CUBE 
BY operator computes GROUP BY corresponding to all possible combinations of attributes in the CUBE BY operator In general a CUBE BY operator on n attributes computes 2\224 GROUP BYs or cuboids The attributes of a relation table can be divided into dimension attributes and measure attributes A tuple with dimension attributes and measure attributes in a data cube is called a cell Given a set of dimension attributes SDc A if r is the only tuple in its partition when the relation table is partitioned on SD we say tuple r is a single tuple on 
SD and SD is called the single dimensions of r 14 Assumed that relation table R containing three relation tuples with attributes A B C M and M is the measure attribute and the aggregate function is SUM function 0,1,1,50 l,l,l,lOO and 2,3,1,60 When R is partitioned on dimension A i.e cuboid A the partition in which the dimension value of A is equal to 0 contains a single tuple 0 1 1 50 Then 0 1, 1 50 is a single tuple and A is its SD A single tuple and its SD can 1028 


Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 express many cells in a cube and these cells can be viewed as being condensed into the single tuple r In general, given a single tuple an and its single dimensions SD=[a,,..,a KiSjSn the complete set of cells condensed by the single tuple r and it\222s SD is denoted as ExpandSet\(r and the dimension value of any cell r\222E ExpandSet\(r can be computed by the following Expand principles 1 r\222\(ak ak if akE SD 2 ak or r\222\(ak if bj 3 r\222\(ak if a SD and llkcj For example suppose the relation table tuple r=\(A=l B=2 C=3 D=4 is a single tuple and its SD=[AC According to the Expand principles, each dimension value of the cells in ExpandSet\(r\is as follows A=l B C=3 and D=4 or D where 223*\224 denotes the value 223all\224 i.e aggregated to the highest level on this dimension In other words, the two cells A=l B C=3 D=4 and A=l B C=3 D in ExpandSet\(r\are condensed into the single tuple r In an and its SD a,,..,a l<i<jSn there are the number of 2n-J cells contained in ExpandSet\(r in other words 2\224-\222 cells are condensed by the single tuple r. Notice that all the cells in ExpandSet\(r\have the same aggregation value since all of them are only aggregated from the same single tuple In addition, the operation Expand is a simple operation that requires copying the original tuple and replacing certain attributes value with a special value Then the Expand operator needs no additional cost and no aggregation or other computation is required According to the above Expand principles and the definition of single tuple, we also have two important properties on a single tuple r I The non dimension values of the cells in ExpandSet\(r 222are all derivedfrom the single tuple rand equal to the corresponding dimensions values of r dimension values on SD and the cells that have the same non  dimension values on SD are contained in the set ExpandSet\(r The properties of single tuple construct the basis of the condensed cube In a condensed cube we only need physically store the single tuple together with an extra field to store the single dimensions information of the single tuple The cells can be expressed by the single tuple that are not stored physically When needed these cells can be generated through the expand operator of the single tuple The SD fields of these non-single tuples are equal to 0 in a condensed cube. These non-single tuples can be viewed as the general cells in a general data cube i.e., non-condensed cube since they don\222t condense any cells A condensed 2 All of cells in ExpandSet\(r\share the same non  221 Notice that we require SD#A where A denotes the all r\(ak denotes the value of dimension ak llkln In this paper, whenever there is no confusion we use the concatenation of dimension names to represent the set consisting of those dimensions For example, the {AB  is a short of  A,B   dimensions of a cube cube can be computed through our BU-BST algorithm 41 which is basically a modified version of the original BUC algorithm 221I Similar to the BUC algorithm BU-BST algorithm explores the data cube space using a bottom-up depth-first munner i.e., the recursive partitioning manner 3 The mining of constrained cube gradient 3.1 The problem definition Given two distinct cells c1 and c2 of a data cube D of a given relation table R with n dimensions c1 is an ancestor of c2 and c2 is a descendant of cl iff on every dimension attributes, either c1 or c2 shares the same value or c1 has value 223*\224 where 223*\224 indicates 223all\224 CI is a sibling of c2 and vice versa iff c1 and c2 have identical values in all dimensions except one dimension in which neither has value  For simplicity we sometimes say c1 is similar to c2 if c1 is a descendant an ancestor or a sibling of c2 A significance constraint Csig is usually defined as conditions on measure attributes A probe constraint cprb is usually defined as conditions on dimension attributes and is used to select a set of user-desired cells A cell c is significant iff Csig\(c\and a cell c is a probe cell iff c is significant and Cp&\(c\Given a single tuple c If c is significant all of the cells condensed into c are significant because they have the same measure to c In addition we say c satisfies the probe constraint Cprb if each dimension of c has the dimension values that satisfy the corresponding dimension value constraints of cprb For example, assume that the cell c=\(1,2,3,4 is a single tuple and its SD={B}. Assume that Cprb=\(A B C=4 D=3 Then the cell c does not satisfy the probe constraint because the dimension of C and D of the cell c have no dimension values that satisfy the value constraints of dimension C and D in Cph. Assume that Cprb=\(A B C=3, D=*\Then the cell c satisfies the probe constraint Cph and dimension C has dimension value 3 satisfies the corresponding dimension constraint of Cprb The complete set of probe cells is denoted as P The set of significant cells that may have gradient relationship with a set of probe cells P are called the gradient cells of P The gradient constraint has the form cgrad\(cg  z\(g\(c cp 0 v where 0 is in I 2   v is a constant value, and g is a gradient function A gradient cell cg is interesting with respect to a probe cell C,EP iff cg is significant cg and cp satisfy similar relationship and Cgrd\(cg  Formally given a relation table R a significant constraint Csig a probe constraint C and a gradient constraint cgrad\(cg c the constrained cube gradient problem is to find all the interesting gradient-probe pairs cg, cp\such that Cgrd\(cg  3.2 The LiveSet algorithm The LiveSet-driven algorithm short for LiveSet is an efficient algorithm for the constrained cube gradient mining The main framework of LiveSet algorithm is as follows: \(1 1029 


Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 1030 Apply a cube computation algorithm such as H-cubing 21 or BUC 221I to compute the set of probe cells P from the relation table R using both the significance and probe constraints 2 Use a cube computation algorithm to find all interesting gradient-probe cell pairs that is, to determine which gradient cell should be associated with which probe cells The live set method is used in second step In general the live set of a gradient cell cg which is denoted as LiveSet\(c is the set of probe cells cp such that it is possible that cg cp\is an interesting gradient-probe pair for some descendant cell cg\222 of cg The major advantage of live set method is that as generating an interesting gradient probe cell pairs we only need to compare the gradient cell with its related probe cells i.e live set but the complete set of probe cells P Thus the derivation of live set of gradient cell is a key issue of LiveSet-style algorithm. The matching analysis method is used to derive the LiveSet Let cp=\(dpl dp2  dpm\be a probe cell and cg=\(dgl dg2  dm be a gradient cell The number of solid-mismatches between cp and cg is the number of dimensions in which both values are not  but are not matched i.e of different values The number of mismatches between cp and cg is the number of dimensions in which cp is  but cg is not It is noticed that the notion of mismatches is not symmetric i.e if cg has  value on a dimension but cp has a non-* value on the same dimension this is not considered a mismatch A probe cell cp is matchable with a gradient cell cg if either cg or cp has no solid-mismatch or they have exact one solid mismatches but no mismatch Two important properties are used to derive the live set Propertyl Assume cgl and cg2 are two gradient cells and cg2 is a descendant of cgl Then LiveSet\(cp cLiveSet\(cgl This property ensures that we can produce the live set of a descendant cell from that of the ancestor cell Property2 Let cp be a probe cell and cg be a gradient cell If cp is matchabel with cg then cp E LiveSet\(c otherwise cp e LiveSet\(c This property shows the way to derive the live set that is using the matching analysis method 4 The algorithm based on condensed cube Definition1 live set Since the probe cells and the gradient probe cells are likely single tuples the definition of LiveSet of gradient cell is extended as follows 1 The gradient cell cg is a non-single tuple For a probe cell cp that is a single tuple we say C~E LiveSet\(c if there exists cell cp ExpandSet\(q and ci cp\222 is a possibly interesting gradient-probe cell pair for some descendant cells ci of cg On the other hand if the cp is a non-single tuple we say cp LiveSet\(c if cg\222 cp is a possibly interesting gradient-probe cell pair for some descendant cells cg\222 of cg The gradient cell cg is a single tuple Since cg likely condenses many gradient cells we define LiveSet\(cg c where C@E ExpandSet\(c and llillExpandSet\(c and I ExpandSet\(c denote the cardinal number of the set ExpandSet Definition2 basic cell Given a single tuple r the cell in ExpandSet r that is obtained by taking the  value for each dimension of r except the dimensions in SD of r, is called the basic cell of the single tuple r and is denoted as cb For example, assume that a gradient cell cg=\(A=lr B=2 C=3 D=4 is a single tuple and its SD=\(BC then the basic cell an ancestor of the other cells in ExpandSet\(r\because those cells are obtained by taking the non value for the dimensions that is not included in SD from cb Definition3 \(potential A gradient cell cg is potential to expand higher dimension i.e further partition if it satisfies the following conditions 1 cg is not a single tuple 2 Csig\(cg 3 C@\(c,,c for some probe cell cp in LiveSet\(cg Two basic lemmas are developed for the derivation of the live set Lemma 1 Given a gradient cell cg and a probe cell cp that is a single tuple If cp is not matchable with cg then for any cell CE ExpandSet\(c c is not similar to cg\222 that is a descendant of cg Proof Suppose that cp is not matchable with cg According to the definition of matchable cp and cg have at least two solid-mismatches \(there are not mismatches between q and cg because cp is a single tuple and has no  dimension value\Without loss of generalization, we assume that cp has different non values with cg on dimensions al  al 22 Both cg and cg have the same non values on dimensions al  a because cg\222 is a descendant of cg Thus cp also has different non-* values with cg on dimensions al  ai 22 Two cases raise here i.e a  al and at _ a,}nSD=0 where SD is the single dimensions of the single tuple cp I  Suppose  al  a Without loss of generalization we suppose al  a,}nSD=\(al  a j2l From the property 2 of the single tuple it is known that all of cells in ExpandSet\(c share the same non values on dimensions a1  a Thus c has different non values with cg on dimensions al  a Suppose j>l that is c and cg\222 have at least two different non values on dimensions al  a According to the definition of similar c is not similar to cg Suppose j=1 Since cp and cg\222 have at least two different non dimension values, there is at least another dimension ad wherelldli in which c and cg\222 have different non value. Again, according to the Expand principles, the value of dimension ad of any cell CE ExpandSet\(c is either equal to a non-* value ad where cp\(%\denotes the value of cp on dimension ad or equal to  If equal to a non value c and cg\222 have different non dimension values on dimension ak and a Then c is not similar to cg\222 If equal to  value c and cg\222 have different non dimension values on a except the dimension in which c has  but cg has non  Thus c is also not similar to cg\222  I1   al   al nSD=0 22 According to the values of dimensions al   which is computed according to the Expand principles the cells in Expandset can be divided into three types a the cells in which the values of Of Cg Cb 2,3 It iS Clear that CbE ExpandSet\(r and Cb iS 


Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 dimensions al  a are all equal to  b the cells in which only one dimension value is non and the others are  c the cells in which more than two dimension values are non For the case of a suppose c is similar to c,\222 there is only a case that c is an ancestor of c,\222 Because the values of dimensions al of cell c are  value whereas the values of dimensions al  aJ of cell cgl are non-*\Again, according to the definition of ancestor c and ci should share the same value or c has  on the remaining dimeusions A-\(al  a where A denotes the all dimensions of cube. Due to c has non dimension value on SD that is contained in A-\(al  ai c and c,\222 should share the same non-* value on SD. Then c should be contained in Expandset according to the property\(2 of the single tuple. If C,\222E ExpandSet\(c the non-* values of dimensions of c,\222 are all derived from the corresponding dimension values of cp Again because cg is an ancestor of c,\222 the non values of the corresponding dimensions of cg should be the same to that of c,\222 Then the non dimension values of cg are also derived from cp So c and cg surely have no two solid-mismatches It is in contradiction to the supposition cp is not matchable with c So c is not similar to c,\222 For the case of b suppose the only one different non value dimension is a lljli and the  value dimension is ak Kkli k#j Then c and c,\222 have one different non value on dimension aj except one dimension ak in which c has  but cg has non value so c is not similar to c,\222 For the case of c c and c,\222 have more than two different non-* dimension values so c is not similar to Lemma 2 Given two gradient cells c cg\222 and c,\222 is obtained from cg in the depth-first order We have LiveSet c,\222 LiveSet cg Proof  I  cg is a non-single tuple For every cpeLiveSet c,\222 there are two special cases cp is either a single tuple or a non-single tuple In the case that cp is a non-single tuple Since cpELiveSet c,\222 it is known that cg cp is a possible gradient-probe cells pair, where cg is a descendant of c,\222 Since c,\222 is obtained from cg in the depth-first order that is c,\222 is obtained by partitioning cg on some dimensions i.e by taking specific values for some dimensions in which cg has  value Then c,\222 is a descendant of cg Since cg is a descendant of c,\222 and c,\222 is a descendant of cg cc is also a descendant of cg according to the definition of descendant Since c,\224 cp\is a possible gradient-probe cells pair we also have cPe LiveSet c On the other hand, assume that cp is a single tuple It is known that there exists cell 222~ExpandSet\(c and cc c is a possible gradient-probe cells pair where cg is a descendant of cg\222 and c Thus we have E LiveSet c In a word for every C~E LiveSet c,\222 we have cPe LiveSet c Therefore LiveSet c,\222 LiveSet c is held 1I c,\222 is a single tuple Then we have LiveSet\(c,\222 cgi\222 where C~\222E ExpandSet\(c,\222 and llill ExpandSet\(c,\222 Assume that cb\222 is the basic cell of cg\222 and we have Cb\222E ExpandSet\(c,\222 For any cell cgi\222 in ExpandSet\(c,\222 that Cg  is not equal to cb\222 it is known that cgi\222 is a descendant of cb\222 Since both cgi\222 and cb\222 are non-single tuples according to the above proof of  I  of lemma2, we have LiveSet\(cgi\222 E LiveSet\(cb\222 So uLiveSet\(c@\222 E LiveSet\(cb\222 where Cgi\222 fcb\222 and llillExpandSet\(c,\222 Then uLiveSet\(cgi\222 LiveSet\(c cb\222 that is uLiveSet\(c@\222 cb\222 for every cell C@\222E ExpandSet\(c,\222 Again c,\222 is a single tuple Then c,\222 is obtained by partitioning cg on the dimensions of SD of c,\222 in the depth-first order and cg has  value on some dimensions of SD. According to the definition of basic cell the basic cell of c,\222 cb\222 is obtained by taking more special values i.e non value for the dimensions of SD of c,\222 Then cb\222 is a descendant of c Similarly according to the above proof of  I  of lemma2 we also have LiveSet\(cb\222 c Therefore we have LiveSet\(c,\222 c,\222 c c LiveSet c  From lemma1 and lemma2 we can derive LiveSet\(c,\222 by pruning the single tuple that is not matchable with cg\222 or the basic cell of c,\222 cb\222 from LiveSet\(c Note that there likely exist another kind of cells in the LiveSet\(c that is the non-single tuples  For these cells they can treated as the cells of a general cube since they don\222t condense any cells and they can be pruned from LiveSet\(c by the propery2 in the section3 In summary for a given potential gradient cell cg and it\222s descendant cell c,\222 we can derive LiveSet\(c,\222 from LiveSet\(c by pruning the cells that are not matchable with c,\222 or cb\222 that is a basic cell of c,\222 from LiveSet\(c Based on the above two lemmas we develop a novel algorithm eLiveSet through the extension of LiveSet algorithm The framework of eLiveSet algorithm is similar to that of the LiveSet algorithm The description of the eLiveSet algorithm is given below Algorithm eLiveSet Input A relation table R a Csig a Cph and a Cpd Output The complete set of interesting gradient-probe cell pairs Method 1 2 3 Apply the BU-BST algorithm to computing set of probe cells P from R Initialize the potential gradient cell to cell cg    and LiveSet\(c Use the bottom-up manner, depth-first order BU-BST cube algorithm to find all interesting gradient-probe cell pairs If cg is significant then for every cp E LiveSet\(c output the interesting gradient-probe cell pairs e,e\222 If cg and cp are single tuples then ee ExpandSet\(c and e\222\200 Expandset otherwise e=cg \(or e\222 If LiveSet\(c is empty or cg has no potential to grow terminate this branch and recursively backtrack to process the next cell according to the depth-first order If cg has potential to grow expand it to the next level according to the depth-first order If a 1031 


Proceedings of the First International Conference on Machine Learning and Cybernetics Beijing 4-5 November 2002 descendant cell cg\222 of cg is processed from this expansion, derive LiveSet\(c,\222 from LiveSet\(c It is worth mentioning that in our study, the probe cells in LiveSet\(c are assumed to be stored in the ascending order according to the measures Thus if the measure of one probe cell cp can not satisfy the constraint Cgrad\(cg,cp\then all the probe cells following it will not satisfy the gradient constraints function and hence can be pruned from LiveSet\(c In the case that the LiveSet\(c is very large set the hash table method can be used to fast access the Liveset cg  5 Experiments In this section we present our experimental results on the performance \(in terms of time of mining constrained cube gradient from a data cube All experiments are conducted on a PC platform with an Intel PentiumIII 500M CPU 218M RAM and Windows 2000 OS Two mining algorithms are compared in our experiments The first algorithm is eLiveSet that is constructed on a condensed cube that is computed by the BU-BST algorithm. The other is LiveSet that is constructed on a general cube that is computed by BUC algorithm. The performance of mining cube gradient from cube with the two algorithms is compared All experiments are performed using synthetic algorithmically generated\datasets of 1M \(=1,024\tuples that is uniformly distributed and the number of dimension is set to 9 and the cardinality of all attributes is set to 100. The aggregate function used in the cube computation algorithm is SUM function. The performance of the algorithms with different constraints is shown in Figl In Figl the significance constraints C,,,>O and the gradient constraints Cd=m\(c cp The value of the non value dimension in probe constraint C is set to 1,50 The number of non-* value dimension is varied from 1 to 9 The non value dimensions in cprb are fixed on the first k dimensions 1%9 respectively The larger the number of non value dimensions, the smaller number of probe cells that satisfy the cprb In Figl the scalability of the two algorithms over probe cells is shown The runtime of the two algorithms is less with the increase of the number of non value dimensions in C because the number of non  value dimensions in Cp,,,\222 larger and the probe cells satisfied Cph are larger However eLiveSet algorithm is faster than LiveSet algorithm The reason is that many cells are condensed into the condensed cube and the search space of the cells including the probe cells and the gradient cells to be handled reduces dramatically in eLiveSet algorithm Similarly eLiveSet algorithm is also faster than LiveSet with Cslg and Cgd Due to the limitation of space the corresponding figures of results are not given P 1300.00 P 1100.00 2 2 900.00 2 700.00 500.00 a2 3 300.00 100.00 123456789 The number of non value dimension of Cprb Fig 1 The scalability with probe cells 6 Conclusions In this paper we study the problem of mining constrained cube gradient using the condensed cube approach A new algorithm, eLiveSet is developed by the extension of the existing efficient algorithm LiveSet The experiment results show eLiveSet is efficient and scalable There are many interesting issues in our future work for example, instead of computing the cube gradient on a non materialized cube, as shown in this paper, we can compute the cube gradient on a materialized cube since a materialized cube is always pre-computed in a datawarehouse environment In addition, further enhancing the performance of eLiveSet algorithm is also our interested topic Acknowledgements This paper is supported by the e-government project of National Science and Technology Ministry of China No 2001BA110B01 References K.Beyer and R.Ramakrishnan Bottom-up computation of sparse and iceberg cubes ACM SIGMOD 1999 J.Han J.Pei G.Dong and K.Wang Efficient computation of iceberg cubes with complex measures ACM SIGMOD 2001 T.Imielinski L.Khachiyan and A.Abdulghani Cubegrades Generalization Association Rules Tech Rep Dept Computer Science Rutgers University Aug. 2000 Wei Wang, Jianlin Feng Hongjun Lu and Jeffrey Xu Yu Condensed Cube An Effective Approach to Reducing Data Cube Size IEEE ICDE 2002 Jim Gray Adam Bosworth Andrew Layman and Hamid Pirahesh Data Cube A Relational Aggregation Operator Generalizing Group-By, Cross Tab, and Sub-Totals. IEEE ICDE 1996 1032 


Proceedmgs of the First International Conference on Machme Learning and Cybernetics Beijing 4-5 November 2002 161 171 81 191 Guozhu Dong, Jiawei Han, Joyee Lam Jean Pei and Ke Wang Mining Multi-Dimensional Constrained Gradients in Data Cubes VLDB 2001 R.Ng L.V.S.Lakshmanan J.Han and A.Pang Exploratory mining and pruning optimizations of constraned association rules. ACM SIGMOD 1998 R.Agrawal T.Imielinski and ASwami Fast algorithms for mining association ruels. VLDB 1994 Jiawei Han, Jianyong Wang Guozhu Dong, Jian Pei Ke Wang CubeExplorer online exploration of data cube ACM SIGMOD 2002 1033 


4.1 Simulation model In the section we evaluate the performances of the three algorithms including BASIC 1121 Cumulate 12 and GMAR on a DELL PowerEdge 4400 Server with Intel" Xeon Processor and 756MB main memory running Windows 2000 server All the experimental data are generated randomly and stored on a local 30GB SCSI Disk Ultra 160 with a RAID controller The relative simulation parameters are shown in Table 1 To make our data representative, we generate two types of databases in the experiments i.e DENSE databases and SPARSE databases. Each item in the DENSE database is randomly generated from a pool P called potentially frequent itemsets with size 300 while each item in the SPARSE database is randomly generated from a pool N i.e the set of all the items\with size 1000 Since the items in the DENSE database are more clustered than those in the SPARSE database, larger frequent itemsets will probably be produced in the DENSE database for the same minimum support Besides we use the notations T for average number of items per transaction I for average number of items in a frequent itemset, and D for number of transactions. For example the experiment labeled with 7lOB.DIK represents the simulation environment with IO items on the average per transaction 3 items on the average in a frequent itemset, and 1000 transactions in total Table 1 Simulation parameters with default values ID INumber of transactions 1000-500,000 7 INumber ofthe items per transaction 15-15 P INumber of potentially frequent 1300  litemsets I I Number of the items in a frequent 12-5 4.2 Experimental results Experiment 1 In the experiment we explore the execution time of BASIC Cumulate and GMAR algorithms for the environment 7lO.L3,DIK under different minimum support and minimum confidence pairs as shown in Figure 9 In the figure, we find that our algorithm GMAR is almost faster 2-16 times than BASIC especially for larger minimum support and minimum confidence pairs whereas Cumulate is only faster 1.3-1.5 times than BASIC although R Srikant and R Agrawal claimed that Cumulate runs faster 2-5 times than BASIC 1121 In general the larger the minimum support and minimum confidence pair is the faster the execution time of the three algorithms becomes To be fair to all algorithms, we have added the extra time of generating original frequent itemsets and association rules for GMAR However the time is helow 1 of total execution time thus we do not show it in the figure h n  B a 3 i.m.26 i.ma 1.~1.3 i.mm 1.740.3 I.w Figure 9 Execution time for different pairs minimum support  Confidence Experiment 2 In the experiment we extend Experiment 1 by fixing the minimum support IS and observe their variations For the minimum support 1.5 all the algorithms except GMAR are not sensitive to the changes of the minimum confidences as shown in Figure IO The reason is that larger minimum confidences will make GMAR prune more irrelevant rules. Nevertheless, GMAR is still in the first rank D  2 4M cm 22 CUlIIUlStt  gm BASIC 8 Irn 0 0 0.26 0.28 0.3 0.32 0.34 0.3 minimum confidence Figure 10 Execution time for different minimum confidences Experiment 3 In the experiment, we explore the execution time of the three algorithms for the environment ZlO.I3.DxK i.e different numbers of transactions generated in the SPARSE database and in the DENSE database as shown in Figure Il.\(a and b respectively. Both cases have the same minimum confidence 0.3 However to get comparable number of frequent itemsets, we set a smaller minimum support 1 in the SPARSE case and a larger 233 


minimum support 2 in the DENSE case As expected GMAR is still the hest one among them in the SPARSE and DENSE case especially when there are a huge amount of transactions From the both cases we find that much more frequent itemsets are generated in the DENSE database than in SPARSE database so that BASIC and Cumulate are not practicable candidates there B Ixa 8 Imo 222ixa 80 0 Imo 3033 m m loo30 number of transactions Figure 11 a Execution time for different numbers of transactions in the SPARSE database cumh?t  6 loo30  rma 2230 lorn m 5m 7cw IwD3 number of transactions Figure 1 l.\(b Execution time for different numbers of transactions in the DENSE database 5 Conclusions In the paper we try to find the association rules between the items at different levels in the taxonomy tree under the assumption that original frequent itemsets and association rules have already been generated beforehand The primary challenge is how to make use of the original frequent itemsets and association rules to directly generate new generalized association rules rather than rescanning the database In the proposed algorithm GMAR we use join methods and pruning techniques to generate new generalized association rules Through several comprehensive experiments we find that the GMAR algorithm is much better than BASIC and Cumulate algorithms since it generates fewer candidate itemsets and furthermore prunes a large amount of irrelevant rules based on the minimum confidence 6 Acknowledgments This research was supported in part by the National Science Council Taiwan under contract NSC-90-22 13-E-224-026 7 References I R Agrawal T Imielinski and A Swami 223Mining Association Rules between Sets of Items in Large Databases,\224 Pmc ACM International Conference on Mananement of Data  1993 pp 207-216 121 R Anrawal and R Srikant 223Fast Alaorithms for Mmine Adsociation Rules,\224 Pmc 2Vh Internationaiconference on Ve Large Data Bases 1994 pp 487-499 131 Yong-Jian Fu 223Data Mining,\224 IEEE Potentials Yol 16 No 4 1997 pp 18-20 141 Jia-Wei Han and Yong-Jian Fu 223Mining Multiplelevel Association Rules in Large Databases,\224 IEEE Transactions on Knowledge and Data Engineering Yo 11 No 5 1999 pp 798-805 5 Iia-Wei Han and Micheline Kamber Data Mining Concepts and Techniques Morgan Kaufmann Publishers 2001 6 Iia-Wei Han lian Pei and Yi-Wen Yin 223Mining Frequent Patterns without Candidate Generation,\224 Pmc ACM International Conference on Management o Data 2000 pp 1-12 7 Mon-Fong Jim Shian-Shyong Tseng and Shan-Yi Lia 223Data Types Generalization for Data Mining Algorithms,\224 Pmc IEEE International Conference on stems Man and Cybernetics 1999 pp 928-933 8 Bing Liu Wynne Hsu and Yi-Ming Ma 223Minin Association Rules with Multiple Minimum Supports,\224 Pmc 5 ACM International Conference on Knowledne Discovery and B DataMining 1999 pp 337-341 191 J S Park M S Cben and P S Yu 223An Effective  H&h-based Algorithm for Mining Association Rules,\224 Pmc ACM Internotional Conjerence on Mamgement o Data 1995 pp 175-186 IO A Savasere E Omiecinski, and S Navathe 223An Efficient Algorithm for Mining Association Rules in Large Databases,\224 P 21\224 lnternationk Conference on Very La Data Bases 1995 pp 432-443 Ill Pradeep Shenoy layant Haritsa S Sudarshan Gaurav Bhalotia, Mayank Bawa and Devavrat Shah 223Turbo-charging Vertical Mining of Large Databases,\224 Pmc ACM International Conference on Management of Data 2000 pp 22-33 I21 R Srikant and R Agrawal 223Mining Generalized Association Rules,\224 Pme 21\224 International Conference on Very Large DataBases 1995 pp 407-419 I31 S Y Sung K Wag and L. Chua 223Data Mining in a Large Database Environment,\224 Pmc IEEE International Conference on Systems Man and Cybernetics 1996 pp 988-993 I41 H Toivonen 223Sampling Large Databases for Association Rules,\224 Pmc 2T\221 International Conference on Very Large Data Bases 1996 pp 134-145 I51 Ming-Cheng Tseng Wen-Yang Lin and Been-Chian Chien 223Maintenance of Generalized  Association Rules with Multiple Minimum Supports,\224 Pmc 9th IFSA World Congress and 20th NAFIPSInternational Conference 2001 pp 1294-1299 234 


In Figures 10 and 11 we see MAFIA is approximately four to five times faster than Depthproject on both the Connect-4 and Mushroom datasets for all support levels tested down to 10 support in Connect-4 and 0.1 in Mushroom For Connect-4 the increased efficiency of itemset generation and support counting in MAFIA versus Depthproject explains the improvement Connect 4 contains an order of magnitude more transactions than the other two datasets 67,557 transactions amplifying the MAFIA advantage in generation and counting For Mushroom the improvement is best explained by how often parent-equivalence pruning PEP holds especially for the lowest supports tested The dramatic effect PEP has on reducing the number of itemsets generated and counted is shown in Table 1 The entries in the table are the reduction factors due to PEP in the presence of all other pruning methods for the first eight levels of the tree The reduction factor is defined as  itemsets counted at depth k without PEP   itemsets counted at depth k with PEP In the first four levels Mushroom has the greatest reduction in number of itemsets generated and counted This leads to a much greater reduction in the overall search space than for the other datasets since the reduction is so great at highest levels of the tree In Figure 12 we see that MAFIA is only a factor of two better than Depthproject on the dataset Chess The extremely low number of transactions in Chess 3196 transactions and the small number of frequent 1-items at low supports only 54 at lowest tested support muted the factors that MAFIA relies on to improve over Depthproject Table 1 shows the reduction in itemsets using PEP for Chess was about an order of magnitude lower compared to the other two data sets for all depths To test the counting conjecture we ran an experiment that vertically scaled the Chess dataset and fixed the support at 50 This keeps the search space constant while varying only the generation and counting efficiency differences between MAFIA and Depthproject The result is shown in Figure 13 We notice both algorithms scale linearly with the database size but MAFIA is about five times faster than Depthproject Similar results were found for the other datasets as well Thus we see MAFIA scales very well with the number of transactions 5.3 Effects Of Compression To isolate the effect of the compression schema on performance experiments with varying rebuilding threshold values we conducted The most interesting result is on a scaled version of Connect-4, displayed in Figure 14 The Connect-4 dataset was scaled vertically three times so the total number of transactions is approximately 200,000 Three different values for rebuilding-threshold were used 0 corresponding to no compression 1 compression immediately and all subsequent operations performed on compressed bitmaps\and an optimized value determined empirically We see for higher supports above 40 compression has a negligible effect but at the lowest supports compression can be quite beneficial e.g at 10 support compression yields an improvement factor of 3.6 However the small difference between compressing immediately and finding an optimal compression point is not so easily explained The greatest savings here is only 11 at the lowest support of Conenct-4 tested We performed another experiment where the support was fixed and the Connect-4 dataset was scaled vertically The results appear in Figure 15 The x-axis shows the scale up factor while the y-axis displays the running times We can see that the optimal compression scales the best For many transactions \(over IO6 the optimal re/-threshold outperforms compressing everywhere by approximately 35 In any case both forms of compression scale much better than no compression Compression on Scaled ConnectAdata Compression Scaleup Connectldata ALL COMP 0 5 10 15 20 25 30 100 90 80 70 60 50 40 30 20 10 0 Min Support  Scaleup Factor Figure 14 Figure 15 45 1 


6 Conclusions We presented MAFIA an algorithm for finding maximal frequent itemsets Our experimental results demonstrate that MAFIA consistently outperforms Depthproject by a factor of three to five on average The breakdown of the algorithmic components showed parent-equivalence pruning and dynamic reordering were quite beneficial in reducing the search space while relative compressiodprojection of the vertical bitmaps dramatically cuts the cost of counting supports of itemsets and increases the vertical scalability of MAFIA Acknowledgements We thank Ramesh Agarwal and Charu Aggarwal for discussing Depthproject and giving us advise on its implementation We also thank Jayant R Haritsa for his insightful comments on the MAFIA algorithm and Jiawei Han for providing us the executable of the FP-Tree algorithm This research was partly supported by an IBM Faculty Development Award and by a grant from Microsoft Research References I R Agarwal C Aggarwal and V V V Prasad A Tree Projection Algorithm for Generation of Frequent Itemsets Journal of Parallel and Distributed Computing special issue on high performance data mining to appear 2000 2 R Agrawal T Imielinski and R Srikant Mining association rules between sets of items in large databases SIGMOD May 1993  R Agrawal R Srikant Fast Algorithms for Mining Association Rules Proc of the 20th Int Conference on Very Large Databases Santiago Chile, Sept 1994  R Agrawal H Mannila R Srikant H Toivonen and A 1 Verkamo Fast Discovery of Association Rules Advances in Knowledge Discovery and Data Mining Chapter 12 AAAVMIT Press 1995 5 C C Aggarwal P S Yu Mining Large Itemsets for Association Rules Data Engineering Bulletin 21 1 23-31 1 998 6 C C Aggarwal P S Yu Online Generation of Association Rules. ICDE 1998: 402-41 1 7 R J Bayardo Efficiently mining long patterns from databases SICMOD 1998: 85-93 8 R J Bayardo and R Agrawal Mining the Most Interesting Rules SIGKDD 1999 145-154 9 S Brin R Motwani J D Ullman and S Tsur Dynamic itemset counting and implication rules for market basket data SIGMOD Record ACM Special Interest Group on Management of Data 26\(2\1997 IO B Dunkel and N Soparkar Data Organization and access for efficient data mining ICDE 1999 l 11 V Ganti J E Gehrke and R Ramakrishnan DEMON Mining and Monitoring Evolving Data. ICDE 2000: 439-448  121 D Gunopulos H Mannila and S Saluja Discovering All Most Specific Sentences by Randomized Algorithms ICDT 1997: 215-229 I31 J Han J Pei and Y Yin Mining Frequent Pattems without Candidate Generation SIGMOD Conference 2000 1  12 I41 M Holsheimer M L Kersten H Mannila and H.Toivonen A Perspective on Databases and Data Mining I51 W Lee and S J Stolfo Data mining approaches for intrusion detection Proceedings of the 7th USENIX Securiry Symposium 1998 I61 D I Lin and Z M Kedem Pincer search A new algorithm for discovering the maximum frequent sets Proc of the 6th Int Conference on Extending Database Technology EDBT Valencia Spain 1998 17 J.-L Lin M.H Dunham Mining Association Rules: Anti Skew Algorithms ICDE 1998 486-493 IS B Mobasher N Jain E H Han and J Srivastava Web mining Pattem discovery from world wide web transactions Technical Report TR-96050 Department of Computer Science University of Minnesota, Minneapolis, 1996 19 J S Park M.-S Chen P S Yu An Effective Hash Based Algorithm for Mining Association Rules SIGMOD Conference 20 N Pasquier Y Bastide R Taouil and L Lakhal Discovering frequent closed itemsets for association rules ICDT 99 398-416, Jerusalem Israel January 1999 21 J Pei J Han and R Mao CLOSET An efficient algorithm for mining frequent closed itemsets Proc of ACM SIGMOD DMKD Workshop Dallas TX May 2000 22 R Rastogi and K Shim Mining Optimized Association Rules with Categorical and Numeric Attributes ICDE 1998 Orlando, Florida, February 1998 23 L Rigoutsos and A Floratos Combinatorial pattem discovery in biological sequences The Teiresias algorithm Bioinfomatics 14 1 1998 55-67 24 R Rymon Search through Systematic Set Enumeration Proc Of Third Int'l Conf On Principles of Knowledge Representation and Reasoning 539 550 I992 25 A Savasere E Omiecinski and S Navathe An efficient algorithm for mining association rules in large databases 21st VLDB Conference 1995 26 P Shenoy J R Haritsa S Sudarshan G Bhalotia M Bawa and D Shah: Turbo-charging Vertical Mining of Large Databases SIGMOD Conference 2000: 22-33 27 R Srikant R Agrawal Mining Generalized Association Rules VLDB 1995 407-419 28 H Toivonen Sampling Large Databases for Association Rules VLDB 1996 134-145 29 K Wang Y He J Han Mining Frequent Itemsets Using Support Constraints VLDB 2000 43-52 30 G I Webb OPUS An efficient admissible algorithm for unordered search Journal of Artificial Intelligence Research 31 L Yip K K Loo B Kao D Cheung and C.K Cheng Lgen A Lattice-Based Candidate Set Generation Algorithm for I/O Efficient Association Rule Mining PAKDD 99 Beijing 1999 32 M J Zaki Scalable Algorithms for Association Mining IEEE Transactions on Knowledge and Data Engineering Vol 12 No 3 pp 372-390 May/June 2000 33 M. J. Zaki and C Hsiao CHARM An efficient algorithm for closed association rule mining RPI Technical Report 99-10 1999 KDD 1995: 150-155 1995 175-186 3~45-83 1996 452 


