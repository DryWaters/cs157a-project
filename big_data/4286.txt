WPI2 3:30 Decilsonmaking with th Boftmunn Machine by k og C  Departmet of ec and Compu Engieern Drexel Universiy Philadelph PA 19104 Department of En ring Princeton University Priceton NJ 08544 The Boltzmann Machine has been hirduced as a means to perform global oibMiztin for obj functions using the principles of simulated armealng In this paper we consider its utility as a s ee content-addressable memory and provide bus ontjIoracr e in this context a We show how to explbit racne's abity to escape bcal minima in order to use i at a constant tenperature for unambiguous associative pattern-retval in nroly enverts An association rule which creates a dwher ktksre around each stored patten is used aong with the MachNe's dynamics to match the machine's noisy inpu with one of the 
pre-stored pattems Spurious fixed poins whose regions of attraion are not recognized by the rule are skipped due to the Macine's fnite prbabiliy to escape from any state b We propose the use of the incremental Hebbian rule as a leaming scheme for the Boltzmann-Machine-based content addressable rnmeory This learnn rule is difterent frorn the probabity-distbutio-matchinig u uls which are used at present for the Machine's weigts in the global-minimizer appliction We interpret the Incremenal Hebbian rule as a steepest-deset algortm maximking the probablity of pattem stabilzation durin leann c We descbe tea n dOne from a sored patte using a birth-and-death Makvcvtin and find bous on t retrieval probabilties Our bwxixs slw as asessment of the Machie's efficency as a content-ad le nemor SpefFaly 
we can compare differernt learning rles used to 
instal pattems in the Machine's state space as well as emperatures o opemrtn length of learning periods length of prductI periods and coding schemes for information to be stred and retrved by te network The resuts apply to the Botmann machiine and to the asynchronous net of binary threshold elements Hopfield model They provide the network designer wih worst-cm and best-case bounds for the network's performance and allow a polynomial-tine tradoff studie of design parameters The su o tht artfi nera network be utled in classification pate r i a ssociive real has be the main theme of numerus studies which appeared In the last decade e.g 10 261 and their references Among the most popular fardlies of neural networks are fly conneded networks of binary threshold elemnrts  2 3 4 5 
6 12 13 171 18 21 23 25D1 Thee structures and the reiated family of fully conneced networks of sigmoidal threshold elenmnts 8I 14 22D have been used as error-corcting decoders in many applications among which were interesting applications in optinization 71,[151,[191,[28D A cwomon drawback of the many studied schemes is the abundance of spurious local minima which trap the decoder in undesirable and often non-interpretable states during the process of input  stored-pattem association The resuk of numerous studies of this issue e.g 2 51 131 21 122 231 251 can be roughly summarized as follows while the nunber of arbitrary binary patterns that can be stored in a fuly-connected newo is of the order of magitude of N where N is the number of neurons the number of created 
loca attractors in the network's state space is exponential in N It was.propose 3W2 ina neural networks which update their states on the basis of aI b state-rassessment ruls could be used for gkbaI optimization when the objetve function is multi-modal The suggested archiltecture fth Boluzrnam machine is bsdon fth principle of simied anagln  1J 91 20 1271 and ha been shown to perform interesting tasks of decision making and opimizaton However the leaming algorihm that was proposed for the Machine along with itRs cooling procedures do not lend themselves to real-timne operation In addiion we note that most studies so far have concentrated on the properi-es of the Machine in global optimization and only few studies have mentioned possible utilization of the Machine at constant emperature as a cortert-addressable memory e.g 
for loca optimization In the present paper we propose to use the Bolzmann machine for assoiative retrieval and develop bounds on its perfomance as a cortert-addressable memory We iruroduce a learning algorthm for the Machine which locafly maximizes the stabilzation probabity of leaned patterns We then proceed to calculate in polynomil time upper and lower bounds on the probabithat a tple at an kinW H I distanoe frm a skd pattern wil get attracted to that pattem A proposed assiaton rule creates a sphere of rtence arund each stored pattem and is indifferent to spurio atactors Due to the fact that the Machine has a nonzero prbabilty of escape from ary state the spurious atflctors are ignored The obtand bounds allw the assessment of retrial p ls diffenrentlearn algoIts and necessary leaming periods network emperatures 
and coding schem for kems to be stored After introdcng the notation we presert the procedure of using the Boltzmann Machine as a coternt addressable memrory along wth the association rule section 11 we then develop an icemental learning rule for th Machi section IIQ In section IV we develop the one-step convergence/divergence probabiles of the MacNne from a given Hanuning distance towards a stored pattem These probabities are then used to find bounds on the convergence performarce using Markov birth-and-death processes setion V The application of th bounds is Nhjsted using a forty-neuron network example with two dcfferent siored pattem sets section VI IL The Bofznann Mmchlrn as a Contn Add sble Menoiary The Boftznann Macie is a nui-wconnected network of N sirle processors caNed probabitic binary neurons The gh neuron 
is chwacterized by N-1 real numbers represeting the synapti weqlits w ij j-l,2.i-1,i,..N wl is assumwd to be zero foraal  a realthresll rijaa binary akyvy ol\(ue B 1,1 Iwhich we shl also refer to as the neuron's state The binary N-tuple U  ui,u2   uN is caled the network's state We distigsh between two phases of the network's opration a Th leaning hase when the network parameters wi1 andri are determined This determination could be achieved through autonomous leaming of the binary pattem environrment by the network unsupervied learning through leaming of the environment with the help of a eacher which supplies evaluattve reiriorcement signas suprised leaming or by an exlernal fixed assignment of paraieter vaus b The aLoduction ghase when the network's state U is 902 


 0 The bradcas word encourers noise and disto n resuling in some of its bits being reversed The Boltzmann Machie at the receiMng end accepts this distorted pattem as its initial state U\(O and observes the consequent fme evoluion of the network's state U At a certain me isant no th Machie wi declare afte irpid patm U\(O is to be a d with pattem Bm i U at hat instart is close encoug to Bmn For thi purpoe we define Dletin 2 The dma arn of ktfence of pattern Bm a Bam 4maxJ is On dmj determined This determination could be performed sycrnuly by al neurons at the sarre predeermined ftie n or asynchronous each neuron reassesses is state independently of the other neurons at random times The decisions of the neurons regarding their states during reassesnwret can be arrived at determinirsitaly the set of neuron ipus detemies the neuron's state or stochasticaly  the set of neuron iuts shapes a prbaliy cisibuton for the staewon lw We shal descrb f t a h and sohasic production rube which we sAid here Al random times during the production phase asynchronously and Kndpenderntly of all other neurons the  Ue d HD\(U 8m dn}\(5 dmax is d LtoZw d Ido bW%h mulinprach ts ffi aX U The nseboefno s Iasex tUPwitattm ri,flL#vShh tmwoh*ed torn U1slISsc t/"G OaCJ,d Due to th finte of escape from any ntum th usually-shallow energ mnimali crsocIg to spuriouxS tbced poit are qucky skd by th network on is way to th deeper vaeys dud by th des e fied vfl ie Bi B AL ProbOINy f S o ud the hebblu We conside the case U  Bm and caihcue the pot of s on br Bm wih a fixed arvhles flxedw1Nridq D n 3 V bW Aty daaxrbr a pn Bm is the one-sep p Pw Pr  v'=B U'V=BJ n  3 We rUe tha rule 3 cr be otdie from rule 1 Te OlThis determdnistic choice of ui guarantees under symmetry coitD on the weght wi=w that the ntw s state wil stalze at a fixed point in the 2N-tuple state space of the ntwrk 13D where DL  A state Uft BN iscald a fredpoint in the state Saace of the N-reuron network I PJU0'CZ=U IUV=ud=1 4 A fixed pfound through appliations of 3 may not be te gbbal nimum of the energy in 2 A n I WhiCh seeks the gbW minimum should aid local-mrinimum traps by allowing uphil dcitng with respect to the value of E The decision scheme 1 is devised for hat purpose alwing an increase in E with nonzero probablity This provision for Progress in the ocaly Wrong direction in ordo reaP a'flal advantage ler is in accordce with the puIcles of si/ated annealing techniques 9],[20J,[27J which are used in ur Odalotimit In our ce the pb i dosing the cally rigt decision equation 3 and t lcaly rn decision ardetermined by t ratio of the ergWy gap AE to the terw@ratre shap*ig constart To The Bokzmann Machine has been proposed for glb minkriz atl[,[31,[12D and a considerab amoru of effor has been im sted in devising a good cooling schenm namely a mneans to contol Te in order to a te f of a global N LEa P 7 piU 4\(bl   1 U1 3 97 6 1.1 AEs whwe N AEnt SIW 1.11.4 6 7a 7b U1 isthe unit stp fundcon and pt probablit fattfiT rwnuron ese8 d f upaing To take into acout the cifferert b s of t cdword paerns we de fntbnA 4 The average yof ab on frthe patterns Bm mn-1,2   is the one-step average prbabity 0 N LEa P,=XHXpr U-Ab+[1-U\(bm 247 Ea1 rn-1 i neuo deckdes tpn is next state using the p c decision JIB 1 wth probably 1 6'r j4 1 wt pobabq ety 1+e  N E XWM-.4 1 b Is caled th fh nerW pap and Te is a predeterined real ub caked I fln The ae-updatg rule 1 is related to the network's snyw  13118J114 D whch is descrbed by ES 2 Ei Ui CE m 2 f the retwork is to W a lOcal nirmm of 2 thn the ih neuron when chsn at random for state udag ShOUId choosedra 1 a mnimum in a short tmw 9]427J However the network may also be used as a selcie conten  nwnvry which does not suffer from inadvertey-inale spuriou loa ninira We consider the folowing appcation of the Bottzmann machine as 1 CeV 8 Incremental Leaning Ruls Using 7 we can develop the increental ocal learnn nrle wtich will maxime the stablon probabilities Incremental local learning wig have the same stochastic form that the state assessment had during production at each tme instari during leamrig a neuron say the kth is mndomly selcted for weight update At tha instarn the environment presents the network with one of the patters say Bm The randomly-selted neuron k chane its weiits and threholds in the direction which wE locall maximize the probait of stazatibo 8m The nework txrore chooses 903 a scheme for pattem classfication under noisy communication conditons let an ermder emit a sequence of NXI binary code vector from a set of 0 codewords each haft a prbailt of occurence of IIm m m 1,2 


am eqwb  at oe ofq th b an V Estiadion Roer P U To estimate the reti p we shas the Hringda of the networis state fronta sored pat The evoktion of the netwrk fro Mast stAte diance from a sue patten can be knerpeed hin tens of a blth-and-dnth Markov poces 16D figure 1 wih the w,b w  w DC max Wt"-{<f4S I  Q i3raI\(13 L MAEj%d E 2x0 pa1 14a 5f dAEd  2twDug*i 14b Thes values r tw maxwzm and rrmrdn values ta the h enrgy gap could asmun when d network is at O of d fron Bm  Using these extema we Ca find we wost case o I A 0 0 0 O 1 p.O 0 o 0 0 0 0 Fe 1 e 0 0 0 o N Vs4t bc 0 P g   where th bit oty pU i the ciegec _Pr of d irreaski t HD fromn i to W ad the death t pg is t _oortrcipn d deo s g the H1D kiom Ib l1 Pw 1 P6cP tqP qP ttd4 P41 P PC PA P4 d   _ _ l_bk 904 the cal steepest-descert direcion aong the rade d Psm Let the k neuron be seleted be larn Of the patern Bm at 8 cerain thie nstnt The deriatis of Psm with repec to st weigh 4 ww enS 9wtheso k NO 6-7 bad bf ar 1 et7 ap 1pk b 9b 1 crf The steepes descen dlans belw ocAth w a the kth tfshold at the presc d S are re b d respeivy ad a graft-Ised lwa ng e wE be wfV w4J bwb 4 10O itO whre SW ad 8r are sp si ch hin sraMed non ar otinizatbwould beo Ibid uS wthsbd ath\(or consitetl urd htd in order to g i in the objecive 1funto S each lwrni step The ru 10 cowlswith the celeb d H Iwhyphsb 11L aid in 9w icrsnw form 10 b use in sevru Schemes of adaptve blarni be rNu d teshd s mfl e.g 4 1173 18J The HNboin hypthesi ca for mi on of a connection between eto elemets in p t the pr ct of their acivaMtons eutn 1tO ir a die rlizatin of tha statement and so I Ob  1za of the theshold Is t gh an t NO 1 with uLNj-1 aNd q Using 9w rule 5 the 6yof  t P,\(HDUPT,hBd,W 1 140 Um,BJ=d  m,Z  1I ber ed_ter*en add0 Inorder  hoS 1t we hd first caulathe 9w folOiWoae-SpS'Mbpbs  1+eST 1+e7 P 6d.,l N-d U-tw L U 1-40W 1 Q co promIss we tuwo used the extreme values of AEmI\(d to underestimate overestimate convergence and oesinse uesMa divee,gin that thr b a ikagrere in d of the N o betw netwod at d Bm  we sw th errs suytlky S aoe tflbb W~~~~~~~~~~~~~~~~wm  PRB.dA Pd[HDA IHPM 12 wher 5-1,P I Fora-1 weoWSainthepIbtofWoWUme ForS+1 we obtan te p A d e For 8 we 0*ain t p t a o Wf stal mate An exac cawlton of the attraction Fitots 1.2 thne-xponerle ad we sha therefore seek lower ad uppe bounds can be cabtated In lM We Sh re-order the w of each neuron ao gto their cortxtn to the A for each pattt ug on WI\(wj  To 1 eCrT 1+e rr ad the besmE e S  pF\(8 d I1 Wd pULI1e 14-e7 j r 1 261  J where boe cae PkBnd O 1 Pw9M d 1 1 iSa It OICe 17b 17 For the woeS\(rsevl,best 


 Prf LP,{-W I HD P 2 1 b  r I Enr ri bb.-i rlbg-1 Whe nr Is the MIblt of dWe rth pam in th environment during the tain period The main between the aim-of er-products assimet aw d t assi n 24 Is the fact that whle both take Ino acoouint th correlaion between pattems the tter takes irno acount also the probability distributions of the pattems in the network's envirnent RuLe 24 is fthrore expected to perform beter than rule 23 even when we equate the bounds on their parameter vae by sett L T  R R X rtnumber of paffemrs Pattern sts we study examples of networks with forty neuros which have eamed using the sInmets 22 and 24 dlferert sAeets othe fooi pattem set Givn ta an ipt pattern was at HO of do frOm Bi the probt*yt Stern pethenetwowrk assat f with Bm is P u0'\242IHD rUP'WB do 4-m Z Pr HD\(U@J r 1 HDV B,,BO=d0 19 r.O were we can use he one-ep bonds found in secto III in order to calculat the Wst-case and best-case prbilties of associton Usin eqr 15 and 16 we deiremmat siesbromh pnB.susme Svfix,4 aids bustcn maMz;I ddreXm msSbpknS otc 1 dabestcm S\\4  P4,0j o P O ii,t1 Pd Fwo.,-1 P t\(Bb.i1 Using hse matrIes k is now possl to cal e wer and upper bounds for the associaon probabUkies needed In equation 19 z4\(YJS Pr  HDet B r HU"D,B Yo I sp na diem xI irnabs srds*Mentftt cbr X.aNd x4IsltwurSt Izn+l wC IX 1 do 10 iO_6 The bluos of o 20\(a cian be uwed to bound the aso_an _y---f i n 19 The boun at th twlm in 19 by thei 4J Yll r-o The bWM bod cWX be mea skr W shoe ki osA W sat 1 for ib PJ,Bi,-t f i=1t,2\266.1 T T m~~-I\(-Q I4    1 1Xo 2T_-}T for a 1 01uls2T+1xT+2T0.'hT 24b Ofr Oq=1 Th parameters Qi and Oq we dlt ratio related t t p lntiilsfIrursB o_sdebnsftd 0-1 1-1,I and of82  1 1 1-1 In the wnedkns a'17 n  The fis is th popular sun-f-outer-prodfs assignmrt eg 51 6 13D o 0 wi=X bkb,j andt=-X 1:tb 22 k-I h-i The secd Is based on th steady sWte aerag values of weights calculaed according to the incremental Hettian larning rule 117D This learr*igrle is a derivative of the updat schm t we h ed in equation 10 4I ffbbv I and L e 1d 1uadf T 242t 4O4 f b1-ld j W il f-4 iX  rnT 4 t4=IwwwN L n i Imwfh i qj iwb l m siwtu L 9 ffb 1 d ddT In 23 we have itrduced lwer and upper bounds for the weiWS L and L amd for fts tlveKfs T ad T We sia use the steady sae average vues of the weights and the trsold 17D as p smp d Ve ntwok for   I  r n XIlr 4 _ _ r--I a2 o 24c 7o for i Pr-1..N..N mid 21 c p.=mkiHD\(BB-d j=l,...,R,j,i 21d Equation 21c and 21d assume that the network wanders it the dmar sphere of influence of a pattem oer than Bi whenver is dtance from Bq Is pi or more ThIs assulTqlis ve _cowevive sne pW spee the shorte Stnc to a competin dmaxr sphere of wdtence and the network ooukd actuy wander to d s ta pi and sti corwerge eventuay It the dm sphere Virfencer*vB  VL Exmles pelannanc assessmentm and copNtsofl id,lt-ssg.et algorthn Our examVes are desige to demonstrate the utW of our approach in comparing weight asiomerts for a netwok Simlarly the seldctn of a temperature Te th selection of a ceword set for repesertation of iormation and the leaing and production periods th ar necessary to attain required pefm ace could be studied Weight anlgnnwnts We shall compare the utility of two static weight sn ris for wq and r-o whe fth undefd matrix is Ome bith-xklth fff 18 iVI,,+l bri12,.L1 i d of w prx to th pes rafp n Om I koi ta8 alead asoiae sae U wk on f h pattM we SW Oww a the kwor bouii on the womgic 11 ineqdo 19 DXkno 2  t   t 134  4H B5se  B7+a 905 t IXafO 7 2L 0m&lLb WI 24a 0 for arQgs 1 I 1 2 1 1 1 I 1 1 4t f1--B2++++-H  for Pd 02 XC t 


A-l1 Fg Case 1 RetiaWprobtli o n We study the set  B4 B5 Be B7 of four mutualy orthogonal pattems In figure 2 we show the p ity of convergence from Hamming ditances of 1 and 2 to B4 versus the number of production steps Here dmaxvs The sum-of-outer-prducts weigt assig*mnt eq 22 P 2 Boung on t co nv my Sun m@-W rlwdcs we%ftl ass%ownent pOm Ao oowo bound kia I distan 2 Abwoer bound l Wii Hmming dMune 2 Bupper bound Sal Hm*g disto 5 B-U bwer bound hid Hwnn*ig Man 5 m Isi ce I A-I upper bound irilal Hamming ditnce 2 A-e lower bound iniia Hamming distance 2 9-1 upper bound nal H{arrTdng distance 2 B-U lower bound initil Hammn distance 2 Numb of prodoO se Figure 3 Bounds on the convergence probablity Sum-o-outer-poducts weigt assigment pattem set B B2 83 A-I upper bound iial Hanmi dam e 1 A-1 lower bound itt INHA*ig deae I B-i upper bound inikial nri dsa 2 Bi1 bwer bound hini Ha I ance 2 Numb of p duc0 0,n a Fgure 4 Bounds on the convegnce Steady-stae Heblanlearing weigt asgnmnt pattn set  B5 Be B7 A-I upper bound h Ha wer B ownd he Haoot n c proab n t AFgr 2 was used The method gives tight bounds for i itial Hamng distan of one For reeal from an iitial Hamming distace of 2 the upper ard bwer bounds separate The same paterns are exanined again in figure 4 where the steady-state Hebbian assignment eq 24 is used The performance with this weight assigment gave better steady-ste bounds for lar e ia Hammng d c we show the bounds for Hamming d tancs of 2 and 5 We have stuxid the retrieval of pattern B-I when the set of arbitrariy-seled paerns fB1,B2,B3 was stored in the 40-euron netwrk using 22 and 24 for wet Figures 3 and 5 depict the retrieval probabilties for the two assignments with dmax 0 Again we have observed an advanaoge to the steady-state Hebblan snent for laW iiial Hamming distances We show the bounds for Hamming distances I and 2 for the sum-ofter-products assignment figure 3 and for Hamming distances of 2 and 5 for the steady-Wate Hebban a fu 5 Man We have preserted how h Bolmann Machin can be used as a cortent-addressable memory exploit the stochstc nwe of its sate-scion p r n escape undesirable mirima An asso n rd in tern of pater her of irdnluece isused in order to match an p wh one of th predetermined stored patterns The system Is therefore Ifferent to spurious sates whose spheres of Influnc are not recognized in the retrieval process We have detald a technique to calculae the upper and loww bounds on retrival probabities of each stred pattern These bourds are functiu of the network's parameters i.e or larnin res and the pattem sets the inta Haing distance from the stored pattern the asociation rule and the number of production steps They aNow a ponymitme assessfent of the networks cpab e as an a e memory for a set of paterw  a copnarison of different codg schemes for pafttr to be strd and retieved an assmen of the ngh of the leamrnng period necessary in order to gartee a presb d oilty rof ievl ard a r o o iffe leamgassignmnnst rules for the ntwork paramets A-I 


Nutrw d prodm*m stwp Figur 5 Bounds on th convernoe Steady-ste Han-ealng assnn pnAtern set fBi B2 83 A-I tqper bound hal Ha g distance 2 A-I bwer bound ithal Hamrig 1 arce 2 4 upper bound nital Ham*g d as 5 Bi wer bound ii Iaming e 5 References 13 Aarts,EH.L Van Laarhoven,P.J.M  he Capacity of the Fio0ied Assodative Memory IEEE Transactions on Infonration Theory Vol IT33 No 4 pp 461-482 1987 1241 McClelland,J.L Rumelhart,D.E editors Parallel Disrce Pnrcssig Volume 2 Psychokogtal and Bbbgda Atds  Carridge:MIT press 1986 25 Newman C.M Memory Capacity in Neural Network Models  Rigorous wer Bounds Neural Nethos Vol 1 No 3 pp 223-238 1988 26 Rumelhart,D.E MClelliand,J.L editors Parallel Dirbed Processing Volume 1 Foundations CantbidgelMIT press 1986 27 Szu,H Fast Simulated Annealing i n Denker,J.S.\(editor  Neural Networks for Computing New YorkArmercan Insttute of Physics Vol 51.,pp 420 425 1986 281 Tank,D.W Hopfleld J.J  Simple Neural Optimization Networks IEEE Transactions on Circuits and Systems Vol CAS-33 No.5 pp 533-541,1986 Reseh suppored by NSF gr HI 8810186 907  Statistical Cooln A Gener Awroach to Coratorlal Optimiation Problems,'P J of d Vol 40 pp 420-425 1985 2 Abu Uostata,Y.S St Jacues,J-M  Irtorat ion Capacity of the Hopfleld Model IEEE Transations on Ifrt Theo Vol FT31 No 4 pp 461464 1985 13 Ackley,D.H Hlnton,J.E Selnowskl,T.J  A Leaning Algrthm for Botzmm Machines Cognre Science Vol.19 pp.147-169 1985 4 AmarIS-1  Learning Patterns and Pattem Sequence by Self-Organizng Nets of Threshold Elements IEEE Transacdons on Compuers Vol C-21 No.11 pp.1197-1206 1972 5 Amlt,D.J Gutfreund,H Sompollnsky,H  Spi-Glss Model of a Neua NMworkPh F*ysia Review A Vol 32 No 2 pp 1007-1018 1985 161 AmIt,O.J Guttreund,Ht Sompollnsky,H Storing Infinie Numbers of Paerms in a Sn-Glss Model Physical Rev*w Letters Vol 55 No.14 pp 1530-1533 198 7 ChuaLO Lin G-N  Nonlinear Programming without Corrptaton IEEE TrrCirns on Ck4s and Systems VoL CAS-31 No.2 pp 182-188,1984 8 Cohen,M Grossberg,S Absolute Stability of Global Pattem Formation and Parallel Memory Storage by Competti Neural Networks IEEE Transacdons on Systems AMan and Cybemetc Vol SMC-13,pp215-826 1983 9 Gen,S Gsan,D  Stochastic Relaxation Gbbs Dlstrb*ns and the Bayesiwi Rarsion of Imags IEEE T^raacto on Paten Ansis and Afatne lntefsgence Vol 6 pp 721-741 1984 1O _gros rg,S  Nonlinear Neural Netw  Pric@es MeclhitsaT Ard ec ar NeuraINeoks VoL 1 1988 113 l1ebb,D-.0  The Organization of Behavior New YorkWliey 1949 123 Hlnton,J.E Sejnowski,T.J  Learning and Releaming in the Boltzmann Machine in Rumelhart,D.E McClelland,J.L editors Parallel Dtributed Processing Vokrne 1 Foundations Canbridge:MIT press 1986 13 Hopfield,LJ  Neural Networks and Physical Systems with Emergent Colective Computational Abilties Proceedings of the National Academy of Science USA Vol 79 pp 2554-2558 1982 114 Hopfleld,J.J  Neurons with Graded Response Have Colletive Computational Properties Lke Those of Two-State Neurons Proceedings of the National Academy of Science USA Vol 81 pp 3088-92 1984 153 Hopfield,J.J Tank,D  Neural Computation of Decison in Optimiation Problers Biological Cybemetis Vol 52 pp 1-12 1985 16 Howard,R.A Dynamic Probabilistic Systems New YotkiWIey 1971 17 Kam,M Cheng,R Guez A  Pattem Retrieval and Learning In Nets of Asynconous Binar Threshld Elements in press IEEE Transiors on Cirls and System 18 Kam,M ChFn,R Guez A On the Stale Space of the Binary Neural Network Proceedings of the 1988 American Contro Cornsrence Atanta GA 1988 19 Kennedy,M.P Chua L.O Circuit Theoretic Soutions for Neural Networks Procedings of the 1st Intemationau Corence on Neural tworks San DWo CA 1987 201 Klrkpatrick,S GeIIat,C.D-,Jr Vecohl,M.P Optimization by Simulated Annealing Science 220 pp 671-680 1983 21 Komlos,J Paturi R Convergence Resus hi an Associatfe Memory Model Neural Networks Vol 1 No 3 pp 239-250 1988 22 LI,J.H Michel,A.N Porod,W ualitative Analysis and Syrthesis of a Class of Neural Networks IEEE Transations on Cicits and Systemns Vol 35 No 8 pp 976-986 1988 231 McEllece,R.J Posner,R.C Rodemich E.R Venkatesh S.S 


acquires a lock on this leaf node for mutual exclusion while inserting the itemset r if we exceed the threshold of the leaf we convert the leaf into an internal node with the lock still set This implies that we also have to provide a lock for all the internal nodes and the processors will have to check if any node is acquired along its downward path from the root This complication only arises at the interface of the s and internal nodes With this locking mechanism each process can insert the itemsets in different parts of the hash tree in parallel r since we start with a hash tree with the root as a leaf there can be a lot of initial contention to acquire the lock at the root r we did not 336nd this to be a signi\336cant factor on 12 processors 3.2 Support Counting r this phase we could either split the database logically among the processors with a common hash tree or split the hash tree with each processor traversing the entire database We will look at each case below 3.2.1 Partitioned vs Common Candidate Hash Tree One approach in parallelizing the support counting step is to split the hash tree among the processors The decisions for computation balancing directly in\337uence the effectiveness of this approach since each processor should ideally have the same number of itemsets in its local portion of the hash tree Another approach is to keep a single common hash tree among all the processors There are several ways of incrementing the count of itemsets in the common candidate hash tree Counter per Itemset Let us assume that each itemset in the candidate hash tree has a single count 336eld associated with it Since the counts are common more than one processor may try to access the count 336eld and increment it We thus need a locking mechanism to provide mutual exclusion among the processors while incrementing the count This approach may cause contention and degrade the performance r since we are using only 12 processors and the sharing is very 336ne-grained at the itemset level we found this approach to be the better than using private or separate counters 1  3.2.2 Partitioned vs Common Database We could either choose to logically partition the database among the processors or each processor can choose to traverse the entire database for incrementing the candidate support counts Balanced Database Partitioning In our implementation we partition the database in a blocked fashion among all the processors r this strategy may not result in balanced work per processor This is because the work load is a function of the length of the transactions If l t is the length of the transaction t  then during iteration k of the algorithm we have to test whether all the 1 r all the databases we looked at on our system the overhead of contention was within 4 which leads us to conclude that contention is not a big problem Other mechanisms like separate counters to eliminate locking and local counters to eliminate false sharing were studied t not shown to be bene\336cial 14 7 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


000 l t k 001 subsets of the transaction are contained in C k  Clearly the complexity of the work load for a transaction is n s O 000 min 000 l t k 000l t l t 000 k 001\001  i.e it is polynomial in the transaction length This also implies that a static partitioning won\325t work r we could devise static heuristics to approximate a balanced partition r example one static heuristic is to estimate the maximum number of iterations we expect say T  We could then partition the database based on the mean estimated work load for each transaction r all iterations n s 000 P T k 000 1 000 l i k 001 001 001T  Another approach is to re-partition the database in each iteration In this case it is important to respect the locality of the partition by moving transactions only when it is absolutely necessary We plan to investigate different partitioning schemes as part of future work 3.3 Parallel Data Mining Algorithms Based on the discussion in the previous section we consider the following algorithms for mining association rules in parallel 000 Common Candidate Partitioned Database CCPD This algorithm uses a common candidate hash tree across all processors while the database is logically split among them The hash tree is built in parallel see section 3.1.4 Each processor then traverses its local database and counts the support see section 3.2.1 for each itemset Finally the master process selects the large itemsets 000 Partitioned Candidate Common Database PCCD This has a partitioned candidate hash tree t a common database In this approach we construct a local candidate hash tree per processor Each processor then traverses the entire database and counts support for itemsets only in its local tree Finally the master process performs the reduction and selects the large itemsets for the next iteration Note that the common candidate common database\(CCCD approach results in duplicated work while the partitioned candidate partitioned database PCPD approach is more or less equivalent to CCPD r this reason we did not implement these parallelizations 4 Optimizations In this section we present some optimizations to the association rule algorithm These optimizations are bene\336cial for both sequential and parallel implementation 4.1 Hash Tree Balancing Although the computation balancing approach results in balanced work load it does not guarantee that the resulting hash tree is balanced Balancing C 2 No Pruning  We\325ll begin by a discussion of tree balancing for C 2  since there is no pruning step in this case We can balance the hash tree by using the bitonic partitioning scheme described  We simply replace P  the number of processors with the fan-out F for 8 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


the hash table We label the n large 1-itemsets from 0 o n 000 1 n exicographical order and use P 000 F to derive the assignments A 0 000 001\001\001 000 A F\000 1 for each processor Each A i is treated as an equivalence class The hash function is based on these equivalence classes which is simply n as h 001 i 002\000 A i  for i 000 0 000 001\001\001 000 F  The equivalence classes are implemented via an indirection vector of length n  For example let L 1 000 f A\000 D 000 E 000 G\000 K 000 M 000 N 000 S\000 T 000 Z g  We 336rst label these as f 0 000 1 000 2 000 3 000 4 000 5 000 6 000 7 000 8 000 9 g  Assume that the fan-out F 000 3 We thus obtain the 3 equivalence classes A 0 000 f 0 000 5 000 6 g  A 1 000 f 1 000 4 000 7 g  and A 2 000 f 2 000 3 000 8 000 9 g  and the indirection vector is shown in table 1 Furthermore this hash function is applied at all levels of the hash tree Clearly this scheme results in a balanced hash tree as compared to the simple g 001 i 002\000 i mod F hash function which corresponds to the d partitioning scheme from section 3.1.1 Label 0 1 2 3 4 5 6 7 8 9 Hash Value 0 1 2 2 1 0 0 1 2 2 Table 1 Indirection Vector Balancing C k 001 k\001 2 002  Although items can be pruned for iteration k 002 3 we use the same bitonic partitioning scheme for C 3 and beyond Below e show that n n this general case bitonic hash function is very good as compared to the d scheme Theorem 1 below establishes an upper and lower bound on the number of itemsets per leaf for the bitonic scheme Theorem 1 Let k 002 1 denote the iteration number I 000 f 0 000 002\002\002\000 d 000 1 g the set of items F the fan-out of the hash table T 000 f 0 000 002\002\002\000 F\000 1 g the set of equivalence classes modulo F  T 000 T k the total number of leaves in C k  and G the family of all size k ordered subsets of I  i.e the set of all k itemsets that can be constructed from items in I  Suppose d 2 F is an integer and d 2 F 000 F\002 k  De\336ne the bitonic hash function h  I\003 T by h 001 i 002\000 i mod F if 0 004 001 i mod 2 F 002 003 F and 2 F\000 1 000 001 i mod 2 F 002 otherwise and the mapping H  G 003 T from k itemsets to the leaves of C k by H 001 a 1 002\002\002\000 a k 002 000 001 h 001 a 1 002 000\002\002\002\000h 001 a k 002\002  Then for every leaf B 000\001 b 1 000\002\002\002\000b k 002 005T  the ratio of the number of k itemsets in the leaf  k H 000 1 001 B 002 k  o the average number of itemsets per leaf  kG k 004 kT k  s bounded above and below by the expression e 000 k 2 d\000 F 004 k H 000 1 001 B 002 k kG k 004 kT k 004 e k 2 d\000 F 002 A proof of the above theorem can be found in W e also obtain the same lo wer and upper bound for the d hash function also r the two functions behave differently Note that the average number of k itemsets per leaf kG k 004 kT k is 000 2 w F k 001 004 F k 006 000 2 w 001 k k   Let 005 001 w 002 denote this polynomial We say that a leaf has a capacity close to the average if its capacity which is a polynomial in w of degree at most k  s f the form 000 2 w 001 k k  003 006 001 w 002  with 006 001 w 002 being a polynomial of degree at most k 000 2 9 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


r the bitonic hash function a leaf speci\336ed by the hash values 000 a 1 000\000\000\001 a k 001 has capacity close to 002 000 w 001 if and only if a i 000 002 a i 000 1 for all i 1 001 i 001 k 002 1 Thus there are F 000 F\002 1 001 k 000 1 such leaves and so 000 1 002F 000 1 001 k 000 1 fraction of the s ave capacity close to 002 000 w 001  Note also that clearly 000 1 002F 000 1 001 k 000 1 approaches 1 On the other hand for the d hash function a leaf speci\336ed by 000 a 1 000\000\000\001 a k 001 has capacity close to 002 000 w 001 if and only if a i 000 002 a i 000 1 for all i  and the number of i such that a i 003a i 000 1 is equal to 000 k 002 1 001 004 2 So there is no such leaf if k is even r odd k 003 3 the ratio of the 322good\323 s decreases as k increases achieving a maximum of 2 004 3 when k 002 3 Thus at most 2 004 3 f the s achieve the average From the above discussion it is clear that while both the simple and bitonic hash function have the same maximum and minimum bounds the distribution of the number of itemsets per leaf is quite different While a signi\336cant portion of the s are close to the average for the bitonic case only a ew are close in the simple hash function case 4.2 Short-circuited Subset Checking  Candidate Hash Tree \(C 3 Hash Function: h\(i DEPTH 0 DEPTH 1 DEPTH 2 01 35 7101113 12986 24 LEAVES ABE ADE CDE A,C,EB,D B,D B,D B,D B,D B,D B,DA,C,E A,C,E A,C,E A,C,E A,C,E A,C,E ABD ACD ACEBCEBCDBDE ABC Figure 2 Candidate Hash Tree  C 3  Recall that while counting the support once we reach a leaf node we check whether all the itemsets in the leaf are contained in the transaction This node is then marked as VISITED to avoid processing it more than once for the same transaction A further optimization is to associate a VISITED 337ag with each node in the hash tree We mark an internal node as VISITED the 336rst time we touch it This enables us to preempt the search as soon as possible We would 10 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


expect this optimization to be of greatest bene\336t when the transaction sizes are large r example if our transaction is T 000 f A\000 B 000 C\000 D\000 E g  k 000 3 fan-out 000 2 then all the 3-subsets of T are f ABC,ABD,ABE,ACD,ACE,ADE,BCD,BCE,BDE,CDE g  Figure 2 shows the candidate hash tree C 3  We ave to increment the support of every subset of T contained in C 3  We egin with the subset AB C  and hash to node 11 and process all the itemsets In this downward path from the root we mark nodes 1 4 and 11 as visited We then process subset AD B  and mark node 10 Now consider the subset CDE  We see in this case that node 1 has already been marked and we can preempt the processing at this very stage This approach can r consume a lot of memory r a n fan-out F  for iteration k  e need additional memory of size F k to store the 337ags In the parallel implementation we have to keep a VISITED 336eld for each processor bringing the memory requirement to P\000F k  This can still get very large especially with increasing number of processors In we sho w a mechanism by which further reduces the memory requirement to only k 000F  The approach in the parallel setting yields a total requirement of k 000F 000P  5 Experimental Evaluation Database T I D Total Size T5.I2.D100K 5 2 100,000 2.6MB T10.I4.D100K 10 4 100,000 4.3MB T15.I4.D100K 15 4 100,000 6.2MB T20.I6.D100K 20 6 100,000 7.9MB T10.I6.D400K 10 6 400,000 17.1MB T10.I6.D800K 10 6 800,000 34.6MB T10.I6.D1600K 10 6 1,600,000 69.8MB Table 2 Database properties 5.1 Experimental Setup All the experiments were performed on a 12-node SGI Power Challenge shared-memory multiprocessor Each node is a MIPS processor running at 100MHz There\325s a total of 256MB of main memory The primary cache size is 16 KB 64 bytes cache line size with different instruction and data caches while the secondary cache is 1 B 128 bytes cache line size The databases are stored on an attached 2GB disk All processors run IRIX 5.3 and data is obtained from the disk via an NFS 336le server We used different synthetic databases with size ranging form 3MB to 70MB 2  and are generated using the procedure described in These databases mimic the transactions in a retailing en vironment Each transaction has a unique ID followed by a list of items bought in that transaction The 2 While results in this section are only shown for memory resident databases the concepts and optimization are equally applicable for non memory resident databases In non memory resident programs I/O becomes an important problem Solutions to the I/O problem can be applied in combination with the schemes presented in this paper These solutions are part of future research 11 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


  0 500 1000 1500 2000 2500 0 2 4 6 8 10 12 Number of Large Itemsets Iterations Large Itemset at Support = 0.5 222T5.I2.D100K\222  222T10.I4.D100K\222   222T15.I4.D100K\222   222T20.I6.D100K\222   222T10.I6.D400K\222   222T10.I6.D800K\222   222T10.I6.D1600K\222  Figure 3 Large Itemsets per Iteration data-mining provides information about the set of items generally bought together Table 2 shows the databases used and their properties The number of transactions is denoted as jD j  average transaction size as j T j  and the average maximal potentially large itemset size as j I j  The number of maximal potentially large itemsets j L j 000 2000 and the number of items N 000 1000 We refer the reader to for more detail on the database generation All the e xperiments were performed with a minimum support value of 0.5 and a leaf threshold of 2 i.e max of 2 itemsets per leaf We note that the  improvements shown in all the experiments except where indicated do not take into account initial database reading time since we speci\336cally wanted to measure the effects of the optimizations on the computation Figure 3 shows the number of iterations and the number of large itemsets found for different databases In the following sections all the results are reported for the CCPD parallelization We do not present any results for the PCCD approach since it performs very poorly and results in a speed-down on more than one processor 3  5.2 Aggregate Parallel Performance Table 3 s actual running times for the unoptimized sequential and a naive parallelization of the base algorithm Apriori for 2,4 and 8 processors without any f the techniques descibed in sections 3 and 4 In this section all the graphs showing  improvements are with respect to the data for one processor in table 3 Figure 4 presents the speedups obtained on different databases and different processors for the CCPD parallelization The results presented on CCPD use all the optimization discussed 3 Recall that in the PCCD approach every processor has to read the entire database during each iteration The resulting I/O costs on our system were too prohibitive for this method to be  12 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


