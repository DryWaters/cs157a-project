Large scale Entity Extraction and Probabilistic Record Linkage Dr. Flavio Villanustre  Reed Elsevier  LexisNexis Risk Solutions  Alpharetta  GA USA  flavio.villanustre@lexisnexis.com     EXTENDED ABSTRA CT    Large scale entity extraction, disambiguation and linkage in 
Big Data can challenge the traditional methodologies developed over the last three decades. Entity linkage, in particular, is cornerstone for a wide spectrum of applications such as Master  Data Management, Data Warehousing, Social Graph Analytics, Fraud Detection and Identity Management Traditional rules based heuristic methods usually don't scale properly, are language specific and require significant maintenance over time  This presentat ion will introduce the audience to the use of probabilistic record linkage, also known as specificity based 
linkage, on Big Data, to perform language independent large scale entity extraction, resolution and linkage across diverse sources The presentation  also includes a live demonstration reviewing the different steps required during the data integration process \(ingestion, profiling, parsing, cleansing standardization and normalization\, and show the basic concepts behind probabilistic record linkage on  a real world application  
using the open source big data platform HPCC Systems f r o m L e xi s N e xi s   Keywords   Big Data, entity extraction, disambiguation public data, identity management, record linking, identity fraud, public data  Attendees will be sho wn how record field and value specificities, together with phonetics, edit distances and other metrics can be used to disambiguate entities. They will also be shown how social graphs can be assembled based on specific dimensions in the data, and how this s 
ocial graph information can be used to detect heavily connected cliques that could represent interesting associations \(like, for example, those that could be related to potential collusion fraud  This session will discuss the challenge of resolving identi ty from billions of identity fragments and why the bigger the data and the higher the redundancy in it, the better the resolution The session will outline what this means for key business challenges such as Master Data Management and Data Warehousing  A 
CK NOWLEDGMENT  I wish to thank the members of the HPCC Systems project team at LexisNexis for editorial support and their contributions to this presentation  BIOGRAPHIES  F LAVIO VILLANUSTRE  Dr. Flavio Villanustre is the Vice President of Infrastructure and Pro ducts for HPCC Systems, the open source Big Data processing platform for LexisNexis. In this position, Flavio is responsible for Information and Physical Security, overall infrastructure 
strategy and new product development. Prior to 2001, Dr Villanustre served in different companies in a variety of roles in infrastructure, information security and information technology. In addition, Dr. Villanustre has been involved with the Opensource community for over 15 years through multiple initiatives. Some of the se include founding the first Linux User Group in Buenos Aires \(BALUG\ in 1994 releasing several pieces of software under different Opensource licenses, and evangelizing Opensource to different audiences through conferences, training and 
education. Prior to his technology career, Dr. Villanustre was a neurosurgeon  R EFERENCES  1  A. M. Middleton, “HPCC Systems: Introduction to HPCC \(High Performance Computing Cluster\,” LexisNexis Risk Solutions Technical Report, May 2011  978-1-4799-5158-1/14/$31.00 ©2014 IEEE 85 


                                     


                                  


                                                


                                


                                    


                                                       


                 


                                     


                                                        


                           


                                        


                  


  


                                               


   


                                


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


