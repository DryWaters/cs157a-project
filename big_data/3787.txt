  1 Entropy Constrained Clustering Algorithm Guided by Differential Evolution Alexandre Guillaume, Seungwon Lee, Amy Braverman and Richard Terrile Jet Propulsion Laboratory 4800 Oak Grove drive Pasadena, CA  91109-8099 818-393-6899 Alexandre.Guillaume@jpl.nasa.gov  Abstract Entropy constrained vector quantization \(ECVQ is a clustering techni t h at has been successful l y  used to describe efficiently large amounts of data collected by the NASA Earth Observing System. The manipulation of this algorithm requires the user to set two parameters: the entropy Lagrange multiplier, and the initial guess for the number of clusters. In this work, we describe an integrated 
solution that uses a differential evolution algorithm to determine these two parameters. By optimizing two objective functions, entropy and distortion, we find that the solution that best describes the data is located at the inflection point in the Pareto front, i.e. at the point where the tradeoff between the two competing objectives does not favor either one 12   T ABLE OF C ONTENTS  1  I NTRODUCTION 1  2  E NTROPY 
C ONSTRAINED V ECTOR Q UANTIZATION   ECVQ........................................................................2  3  D IFFERENTIAL E VOLUTION   DE...........................3  4  R ESULTS 3  5  I NTERNATIONAL S ATELLITE C LOUD C LIMATOLOGY P ROJECT DATA 
7  6  C ONCLUSION 8  R EFERENCES 8  B IOGRAPHY 8  1  I NTRODUCTION  The ability to capture the essential characteristics of a large amount of information in a substantially smaller information space is important for many applications, such as data management, pattern recognition and artificial intelligence One such application is in the construction of so-called 
Level 3 satellite data products. Earth-orbiting satellites routinely produce massive data sets containing measurements of various characteristics of the Earth’s atmosphere, biosphere, oceans and cryosphere. Such data sets are one of the best ways to capture observational information that is crucial to the study of climate, but only if the information contained in these data can be condensed 1  1 1-4244-1488-1/08/$25 00 ©2008 IEEE 2 IEEEAC paper #1286, Version 4, Updated October 23, 2007 into an understandable and usable picture of the Earth’s systems. Level 3 data products are intended for this purpose: to summarize vast quantities of data produced at 
high-resolution in a way that can be ingested into models and compared against their output   Traditional Level 3 data products summarize satellite data by sorting them into space-time bins defined by say, a monthly, one-degree latitude-longitude spatial grid, and providing the means and standard deviations of parameters in each grid cell. Braverman et al  proposed a new approach in which the multivariate distributions of such parameters are provided for each grid box thereby capturing more information about the relationships among parameters Their method is based on a modified version of the Entropyconstrained Vector Quantization \(ECVQ\algorithm originally suggested by Chou, Lookabaugh and Gray   ECVQ can be seen as a penali 
zed clustering algorithm that balances the trade-off between the information-theoretic complexity and the fidelity of a clustering of a data set. The clustering” assigns each raw data point to one of  a number of groups, and records  the group centroid and number of observations in the group. The set of centroids and their associated \(normalized\bers of cluster members constitute an estimate of the empirical distribution of the raw data   This method has been implemented to summarize a number of different remote sens ing data sets including data from NASA’s Atmospheric Infrared Sounder \(AIRS instrument, and the International Satellite Cloud Climatology Project \(ISCCP\he 
drawbacks to the method as currently implemented is the ad-hoc way in which two of the ECVQ algorithm parameters are set. These are the upper bound on the final number of clusters \(K\he so-called rate-distortion paramater  which determines the way entropy \(a measure of information-theoretic complexity\distortion a measure of fidelity\ed off.  In this paper we describe a method that automates the selection of these two parameters  


  2 2  E NTROPY C ONSTRAINED V ECTOR Q UANTIZATION   ECVQ In this section, we describe briefly the Entropy-constrained Vector Quantization introduced by P. A. Chou et al    The main idea is to represent a set x of N points by a smaller set of points y Each data point x is associated with a representative point y i  called a cluster, or centroid with a certain probability. Tw o quantities are introduced to evaluate the quality of the clustering. The first one is the distortion D\(x,y between the original data points x and its representative points. Intu itively, a good solution will minimize this distortion. Here the distortion is evaluated using the squared Euclidian distance D  x  y   x i  y i  i  2  The second quantity is the information entropy, also called in this case index entropy. It is defined as H  p  i log 2 p  i  i  I   where p\(i is the proportion of data points x assigned to y i  and I is the set of all the clusters indices. Intuitively, the entropy will increase with the number of clusters and the distortion decreases. A tradeoff between the minimization of the distortion and the minimization of the entropy has to be made in order to find a good solution. To this end, P. A Chou et al introduced a loss function that balances the two competing criteria J  x  y   D  x  y     H  x  y   The resemblance between this loss function and Helmoltz free energy used in Physics is misleading. The plus sign in the above equation indicates th at to reach a minimum loss both the distortion and the entropy should be minimized whereas the minimum free ener gy is reached when the entropy is maximum. It’s worth mentioning that there are other algorithms that use the framework of Physics to derive clustering algorithms, such as Deterministic Annealing DA\[4 DA, wh ich is a d e term in istic o p tim izatio n  algorithm, includes ECVQ as a specific case To initialize the ECVQ algorithm the following quantities are required: a\urce probability distribution, b\ a distortion measure, c\range multiplier  d convergence threshold, e\ndex set I and f\ initial set of clusters and entropies. He re we simply associate a 1/N  probability with each point \(N is the number of data points We use the squared Euclidian distance \(ref eq. 1\as our distortion measure The ECVQ main loop consists of the three successive steps 1  Fixing the number and size of clusters, search for the association between the data points x and the members of I that minimizes the loss function J  2  With the association found in the preceding step e.g with fixed probabilities \(associating each data point to an index uate the entropy for each index 3  With the entropies found in the preceding step fixed \(per index e centroid for each index that minimizes the loss function, e.g. the distortion since the entropy is fixed in this step After this last step, the loss function J is updated. A stopping condition then checks whether the new loss function differs from the preced ing one by more than the convergence threshold The idea behind ECVQ is easily understood from steps 3 Up to that point in the description, the algorithm does not adjust \(find\he number of clusters; it merely finds the best association between data points and a pre-set number of clusters. A strategy to find the number of cluster that approximate the data best would then be to iterate the ECVQ algorithm for different choices of initial clusters and search among the different solutions for the best solution according to a given some criterion \(such as minimum distortion\Moreover, the solution achieved with ECVQ is valid for one Lagrange multiplier  which implies that multiple solutions correspond to multiple choices of  In their original paper, Chou et al proposed a method to explore the parameter space of Lagrange multipliers, thus defining the convex hull or line of solutions in the graph D\(H However even with this systematic method, the question of the quality of the solution and the criterion to gauge this quality still remain. In other words, which solution on the convex hull does the best job of describing the data Amy Braverman et al proposed a solution tailored specially for data collected for the Atmospheric Infrared Sounder AIRS  The prot ocol for set t i ng  there is to first stratify the data in geographic and temporal bins \(e.g. one degree latitude by one degree longitude by day\, apply ECVQ with the same value of  to all bins. This procedure is performed for multiple values  and the value for which the variance of the distortions across all bins is minimized is selected. The reasoning is that the algorithm should treat the data homogenously across different bins so that differences in the output should only stem from real differences in the data and not from differences in the quality of the clustering that results 


  3 We propose a method that addresses both the general question raised above concerning the choice of specific solutions \(among all the solutions provided by the ECVQ algorithm in the D\(H plane\he dependence of the solutions on two algorithm parameters \(the Lagrange multiplier  and the initial number of clusters\In order to solve this problem, we decided to generate our own training data sets. Using several Gaussi an distributions, we could set the dimensionality \(number of distributions\e number of clusters, the localization \(the mean of each Gaussian\the spread \(the Gaussian varian ce\overlap between groups of points. This prior knowledge was instrumental in devising a criterion to choose a solution We use differential evolution \(DE t o fi nd t h e opt i m um  solutions to this problem. Such evolutionary computation techniques are known to be well suited for multi-objective optimization problems. ECVQ is equivalent to a biobjective optimization problem reduced to a single objective one: instead of minimizing the distortion and the entropy separately, a single function equal to a weighted sum of the objectives, a single loss function, is minimized Beside the obvious dependence on the Lagrange multiplier  the ECVQ solutions also depend on the number of clusters used to initialize the algorithm. Since  is a real number and the initial number of clusters is an integer, so DE is an especially appropria te choice because of the ease of implementing a mixed variables algorithm with this technique 3  D IFFERENTIAL E VOLUTION   DE Differential evolution is a stochastic optimization technique put forth in 1995 by Rainer Storn and Kenneth Price   It  belongs to the category of Evolutionary Algorithms \(EAs which means that it is a population based algorithm: an initial population of potential solutions is formed by creating individuals randomly within the problem bounds and after an iterative process of modification and selection  of individuals, the population contains individuals that converge toward the problem’s optimal solutions A specific mutation operation is at the heart of DE. In other EAs, the mutation usually consists in adding a small increment chosen from a Ga ussian distribution to a randomly selected individual. In DE, the difference between two distinct individuals randomly chosen from the population is formed and added with a scale factor F to a third distinct individual also chosen randomly and distinct from the two other ones. The individual resulting from this differentiation process, analogous to the mutation in classical genetic algorithm, is called the trial vector Subsequently, this vector undergoes a crossover depending on whether a random number is smaller than a crossover rate Cr It is also customary to add another condition for the crossover to guarantee that at least one of the genes will be changed. The trial vector b ecomes the target vector if another random number chos en between one and the number of genes is equal to the gene number considered while going through all the genes of an individual\. Finally the objective function is evaluated using the resulting trial vector and compared to th e individual of the preceding generation. If it dominates the solution from the preceding generation, the trial vector becomes the new individual for this rank \(the index of the individual in the population This procedure is repeated N pop times N pop being the population size. This DE strategy is also known as DE/rand/1/bin \(“rand” means that the base vectors are chosen randomly and “bin” standing for binomial and 1 for uniform crossover We use a straightforward m odification of the classical differential evolution algorithm to incorporate multiple objectives: the trial vector is selected if it dominates the target vector for all objectives. At the last generation all dominated solutions are removed. In the literature, this version is referred to as Gene ralized Differential Evolution GDE   4  R ESULTS  We generated a training set from a mixture of ten twodimensional Gaussian distributions. Their exact characteristics are unimportant here and only the relative changes between two training sets will be indicated. The set contains 1000 points. We used an algorithm that ran for 6 generations with a population of 100 individuals. The crossover Cr constant was set to 0.6 and the differentiation constant F to 0.9. We allowed the Lagrange multiplier  to vary between 0 and 2.2 and the initial number of clusters K between 1 and 1000 e.g the number of points. The population at the last generation contained 24 nondominated individuals. Since these results are unevenly spaced in the   K plane, we use a linear interpolation to generate a regular grid and represent the result in Figure 1 as a contour plot. The color scale shows the magnitude of either the entropy \(bottom\ or the distortion \(top\e also show the solutions with the true number of clusters \(ten and represent them with red points on the same figure 


  4  Figure 1: Contour plot the entropy \(bottom\the distortion \(top\on of the Lagrange multiplier   and the initial number of clusters K  The first conclusion drawn from Figure 1 is that the entropy and the distortion are indeed opposite objectives: entropy maxima correspond roughly to distortion minima. The second conclusion is that the placement of the solution indicat ed by the red dots doesn’t seem to obey any trend other than the simultaneous minimization of both the entropy and the distortion. The presence of one red point outside of the distortion contour plot is an artifact of the interpolation; there are not enough points in this region to interpolate To further our understanding of the results, we introduce another representation. The so-called Pareto front is a useful concept \(and repres entation degree of correlation \(conflict\ between different objectives The set of solutions to a problem with multiple objectives can include solutions that are dominated by others. A solution dominates a second one if it, at least, dominates for all objectives and dominates strictly, for at least one objective. The Pareto front is obtained when all the dominated solutions have been removed The Pareto front for the ECVQ problem is depicted on Figure 2. The shape of the curve D\(H indicates clearly the antagonism between the two objectives. Even though there is no obvious systematic evolution of the pair of parameters   K  ere is a systematic evolution of the final number of clusters on the plot. It increases steadily w ith the entropy. We also plot in red point on Figure 2 the solutions containing ten clusters These are located at the inflec tion point of the Pareto front We observe this trend for different data training sets. Hence we propose to use this feature as a criterion to choose the best solutions when working on a realistic set of data, e.g with unknown a priori number of clusters. However intuitive, the inflexion is not an easy quantity to calculate in the context we are considering. First, the density of points in the Pareto front might be uneven \(which is the case in Figure 2\herefore the derivative might be hard to compute. Secondly, for a typical Pareto front as the one sketched on Figure 2, the derivative does not permit to single out a specific point. Th erefore, for all practical purposes, we propose the following method to select a solution we choose the solution in the Pareto front for which the distance to the line joining the two points H\(D=0 and D\(H=0\, is maximal   Figure 2: Pareto front \(in blue The red point indicates the solutions with 10 clusters The black line joins the two points H\(D=0 and D\(H=0  A magenta point indicates the solution corresponding to the largest distance between the black line and the Pareto front  This method is simple. The calculation of both points needed for the line is st raightforward. For instance the entropy when the distortion is zero is simply the logarithm of the number of data points lo g 2  N  In the remainder of this paper, we will refer to the solution selected by our method as the inflexion point. The implementation of the ECVQ algorithm we use, calculates the distortion in normalized units in order to compare the distortion of the different dimensions. At the end of this article, we will give an example where the two dimensions will be of a different nature 


  5                                        For clarity purposes, we decided to normalize both the maximum distortion and entropy to one for all the Pareto fronts shown in this paper \(dividing the maximum distortion by two and the maximum entropy by log 2  N  This guarantees that the scale ra tio is one and therefore that the line joining the point chosen with our method to the line joining the points D\(H=0\ and H\(D=0\, will be perpendicular. On Figure 2, the solution found with our method is indicated by a magenta dot and is distinct from the solutions that contain the true number of clusters \(ten This difference could seem like a flaw of our method, but we will show that it is not. For this purpose, we represent on Figure 3 four synthetic data sets along with the corresponding solution. Theoretically, all points on the Pareto front are equally good. However as a visual examination of Figure 3 reveals, there are obvious qualitative differences if one makes the extra assumption of the existence of a best compro mise. In Figure 3, the plot labeled a\ows the solution with the highest entropy in the last population. With a total of 69 clusters \(in red the ten clusters is represented by several red points indicating an insufficient compression. On the other hand the solution with the highest distortion, shown in plot b has only one cluster. These two extreme cases support our claim that there are better solutions than these along the Pareto front. Note however that one cannot extend this claim to the limit where a unique solution would be better than all the other ones. Indeed, the program found two solutions with 10 clusters. The solution found by the method we propose in this paper, is shown in Figure 3 plot d\s solution has seven cl usters located at the exact location of the well-separated clusters. Two pairs of overlapping blue clusters are represented each by one red cluster each To further test our approach, we generated four sets of data that differed only in their spread \(e.g. variance since we are using Gaussian distributions\ncreased spread should increase the distortion per cluster. All other Figure 3: The data training set is represented with blue points. Clusters are represented with red points. Plot a corresponds to the right most solution on Figure 2 with the highest entropy and the lowest distortion. Plots b corresponds to the left most solution on Figure 2, with th e highest distortion and the lowest entropy \(null c s which are represented by a red dot and a magenta dot respectively on Figure 2 a  b  c  d   


  6 things being equal, the distortion when the entropy is zero should increase as well, when the spread increases                    In other words, for the same number of data points, the point H\(D=0 should remain the same while the point D\(H=0 should move to higher values as the spread increase. We increase the spread from the level of the data presented above to values equal or greater than the distance between clusters, level at which the notion of cluster itself is irrelevant. The results of this study are shown in Figure 4 As long as the clusters are well separated, our algorithm finds solutions close to the true locations \(synthetically generated increases the distortion of all solutions. This effect can be seen on the Pareto Fronts shown in Figure 5. After normalization, the higher distortion results in a deeper inflexion point, or higher curvature at the inflexion point Our criterion is able to find a good solution in all cases. The spread is so wide on plot d of Figure 4, that the data distribution is nearly uniform. To test this limit, we generated another set of synthetic data not shown\th a uniform distribution. The resulting Pareto front was similar to the green Pareto front shown on Figure 5. Hence, Figure 5 is an experimental illustration of our phenomenological criterion; the uniformity of the data influences the bend of the inflexion point and the distance of this point to the line joining the two points D\(H=0 and H\(D=0  Next we address the ultimate goal of this paper, finding a method to select the best pair of parameters  K 1, we list the parameters corresponding to the four runs that lead to plot a\o d\gure 4. We see that the entropy is more or less constant between r uns a\ch have the same number of clusters. Since we didn’t change the location of the clusters or the probability that a given data  Figure 4: The blue points represent generated data sets. Th e spread of each cluster increases from plot a\d The solutions found by our algorithm are indicated in magenta In plots a\to d generated clusters are indicated in black a  b  c  d   


  7 point belongs to a given cluster, the entropy per cluster should stay the same \(even with the different spreads entropy increases when one clus ter is added to the solutions The distortion does increase with spread. There is no systematic evolution of the initial number of clusters, K provided to the ECVQ algorithm  The Lagrange multiplier  logically decreases with spread to counteract the distortion     K D H Cluster number a\0.596324 89 0.102061 2.33139 7 b\0.449604 103 0.184047 2.33007 7 c\0.285814 138 0.267772 2.3982 8 d\0.285814 67 0.317165 2.58782 10 Table 1: Algorithm parameters \(K    entropy, distortion and cl uster number to the four plots of Figure 4   Figure 5: The blue, cyan, black and green Pareto fronts correspond to the data sets used in Figure 4, plots a\to d 5  I NTERNATIONAL S ATELLITE C LOUD C LIMATOLOGY P ROJECT DATA  The International Satellite Cl oud Climatology Project or ISCCP was est a bl i s hed i n 1982 as part of t h e W o rl d Climate Research Programme \(WCRP analyze satellite radiance measurements to infer the global distribution of clouds, their properties, and their diurnal seasonal, and inter-annual vari ations. In this context clustering techniques have been used to study cloud formation and patte  Jakob et al used the K-mean technique to analyze the data of 1999. This technique, like the ECVQ technique does not single out a solution. In their work, the authors performed several runs and found several solutions corresponding to different numbers of clusters After a statistical analysis of the different results, they conclude that the solution with four clusters is the best They also admit that this c hoice is “somewhat subjective In a more recent work, Rossow et al 9 ad d r ess th e shortfall of the K-mean technique e.g the need for a subjective choice, by selecting the different solutions using four, rather stringent, criteria. Following this procedure they elect the solution with six clusters the best  To test our technique, we used ISCCP data collected in January 2000, between 10 degrees of latitude south and 10 degrees of latitude north and between 130 degrees of longitude east and 170 degrees of longitude east. We display the cloud optical thickness versus the cloud top pressure on Figure 6 and the corresponding Pareto front on Figure 7  Figure 6: Sample of ISCCP data of January 2000 along with the clusters solutions found by our method magenta points  The solution we find has eight clusters. Our method therefore finds a number of clusters very close to the results of Rossow et al  and i t has t h e addi t i onal  advant age t h at  it doesn’t need experts input to do so. It is not the goal of the present work to undertake the exact same study as  i n order t o com p are wi t h ours. Thi s  woul d require an entire article by itself. Rather, we want to stress with the ISCCP data illustration that our phenomenological criterion is, at least, as good as other criteria and can be automated regardless of the nature of the problem. Rossow et al improve their early “subj ective introducing four objective crite ria. The multiplication of criteria is certainly a good way to single out a solution but makes the process problem and expert dependent without absolutely removing a level of arbitrary. Indeed, three out of the four criteri requi re arbi t r ary t h reshol ds. The solution immediately to the left of the magenta point on Figure 7, has six clusters and is the third in terms of 


  8 distance to the line joining the two intercepts D\(H=0 and H\(D=0 In this case, the classi fication according to this distance gives results very cl ose to the expert analysis Furthermore, our approach is general; it does not depend on the nature of a specific problem   Figure 7: Pareto front in blue point along with the solution found with our method   6  C ONCLUSION  In this paper, we described a method to automate the choice of parameters needed by an Entropy Constrained Vector Quantization algorithm. Provided with an initial guess of clusters to represent a set of data, the ECVQ minimizes two conflicting quantities, the distortion and the entropy. We used a differential evolution approach to find the best solutions among a population of randomly generated solutions. We then proposed a method to choose a unique solution by picking in the Pareto front the point with the greatest distance to a line join ing the points of zero entropy and zero distortion. This phenomenological criterion was tested and validated on different synthetic and experimental data sets.  In particular, we used data from the International Satellite Cloud Climatology Pr oject and found a similar number of clusters as Jakob et al   As l ong as a sufficient level of inhomogeneity justifies the clustering process, our method systematically provides a unique solution with its corresponding pair of parameters K and    R EFERENCES   Philip A. Chou. Tom L ookabaugh and Robert M. Gray Entropy-Constrained Vector Quantization”, IEEE Trans Acoustics, Speech, and Signal Processing 37 1989 2 Brav erm a n   A Fetzer, E., Eld e rin g  A., Nittel, S. an d  Leung K., “Semi-streaming Quantization for Remote Sensing Data  Journal of Computational and Graphical Statistics, Volume 12, Number 4, pages 759-780, 2003   B r averm a n, A. “C om pressi ng M a ssi ve Dat a  Set s  Usi ng Quantization  Journal of Computational and Graphical Statistics, Volume 11, Number 1, pages 44-62, 2002  Kennet h R o se, “Det erm i ni st i c  Anneal i ng for C l ust e ri ng Compression, Classification, Regression and Related Optimization Problems”, Proc. IEEE 86  2210, 1998   Kennet h  V Pri ce, R a i n er M  St orn and Jouni A Lampinen, Differential Evolution, Springer, 2005  J. Lam p inen http://www.it.lut.fi/kurssit/0304/010778000/MODE.pdf   e b site: http://isccp.giss.nasa.gov  C h ri st i a n Jakob and George Tsel i oudi s, “Object i v e identification of clouds regimes in the Tropical Western Pacific”, Geophysical Research Letters 30, 1-4, 2003  illiam B. Rossow, George Tselioudis, Allyson Polak and Christian Jakob, “Tropical climate described as a distribution of weather states indicated by distinct mesoscale cloud property mixtures” , Geophysical Research Letters 34,  L04701, 2005  B IOGRAPHY  Alexandre Guillaume  is a member of the information systems and computer science staff at the Jet Propulsion Laboratory. His recent work includes genetic algorithm application, quantum computing theory, experiments on superconducting devices and materials modeling. His work has been published in peer reviewed journals. He has a Ph D. in Physics from the Joseph Fourier University in Grenoble \(France Seungwon Lee  is a senior member of information systems and computer science staff at the Jet Propulsion Laboratory. Her recent work includes genetic algorithm application, high performance computing, materials modeling, and nonlinear dynamics system. Her work is documented in numerous journals and conference 


  9 proceedings. She has a B.S. and M.S. in Physics from Seoul National University in Korea, and has a Ph. D in Physics from Ohio State University Amy Braverman is a Senior Member of the Information Systems and Computer Science Staff at the Jet Propulsion Laboratory, California Institute of Technology. She holds a B.A. in Economics from Swarthmore College, an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Postdoctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr. Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams, statistical data fusion, highdimensional data analysis and visualization, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities. She serves on the Committee on Applied and Theoretical Statistics of the US National Academy of Science, and is the president of the Interface Foundation of North America- a professional society of statisticians, computer scientists and domain scientists who focus on applied statistics and computing in a wide variety of discipline areas including Earth science genomics and proteomics, telecommunications, defense and security, astrophysics and astronomy, information technology, and social sciences Richard J. Terrile created and directs the Center for Evolutionary Computation and Automated Design at NASA’s Jet Propulsion Laboratory His group has developed genetic algorithm based tools to improve on human design of space systems and has demonstrated that computer aided design tools can also be used for automated innovation and design of complex systems. He is a planetary astronomer and the codiscoverer of the Beta Pict oris circumstellar disk. Dr Terrile has B.S. degrees in Physics and Astronomy from the State University of New York at Stony Brook and an M.S and a Ph.D. in Planetary Sc ience from th e California Institute of Technology in 1978  


of other processes, standardizing them and linking with technology in an ES-enabled environment made those processes very rigid and inflexible. The best practice processes embedded in the software are considered truly best practices in this organization With regular updates ensuring the incorporation of latest developments in practices and technologies, it is generally believed that the requisite agility is also part of the best practices. This study results suggest the influence of integration and standardization characteristics on the firm s ability to build agility in their processes. The effects, however, vary depending upon the type of integration and the extent of that integration achieved by the organization after the implementation of enterprise system. While the technical tight-coupling of the enterprise system infrastructure may limit the firm s ability to build agile processes, both vertical and horizontal integration, and standardization of the processes and information appears to be contributing positively While demanding their IT infrastructures to be tightly integrated for control and visibility purposes with the help of enterprise systems, firms are seeking to deliver agility with loosely coupled systems and technologies simultaneously. Though it is considered difficult to achieve both agility and control simultaneously in the past, today introduction of web services and service oriented architecture are expected to deliver both. Recognizing the weaknesses of enterprise systems, ES software vendors and business organizations are expending a significant proportion of their IT investment in Web services and business process management technologies The leading enterprise systems software vendors such as SAP and Oracle are now incorporating Web services standards into their next generation software solutions. For example, SAP has so far delivered 1000 Web service components and recently released the first Web services enabled ES suite. These emergent technologies are centered on the goal of providing the requisite process agility to enterprises by offering a competent business process platform from which to dynamically compose processes [70 37  B u ild in g ag i lit y  in to b u si n e s s p r o cesses a n d  implementing them is not easy and is dependent not just on the IT infrastructure including enterprise systems, but also on other factors such as business process management maturity levels and process characteristics specific to a particular organization Further research on building and implementing agile processes in dynamic business environments is necessary   7. References  1 Ma lone T  W Crow s t on, K  H e r m a n G  Eds  Organizing Business Knowledge: The MIT Process Handbook, Cambridge, MA: MIT Press, 2003 2 G a rtne r Re se a r c h  Achieving agility: BPM delivers business agility through new management practices  Gartner Research Note, ID: G00137553, downloaded on 1 Feb 2007 from http://www.gartner.com 2006 3 C. M e y e r The Second Generation of Speed  Harvard Business Review 79\(4\ 2001, pp. 24-26 4 N  Sla c k S. C h a m be rs a nd R  J ohns to n, O p e r a t i ons  Management, 4 th edition, Prentice Hall, Essex, 2004 5 G  Ra y  J.B. Ba rne y a nd W  A  Muha nna  C a p a b ilitie s  Business Processes, and Competitive Advantage: Choosing the Dependent Variable in Empirical Tests of the ResourceBased View Strategic Management Journal 25\(1\004 pp. 23-37 6 G a rtne r R e se a r c h  Gartner position on Business Process Management Gartner Research Note, ID G00136533, downloaded on 1 Feb 2007 from http://www.gartner.com 2006  F o rrest er Research   Market Overview of ERP Applications The Technology And Industry Battle Heats Up downloaded on 25 June 2006 from http://www.foresterresearch.com 2005 8 T  H  D a ve npor t Process view of Management  Process Days Conference 20-22 September, Downloaded from http://www.leonardoconsulting.com/processdays05 on 4 October 2005 2005 9 D  M. U p to n T he Ma na gem e nt of Ma nuf a c t uring  Flexibility California Management Review 36\(2\1994 pp. 72-89 10 Y  Y  Y u s u f  M. Sa r h a d i a n d A  G una s e k a r a n Agile manufacturing: The drivers, concepts and attributes  International Journal of Production Economics 62\(2 1999, pp. 33-43 1 C  Cand ra an d S  K u m a r  Enterprise Architectural Framework for Supply Chain Integration Industrial Management Data Systems, 101\(6\2001, pp.290-303 12 R.D. G a llie rs Strategizing for Agility: Confronting Information Systems Inflexibility in Dynamic Environents In Desouza, K.C. \(ed Agile Information Systems: Conceptualization, Construction and Management Oxford: Butterworth-Heinemann, 2007, pp.115 13  V. Sa m b am ur thy   A  B h a r a d w a j a nd V. G r ov e r   Shaping Agility through digital options Reconceptualizing the Role of Information Technology in Contemporary Firms  MIS Quarterly 27\(2\2003, pp 237-263 1 R  S eeth a m r aju   Influence of Enterprise Systems on Business Process Agility  International Conference on Electronic Business ICEB +eBRF 2006 Tampere, Finland 28 Nov to 2 D ecember 2006 15 M  Va n O o s t e r ho ut E  W a a r ts a nd J  v a n  Hillegersberg Change factors requiring agility and implications for IT  European Journal of Information Systems 15\(2\2006, pp.132-145 16 R  Ra sc hk e a nd J S. Da v i d  Business process agility  Proceedings of the 11th Americas Conference on Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 10 


Information Systems Omaha, NE, USA, 11-14 August 355-360, 2005 1 R  J  V o ku r k a an d J F l i e dn er  J  The Journey toward Agility  Industrial Management & Data Systems 98\(4 1998, pp. 165-171 18 F. Be c k e r  Organizational agility and the knowledge infrastructure Journal of Corporate Real Estate, 3\(1 2001, 29-37 19 R  E. G r a c h e tti, L  D. Ma rti n e z  O.A  Sa e n z  a nd C S  Chen Analysis of the structural measures of flexibility and agility using measurement theoretical framework  International Journal of Production Economics, 86\(1 2003, pp.47-62 2 S  M a t h i y akal an  N  A s h r a f i  W  Z h an g an d F  W aage  Defining Business Agility:An Exploratory Study  Proceedings of the 16 th Information Resources Management Conference, 2005 21 K  C o n boy a nd B  Fitz g e ra ld Toward a Conceptual Framework of Agile Methods: A Study of Agility in Different Disciplines  The Nineteenth International Forum on COCOMO and Software Cost Modeling Los Angeles CA, 2004 2 L  M  M ead e an d K J Ro gers  A n a l y zi n g  organizational project alternatives for agile manufacturing processes: an analytical network approach." International Journal of Production Economics 37\(2\1999, pp241-261 23 N  Ca rr, N D o e s IT Ma tte r Inf o r m a tion T e c hnolog y  and the  Corrosion of competitive Advantage  Boston Harvard Business School Press, 2004 24 A S  Bh arad waj   A Resource-Based P e rsp ectiv e o n  Information Technology Capability and Firm Performance An Empirical Investigation MIS Quarterly 24:1\, 2000 pp. 169-196 2 S  Devraj an d R Koh l i   Performance Impacts of Information Technology: Is Actual Usage the Missing Link Management Science, 49\(3\2003, pp.273-289 26 J.W  Ross, a nd M.R V ita le  The ERP revolution Surviving vs. thriving Information Systems Frontiers, 2 2000, pp. 233-241 27 R. A g a r w a l, R  V  Sa m b a m urthy  V  P ri nc iple s  and Models for Organizing the IT Function," MIS Quarterly Executive, 1\(1\01, pp. 1-16 28 B a r u a  A  a nd T  M u k h o p a d hy ay  200 0  I nf or m a tion Technology and Business Performance: Past, Present, and Future," in Framing the Domains of IT Management Projecting the Future through the Past  Zmud, R.W. \(Ed Pinnaflex Educational Resources, Cincinnati, Ohio, 2000 pp. 65-84 29  P  W e il, M. Subra m a n i a nd M. Broa dbe nt  Building IT infrastructure for strategic agility  Sloan Management Review 44 \(1\, 2002, pp. 57-65 30 IBM 2 0 0 8   Information on Demand downloaded on 24 August 2008 from http://www01.ibm.com software/data information-on-demand/faq.html, 2008 31 P e isl   Delivering on demand business agility with business process management Hampshire: IBM Corporation, 2003 32 H P  Adaptive Enterprise downloaded on 24 August 2008 from http://www.hp.com/enterprise/483408-0-0-0121.html, 2008 33  SA P   Delivering Operational Excellence with Innovation: ESOA for ERP White paper, SAP AG downloaded on 25 August 2008 from http://viewer.bitpipe.com/viewer/print/document.do?access id=8041050&pages 2006 34 SA P   SAUG User group Summit: Opening presentation by Allan Fairhurst, General Manager, SAP Australia New Zealand, downloaded from http://www.saug.com/documents on 25 August 2008 2007 35 L  Fi nk a nd S  Ne um a nn Gaining Agility through IT Personnel Capabilities: The Mediating Role of IT Infrastructure Capabilities  Journal of the Association of Inform ation Systems  8\(2\, 2007, pp.440-462 36 A  E. Mondra g o n A  C   Ly ons a nd D  F. K e hoe  Assessing the value of information systems in supporting agility in high-tech manufacturing enterprises International Journal of Operations & Production Management 24\(11/12\, 2004, pp. 243-252 37 S P a dm a n a bhu ni, G   G a ne s h  D  Moitra   Web Services, Grid Computing, and Business Process Management: Exploiting Complementarities for Business Agility, Proceedings of the IEEE International Conference on Web Services \(ICWS'04\004 3 N Bi eb erst ei n  S  Bo se  L   W a l k er an d  A   L y n c h  Impact of service-oriented architecture on enterprise systems, organizational structures, and individuals IBM Systems Journal, 44\(4\, 2005, pp. 691-708 39 T  H  D a ve nport, J  G  H a rris a nd S. Ca ntre ll Enterprise systems and ongoing process change  Business Process Management Journal 10\(1\, 2004, pp. 16-26 40 S  K  Sia   M. T a ng C  S oh, a nd W  B o h, 2 0 0 2   Enterprise Resource Planning \(ERP\ Systems as a Technology of Power: Empowerment or Panoptic Control  The Database for Advances in Information Systems 33\(1\002, pp. 23-37 41 Som m e r  Business process flexibility: a driver for outsourcing  Industrial Management & Data Systems  103\(3\, 2003, pp.177-183 4 S New e l l  E  L  Wa gn er  an d G  Davi d   Clumsy Information Systems: A Critical Review of Enterprise System In Desouza, K.C. \(ed Agile Information Systems Conceptualization, Construction and Management Oxford Butterworth-Heinemann, 2007, pp.163-177 43 L  Ma rk us 20 01 Reflections on the System Integration Enterprise  Business Process Management Journal 7\(3\01, pp.1-9 44 L  Ma rk us, S. A z line  D P e trie a nd C   Tanis Learning from Adopters Experiences with ERP Successes and Problems  Journal of Information Technology 15\(4\2000, pp.245-265 45 T  W  Ma lo ne K  Crow s t on L  L e e  B P e ntla n d  C  Dellacoras, G. Wyner, J. Quimby, C.S. Osborn, A Bernstein, G. Herman, M. Klein and E. O Donnell Tools for Inventing Organizations: Toward a Handbook of Organizational Processes  Management Science 45\(3 1999, pp.425-433 46 R  H  C h ia ng E. L i m a nd V.C  Store y  A Framework for Acquiring Domain Semantics and Knowledge for Database Integration Data Base  31\(2\, 2000, pp.46-64 Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 11 


4  P  R L a w r en c e an d J W  L o rsch   Organization and Environment: Management Differentiation and Integration  Boston, MA: Harvard Business School Press, 1986  48 S  G l oube rm a n S. a n d H  Mintz b e r g  2 00 1  Managing the Care of Health and the Cure of Disease  Part II Integration  Health Care Management Review  26\(1\001, pp.70-84  49 S  U w iz ey em ung u a nd L  R a y m ond Integration flexibility and transversality: essential characteristics of ERP systems International Conference ofn Enterprise Information Systems \(ICEIS\04, pp.70-77  5 J D Ort o n and D E  W e i c k  Loosely Coupled Systems: A Reconceptualization Academy of Management Review, 15\(2\1990, 203-223  51 D a v e nport, T  H Mis s i o n Critic a l: Re a liz ing the  Promise of Enterprise Systems, Boston, MA: Harvard Business School Press, 2000  52 A  We nsle y  a nd E.V  St ijn  Enterprise Information Systems and the Preservation of Agility In Desouza, K.C ed Agile Information Systems: Conceptualization Construction and Management Oxford: ButterworthHeinemann, 2007, pp.178-187  53 D a v e nport, T   The Coming Commoditization of Processes Harvard Business Review, June, 2005, pp.101108  5 D Ken t and H A t t r i  2 006  SCOR, Lean and Six Sigma Supply Chain Synergy downloaded on 12 July 2007 from http://www.supply-chain.org/whitepapers, 2006  5  L ee M  and T  C h an g 2 00 6 Applying TQM CMM and ISO9001 in Knowledge Management for software development process improvement International Journal of Services and Standards, 2\(1\2006, pp.101-115  56 L  Ra y m ond S. Riv a rd a n d D  J u tra s  Small enterprises predisposition to adopt an ERP  Sixth International Conference on Enterprise Information Systems \(ICEIS Portugal, 10-14 April, 2004, pp.614-618  5  P  B i n g i   M  K S h arm a  an d  J K G o d l a J K   Critical issues affecting ERP Implementation Information Systems Management, 16\(3\1999, pp. 7-14  58 S. Ko nic k i Nike just didn t do it right, says i2 Technologies  Information Week downloaded on 5 March2006 from http://www.informationweek.com/827 nike.htm 2001  59 A  W  Sc he e r a n d F  H a be r m a nn Enterprise resource planning: making ERP a success  Communications of the ACM 43\(4\2000, 57-61  60 E.L  W a g n e r S.V  Sc ott a nd R.D. G a llie rs The creation of best practice software: Myth, reality and ethics  Information and Organization 16\(3\, 2006, pp. 251-275  61 T  Fors be rg L  N ils s o n a nd M A n toni  Process orientation : The Swedish experience  Total Quality Management, 10 4&5\:, 1999, pp. 540-547  62  P  Be s s o n, a nd F.Row e 20 01 ERP Project Dynamics and Enacted Dialogue: Perceived Understanding Perceived Leeway, and the Nature of Task-related Conflicts Database for Adavances in Information Systems, 32\(4\2001, pp. 47-66  63 R. E l Am ran i F. Ro w e  M. Bid a n  B. G e ff ro y Maronnat, and R. Marciniak, \(2003 ERP Implementation and Change: Towards a Cross-Functional View  Proceedings of the 11th European Conference on Information Systems Naples, Italy, June 16-21, 2003  64 J  D   Mc K e e n a nd H  A  S m ith Making it happen Critical issues in IT management Wile, Chichester, UK 2003  65 J.C  Hua n g  a n d S. Ne w e ll Knowledge Integration Process and Dynamics Within the Context of CrossFunctional Projects  International Journal of Project Management 21, 2003, 167-176  6  M  Hut t  B W a l k er an d G  F r an kw i c k  Hurdle the Cross-Functional Barriers to Strategic Change  Sloan Management Review 3, 1995, pp. 22-30  6 G u mmesso n  E    Qualitative Methods in Management Research Sage Publications Inc, Newbury Park, 1991  6 Yi n R   Case Study Research fourth edition, London Sage Publications, 2003  69 H  Ba rk i a nd A   P i ns onne a u lt A Model of Organizational Integration, Implementation Effort, and Performance Organizational Science, Vol. 16, No.2 2005, pp.165-179  70 J  G  Moone y a nd D   G a nle y  Enabling Strategic Agility Through Agile Information Systems: The Roles of Loose Coupling and Web Service Oriented Architecture  In Desouza, K.C. \(ed Agile Information Systems Conceptualization, Construction and Management Oxford Butterworth-Heinemann, 2007, pp.97-109  Proceedings of the 42nd Hawaii International Conference on System Sciences - 2009 12 


Time Time Time 50 350   10 0                   10 1                   10 2 50 350   10 1                   10 0                   10 1 50 350   10 0                   10 1  13         Isolated No SNR Adjust  Figure 11 - Centroiding for isolated range cells Filter Tuning Now that the centroided m easurements are reasonably consistent, the parameters that govern track filtering can be examined.  As previously promised, the effects and corrections for atmospheric refr action and sensor bias have been disabled so that 2 8 k D can be analyzed using a sliding window.  Of course the full analysis would include these effects and 2 8 k D at each time step would be collected and averaged over many trials Plots of the effect of changing process noise in a nearlyconstant-velocity filter are shown in Figure 12 and Figure 13 for Cartesian position and velocity respectively.  In both figures, the plotted values have been divided by 3 so that the desired value is always 1.  Increasing the process noise up to a point should increase the updated uncertainty and reduce 2 8 k D values.  Except near th e end of the trajectory when the measurements are off of beam center, the curves in Figure 12 and Figure 13 appear inconclusive for this expected trend If 2 8 k D values are way out of range there are additional intermediate filter values that can be examined.  For example, the state extrapolati on algorithms can be examined by comparing the consistency of 1 210 Isolated With SNR Adjust 300 300 300 0.005 212 212 212 212 212 kkkk T kkkk D xhzSxhz 35        s D 2 k Range   D 2 k,2 biased D 2 k,1 biased D 2 k,2  Figure 10 - Range bias error in detection primitive Centroiding Algorithm From Section 3, assuming that the centroided-range uncertainty for an isolated range cell is the same as its detection-primitive uncertainty may be incorrect Collecting and plotting 2 3 k D values only from isolated range-cell measurements can be used to analyze such assumptions.  The plots in Figure 11 compare differences between the isolated-cell algorithm defined in Section 3, an algorithm that modifies the uncertainty based on the SNR in the isolated cell, and the 2 3 k D values from all measurements 34\was used to modify the range uncertainty for the upper line labeled Isolated with SNR Adjust    4 22  2 2  resRi o R R Rn bdp bm  s D 2 k,3 Range    s D 2 k,8 Position     212 1 can also be examined using \(35 The residual is also commonly used to determine the assignment cost  212 kk z  P  k  k1 with z k The consistency of the innovation covariance k T kkkkk RHPHS 100 150 200 250 10 1                    D 2 k,3 biased 100 150 200 250 10 2                    100 150 200 250 10 1                    0.5 50  Figure 12 \226 Position consistency, filter tuning example  r  t t 34 If the All Centroided curve \(middle\as the baseline doing nothing \(lower\imates the uncertainty and 33\imates the uncertainty.  Dividing by the square root of the observed SNR leads to a more consistent covariance; however, there is currently no statistical evaluation to justify it             210 210 1 1 1 2 All Centroided 


Time 50 350   10 1                   10 0                   10 1  14         300 0.005  s D 2 k,8 Velocity   100 150 200 250 10 2                    0.5 50  Figure 13 \226 Position consistency, filter tuning example 5  C ONCLUSION  Calculating and observing the behavior of covariance consistency at different levels  in the radar signal processing chain represents a very powerfu l tool that can be used to assess the accuracy and softwa re implementation of radar signal-processing algorithms.  Analyzing covariance consistency is applicable to radar systems both in the field and in simulations.  The primary challenge in both arenas comes down to properly accounting for the true target states that contribute to detections, detection primitives measurements, and state estimates For a fielded radar syst em, achieving covariance consistency is usually a s econdary consideration behind achieving and maintaining track s.  Indeed, until recently radar specifications did not even include requirements for covariance consistency.  Recent covariance consistency requirements stem from the fact that the use of radar systems in sensor netting applications is on the rise Currently the combined e ffects of off-beam-center measurements, atmospheric correction, bias correction clustering and centrioding, data association, and filtering on state covariance consistency throughout a target\222s trajectory are not well known.  This is particularly true for radars using wideband waveforms and multiple hypotheses or multiple frame trackers.  Numerical results presented here indicate that algorithms early in the radar signal processing chain can significantly degrad e covariance consistency and that some errors are better tolerated than others For a simulated target in a modeled system, truth relative to some global reference is known. However, transforming truth through different refere nce frames and accounting for changes that occur during various radar processing algorithms is not as simple as it appears.  The techniques in this paper help expose this hidden complexity and provide a framework for discussing and expanding the future development of covariance consistency techniques.  Such future developments include issues related to mapping truth through the convolution operation typically used to simulate wideband signal processing and the fast Fourier transforms typically used in pulse-Doppler processing.  Sophisticated tracking algorithms that carry multiple hypotheses, associate across multiple frames, or weight the association of multiple targets within a single frame pose significant challenges in properly associating truth with state estimates.  Additional work, including an investigation of track-to-truth assignment, is needed before covariance consistency techniques can be applied to these algorithms Another area that needs furthe r analysis is the use of a sliding window to approximate the covariance behavior expected during a set of Monte-Carlo trials.  Various timedependent variables such as the target\222s range and orientation, the transmit waveform, the radar\222s antenna patterns toward the target, missed detections, and false alarms could easily viol ate the assumption that measurement conditions are nearly stationary over the time of the window.  It is importa nt to understand the conditions when this assumption is violated Finally, the examples presented here included a relatively benign arrangement of targets.  Further analysis in dense target environments with the related increase in merged detections, merged measurements, and impure tracks is needed.  Further analysis for targets traveling over different trajectories is also needed Even so, the techniques presented here can be extended to many of these analyses R EFERENCES  1  S. Blackman and R. Popoli Design and Analysis of Modern Tracking Systems Artech House, 1999 2  Y. Bar-Shalom and X. R. Li Multitarget-Multisensor Tracking: Principles and  Techniques YBS Publishing, Storrs, CT, 1995 3  Y. Bar-Shalom, Editor Multi-target-Multi-sensor Tracking: Advanced Applications and  Vol. I Artech House, Norwood, MA, 1990 4  D. B. Reid, \223An Algorithm for Tracking Multiple Targets,\224 IEEE Trans. on Automatic Control Vol. 24 pp. 843-854, December 1979 5  T. Kurien, \223Issues in the Design of Practical Multitarget Tracking Algorithms,\224 in Multitarget-Multisensor Tracking Y. Bar-Shalom \(ed.\43-83, Artech House, 1990 6  R.P.S. Mahler, Statistical Multisource-Multitarget Information Fusion, Artech House, 2007 


 15 7  B.-N. Vo and W.-K. Ma, \223The Gaussian Mixture Probability Hypothesis Density Filter,\224 IEEE Trans Signal Processing Vol. 54, pp. 4091-4104, November 2006 8  B. Ristic, S. Arulampalam, and N. Gordon Beyond the Kalman Filter Artech House, 2004 9  Y. Bar-Shalom, X. Rong Li, and T. Kirubarajan Estimation with Applications to Tracking and Navigation, New York: John Wiley & Sons, pg. 166 2001 10  X. R. Li, Z. Zhao, and V. P. Jilkov, \223Estimator\222s Credibility and Its Measures,\224 Proc. IFAC 15th World Congress Barcelona, Spain, July 2002 11  M. Mallick and S. Arulampalam, \223Comparison of Nonlinear Filtering Algorithms in Ground Moving Target Indicator \(GMTI Proc Signal and Data Processing of Small Targets San Diego, CA, August 4-7, 2003 12  M. Skolnik, Radar Handbook, New York: McGrawHill, 1990 13  A. Gelb, Editor Applied Optimal Estimation The MIT Press, 1974 14  B. D. O. Anderson and J. B. Moore Optimal Filtering  Prentice Hall, 1979 15  A. B. Poore, \223Multidimensional assignment formulation of data ass ociation problems arising from multitarget and multisensor tracking,\224 Computational Optimization and Applications Vol. 3, pp. 27\22657 1994 16  A. B. Poore and R. Robertson, \223A New multidimensional data association algorithm for multisensor-multitarget tracking,\224 Proc. SPIE, Signal and Data Processing of Small Targets Vol. 2561,  p 448-459, Oliver E. Drummond; Ed., Sep. 1995 17  K. R. Pattipati, T. Kirubarajan, and R. L. Popp, \223Survey of assignment techniques for multitarget tracking,\224 Proc  on Workshop on Estimation  Tracking, and Fusion: A Tribute to Yaakov Bar-Shalom Monterey CA, May 17, 2001 18  P. Burns, W.D. Blair, \223Multiple Hypothesis Tracker in the BMD Benchmark Simulation,\224 Proceedings of the 2004 Multitarget Tracking ONR Workshop, June 2004 19  H. Hotelling, \223The generalization of Student's ratio,\224 Ann. Math. Statist., Vol. 2, pp 360\226378, 1931 20  Blair, W. D., and Brandt-Pearce, M., \223Monopulse DOA Estimation for Two Unresolved Rayleigh Targets,\224 IEEE Transactions Aerospace Electronic Systems  Vol. AES-37, No. 2, April 2001, pp. 452-469 21  H. A. P.  Blom, and Y. Bar-Shalom, The Interacting Multiple Model algorithm for systems with Markovian switching coefficients IEEE Transactions on Au tomatic Control 33\(8  780-783, August, 1988 22  M. Kendall, A. Stuart, and J. K. Ord, The Advanced Theory of Statistics, Vol. 3, 4th Edition, New York Macmillan Publishing, pg. 290, 1983 23  T.M. Cover and P.E. Hart, Nearest Neighbor Pattern Classification, IEEE Trans. on Inf. Theory, Volume IT-13\(1 24  C.D. Papanicolopoulos, W.D. Blair, D.L. Sherman, M Brandt-Pearce, Use of a Rician Distribution for Modeling Aspect-Dependent RCS Amplitude and Scintillation Proc. IEEE Radar Conf 2007 25  W.D. Blair and M. Brandt-Pearce, Detection of multiple unresolved Rayleigh targets using quadrature monopulse measurements, Proc. 28th IEEE SSST March 1996, pp. 285-289 26  W.D. Blair and M. Brandt-Pearce, Monopulse Processing For Tracking Unresolved Targets NSWCDD/TR-97/167, Sept., 1997 27  W.D. Blair and M. Brandt-Pearce, Statistical Description of Monopulse Parameters for Tracking Rayleigh Targets  IEEE AES Transactions, Vol. 34 Issue 2,  April 1998, pp. 597-611 28  Jonker and Volgenant, A Shortest Augmenting Path Algorithm for Dense and Sparse Linear Assignment Problems, Computing, Vol. 38, 1987, pp. 325-340 29  V. Jain, L.M. Ehrman, and W.D. Blair, Estimating the DOA mean and variance of o ff-boresight targets using monopulse radar, IEEE Thirty-Eighth SSST Proceedings, 5-7 March 2006, pp. 85-88 30  Y. Bar-Shalom, T. Kirubarajan, and C. Gokberk 223Tracking with Classification-Aided Multiframe Data Association,\224 IEEE Trans. on Aerospace and Electronics Systems Vol. 41, pp. 868-878, July, 2005   


 16 B IOGRAPHY  Andy Register earned BS, MS, and Ph  D. degrees in Electrical Engineering from the Georgia Institute of Technology.  His doctoral research emphasized the simulation and realtime control of nonminimum phase mechanical systems.  Dr. Register has approximately 20 years of experience in R&D with his current employer, Georgia Tech, and product development at two early-phase startups. Dr. Register\222s work has been published in journals and conf erence proceedings relative to mechanical vibration, robotics, computer architecture programming techniques, and radar tracking.  More recently Dr. Register has b een developing advanced radar tracking algorithms and a software architecture for the MATLAB target-tracking benchmark.  This work led to the 2007 publication of his first book, \223A Guide to MATLAB Object Oriented Programming.\224  Mahendra Mallick is a Principal Research Scientist at the Georgia Tech Research Institute \(GTRI\. He has over 27 years of professional experience with employments at GTRI \(2008present\, Science Applications International Corporation \(SAIC Chief Scientist \(2007-2008\, Toyon Research Corporation, Chief Scientist 2005-2007\, Lockheed Martin ORINCON, Chief Scientist 2003-2005\, ALPHATECH Inc., Senior Research Scientist 1996-2002\, TASC, Principal MTS \(1985-96\, and Computer Sciences Corporation, MTS \(1981-85 Currently, he is working on multi-sensor and multi-target tracking and classification bas ed on multiple-hypothesis tracking, track-to-track association and fusion, distributed filtering and tracking, advanced nonlinear filtering algorithms, and track-before-detect \(TBD\ algorithms He received a Ph.D. degree in  Quantum Solid State Theory from the State University of New York at Albany in 1981 His graduate research was also based on Quantum Chemistry and Quantum Biophysics of large biological molecules. In 1987, he received an MS degree in Computer Science from the John Hopkins University He is a senior member of the IEEE and Associate Editor-inchief  of the Journal of Advances in Information Fusion of the International Society of Information Fusion \(ISIF\. He has organized and chaired special and regular sessions on target tracking and classific ation at the 2002, 2003, 2004 2006, 2007, and 2008 ISIF conferences. He was the chair of the International Program Committee and an invited speaker at the International Colloquium on Information Fusion \(ICIF '2007\, Xi\222an, China. He is a reviewer for the IEEE Transactions on Aerospa ce and Electronics Systems IEEE Transactions on Signal Pr ocessing, International Society of Information Fusion, IEEE Conference on Decision and Control, IEEE Radar Conference, IEEE Transactions on Systems, Man and Cybernetics, American Control Conference, European Signal Processing Journal and International Colloquium on Information Fusion ICIF '2007   William Dale Blair is a Principal Research Engineer at the Georgia Tech Research Institute in Atlanta, GA. He received the BS and MS degrees in electrical engineering from Tennessee Technological University in 1985 and 1987, and the Ph.D. degree in electrical engineering from the University of Virginia in 1998. From 1987 to 1990, he was with the Naval System Division of FMC Corporation in Dahlgren, Virginia. From 1990 to 1997, Dr Blair was with the Naval Surface Warfare Center, Dahlgren Division NSWCDD\ in Dahlgren, Virg inia. At NSWCDD, Dr Blair directed a real-time experiment that demonstrated that modern tracking algorithms can be used to improve the efficiency of phased array radars. Dr Blair is internationally recognized for conceptualizing and developing benchmarks for co mparison and evaluation of target tracking algorithms Dr Blair developed NSWC Tracking Benchmarks I and II and originated ONR/NSWC Tracking Benchmarks III and IV NSWC Tracking Benchmark II has been used in the United Kingdom France, Italy, and throughout the United States, and the results of the benchmark have been presented in numerous conference and journal articles. He joined the Georgia Institute of Technology as a Se nior Research Engineer in 1997 and was promoted to Principal Research Engineer in 2000. Dr Blair is co-editor of the Multitarg et-Multisensor Tracking: Applications and Advances III. He has coauthored 22 refereed journal articles, 16 refereed conference papers, 67 papers and reports, and two book chapters. Dr Blair's research interest include radar signal processing and control, resource allocation for multifunction radars, multisen sor resource allocation tracking maneuvering targets and multisensor integration and data fusion. His research at the University of Virginia involved monopulse tracking of unresolved targets. Dr Blair is the developer and coordinator of the short course Target Tracking in Sensor Systems for the Distance Learning and Professional Education Departmen t at the Georgia Institute of Technology. Recognition of Dr Blair as a technical expert has lead to his election to Fellow of the IEEE, his selection as the 2001 IEEE Y oung Radar Engineer of the Year, appointments of Editor for Radar Systems, Editor-InChief of the IEEE Transactions on Aerospace and Electronic Systems \(AES\, and Editor-in- Chief of the Journal for Advances in Information Fusion, and election to the Board of Governors of the IEEE AES Society,19982003, 2005-2007, and Board of Directors of the International Society of Information Fusion   


 17 Chris Burton received an Associate degree in electronic systems technology from the Community College of the Air force in 1984 and a BS in Electrical Engineering Technology from Northeastern University in 1983.  Prior to coming to the Georgia Institute of Technology \(GTRI\ in 2003, Chris was a BMEWS Radar hardware manager for the US Air Force and at MITRE and Xontech he was responsible for radar performance analysis of PAVE PAWS, BMEWS and PARCS UHF radar systems Chris is an accomplished radar-systems analyst familiar with all hardware and software aspects of missile-tracking radar systems with special expertise related to radar cueing/acquisition/tracking for ballistic missile defense ionospheric effects on UHF radar calibration and track accuracy, radar-to-radar handover, and the effects of enhanced PRF on radar tracking accuracy.  At GTRI, Chris is responsible for detailed analysis of ground-test and flight-test data and can be credited with improving radar calibration, energy management, track management, and atmospheric-effects compensation of Ballistic Missile Defense System radars   Paul D. Burns received his Bachelor of Science and Masters of Science in Electrical Engineering at Auburn University in 1992 and 1995 respectively. His Master\222s thesis research explored the utilization of cyclostationary statistics for performing phased array blind adaptive beamforming From 1995 to 2000 he was employed at Dynetics, Inc where he performed research and analysis in a wide variety of military radar applications, from air-to-air and air-toground pulse Doppler radar to large-scale, high power aperture ground based phased array radar, including in electronic attack and protection measures. Subsequently, he spent 3 years at MagnaCom, Inc, where he engaged in ballistic missile defense system simulation development and system-level studies for the Ground-based Midcourse defense \(GMD\ system. He joined GTRI in 2003, where he has performed target tracking algorithm research for BMD radar and supplied expertise in radar signal and data processing for the Missile Defense Agency and the Navy Integrated Warfare Systems 2.0 office.  Mr. Burns has written a number of papers in spatio-temporal signal processing, sensor registration and target tracking, and is currently pursuing a Ph.D. at the Georgia Institute of Technology  


  18 We plan to shift the file search and accessibility aspect outside of the IDL/Matlab/C++ code thereby treating it more as a processing \223engine\224. SciFlo\222s geoRegionQuery service can be used as a generic temporal and spatial search that returns a list of matching file URLs \(local file paths if the files are located on the same system geoRegionQuery service relies on a populated MySQL databases containing the list of indexed data files. We then also plan to leverage SciFlo\222s data crawler to index our staged merged NEWS Level 2 data products Improving Access to the A-Train Data Collection Currently, the NEWS task collects the various A-Train data products for merging using a mixture of manual downloading via SFTP and automated shell scripts. This semi-manual process can be automated into a serviceoriented architecture that can automatically access and download the various Level 2 instrument data from their respective data archive center. This will be simplified if more data centers support OPeNDAP, which will aid in data access. OPeNDAP will also allow us to selectively only download the measured properties of interest to the NEWS community for hydrology studies. Additionally OpenSearch, an open method using the REST-based service interface to perform searches can be made available to our staged A-Train data. Our various services such as averaging and subsetting can be modified to perform the OpenSearch to determine the location of the corresponding spatially and temporally relevant data to process. This exposed data via OpenSearch can also be made available as a search service for other external entities interested in our data as well Atom Service Casting We may explore Atom Service Casting to advertise our Web Services. Various services can be easily aggregated to create a catalog of services th at are published in RSS/Atom syndication feeds. This allows clients interested in accessing and using our data services to easily discover and find our WSDL URLs. Essentially, Atom Service Casting may be viewed as a more human-friendly approach to UDDI R EFERENCES   NASA and Energy and W a t e r cy cl e St udy NEW S website: http://www.nasa-news.org  R odgers, C  D., and B  J. C onnor \(2003 223Intercomparison of remote sounding instruments\224, J Geophys. Res., 108\(D3 doi:10.1029/2002JD002299  R ead, W G., Z. Shi ppony and W V. Sny d er \(2006 223The clear-sky unpolarized forward model for the EOS Aura microwave limb sounder \(MLS Transactions on Geosciences and Remote Sensing: The EOS Aura Mission, 44, 1367-1379  Schwartz, M. J., A. Lam b ert, G. L. Manney, W  G. Read N. J. Livesey, L. Froidevaux, C. O. Ao, P. F. Bernath, C D. Boone, R. E. Cofield, W. H. Daffer, B. J. Drouin, E. J Fetzer, R. A. Fuller, R. F. Jar not, J. H. Jiang, Y. B. Jiang B. W. Knosp, K. Krueger, J.-L. F. Li, M. G. Mlynczak, S Pawson, J. M. Russell III, M. L. Santee, W. V. Snyder, P C. Stek, R. P. Thurstans, A. M. Tompkins, P. A. Wagner K. A. Walker, J. W. Waters and D. L. Wu \(2008 223Validation of the Aura Microwave Limb Sounder temperature and geopotential height measurements\224, J Geophys. Res., 113, D15, D15S11  Read, W G., A. Lam b ert, J Bacmeister, R. E. Cofield, L E. Christensen, D. T. Cuddy, W. H. Daffer, B. J. Drouin E. Fetzer, L. Froidevaux, R. Fuller, R. Herman, R. F Jarnot, J. H. Jiang, Y. B. Jiang, K. Kelly, B. W. Knosp, L J. Kovalenko, N. J. Livesey, H.-C. Liu1, G. L. Manney H. M. Pickett, H. C. Pumphrey, K. H. Rosenlof, X Sabounchi, M. L. Santee, M. J. Schwartz, W. V. Snyder P. C. Stek, H. Su, L. L. Takacs1, R. P. Thurstans, H Voemel, P. A. Wagner, J. W. Waters, C. R. Webster, E M. Weinstock and D. L. Wu \(2007\icrowave Limb Sounder upper tropospheric and lower stratospheric H2O and relative humidity with respect to ice validation\224 J. Geophys. Res., 112, D24S35 doi:10.1029/2007JD008752  Fetzer, E. J., W  G. Read, D. W a liser, B. H. Kahn, B Tian, H. V\366mel, F. W. Irion, H. Su, A. Eldering, M. de la Torre Juarez, J. Jiang and V. Dang \(2008\omparison of upper tropospheric water vapor observations from the Microwave Limb Sounder and Atmospheric Infrared Sounder\224, J. Geophys. Res., accepted  B.N. Lawrence, R. Drach, B.E. Eaton, J. M. Gregory, S C. Hankin, R.K. Lowry, R.K. Rew, and K. E. Taylo 2006\aintaining and Advancing the CF Standard for Earth System Science Community Data\224. Whitepaper on the Future of CF Governance, Support, and Committees  NEW S Data Inform ation Center \(NDIC http://www.nasa-news.org/ndic 


  19   Schi ndl er, U., Di epenbroek, M 2006 aport a l based on Open Archives Initiative Protocols and Apache Lucene\224, EGU2006. SRef-ID:1607-7962/gra/EGU06-A03716 8] SciFlo, website: https://sci flo.jpl.nasa.gov/SciFloWiki 9 ern a, web s ite: h ttp tav ern a.so u r cefo r g e.n et  Java API for XM L W e b Services \(JAX-W S https://jax-ws.dev.java.net  Di st ri but ed R e source M a nagem e nt Appl i cat i on DRMAA\aa.org  Sun Gri d Engi ne, websi t e   http://gridengine.sunsource.net  W 3 C R ecom m e ndat i on for XM L-bi nary Opt i m i zed Packaging \(XOP\te: http://www.w3.org/TR/xop10  W 3 C R ecom m e ndat i on for SOAP M e ssage Transmission Optimization Mechanism \(MTOM website: http://www.w3.org/TR/soap12-mtom  W 3 C R ecom m e ndat i on for R e source R e present a t i on SOAP Header Block, website http://www.w3.org/TR/soap12-rep 16] OPeNDAP, website: http://opendap.org  Yang, M Q., Lee, H. K., Gal l a gher, J. \(2008 223Accessing HDF5 data via OPeNDAP\224. 24th Conference on IIPS  ISO 8601 t h e Int e rnat i onal St andard for t h e representation of dates and times http://www.w3.org/TR/NOTE-datetime 19] ITT IDL, website http://www.ittvis.com/ProductServices/IDL.aspx 20] Python suds, website: h ttps://fedorahosted.org/suds  The gSOAP Tool ki t for SOAP W e b Servi ces and XM LBased Applications, website http://www.cs.fsu.edu/~engelen/soap.html  C hou, P.A., T. Lookabaugh, and R M Gray 1989 223Entropy-constrained vector quantization\224, IEEE Trans on Acoustics, Speech, and Signal Processing, 37, 31-42  M acQueen, Jam e s B 1967 e m e t hods for classification and analysis of multivariate observations\224 Proc. Fifth Berkeley Symp Mathematical Statistics and Probability, 1, 281-296  C over, Thom as. and Joy A. Thom as, \223El e m e nt s of Information Theory\224, Wiley, New York. 1991  B r averm a n, Am y 2002 om pressi ng m a ssi ve geophysical datasets using vector quantization\224, J Computational and Graphical Statistics, 11, 1, 44-62 26 Brav erm a n  A, E. Fetzer, A. Eld e rin g  S. Nittel an d K Leung \(2003\i-streaming quantization for remotesensing data\224, Journal of Computational and Graphical Statistics, 41, 759-780  Fetzer, E. J., B. H. Lam b rigtsen, A. Eldering, H. H Aumann, and M. T. Chahine, \223Biases in total precipitable water vapor climatologies from Atmospheric Infrared Sounder and Advanced Microwave Scanning Radiometer\224, J. Geophys. Res., 111, D09S16 doi:10.1029/2005JD006598. 2006 28 SciFlo Scien tific Dataflo w  site https://sciflo.jpl.nasa.gov  Gi ovanni websi t e   http://disc.sci.gsfc.nasa.gov techlab/giovanni/index.shtml  NASA Eart h Sci e nce Dat a Sy st em s W o rki ng Groups website http://esdswg.gsfc.nasa.gov/index.html   M i n, Di Yu, C h en, Gong, \223Augm ent i ng t h e OGC W e b Processing Service with Message-based Asynchronous Notification\224, IEEE International Geoscience & Remote Sensing Symposium. 2008 B IOGRAPHY  Hook Hua is a member of the High Capability Computing and Modeling Group at the Jet Propulsion Laboratory. He is the Principle Investigator of the service-oriented work presented in this paper, which is used to study long-term and global-scale atmospheric trends. He is also currently involved on the design and development of Web Services-based distributed workflows of heterogeneous models for Observing System Simulation Experiments OSSE\ to analyze instrument models. Hook was also the lead in the development of an ontology know ledge base and expert system with reasoning to represent the various processing and data aspects of Interferometric Synthetic Aperture Radar processing. Hook has also been involved with Web Services and dynamic language enhancements for the Satellite Orbit Analysis Program \(SOAP\ tool.  His other current work includes technology-portfolio assessment, human-robotic task planning & scheduling optimization, temporal resource scheduling, and analysis He developed the software frameworks used for constrained optimization utilizing graph search, binary integer programming, and genetic algorith ms. Hook received a B.S in Computer Science from the University of California, Los  


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


