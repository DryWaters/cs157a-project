Proceedings of the Second International Conference on Machine Learning and Cybernetics Wan 2-5 November 2003 A FAST ALGORITHM ON DISCOVERY LARGE ITEMSETS XIONG-FE1 LI, XUE-BA1 ZANG College of Computer Science and Technology, Jilin University,Changchun,China EMAIL:lxf@jlu.edu.cn Abstract Discovering large itemsets is the key problem of algorithm for data mining. In this paper the support vector of itemsets is presented The algorithm LIG can prognosticate the capability of a large k-itemset extending a candidate k+l-itemset 
by calculating support vector so the size of candidate set has been reduced and the efficiency of algorithm has been raised As the number of items is very large, the size of candidate set is huge. Main memory can\222t load all of candidate set. For reduce IIO swap, the algorithm builds the candidate hash tree based on the estimated support of candidate itemsets The performance of algorithm is better Keywords Data mining Large itemsets Algorithm 1 Introduction Association rule plays an essential role in data 
mining l1 Some algorithms for mining associations have been proposed in recent years. Apriori algorithm 222I is one of the best-known. There are three kind of design pattern Apriori-like candidate set, Lattice-based algorithm 222I and Han\222s FP-tree 41 To discovery large itemsets is the key for all of them The subset of large itemset must be a large itemset 222I In Apriori, the large itemset Lk-1 found in the k-1 pass are used to generate candidate itemsets ck then Lk is generated from ck The size of ck is much larger than 
222S Most of algorithms are derived from Apriori The common weakness of them is that the size of candidate set is too large When number of items is huge Cz is the bottleneck of algorithms When the number of items grows to lOOk or lager the efficiency of CPU will be down to zero The disk T/O swaps is frequent for main memory can\222t load all of candidate sets. In this paper,we propose a fast algorithm for discovering large itemsets. It improves the efficiency by two ways 1 Gave a concept of support vector to count itemsets\222 support in transactions of different size, form a 
support vector. Reduce the size of candidate set 2 Optimised the candidate hash tree to decide nodes\222 location in hash tree by their support. The nodes accessed frequently can be preserved in main memory The rest of this paper is organized as follows: Section 2 is our theory and algorithm on support vector and performance study. In section 3 we describe an optimised hash tree, and evaluate the performance of the algorithm Finally, we conclude the paper in section 4 2 The algorithm based on support vector 2.1 Basic theory and concept Let ipD\(I is the support of itemset 
I in the dataset D minsupport is the threshold of minimum support IT represents the size of the transaction T that is the number of items in T TED Let d be a itemset d is the size of d that is the number of items in d Lemma 2.1 In dataset D the support of the itemset I 222 is independent of the transactions whose size is less than All of proofs are omitted 
in the paper Lemma 2.2 In dataset D Given the large k-itemset Ik Let D\221 d/dED A Idl2m>k if ipD\(&J<minsupport then for a large m-itemset I on D there must be Definition 2.1 Whenplk<q sk is the times of itemset Z occurring in k-size transactions; when k=q sk is the times of itemset I occurring in the transactions whose size is not less than k Here sk=pD\(I The vector 
sp,sp+l   s sq is named as the support vector of itemset I Lk is the set of large k-itemsets LCk={itemseteLkl 2 itemset.s,>minsupport LCk is named as the seed k-itemsets Lemma 2.3 Vector ibik   is and jbjk+l   are the support vector of large k-itemset itemset and itemset PI Q I 4 k=p Definition 2.2 i=k+l 0-7803-7865-2/03/$17.00 02003 IEEE 204 


Proceedings of the Second International Conference on Machine Learning and Cybernetics Xi'an 2-5 November 2003 respectively so f min\(ipjp and  n itemsetjl=k-1 are the necessary condition that the itemset generated by itemseti and itemsetj is the element of ck p=k+l 2.2 Algorithm LIG Now we give the LIG algorithm for discovering large itemsets. According to lemma 2.1 while passing the data to determine the set of large k-itemsets the algorithm can delete the k-size transactions from the data Lk is the set of large k-itemsets LCdk is the set of seed k-itemsets, the algorithm generates candidate set Ck+l fiom Lek not from Lk Using the support vector can reduce the size of Lek LIG algorithm is as follows 6 if @>k\then Lck=Lcku C 7 end 8 end LIG algorithm generates the candidate itemsets using the seed itemsets, but not the large itemsets. Since the size of the seed set is less than the size of the large set, so the size of the candidate set is less, and LIG is faster Lemma 2.3 strengthens the restrict conditions for generating candidate set this can reduce the size of the candidate set JOIN function if \(k>2\then insert into Ck select p.iteml,p.itemz  p.itemk+q.itemk.I from LCk-1 P,LCk-I q,LCz s where p.iteml=q.iteml   p.itemkS2=q.itemk p.itemk Cl candidate 1 itemset L1=\(large 1-itemset q itemk p  item I s iteml q itemk I s itemz 4 and 2 min\(p.r,q.r,s.r i=2 else LC1={itemsetEL1l itemset.si2minsupport r=k D'=D 1-itemset insert into Ck for k=2 initializing q;ILCk-lI>l;k++,q do Ck=JOIN\(LCk  New candidates select p  iteml p  item2 from LCk-1 Ck-l where p.iteml<q.iteml forall transactions tE D do begin Ct=subset\(Ck,t  Candidates contained in t if \(r>q\then I-q 4 1 0 11  12 13 c.sr 14 end 1 5 LCk=limit_gen\(k,Ck  Generating Lk and LCk 1 6 end if Fk then D'-=t Deleting k-size transactions forall candidates c E Ct do 17 L=u kLk 4 Based on lemma 2.2 when si<minsupport m is a up-limit size of candidate Also we can using it to determine the seed set i=m The limitsen function 1\forall ceck do begin 2 for p=q,sum=c.s,;sum<minsupport and p>k,p sum+=c.s 4 if \(sum2minsupport\then begin 5 do Lk=LkU c  The set of large itemsets and f min\(p.r,q.r itemsets, it is necessary to take it as joining condition r=k Since the seed 2-itemset is the bridge linking two k 2.3 Deciding the length of support vector In LIG algorithm, each itemset has a support vector For a k-itemset I its support vector is s,@k+l  sq-~,sq We'd better let the length of support vector as big as we want In the dataset D of transactions the size of the maximal transactions is len if let the length of support vector be len, LIG will perform best. But in general len is very large, the number of itemsets is huge it will take large space of main memory The length of transaction is given from a Poisson distribution[21, let mean be til we can let the length of support vector be Iil+ATI For example given a database with the size of maximal transactions being 10 and average size of transactions being 5 if we let the length of support vector be 7 the support vector of 1-itemset for the first pass will be sI,s2  S~,S the support vector of 2-itemset for the second pass will be s2,s3   s7,sa In the same way we can set the low-limit length of 205 


the support vector For example given a database with the size of maximal transactions being 20 and average size of transactions being 10 let the up-limit length of support vector be 13, then the support vector of 1-itemset for the first pass will be sI,sz S,~,S However since few short transactions contain this itemset if the support vector of Z is 0,0,0,0,0,2,5,7,3   and the support vector of Z2 is 0,0,0,0,1,6,3,4,8   when joining these two itemsets to generate candidate itemset we get a united support vector 0,0,0,0,0,2,3,4,3  in which the front sections are all zero To reduce the space cost we merge the front sections of vector, and set a low-limit length If the average size of transactions is 14 we may set the low limit length of support vector 171-AT2 In this situation we define the support vector again For a k-itemset I its support vector is sb sp s,+l  sq-,,sq when i=k si is the times that itemset I presented in transactions whose size is equal to i when i=p s is the times that itemset Ipresented in transactions whose size is bigger than k but not bigger than i when q<Zq s is the times that itemset Z presented in transactions whose size is equal to i when i=q s is the times that itemset I presented in transactions whose size is not less then i In this way the low-limit length of the support vector in the former example may be set 7 then the support vector for the first pass will be s,,sg sa  Sl,>S2,3 Pass 2.4 Performance Apriori algorithm 1 LIG algorithm the size of I the size 1 the size of I the size 2.4.1 Example 2 3 4 Table 2-1 as an example, where minsupport is 2 13 10 11 6 13 2 8 1 13 0 4 0 Proceedings of the Second International Conference on Machine Learning and Cybernetics Xi'an 2-5 November 2003 206 Table 2 1 Database D  Tid I Items I Tid I Items I Tid I Items 100 I2 I 150 I 4,8 I 120 I 1,3,7,9 I 170 1 1,5,7 130 1,3,5,9 180 1,3,4,5,9 140 1.5.6.9 190 3.5.7 I 220 I 1,5,7 I After first passing over the data the algorithm gets the set of large 1-itemset L1 the support vector of itemset 2 in LI is l,l,O,O\according to lemma 2.2, it can not be extended it isn't the member of the seed set LC Meanwhile the algorithm deletes the transactions whose size is 1 in the data Using the JOIN function, the seed itemsets LC are used to generate the candidate itemsets Cz itemset 3 and 4 are the element of the seed itemsets LC the support vector of them are 0,0,1,3 and 0,3,0,1 respectively so the united support of two is O,O,O,l this don't meet minsupport 3,4 isn't the member of C In the same way 1,4},{1,8>,{3,8>,{4,7 4,8 4,93,\(7,8},{7,9 and 899 aren't in C Then, using limit-gen function, the algorithm generates L2 and LC2 since the support vector of itemset 43 in L2 is l,O,O,l\it isn't in the seed set LC2 The seed itemsets LC2 are used to generate the candidate itemsets C3 the subset 7,9 of  1,7,9 isn't in LC2 so 1,7,9 isn't contained C3 the same to 3,7,9 and 5,7,9 the subset 3,7 of 1,3,7} is in LC2 but the united support of all its subset 1,3},{1,7 and 3,7} is O,l,O this don't meet the minimum support 2 so  1,3,7 isn't the element Also 3,5,7} isn't in the potentially large 3-itemsets c3 so c3 is algorithm passes the data to count the support vector of each itemset in C3 while deleting the 3-size transactions in the data. Then, using limitsen function, the algorithm generates L3 and LC3 since the support vector of itemset  1,5,7 in L3 is 2,0,0,0 it isn't in the seed set LC3.and so no Using this example we compare LIG algorithm with Apriori algorithm, the result is given in table 2-2 The table of result shows LIG algorithm reduced both the size of the data and the candidate itemsets effectively improved'the efficiency of algorithm  1,395  1,3,9 1,5,7 1,5,9 11 Then the I database I ofck I database 1 ofck 1 I 13 I 36 I 13 I 11 2.4.2 Experimental Evaluation In order to study the performance the algorithm mining rules in very large database we make a lot of experiments All the experiments are performed on a Dell Edage24001667 server with 256 megabytes main memory running on Microsoft Windows 2000 advanced server SQL Server7.0 DBMS All the programs are written in MicrosoftNisual C 6.0 access the database in the way of ADO The synthetic data sets were generated using the procedure described in 2 


Proceedings of the Second International Conference on Machine Learning and Cybernetics Wan 2-5 November 2003 2403 2100  g I I200  900  600 200 IS0 I00 0.50 0.40 0.20 0.15 010 a N=100 So00  Apriori 3SW 3wo 2500 2wo so00  I  Apriori Y4 2.00 1.50 1.00 OS0 0 20 b N=500 nqber of item 10 100.00 Ik Sk IOk 2Ok c\2-itemsets Figure 2.2 Execution result As the minimum support decreases the execution times of both Apriori algorithm and LIG algorithm because of increases in the total number of candidate and large itemsets The performance of LIG algorithm is better than Apriori Algorithm, especially when the threshold is small the number of large itemsets increases or the number of items is very large LIG algorithm beats Apriori algorithm by more than an order of magnitude Figure 2.2a and Figure 2.2b compare the execution times for decreasing values of minimum support The number of items in Figure 2.2a is 100 in Figure 2.2b is 500 clearly the effect of LIG algorithm is better when the number of items is large and the threshold is small. In the actual application, the number of items is usually very large \(such as the kind of commodity in the supermarket For different support, the performance of LIG algorithm is steady and there isn't abnormity Figure 2.2 compares the size of the set of candidate 2-itemsets, the number of candidate 2-itemsets using Apriori algorithm increase exponentially with the number of items, LIG algorithm solves this problem to a certain extent especially when the number of items is very large the algorithm reduces U0 cost markedly. Number of transactions of database has reduced while pass The experiment result shows that the number of items is more the effect of LIG algorithm is better and the size of the set of candidate 2-itemsets is reduced obviously 3 Optimizing candidate hash tree Candidate itemsets of Apriori I are storied in the Hash tree Park put forward an algorithm basing on Hash tree to make the large itemsets 51 The document has been inquired into the Hash tree problem hrther All these methods are based on the premises that the hash tree is stored completely in RAM and not involve the problem of disc YO By way of the experiment we find that frequent disc I/O declined the utilization rate of CPU when number of item is increasing and the size of the candidate set is large \(specially the candidate set 2 for they cannot be stored completely in the ROM At the odious circumstances, the utilization ratio of CPU is close to zero therefore reducing U0 spending becoming the important way that raises the algorithm capability There are two ways to solve the mentioned above issue the first is concept concluding using the thought of multiple level association rules to reducing the number of items Another kind of scheme is improving the space dedicating method, and reduces disc I/O's times. Therefore we decide to optimise the candidate Hash tree and the management with the memory 207 


Proceedings of the Second International Conference on Machine Learning and Cybernetics Xi'an 2-5 November 2003 We find the itemsets as for candidate in the hash tree when the passing database to counts the supports the itemsets with higher frequency have higher access frequencies Based on this idea we decide to foretell the access frequencies of itemsets in the candidate hash tree and according to its frequency we put the itemsets which will be accessed more frequent into memory and put the itemsets rarely accessed into disk and switch them back to RAM when accessed Then how to decide the frequent level of itemsets in the hash tree As we known every item in the candidate itemsets must belong to large 1 itemsets so we can use the support of its first item to foretell the access frequency of itemsets in the candidate hash tree As for every item in the candidate set we calculate its frequency with his the support of its first item. When building the candidate hash tree, order the itemsets in the hash tree by its support decreasingly, then put frequently accessed itemsets in RAM while the less frequently accessed itemsets into disc For example let's describe this procedure that building the candidate 2-itemset and computing the support Firstly we built large 1-itemsets LI and then we order the itemset in LI according to the dimension of supports We give every itemset a placement and the precedence is continuously arranging the placement of itemset i is for gi The placement of the biggest support itemset in LI is I and the placement of the second support itemset is 2,and so on As we know till now our hash function use item i as parameter, named Hash\(i like i mod 2  Now we decide to use a new hash function to let the items in the candidate hash tree distributed according to their supports, named Hash If the number of the items is N for the candidate k-itemsets, it has k items altogether, and the scope is from 1 to N and the scope of placement of all items in the large 1-set is from 1 to N For example we can define a hash function like this Hash  the size of Hash table is L t is the quotient of L divided by items number N After finishing the definition of hash function we begin to build the hash tree of,candidate 2-set. Given any candidate 2-set c={Zi,Zj  ZHj  Zi is the first item of c is a member of large 1-itemset L1 And the placement ofZi is Gi. The process of inserting itemset c into Hash tree is as follows firstly apply the new hash function Hash\(gi to the root of the tree, that is to let gi decide the forestalled support of candidate set c .compare to others Such a process puts the itemsets into hash tree ordered by its accessed frequency. When coming to the next layer, we use the original hash function Hash  and use the second item of itemset to select branch of tree And the leaf layer is arrived at the next layer Finally we insert this candidate itemset 30000 lOOk a 5k  45000  40000  35000  30000  25000  20000  lS000  10000  Figure 3.1 Then we let candidate itemsets accessed frequency be resident in memory and others resident in disk For example, if the size of the Hash table in root node is 9 then we can put the candidate itemsets whose value in root node is from 1 to 6 into memory and put the candidate itemset whose Hash value is from 7 to 9 to disk When passing database to counting the support of candidate itemsets, usually the itemsets in memory can be accessed frequently and the itemsets in disk can be accessed rarely. When need to access the itemsets in disk we swap them with the itemsets in memory whose accessed frequency is relatively lowest This principle is obeyed by every swap. For a transaction t the process that we search Hash tree to find all the candidate itemsets contained in it is as follow at root node we hash the order in Ll of every item in t to choose branch node. Reaching the internal node at next level we go on to hash the next item itself till arriving a leaf node then find candidate itemsets contained in transaction t in this leaf node We compare the required space of optimised hush tree with traditional hush tree given in figure 3.1 I/O times of optimised hush tree are markedly less The relation between number of transactions and run time is 208 


Proceedings of the Second International Conference on Machine Learning and Cybernetics Xi 2-5 November 2003 showed in figure 3.l\(a Number of transactions is from 5k to 100k.Minmum support is 0.5 and number of items is 1OOO.We show in figure 3.l\(b the run time between optimised and unoptimised for the T10.D10k.N1000 dataset When minimum support is 0.7 size of candidate itemset 283619,almost all nodes of tree are storied in memory When minimum support is 0.5 size of candidate itemset 474315,about half of nodes are storied in memory.Compare with traditional hush tree the more nodes of tree are in RAM, the better is the performance of optimised hush tree The optimised hush tree rarely access disk because it put frequent accessed itemsets in RAM and rarely accessed in disk which improve the performance of the algorithm. Since candidate 2-set's size is often very large and RAM is especially insufficient the optimised hush tree performs even better in such situation 4 Conclusions and Future Work In this paper we presented an algorithm LIG for discovering large itemsets, to solve the problems of huge number of candidate itemsets and frequent 110 operating First we presented the concept of the support vector Since counting the support by the support vector we could get more information while passing the database Therefore it could predict the value of some itemsets as the potentially large itemsets remove the itemsets whose superset is not large itemsets in time and reduce the size of the candidate set meanwhile reduce the scale of the database and improve the efficiency of discovering large itemsets LIG generated candidate itemsets from seed itemsets but not large itemsets Since the scale of seed set is smaller than that of corresponding large set it reduced the size of candidate set generated. Furthermore we discuss how to decide the length of support vector Finally we proposed the method of reducing disk 110 for the problem that candidate itemset can not fit in memory  the optimisation of candidate Hash tree We decide their locations in the Hash tree based on the frequency of candidate itemsets and the frequency of candidate itemsets is determined by the support of its first item Then we let the itemset that is more accessed frequency in the Hash tree resident in memory, and others resident in disk So most work can be satisfied in memory when passing the database to count the support and need rarely to access disk The result of experiments showed that the optimised hash tree performed better The problem that remains in further to study have two, one is that the number of candidate itemsets is still large we can use association-scope to reduce the size of candidate set further the other one is that the frequency of candidate itemsets is predicted by the support of large 1 itemsets though the result of experiments is well we should study the corruption between transactions so that we could get a more accurate frequency of candidate itemsets References l R.Agrawa1 T Imielinski and A Swami Mining Association Rules between Sets of Items in Large Databases In Proc of the ACM SIGMOD Conference on Management of Data Washington D.C May 1993.~~207-216 2 R.Agrawa1 and R.Srikant Fast Algorithms for Mining Association Rules In Proc. of the 20th VLDB Conference Santiago,Chile,Sept, 1994 pp.487-499 3 C.C.Aggarwa1 and P.S.Yu Online generation of association rules In http:Ndb.nthu.edu.tw/present/present-15 ha 41 J.Han,J.Pei and Y.Yin.Mining frequent patterns without candidate generation. In Proc.2000 ACM Data.Dallas,TX.May 2000.p 1  12 5 J.S.Park,M.S.Chen and P.S.Yu An effective hash based algorithm for mining association rules.In Proc 1995 ACM-SIGMOD 1nt.Conf.Management of Data.San Jose, CA,May 1995.~~175-186 6 J.Kleinberg C Papadimitriou and P.Raghavan Segmentation problems Proceedings of the 30 Annual Symposium on Theory of Computing,ACM 1999 SIGMOD Int.Conf.Management of 209 


FUZZ-IEEE\22297 223Fuzzy Tracking methods for Mobile Robots\224 Applications of Fuzty Logic: Towards High Machine Intelligence Quotient Systems Vol 17 Ch 17. Prentice Hall series on Environmen tal and Intelligent Manufacturing Systems, forthcoming 1997  101 S M Smith D J Comer 223Automated Calibration of a Fuzzy Logic Controller Using a Cell State Space Algorithm,\224 IEEE Control Systems 1 99 1 pp 18 28  1 I A Saffiotti E H Ruspini K Konolige. \223Using Fuzzy Logic 200or Mobile Robot Control,\224 International Handbook of Fuzzy Sets and Possibiliv Theory D Dubois H Prade and H.J Zimmermann, Eds Kluwer Academic, forthcoming in 1997  121 223M Sugeno and M Nishida,\224Fuzzy Control of a Model Car\224 Fuzzy Sets andsystems 16 1985 103-1 13 North Holland  13 J T Takagi M Sugeno 223Fuzzy Identification of Systems and Its Applications to Modeling and Control,\224 IEEE Transac tions on Systems, Man and Cybernetics vol SMC-15 No 1 JanuaryBebruary 1985 14 G Ulivi 223Emergent Techniques for Mobile Robots and Autonomous Vehicles.\224 ICASAV-95 IFAC Workshop Tou louse, France, October 1995 Is H Wu W Chang H He P Xi 223A Fuzzy Control Method 200or Lateral Control of Autonomous Land Vehicle\224 Mobile Robots X W J Wolfe and C H Kenyon editors SPIE Pro ceedings Vol 2591, pp 125-131, SPIE Oct 1995 Appendix A Applying least squares to fuzzy consequents The development that follows leads to an expression of the output value U that allows the use of the least squares optimization in the proposed modelling method It assumes a Center of Sums COS defuzzification approach in which the fmal output set is obtained through summed aggregation, as well as a constraint that makes all the fuzzy consequents have the same area The output value U in the discrete universe case is given by the equation that represents the center of the area of the final output fuzzy set p U through summed aggregation d c PcI U  uj j=1 where pc U represents the membership function associated with the output inferred by the set of rules defined in the rule base For the Sum operator pc U is calculated as U  4 r 1 In the case of scaled fuzzy outputs the output activation level ai is used to scale the fuzzy consequent of rule Ri Thus the membership function of the scaled output fuzzy r 1 r 1 If all the consequent fuzzy sets are defined to have the same area, the following condition applies d J=1 where C is a constant Defining ai ai  7 c ar r=l then ai  ai because of ai and 0  ai 1 so the value of the control action can be obtained as 1 UCGm 1 It must be considered that any type of sets that satisfies the condition expressed in 7 can be used for the consequents whenever their centroids coincide with the uCG in the parameter vector This method is applied in the presented problem conditions to an algebraic equation of the type where represents measured output data A represents input data and is the vector of consequent rule parameters to be identified If antecedents are fixed, the least squares method can be easily adopted by using 8 where the set of input data is given by vector represents the vector of measured outputs and the parameter to be identified is given by vector UCG   uCG,]T  which represents the centers of gravity of the consequents With N sets of measured input-output data, the algebraic equation to be solved can be expressed as   A.p a  al X a  am  and consequently the vector j of U by the recursive least squares method can be computed 223r 1345 


3 3.5 fixed-style adap-style Er 4 S1 White burst the transient signal s1 is white and Gaus40 02~45\260\260 10 rn 0 sian with zero mean S2 Single exponentially-decaying sinusoid 30 60 20 X 4050 f S2\(i Ce f cos\(27f i/foq5 33 20  0   260 30   for i I l  M with the phase  randomly chosen from   20   0 2 7 and the frequency f randomly generated in the range 10   10       1 Js 4 1\260 10 1 0 0 0 0.75 0.7~~~~~~~~~~~~~~~~~~~~~~0 0.9 0.85 00 0 0 0 000 ogpq 0 0 0 1 02 1 03 legt le 1 1h02 1033 3 Exponentially-enveloped white burst transient length transient length Figure 9 Exponential case The right plot gives compariS3 i efS si i 34 son of the bias and thresholds used in the new adaptive Page procedure to those of the fixed-style Page scheme tailored to for i 1   M each specific transient length In all cases T 10l6 The agS4 Narrowband burst S4 iS created by passing white gregate SNR SL corresponding to the bias used in the fixed Gaussian noise through a narrowband filter whose bandwidth Page schme is pltted in he left igure.1S 0.3wr and whose center frequency iS chosen randomly 8 10\26065 _F 0~~~~~~~~~~~~~~~~~~~0 bL where bL denotes the bias for the fixed-style comparison only Page designed for length L 4 SIMULATION STUDY Again considering the fixed-style Page scheme first based The purpose of this section is to study the performance of the on the thresholds h's calculated by the FFT approach we above designed adaptive Page test regardless of the transient find from figure 8 that the behavior of the Page test designed signal's form strength and location The signal model is for particular transient lengths L does not provide constant detectability and thus modification according to our adaptive Ho x  w 31 Page procedure is desirable Therefore b and h\(L are ob tained according to 23 and 21 The corresponding fixed H1 x\(n  s\(n n men  n  n  nd bias b and variable threshold h\(L are plotted and compared s\(n  w\(m nS  n  mS  Td to those used in the fixed-style Page tests in figure 9 The 32 performance of the adaptive Page procedure is given in figure 10 and a gratifying detection improvement is observed in which x denotes the observation vector w is white Gausespecially for shorter transient lengths sian noise with zero mean and unit variance the vector s is the transient signal of interest The transients are of short du-1 ration M compared with the observation length N In our 60r 7 r 1.5 simulations we always use N  128 M  30 fs  16  2  and A  0.5 unless stated otherwise The transients are as 2.5 50   2.5  follows 50 10\260 1021o2 100 t analysis fixed-style envelope sient lengh0 0 simulation fixed-styld te envelope betwen7flse0larm0is  l6 Te udatetake theform Pagetestoptiize  anansiena adap style simulation adap style Page 0.4 designed for L200 loo 101 0 2 10 oo 101102103 transient length transient length Figure 8 Detect"ab"ility of exponential transients using fixedFigure 10 Performance of the adaptive Page scheme in style Page procedures designed for various but specific tranExponential transient problem The Pd envelope from the sient length L In all cases Pd 0.8 and the average time normal fixed-style Page procedure and the performance of the between false alarms is T 106  The update takes the form Page test optimized for transient length L=200 are shown for g\(x x 


2 35 04 T 2 0.4 0.2 0.2 provides the best performance over a wide range of transients 10 15 20 25 10 15 20 Unlike the Page detector the transient duration M is required here and Tmax does indeed show some sensitivity as regards c d this parameter as illustrated in figure 11 for transient signal 7  sl Similar relationship between Tmax and M is observed for 0.8 0.8 other types of transients 0.2 0.75 0 Cl 10 15 20 25 10 15 20 0.7  a aggregate Certainly this is not an exhaustive menu of transients but a for v is 2.5 when information about M is completely unavailwide range is covered able In fact several new power-law detectors were developed in 19 for instance we have We apply the adaptive Page detector designed in section 3 the Gaussian shift-in-variance case to the above transient's T a _da Page 0.6 Tma 0.6 Mf-1+i 0.6 l0.6 0.8 0.4 0.4 0.2 X X XN 37 detection The b and h\(L derived according to 21 is 12\(N Z\(Xi  X+\261 3 used in this adaptive detector where Pd  8 T  106 and t N  128 The assumptions on which the adaptive Page proSimilarly by combining 3 contiguous FFT bins we can write cedure is built are those of Si the detector is weakly suited Tf 3 It was found that for most practical transient signals Tf 2 to S3 and would seem to bear little affection for either S2 or and Tf 3 were preferable to Tpl S4 a b To illustrate the performance of our adaptive Page detector we compare it to other detectors In 18 it was found that 0.8  0.8 Nuttall's maximum detector 14 SNR dB aggregate SNR dB 0.65  Figure 12 Detection performances of the adaptive Page scheme The transient duration is M  30 samples different oT 0.6  panels refer to different transient signals with a transient signal sl b transient signal 82 c transient signal 83 and 0.55  d signal 84 0.5  We plot Pd versus the aggregate SNR in figure 12 in which Pf a  10-4 It is noted that the adaptive Page procedure 0.45 provides very close performance to that of the maximum detector in all four situations which is best in all cases as 0.4 0 20 40 60 M 80 100 120 in 18 This is exciting as we recall that M is tuned in the maximum detector and that our adaptive Page test reFigure 11 Detection performances of Tmax vs Al for quires no such prior information It is additionally noted that transient signal sl The true transient duration is M  30 qursnschpirnfmao.Itsadtoalyoedht transient  sina  Th tretasetduaini.l 3 the adaptive Page procedure provides performance superior samples and the aggregate SNR is 18dB The dash-dotted line indicates the best performance of Tmax when true M to even the improved power-law detector Tf12 in most cases is chosen The dotted line indicates the performance of the with the exception of S2 in which the transient is highly naradaptive Page scheme rowband Nuttall's Tmax looks for an increase in empirical variance as 5 SUMMARY does our Page processor In  1 8 it was found that another detector due to Nuttall works particularly well for narrowband The standard Page test is designed to detect a change in distritransient signals this based on the power-law statistic 15 bution amongst a conditionally independent observation pool defined as but works nicely at detecting even transient changes provided N they are of known character The standard Page update has Tp1 N 5 i 36 implicit a fixed negative bias and a detection is recorded i upon passage by this update of a fixed threshold these the In 36 v is an adjustable exponent and the Xi  are bias and threshold are determined by the ambient and tranmagnitude-squared FFT bins corresponding to the observasient distributional models fO and fil Specifically when fO tions x It has been found that the best compromise value and fi are close the bias is light and threshold high and 9 1 0 T with v=2.5 0.4 Tmax max S 


when fo and fi are distinct the Page test uses heavy bias and quence of Random Variables Biometrika Vol 57 No low threshold 1 pp 1-17 1970 6 B Broder and S Schwartz Quickest Detection ProceNotionally a transient signal that is long-and-quiet and one dures and Transient Signal Detection ONR Technical that is short-and-loud ought to have approximately the same Res and Transier 1990 detectability However these two engender very different Report 2 November 1990 Page tests and unfortunately the test designed for one can 7 D Casasent J-S Smokelin A Ye Wavelet and Gabor work quite poorly for the other Consequently in this paper Transforms for Detection Optical Engineering Vol an adaptive Page processor has been developed it uses a con31 No 9 pp 1893-1898 September 1992 stant bias but has a threshold that adaptively changes with the 8 S Del Marco and J Weiss M-band Wavepacket-Based number of samples since the most recent reset Transient Signal Detector Using a Translation-Invariant Wavelet Transform Optical Engineering Vol 33 No The new detector has been studied extensively in the Gaus7 pp 2175-2182 July 1994 sian shift-in-mean and shift-in-variance and also in the expo[9 T Dyson Topics in Nonlinear Filtering and Detecnential shift-in-scale cases It works very well and essentially tion PhD Thesis Princeton University Princeton NJ traces the envelope of performance achievable with the best 1986 Page processor tuned to each transient length the proposal is reasonable but ad-hoc but apparently we hardly could do 10 B Friedlander B Porat Performance Analysis of better Transient Detectors Based on a Class of Linear Data Transforms IEEE Transactions on Information TheTransient detection is interesting because one does not know ory Vol 38 No 2 pp 665-673 March 1992 in advance the sort of transient signal one has to look for it 11 C Han P Willett and D Abraham Some Methods could be narrowband or not it could have a sharp attack or it to Evaluate the Performance of Page's Test as used to could increase slowly and disappear abruptly Many transient Detect Transient Signals IEEE Transactions on Signal detectors are tuned to one type of transient and comparatively Processing pp 2112-2127 August 1999 blind to others What tends to unite transient signals of prac[12 G Lorden Procedures for Reacting to a Change in Distical interest however is that they are an organized agglomtribution Annals o Mathematical Statistics vol 42 eration of energy into contiguous or nearby time samples 1897-1908 1971 Now assuming a unit-normal ambient a transient detector pp 1 that assumes nothing but this local scale-change and one that 13 G Moustakides Optimal Stopping Times for Detectis reasonably insensitive to other characteristics such as specing Changes in Distributions Annals of Statistics vol trum is that based on the Page structure for Gaussian shift14 pp 1379-87 1986 in-variance The adaptive Page test developed here is also 14 A Nuttall Detection Capability of linear-And-Power insensitive to transient length it has here been tested for a Processor for Random Burst Signals of Unknown Locavariety of transient signals for which it is not on the surface tion NUWC-NPT Tech Rep 10,822 August 1997 well-suited and its performance has been found remarkably N o-Law Progood 15 A Nuttall Detection Performance of Power-LwPo good.a cessors for Random Signals of Unknown Location REFERENCES Structure Extent and Strength NUWC-NPT Technical Report 10,751 September 1994 1 D Abraham Asymptotically Optimal Bias for a Gen[16 E Page Continuous Inspection Schemes Biometrika eral Nonlinearity in Page's Test IEEE Transactions vol 41 pp 100115 1954 on Aerospace and Electronic Systems pp 1-8 January 17 B Porat and B Friedlander Performance Analysis of 1996 a Class of Transient Detection Algorithms-A Unified 2 M Basseville and I Nikiforov Detection of Abrupt Framework IEEE Transactions on Signal Processing Changes Theory and Application Englewood Cliffs Vol 40 No 10 pp 2536-2546 October 1992 NJ Prentice Hall 1993 18 Z Wang and P Willett A Performance Study of Some 3 A Shiryaev On Optimum Methods in Quickest DetecTransient Detectors IEEE Transactions on Signal Protion Problems Theory Prob Appl Vol 8 No 1 pp cessing Vol 48-9 pp 2682-2686 September 2000 22-46 1963 19 Z Wang and P Willett All-Purpose and Plug-In 4 M Basseville Edge Detection Using Sequential MethPower-Law Detectors for Transient Signals IEEE ods for Change in Level-Part  Sequential Detection Transactions on Signal Processing November 2001 Of Change in Mean IEEE Transactions on Acoustic 20 P Willett and B Chen A New Sequential Detector Speech and Signal Processing Vol ASSP-29 No 1 for Short Duration Signals Proceedings of ICASSPFeb 1981 98 Seattle WA May 1998 5 D Hinkley Inference About the Change-Point in a Se[21 P Willett and Y Bar-Shalom Track Testing for Single 10 


Targets in Clutter Proceedings of the SPIE Aerosense Now using the above procedure we can calculate hL for tranConference on Signal Processing for Small Targets sient duration L L  1  N given T and thus the timeApril 2000 varying threshold h\(L via 21 and also the performance in terms of Pd of the Page and our adaptive Page tests APPENDIX 2 EVALUATE PERFORMANCE OF ADAPTIVE 1 THE FFT APPROACH TO EVALUATE PAGE TEST PERFORMANCE OF FIXED-STYLE PAGE TEST For the transient change problem modeled as in 10 the runlength metrics 5 and 1 are of less interest than they would be for the permanent change problem Further and perhaps more important given their context in this paper these approximations do not apply at all in the case of a time-varying Uration A Page update Thus given the update rule and the average time-between-false alarms T we employ the FFT approach introduced in 11 to obtain the requisite threshold h that satisfies it and then to get the detection performance Pd Interested readers please refer to 11 for detail since here only a j l brief description is given AYvvfj Consider Page's test as an iterated sequential test ST with 7 n sample ihdex n    sbti-Sb lower and upper thresholds 0 and h Each individual ST is defined as an update rule Figure 13 Illustration of a Page implementation 20 with Z Z.i  g\(x non-zero initial point The change starts at point nm indicated by the dotted line and a decision rule as in 20 The procedure to calculate the probability of detection for the Thus the pdf of Zn is adaptive Page scheme is complicated by the fact that the Page bn Z  fn\(Z  fg Z 38 statistic be non-zero at the start point of a change Figure 13 shows such an example where the transient change begins at where fg is the pdf of the update g\(xn fn-i denotes the point nm and the threshold index i  5 at the start point nm pdf of Zn given that the test has continued to time n and due to the non-zero initialization Since the threshold index  denotes convolution the convolution can be made both i plays an important role in the adaptive scheme a non-zero accurate and quick via a fast Fourier transform FFT Then initialization could result in a different detection decision It we compute is thus necessary to calculate detection probabilities for difOn Z ferent threshold index i corresponding to the start point nm fn\(z f h n  0  z  h 39 Overall under the H1 hypothesis we have JO fn z dz 00 as a direct normalization In a straightforward manner under Pd\(nd S p\(i nd i 42 the Ho hypothesis one can express T as i=l F Eo  F1 N 40 where nd is the transient length and i is the threshold index T n=Z P1\(n corresponding to the start point nd According to the definiwhere Ei N is the expected number of samples to a decision tion the pmf p\(i is decided by the characteristics of the test for hypothesis Hi and pi n Pr\(ST ends at n and decides Hl Ho Under the H hypothesis assuming the standard situation we p\(i Pon i 1 Ho 43 have E 0 Pon nm Ho ndl1 Pd nd 5 Pr detect k resets 41 where Pon n l Ho Pr ST will continue to time-step n I I Ho k=o Under the Ho hypothesis assuming fo z  d\(z with the where nd is the transient length6 update g  we can calculate the pdf fr,\(z H0 according to 39 and thus calculate Pon nm Ho0 correspondingly 61n 11 it was noted that the probability of detection is increased both by latent detections caused by diffusive threshold-crossings after a transient's For each index i to calculate the corresponding Pd md i in end and also by a non-zero CUSUM value at the inception of a transient  Both of these can be accounted for via the direct FFT approach and for 42 we need to study stopping probabilities both for the case details we invite the reader to examine 11 fo Z f&i-1\(Z Ho and for fo z 5\(z  For the case that 11 


fo z fi 1 z Ho we consider a decision rule Z Jane Wang Z Jane Wang received the BSc degree from Tsinghua Univer[h\(n+i-1 stopanddecideH sity China in 1996 and the MS and ZC  h[\(n  i1 c ntine dtest 44 PhD degrees from the University of Con 0 h\(m  i 1 continue test necticut in 2000 and 2002 respectively oc 0 stop and decide Ho all in electrical engineering She spent two years as Research Associate of ElecAd b d ts d rtrical and Computer Engineering DeAnd based on this decision rule 44 we compute the imporpartment and Institute for Systems Research at the Univertant quantities sity of Maryland She is now an Assistant Professor in the Department of Electrical and Computer Engineering at the pO\(m  Pr\(ST ends at n and decides Ho University of British Columbia Her present research interests are in the broad areas of statistical signal processing p n  Pr ST ends at n and decides Hi r4 in Pr\(ST ends at in and decides H1 information security and wireless communications Respectively for the case that fo z  d\(z we consider a decision rule Peter Willett Peter Willett is a Profesh c s and decide H sor of Electrical and Computer EngiZne 0,h cstop 1 neering at the University of ConnectiZn E 1 0,h\(n continue test 45 _ cut Previously he was at the University oo 0 stop and decide Ho of Toronto from which he received his BASc in 1982 and at Princeton University from which he received his PhD in and compute the corresponding quantities p\260\(n and po n 1 H h w e mrt Now using p n and p n and pog\(n and po n we can  E 18.Hha rte,mogterop Now,'using a a  ics about the processing of signals from volumetric arrays calculate Pd nd i as in 41 and finally calculate the overall decentralized detection information theory CDMA learPd nd in 42 It is worth to mention that for theoretical d  ing from data target tracking and transient detection He analysis we use the infinite as the upper bound of i in 42 is a Fellow of the IEEE and is a member of the IEEE Sighowever we use a finite reasonable upper bound in practical nal Processing Society's SAM technical committee He is an calculation For instance in the Gaussian shift-in-mean appliassociate editor both for IEEE Transactions on Aerospace cations we plot p\(i vs i in figure 14 From this figure it is and Electronic Systems and for IEEE Transactions on Sysclear that p\(i decays quickly with the increase of i therefore tems Man and Cyberetics He is a track organizer for it is reasonable to consider the truncated pmf Similar obserRemote Sensing at the IEEE Aerospace Conference 2001vations could be found for the Gaussian shift-in-variance and 2003 and was co-chair of the Diagnostics Prognosis and the Exponential cases System Health Management SPIE Conference in Orlando He also served as Program Co-Chairfor the 2003 IEEE Systems 02 Man  Cybernetics Conference in Washington DC 0.18 0.16  0.14 0.12 a 0.1 0.08 0.06 0.04 0.02 100 101 102 103 Figure 14 The pmf p\(i in the Gaussian shift-in-mean case 12 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


