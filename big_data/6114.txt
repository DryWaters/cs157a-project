 Wuhan National Laboratory for Optoelectronics Wuhan China  School of Computer Huazhong University of Science and Technology Wuhan China 
Wen Xia  Hong Jiang  Dan Feng  Lei Tian  xia@hust.edu.cn jiang@cse.unl.edu dfeng@hust.edu.cn tian@cse.unl.edu 
002 
Combining Deduplication and Delta Compression to Achieve Low-Overhead Data Reduction on Backup Datasets 
002 
 Dept of Computer Science and Engineering,University of Nebraska-Lincoln Lincoln NE USA 
Abstract 
Data reduction has become increasingly important in storage systems due to the explosive growth of digital data in the world that has ushered in the big data era In this paper we present DARE a Deduplication-Aware Resemblance detection and Elimination scheme for compressing backup datasets that effectively combines data deduplication and delta compression to achieve high data reduction efìciency at low overhead The main idea behind DARE is to employ a scheme call Duplicate-Adjacency based Resemblance Detection  
 by considering any two data chunks to be similar i.e candidates for delta compression if their respective adjacent data chunks are found to be duplicate in a deduplication system and then further enhance the resemblance detection efìciency by an improved super-feature approach Our experimental results based on real-world and synthetic backup datasets show that DARE achieves an additional data reduction by a factor of more than 2 2X on top of deduplication with very low overhead while nearly doubling the data restore performance of deduplication-only systems by supplementing delta compression to deduplication 
DupAdj 
Data deduplication is a dictionary based data reduction approach popular in the backup/archiving storage area due to its demonstrated ability to effectively to compress backup/archiving datasets by a factor of 4-40X  1  2  In general a chunk-le v el data deduplication scheme splits data blocks of a data stream e.g backup les and databases into multiple data chunks of average size 8K or 4K with each being uniquely identiìed and duplicate-detected by a secure SHA-1 or MD5 hash sig 
1 Introduction 
nature also called a ngerprint  3  4  5  1  This secure ngerprint-based deduplication technique eliminates redundancy at the chunk or le level and thus scales better than the traditional LZ77 and Huffman coding based GZ compression  6  4  Delta compression however has been gaining increasing attention in recent years for its ability 
002 
is similar to chunk the base-chunk the delta compression approach calculates and then only stores the differences delta 
to remove redundancy among non-duplicate but very similar data les and chunks for which the data deduplication technology often fails to identify and eliminate  7  2  F or e xample if chunk  
A A 
2 1 1 
 and the mapping information between and  8  Thus it is considered a promising technique to effectively complement and supplement the ngerprint-based deduplication approaches by detecting and compressing similar data missed by the latter In this paper we propose DARE a low-overhead Deduplication-Aware Resemblance detection and Elimination scheme that effectively combines data deduplication and delta compression to 
A A 
2 2 1 
achieve high data reduction efìciency at low overhead The main contributions include  A DupAdj approach is proposed to exploit existing duplicate-adjacency information after deduplication to detect similar data chunks for delta compression Speciìcally due to locality of similar data in backup datasets the non-duplicate chunks that are adjacent to the duplicate Part of the work was done when the rst author was a visiting student at University of Nebraska-Lincoln 
002 
2014 Data Compression Conference 1068-0314/14 $31.00 © 2014 IEEE DOI 10.1109/DCC.2014.38 203 


2 Related Work 
Table 1 Duplicate detection vs resemblance detection for data reduction Duplicate Detection Resemblance Detection Objects Duplicate data Similar data Granularity Chunk-level Byte-level Representative Methods Secure-Fingerprint based Deduplication Super-Feature based Delta Compression Scalability Strong Weak Representative Systems LBFS  3  V enti 9  DDFS  5  REBL 7  DERD 10  SIDC 2  ones are considered good delta compression candidates for further data reduction  A theoretical and empirical study of the traditional super-feature approach is conducted which suggests that an improved resemblance detection for further delta compression is possible when the aforementioned existing duplicate-adjacency information is lacking or limited  An investigation into the restoration of deduplicated and delta compressed backup data suggests that delta compression has the potential to improve the data-restore performance of deduplication-only systems by further removing redundancy after deduplication and thus enlarging the logical space of the restoration cache  Our experimental evaluation results based on real-world and synthetic backup datasets show that DARE only consumes about 1/4 and 1/2 respectively of the computation and indexing overhead required by the traditional super-feature approach for resemblance detection while achieving a superior data reduction performance The rest of the paper is organized as follows Section 2 presents the background for this work The architecture key data structures and data reduction schemes of DARE are described in Section 3  We present and discuss the experimental evaluation of the DARE prototype in Section 4  Section 5 draws conclusions and provides directions for future work 
Data deduplication is gaining increasing traction in data-intensive storage systems as one of the most efìcient data reduction approaches in recent years Fingerprint-based deduplication techniques eliminate duplicate chunks by checking their secure ngerprints i.e SHA-1/SHA-256 signatures which has been widely used in commercial backup and archiving storage systems  5  1  One of the main challenges facing data deduplication is how to maximally detect and eliminate data redundancy in storage systems at low overhead In order to nd more redundant data the Content-Deìned Chunking CDC approach was proposed in LBFS to nd the proper cut-point of each chunk in the les and address the boundary-shift problem  3  Resemblance detection with delta compression  7  10  11  as another approach to data reduction in storage systems was proposed more than ten years ago but was subsequently overshadowed by ngerprint-based deduplication  5  1  due to the former s poor scalability  T able 1 compares these two data reduction approaches Resemblance detection detects redundancy among similar data at the byte level while duplicate detection nds totally identical data at the chunk level which makes the latter much more scalable than the former in large-scale storage systems REBL 7  and DERD  10  are typical super feature based resemblance detection approaches for data reduction They compute the features of the data stream e.g Rabin Fingerprints  12  and group features into super-features to capture the resemblance of data and then delta compress these data All these approaches require high computation and indexing overhead for resemblance detection As a result the simpler and faster deduplication method has become a more popular data reduction technology in the recent ve years  1  Nevertheless resemblance detection is gaining increasing traction in storage systems because of its ability to capture and eliminate data redundancy among similar but non-duplicate data chunks 
204 


3 Design and Implementation 3.1 An Overview of the DARE Data Reduction Approach 
DARE is designed to improve resemblance detection for additional data reduction in deduplicationbased backup/archiving storage systems Figure 1 shows a detailed case of the workîow of the DARE system For an incoming backup stream DARE goes through the following four key steps 1 The data stream is rst chunked by the CDC approach  3  ngerprinted by SHA-1 duplicate-detected and then grouped into container of sequential chunks to preserve the backup-stream locality  5  2 The DupAdj resemblance detection module in DARE rst detects duplicate-adjacent chunks in the containers formed in Step 1 see Section 3.2  After that DAREês improved super-feature module further detects similar chunks in the remaining nonduplicate and non-similar chunks that may have been missed by the DupAdj detection module when the duplicate-adjacency information is lacking or weak see Section 3.3  3 For each of the resembling chunks detected in Step 2 DARE reads its base-chunk then delta encodes their differences by the Xdelta algorithm  8  In order to reduce the number of disk reads an LRU and locality-preserved cache is implemented here to prefetch the base-chunks in the form of locality-preserved containers 4 The data NOT reduced i.e non-similar or delta chunks will be stored as containers into the disk The le mapping information among the duplicate chunks resembling chunks and non-similar chunks will also be recorded as the le recipes to facilitate future data restore operations in DARE For the restore operation DARE will rst read the referenced le recipes and then read the duplicate and non-similar chunks one by one from the referenced containers on disk according to the mapping information in the le recipes For the resembling chunks DARE needs to read both their delta data and base-chunks and then delta decode them 
Duplicate Detection Resemblance Detection Delta Compression Storage Management 
Figure 1 The data reduction workîow of DARE showing an example of resemblance detection for delta compression rst by the DupAdj approach and then by the super-feature approach D S and N here refer to a duplicate chunk a similar chunk and a chunk that is neither duplicate nor similar respectively which effectively complements and supplements ngerprint-based deduplication Difference Engine  13  and I-CASH  14  mak e full use of the delta compression technology to eliminate redundancy in memory pages and SSD caches respectively Shilane et al  2  proposed a stream-informed delta compression approach to reducing similar data transmission and thus accelerating data replication in a WAN environment This approach is also super-feature based and complements deduplication by only detecting resemblance among non-duplicate chunks in the cache that preserves the backup stream locality It avoids the costly global indexing at a limited loss of data reduction 
002\003\004\005\006\007\010\011\006\011 010\005\004\012\013\014\011\006\015\016 017\020\003\021\022\005\004\012\013\014\011\006\015\016 023\013\024\013\012\011\025 017\020\003\021\016\013\024\013\012\011\025 023\013\024\013\012\011\025 017\020\003\021\016\013\024\013\012\011\025 010\005\004\012\013\014\011\006\015\007\026\015\022\005\014\015\022 010\011\006\011\007\023\006\020\025\015\022 026\015\016\015\024\027\012\011\003\014\015\007\026\015\022\005\014\015\022\007 011\003\022\007\010\015\012\006\011\007\023\006\020\025\015\022 030\031\007\010\005\004\012\013\014\011\006\015\007\010\015\006\015\014\006\013\020\003 032\031\007\010\005\004\030\022\033\007\010\015\006\015\014\006\013\020\003 034\031\007\002\024\004\025\020\035\015\022\007\023\005\004\015\025\021\036\015\011\006\005\025\015\007\030\004\004\025\020\011\014\037 030!\007\010\015\006\015\014\006\013\020\003\007\020"\007\022\005\004\012\013\014\011\006\015\007\014\037\005\003#\016\007\027$\007\006\037\015\013\025\007"\013\003%\015\025\004\025\013\003\006\016 010 010 010 032!\007&\037\015\007\010\005\004\030\022\033\007\025\015\016\015\024\027\012\011\003\014\015\007\022\015\006\015\014\006\013\020\003\007\020"\007\016\013\024\013\012\011\025\007\014\037\005\003#\016 010 023 010 023 010 034!\007\030\022\022\013\006\013\020\003\011\012\007\022\015\006\015\014\006\013\020\003\007\020 006\037\015\007\002\024\004\025\020\035\015\022\007\007\023\005\004\015\025\021\036\015\011\006\005\025\015\007\024\020\022\005\012\015 017 017 023 013\003\016\015\025\006 007\002\003\004\005\006\007\027\011\014#\005\004\007\016\006\025\015\011\024'\007\024\020\022\013"\013\015\022\007"\025\020\024\007\006\037\015\007\007\004\025\015\035\013\020\005\016\007\027\011\014#\005\004\007\016\006\025\015\011\024 013\003\016\015\025\006 010!\007\023\006\020\025\015\007\003\020\003\021\025\015\022\005\014\015\022\007\022\011\006\011 024\020\022\013 017\020\003\021\023\013\024\013\012\011\025 010\015\012\006\011 
205 


    2    
 002  
036\013\012\015\007\032 036\013\012\015\007\030 036\013\012\015\007\034 036\013\012\015\007 036\013\012\015\007\010 036\013\012\015\007\036 032  001\002        007\007\007\010\005\004\012\013\014\011\006\015  007\007\007\007\007\007\007\007\007\007\023\013\024\013\012\011\025  023\013\024\013\012\011\025  023\013\024\013\012\011\025 010\005\004\012\013\014\011\006\015 
  002 002  
3.2 DupAdj Duplicate-Adjacency based Resemblance Detection 3.3 Theoretical Analysis of Super-Feature based Resemblance Detection 
As a salient feature of DARE the DupAdj approach detects resemblance by exploiting the existing duplicate-adjacency information of a deduplication system This is based on our observation that the modiìed chunks may be very similar to their previous versions in a backup system while unmodiìed chunks will remain duplicate and are easily identiìed by the deduplication process Figure 2 illustrates a case of duplicate data chunks and their immediate non-duplicate neighbors DARE records the backup-stream locality of chunk sequence by a  which allows an efìcient search of the duplicate-adjacent chunks for resemblance detection by traversing to prior or next chunks on the list as shown in Figure 2 When the DupAdj Detection module of DARE processes an input le it will traverse all the chunks by the aforementioned to nd the already duplicate-detected chunks If chunk of the input le E was detected to be a duplicate of chunk of le B DARE will traverse the doubly-linked list of in both directions e.g  and   in search of potentially similar chunk pairs between les E and B until a dissimilar chunk or an already detected duplicate or similar chunk is found Since the DupAdj detection approach only adds a doubly-linked list to an existing deduplication system DARE avoids the computation and indexing overhead of the conventional super-feature approach for resemblance detection in data reduction systems In case where the duplicate-adjacency information is lacking or limited DARE uses an improved super-feature approach to further detecting and eliminating resemblance as discussed in the next subsection As mentioned in Section 2  traditional super-feature approaches generate features by Rabin ngerprints and group these features into super-features to detect resemblance for data reduction  7  10  For example of a chunk length  N is uniquely generated with a randomly pre-deìned value pair  and N Rabin ngerprints as used in Content-Deìned Chunking  3  as follo ws 1 A super-feature of this chunk  can then be calculated by several features as follows 2 For example to generate two super-features with k=4 features each we must rst generate 8 features namely features 0...3 for and features 4...7 for  For similar chunks that differ only in a tiny fraction of bytes most of their features will be identical due to the random distribution of the chunkês maximal-feature positions  15  Thus tw o data chunks can be considered very similar if any one of their super-features matches The state-of-the-art studies  7  2  recommend 
doubly-linked list doubly-linked list 
032\011\014#\005\004\007\023\006\025\015\011\024\007 032\011\014#\005\004\007\023\006\025\015\011\024\007 032  032  032  032   001\002 
E B B E B E B F eature m a F eature Max m Rabin a mod SF eature SF eature Rabin F eature   F eature SF eature SF eature 
m n n m n m n i i i i N j i j i x x x k x k k 
Figure 2 A conceptual illustration of the duplicate adjacency The non-duplicate chunks adjacent to duplicate ones are considered potentially similar and thus good delta compression candidates 
1 1 1 1 1 32  1 1 2 
206 


4 Performance Evaluation 4.1 Experimental Setup Platform of the DARE Prototype 
1 2 1 1 2 1 2 1 2 1 0 1 1 
  003   004        002 
the use of 4 or more features to generate a super-feature to minimize false positives of resemblance detection for delta compression However our theoretical analysis and experimental observations suggest that the probability of false positives resulting from feature collision is extremely low but increasing the number of features per super-feature actually decreases the efìciency of resemblance detection First the false positives of 64-bit Rabin ngerprints tend to be very low as discussed in previous studies  12  16  This means that two chunks will have the same content of a hashing region 32 or 48 bytes with a very high probability if they have the same Rabin ngerprint Next the probability of two similar chunks having the same feature is highly dependent upon their similarity degree according to Broderês theorem  17  The less similar tw o data chunks are to each other  the smaller the probability of them having the same feature Thus the probability of two data chunks and being detected as resembling to each other by N features can be computed as follows 3 This probability is clearly decreasing as a function of the number of features used in a superfeature as indicated by the above probability expression Nevertheless all recent studies on delta compression suggest to increase the number of super-features  2  If an y one of the super features of two data chunks matches the two chunks are considered similar to each other Thus the probability of resemblance detection expressed as  can be increased by the number of superfeatures M For simplicity assume that the similarity degree follows a uniform distribution in the range 0  note that the actual distrib ution may be much more complicated in real w orkloads the e xpected value of resemblance detection can be expressed as a function of the number of features per superfeature and the number of super-features under the aforementioned assumption as 4 This expression of resemblance detection suggests that the larger the number N of features used in obtaining a super-feature is the less capable the super-feature is of resemblance detection On the other hand the larger the number M of super-features is the more resemblance can be detected and the more redundancy can be eliminated Thus DARE employs an improved super-feature approach with fewer features per super-feature to effectively complementing the DupAdj resemblance detection as discussed in Section 3.1  And our experimental results suggest that two features per super-feature appear to hit the sweet spot of resemblance detection in deduplication systems in terms of cost effectiveness We have implemented a prototype of DARE and tested it on the Ubuntu 12.04 operating system running on a quad-core Xeon E5606 processor at 2.13 GHz with a 16GB RAM a 14TB RAID6 disk array that consists of sixteen 1TB disks and a 120GB SSD of KINGSTON SVP200S37A120G DARE employs the widely used Rabin algorithm  12  16  and SHA-1 hash function  5  respecti v ely for the chunking and ngerprinting processes in data deduplication with an average chunk size of 8KB For the resemblance detection DARE adopts 
S S Pr Max H S Max H S S S S S 002 002 002 x x dx C N i 
          1 1  1 1     1 1 2 
002 003 004 
Conìgurations for Data Reduction 
N i i i N N N M N M M i i M i 
207 


4.2 An Empirical Study of the Super-Feature Approach 
Six well-known open-source projects representing typical backup workloads of deduplication and resemblance detection are used in this evaluation as shown in Table 2  These datasets consist of large tarred les representing sets of source code les or objects concatenated together by backup software  1  2  In order to assess the scalability of D ARE we generate tw o lar ger synthetic backup datasets according to the principles of synthesizing datasets outlined in recent studies  18  19  W e obtain the rst v ersion of the datasets from our research group of 16 users with 192K les and totaling 42GB mutate the data by the operations of modify delete and new for 20 1 and 1 of the les respectively in the Freq dataset and 10 0.5 and 0.5 of the les respectively in the Less dataset and then concatenate individual les to the tarred les The le modiìcations are also applied in the beginning middle and end of the les as suggested in  18  We rst examine the impact of the number of features per SF and the number of SFs used in resemblance detection via an evaluation driven by a real-world dataset In Figure 3\(a we plot the trend of data reduction of the traditional super-feature based approach as a function of the number N of features per SF and the number M of SFs We nd a general tendency of redundancy elimination being a decreasing function of N and an increasing function of M which is consistent with the results of our mathematical analysis presented in Section 3.3  That is the more SFs are used the more resemblance can be exposed to eliminate more redundancy On the other hand the more features used in generating each SF the less redundancy will be eliminated because the probability of more features in an SF being identical is smaller than that of fewer features in an SF To evaluate the overall performance impact of the number of features per SF we plot in Figure 3\(b the data-reduction throughput of the super-feature based approaches running on the RAID as a function of the number of features per SF We nd that the one-feature-per-SF approach has a lower throughput than the twoor three-features-per-SF approaches where the highest throughputs 6 
Datasets 
Table 2 Workload characteristics of the six real-world and two synthetic backup datasets Datasets Emacs GDB Glibc SciLab GCC Linux Freq Less versions 8 10 35 10 20 40 20 30 Size 1.15 GB 1.37 GB 3.18 GB 4.94 GB 8.91 GB 16.8 GB 857 GB 1372 GB a Compression vs features and SFs b Throughput vs features c Data reduction vs avg chunk size Figure 3 An empirical study of the super-feature based resemblance detection approach for data reduction a Data reduction of the super-feature approach as a function of the number N of features per super-feature and the number M of super-features shaded segments on each bar b Data-reduction throughput of the four super-feature based approaches as a function of the number N of features per super-feature c Data reduction as a function of the average chunk size by deduplication and the 2-features-per-SF SF-2F and the 4-features-per-SF SF-4F Total-2F and Total-4F refer to Dedupe  SF-2F and Dedupe  SF-4F respectively Note that results of subìgures a and c are evaluated on the GCC dataset and results of other datasets are similar to GCC thus are omitted due to space limit a CDC sliding window size of 48 bytes to generate features and uses the Xdelta algorithm  8  o compress the detected similar chunks 
208 


4.3 Deduplication-Aware Resemblance Detection 
Table 3 A comparison among Deduplication DupAdj DARE SF-2F and SF-4F approaches in the data reduction measure under six real-world datasets and both tarred and untarred versions for a total of 12 backup datasets Datasets Tarred UnTarred Emacs GDB Glibc SciLab GCC Linux Emacs GDB Glibc SciLab GCC Linux Versions 8 10 35 10 20 40 8 10 35 10 20 40 Dedupe 37.1 48.7 52.2 56.9 39.1 40.9 43.5 70.6 87.9 77.5 83.5 96.7 DupAdj 32.1 33.5 29.2 19.5 38.2 53.4 29.6 10.9 2.9 5.2 7.2 0.7 DARE 41.0 40.8 36.9 25.2 46.7 54.1 37.7 18.2 7.3 10.4 9.9 1.0 SF-2F 33.7 36.4 35.3 22.6 45.2 54.4 31.7 16.5 6.6 9.6 9.1 0.9 SF-4F 28.2 33.4 30.4 18.8 40.6 53.5 28.0 14.2 5.7 8.1 7.3 0.6 a Computation Overhead b Indexing Overhead Figure 4 A comparison among DARE SF-2F and SF-4F in terms of computation and memory indexing overhead are achieved This is because the former detects more similar chunks than the latter and thus induces more random I/Os in reading the base-chunks of the resemblance-detected chunks for delta compression On the other hand while approaches based on four or more features per superfeature detect less resemblance they incur higher computation latency overhead which lowers the throughput Figure 3\(c shows the data reduction results of deduplication and delta compression approaches as a function of the average chunk size It demonstrates that delta compression is very efìcient in supplementing deduplication for data reduction The larger the average chunk size is the less duplicate data are detected But the resemblance detection approach can detect almost all the redundant data e.g similar data that deduplication fails to identify regardless of the average chunk size 
In this subsection we evaluate DAREês resemblance detection and elimination schemes and compare DARE with the super-feature-only approaches based on 3 SFs with two-features per SF SF-2F and 3 SFs with four-features per SF SF-4F the same conìguration as SIDC  2  Note that D ARE s resemblance detection here is thus DupAdj supplemented by SF-2F where SF-2F is applied only to chunks that DupAdj has failed to detect as similar as discussed in Section 3.3  Table 3 shows the additional data reduction on top of the conventional deduplication Dedupe achieved by the four resemblance detection schemes DupAdj DARE SF-2F and SF-4F on the six real-world backup datasets both tarred and untarred Generally the DupAdj approach achieves a resemblance-detection efìciency similar to the SF-4F approach and DARE detects about 2-6 and 3-10 more redundancy than the SF-2F and SF-4F approaches respectively As indicated in Figure 3\(a and discussed in Section 4.2  SF-2F is more effective than SF-4F for resemblance detection Thus DARE detects the most resemblance by combining the DupAdj and SF-2F resemblance detection approaches In fact our evaluation results suggest that the average similarity degree of the DupAdj-detected chunks is 0.895 while that of the SF-2Fand SF-4F-detected chunks are 0.892 and 0.934 on the above six datasets which demonstrates that DupAdj is very effective and efìcient in detecting resemblance among the post-deduplication chunks with a very low false-positive rate Figure 4 shows the computation and indexing overhead incurred by the three resemblance detec 
209 


4.4 Scalability of DARE Data Reduction 
In order to better evaluate the scalability of DARE on larger backup datasets we have implemented the schemes of Stream-Informed Delta Compression SIDC  2  in SiLo  20  a memory-ef cient near-exact deduplication system that exploits the backup-stream similarity and locality As introduced in Section 2.1 SIDC only detects resemblance in the backup-stream locality-preserved cache that can reduce the indexing overhead of SFs and scales well in large-scale deduplication system Thus we employ their method to assess the scalability of different resemblance detection schemes and implement SiLo with a 20MB locality cache similar to SIDC  2  with a se gment size of 1MB The stream-informed approaches are denoted by the Cached/Cac preìx in Table 4 and Figure 5 The rst column under a dataset Freq or Less in Table 4 shows the percentages of data reduction and the second column shows the data reduction factor the ratio of before/after data reduction Since SiLo achieves nearly 99 deduplication efìciency of the exact deduplication i.e full index in memory while requiring substantially less memory overhead about 1/250 of the exact deduplication approach for ngerprint indexing  20  we only discuss the results of resemblance detection after SiLoês deduplication here in Table 4  As shown in Table 4  resemblance detection further reduces the storage space after the deduplication process by a factor of about 2.5 meaning that we can save about 60 of the post-deduplication storage space by resemblance-detecting post-deduplication chunks DARE achieves a superior data reduction efìciency on both datasets while the cached SF-2F and SF-4F schemes detect less resemblance which is consistent with the results on our real-world datasets summarized in Table 3  Table 4 also shows that the cached SF-2F and SF-4F schemes fail to detect a noticeable amount of resemblance due to the sometimes weak or lack thereof locality in the backup stream as the limitations of the stream-informed cache approach  2  To further understand the scalability of DARE Figure 5 shows the percentages of resemblance detected by its DupAdj detection and by the SF-2F and SF-4F approaches showing the individual contributions by the SF the SF and the SF on the two synthesized datasets DupAdj is very effective and efìcient in detecting resemblance in the deduplication system with abundant duplicate-adjacency information i.e dedupe factor 2 leaving DAREês improved super-feature approach very little smaller than 1 if any additional resemblance to detect see the DARE bars Figure 6 shows that DARE achieves the highest throughputs among all the approaches compared running on both RAID-structured HDDs and the SSD Cached SF-4F has the lowest throughput 
st nd rd 
Table 4 A comparison among the duplicate-detection and resemblance-detection approaches in terms of data reduction efìciency on the two synthesized larger backup datasets Freq and Less The   sign in front of a reduction percentage factor indicates the additional post-deduplication data reduction Datasets Freq Less Exact Dedupe 84.6 6.5X 91.2 11.4X SiLo Dedupe  20  84.4 6.4X 91.2 11.4X Cached Index of DARE DupAdj+SF-2F 9.28 2.5X 5.01 2.3X Cached Index of SF-2F 7.01 1.8X 4.31 2.0X Cached Index of SF-4F SIDC  2  5.32 1.5X 3.27 1.6X Full Index of SF-2F 9.61 2.6X 5.04 2.3X tion schemes Obviously SF-4F which computes more features but detects less resemblance consumes the most amounts of computation and indexing resources for resemblance detection DARE uses the same super-feature parameters as SF-2F but incurs only half of the computation and indexing overheads of the SF-2F approach because of DupAdjês very effective pre-screening of similar chunks by exploiting existing duplicate-adjacency information after data deduplication 
 
1 2 3 
210 


C C C 
Intuitively delta compression should slow down the data-restore performance of a data-reduction system since it needs to restore the resembling chunks by two reads one for the delta data and the other for the base-chunk and then delta decode them But in our evaluation of the restore operations for resembling chunks we nd that the speed of delta decode i.e Xdelta  8  tends to be v ery f ast about 1GB/s in the DARE system Another interesting observation is that for a restoration cache of a given size with a delta chunk and its based chunk  DARE actually caches more logical content of the two chunks and than a deduplication-only system and thus improves the datarestore performance by virtual of the enlarged restoration cache due to delta compression Figures 7\(a and 8\(a show that DARE on average doubles the data-restore speed of the deduplication-only system both running on the RAID Figures 7\(b and 8\(b clearly show that the reason lies in the fact that DARE reads half as many containers for restoration as the deduplicationonly system The superior data-restore performance of SF-2F and SF-4F to the deduplicationonly system is attributed to their data reduction efìciency see Tables 3 and 4  Since the restoreperformance for the other six datasets is similar to and consistent with that of the Less dataset they are omitted to save space The sudden increase in the data-restore performance of the deduplicationonly approach at the backup version 17 Figure 8\(a we observe is due to the fact that most of the backed-up sources targeted for restoration are from the current and recent backups and thus have fewer random reads for restoration 
Figure 5 Percentages of data reduced by DupAdj and the SF SF SF of the super-feature approach respectively in the streaminformed DARE SF-2F and SF-4F approaches a Throughput on RAID b Throughput on SSD Figure 6 Throughputs of four resemblance detection enhanced data reduction approaches on the two synthesized datasets a Restoration throughput b Containers read Figure 7 Data-restore performance versus backup version on the Linux dataset with an LRU cache of size 256MB a Restoration throughput b Containers read Figure 8 Data-restore performance versus backup version on the Less dataset with an LRU cache of size 512MB because it incurs the largest computation overhead for resemblance detection It is noteworthy that DAREês average data-reduction throughput on RAID at 50MB/s is much lower than DAREês average throughput of 85MB/s on SSD The root cause of RAIDês inferior data-reduction performance in Figure 6\(a mainly lies in the random reads of the base-chunks In general DARE achieves superior performance of both throughput and data reduction efìciency among all the resemblance detection approaches as indicated in Figure 6 and Table 4  
002 
4.5 Restoration Performance 
i,k i i k 
1 2 3 
st nd rd 
211 


In this paper we present DARE a deduplication-aware low-overhead resemblance detection and elimination scheme for delta compression on the top of deduplication on backup datasets DARE uses a novel resemblance detection approach DupAdj which exploits the duplicate-adjacency information for efìcient resemblance detection in existing deduplication systems and employs an improved super-feature approach to further detecting resemblance when the duplicate-adjacency information is lacking or limited Our preliminary results on the data-restore performance suggest that supplementing delta compression to deduplication can effectively enlarge the logical space of the restoration cache but the data fragmentation in data reduction systems remains a serious problem  19  W e plan to further study and improve the data-restore performance of storage systems based on deduplication and delta compression in our future work This work was supported in part by National Basic Research 973 Program of China under Grant No 2011CB302301 NSFC No 61025008 61173043 and 61232004 863 Project 2013AA013203 US NSF under Grants IIS-0916859 CCF-0937993 CNS-1116606 and CNS-1016609 This work was also supported by Key Laboratory of Information Storage System Ministry of Education China 
 G W allace F  Douglis H Qian P  Shilane S Smaldone M Chamness and W  Hsu Characteristics of backup workloads in production systems in  2012  P  Shilane M Huang G W allace and W  Hsu W AN optimized replication of backup datasets using streaminformed delta compression in  2012  A Muthitacharoen B Chen and D Mazieres A lo w-bandwidth netw ork le system  in  2001  C Constantinescu J Glider  and D Chambliss Mixing deduplication and compression on acti v e data sets  in  IEEE 2011 pp 393Ö402  B Zhu K Li and H P atterson A v oiding the disk bottleneck in the data domain deduplication le system  in  USENIX Association 2003  J Gailly and M Adler  The gzip compressor   http://www gzip.or g 1991  P  K ulkarni F  Douglis J LaV oie and J T race y  Redundanc y elimination within lar ge collections of les  in  USENIX Association 2004  J MacDonald File system support for delta compression  Masters thesis Department of Electrical Engineering and Computer Science University of California at Berkeley 2000  S Quinlan and S Dorw ard V enti a ne w approach to archi v al storage  in  2002  F  Douglis and A Iyengar   Application-speciìc delta-encoding via resemblance detection  in  USENIX Association 2003  L Arono vich R Asher  E Bachmat H Bitner  M Hirsch and S Klein The design of a similarity based deduplication system in  ACM 2009  M Rabin  Center for Research in Computing Techn Aiken Computation Laboratory Univ 1981  D Gupta S Lee M Vrable S Sa v age A C Snoeren G V ar ghese G M V oelk er  and A V ahdat Dif ference engine harnessing memory redundancy in virtual machines in  2008  Q Y ang and J Ren I-CASH Intelligently coupled array of ssd and hdd  in  2011  A Broder  Identifying and ltering near duplicate documents  in  2000   Some applications of Rabins ngerprinting method  in  1993   On the resemblance and containment of documents  in   V  T araso v  A Mudrankit W  Buik P  Shilane G K uenning and E Zadok Generating realistic datasets for deduplication analysis in  2012  M Lillibridge K Eshghi and D Bhagw at Impro ving restore speed for backup systems that use inline chunkbased deduplication in  2013  W  Xia H Jiang D Feng and Y  Hua SiLo A Similarity-Locality based Near Exact Deduplication Scheme with Low RAM Overhead and High Throughput in  2011 
Proc USENIX FAST Proc USENIX FAST Proc ACM SOSP Data Compression Conference DCC 2011 Proc USENIX FAST USENIX Annual Technical Conference Proc USENIX FAST Proc USENIX FAST Proceedings of SYSTOR 2009 The Israeli Experimental Systems Conference Fingerprinting by random polynomials Proc USENIX OSDI Proc IEEE HPCA Combinatorial Pattern Matching Sequences II Methods in Communications Security and Computer Science Compression and Complexity of Sequences 1997 USENIX Annual Technical Conference Proc USENIX FAST USENIX Annual Technical Conference 
5 Conclusion and Future Work Acknowledgments References 
212 


method increases slightly which is due to two reasons 1 the data before In this experiment we investigate the performance of OLTP queries when OLAP queries are running The workload is update-only and the keys being updated are uniformly distributed We launch ten clients to concurrently submit the updates when the system is deployed on 100 nodes Each client starts ten threads each of which submits one million updates 100 updates in batch Another client is launched to submit the data cube slice query That is one OLAP query and approximately 50,000 updates are concurrently processed in R-Store The system reaches its maximum usage in this setting based on our observation When the system is deployed on other number of nodes the number of clients submitting updates is adjusted accordingly Figure 11\(a shows the throughput of the system The throughput increases as the number of nodes increases which demonstrates the scalability of the system However when OLAP queries are running the update performance is lower than running only OLTP queries This result is expected because the OLAP queries compete for resources with the OLTP queries We also evaluate the latency of updates when the system is approximately fully used As shown in Figure 11\(b the aggregated response time for 1000 updates are similar with respect to varying scales VII C ONCLUSION MapReduce is a parallel execution framework which has been widely adopted due to its scalability and suitability in 0    500    1000    1500    2000  0  10  20  30  40  50  60  70  80  90  100  IncreQueryScan             IncreQueryExe              DC DC DC  Q i i i i T part  Q a Data Cube Slice Query                                                                                                b TPC-H Q1 Fig 7 Performance of Querying    Fig 8 Accuracy of Cost Model    Fig 9 Performance vs Freshness On each HBase-R node the key/values are stored in format Though only one or two versions of the same key are returned to MapReduce HBase-R has to scan all the of the table Since the is materialized to HDFS when it is full these 223les are sorted by time Thus instead of scanning all the and between  only the between   are scanned The value of decides the freshness of the result There is a trade-off between the performance of the query and the freshness of the result the smaller is the fewer real-time data are scanned Figure 9 shows the query processing time with different freshness ratios which is de\223ned as the percentage of the real-time data we have to scan for the query In this experiment  1600 million and 800 million updates on 1 distinct keys are submitted to HBase-R When the freshness ratio is 0 the input of the query is only the data cube Thus the cost of scanning the real-time data is 0 When the freshness ratio increases to 10 the cost of scanning the real-time data is around 1500 seconds because the cost of scanning the real-time table dominates the OLAP query As the freshness ratio increases the running time of and  and when it is not  and  We submit 800 million updates to the server each day and the percentage of keys updated is 223xed to 1 The data cube is refreshed at the beginning of each day and the OLAP query is submitted to the server at the end of the day Since the data are compacted after the data cube refresh the amount of data stored in the real-time table are almost the same at the same time of each day The processing time of and are thus almost constant In contrast when the compaction scheme is turned off HBase-R stores much more data and the cost of locally scanning these data becomes larger than the cost of shuf\224ing the data to MapReduce As a result the processing time of and increases over time and and a user speci\223ed timestamp still need to be scanned and 2 the amount of data shuf\224ed to mappers are roughly the same with different ratios Figure 10 depicts the effectiveness of our compaction scheme In this experiment we measure the processing time of the data cube slice query when the compaction scheme is applied  0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan Processing Time \(s I/Os \(X10 11  Percentage of Keys Updated CubeScan        Processing Time \(s Freshness Ratio CubeScan                                                                                                            50 0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan store\223le store\223les part memstore store\223les memstore store\223les IncreQuerying Baseline IncreQuerying Baseline-NC IncreQuerying-NC Baseline IncreQuerying Baseline-NC IncreQuerying-NC C Performance of OLTP 0    1200    2400    3600    4800    6000  1  5  10  15  20  25  0  0.8  1.6  2.4  3.2  4  IncreQueryScan        IncreQueryExe        I/Os estimated for IncreQuery                               I/Os estimated for  Baseline                 T T T T T T T T 


3000    6000    9000    12000  1  2  3  4  5  6  7  IncreQuerying                                   Baseline-NC                   IncreQuerying-NC                       51 002 Fig 10 Effectiveness of Compaction    a Throughput    b Latency Fig 11 Performance of OLTP Queries a large scale distributed environment However most existing works only focus on optimizing the OLAP queries and assume that the data scanned by MapReduce are unchanged during the execution of a MapReduce job In reality the real-time results from the most recently updated data are more meaningful for decision making In this paper we propose R-Store for supporting real-time OLAP on MapReduce R-Store leverages stable technology HBase and HStreaming and extends them to achieve high performance and scalability The storage system of R-Store adopts multi-version concurrency control to support real-time OLAP To reduce the storage requirement it periodically materializes the real-time data into a data cube and compacts the historical versions into one version During query processing the proposed adaptive incremental scan operation shuf\224es the real-time data to MapReduce ef\223ciently The data cube and the newly updated data are combined in MapReduce to return the real-time results In addition based on our proposed cost model the more ef\223cient query processing method is selected To evaluate the performance of R-Store we have conducted extensive experimental study using the TPCH data The experimental results show that our system can support real-time OLAP queries much more ef\223ciently than the baseline methods Though the performance of OLTP degrades slightly due to the competition for resources with OLAP the response time and throughput remain good and acceptable A CKNOWLEDGMENT The work described in this paper was in part supported by the Singapore Ministry of Education Grant No R-252000-454-112 under the epiC project and M.T 250 Ozsu\220s work was partially supported by Natural Sciences and Engineering Research Council NSERC of Canada We would also like to thank the anonymous reviewers for their insightful comments R EFERENCES  http://hbase.apache.or g  http://hstreaming.com  http://www comp.nus.edu.sg epic  M Athanassoulis S Chen A Ailamaki P  B Gibbons and R Stoica Masm ef\223cient online updates in data warehouses In  pages 865\205876 2011  Y  Cao C Chen F  Guo D Jiang Y  Lin B C Ooi H T  V o S W u and Q Xu Es2 A cloud data storage system for supporting both oltp and olap ICDE pages 291\205302 2011  S Ceri and J W idom Deri ving production rules for incremental vie w maintenance In  pages 577\205589 1991  T  Condie N Conw ay  P  Alv aro J M Hellerstein K Elmelee gy  and R Sears Mapreduce online In  pages 313\205328 2010  J Dean S Ghema w at and G Inc Mapreduce simpli\223ed data processing on large clusters In  pages 137\205150 2004  L Golab T  Johnson and V  Shkapen yuk Scheduling updates in a real-time stream warehouse ICDE pages 1207\2051210 2009  M Grund J Kr 250 uger H Plattner A Zeier P Cudre-Mauroux and S Madden Hyrise a main memory hybrid storage engine  4\(2 Nov 2010  A Gupta I S Mumick and V  S Subrahmanian Maintaining vie ws incrementally extended abstract In  pages 157\205166 1993  S H 264 eman M Zukowski N J Nes L Sidirourgos and P Boncz Positional update handling in column stores In  pages 543\205 554 2010  D Jiang G Chen B C Ooi and K.-L T an epic an e xtensible and scalable system for processing big data 2014  D Jiang B C Ooi L Shi and S W u The performance of mapreduce an in-depth study  3\(1-2 Sept 2010  D M Kane J Nelson and D P  W oodruf f An optimal algorithm for the distinct elements problem PODS 22010 pages 41\20552  A K emper  T  Neumann F  F  Informatik T  U Mnchen and DGarching Hyper A hybrid oltp&olap main memory database system based on virtual memory snapshots In  2011  T W  K uo Y T  Kao and C.-F  K uo T w o-v ersion based concurrenc y control and recovery in real-time client/server databases  52\(4 Apr 2003  K Y  Lee and M H Kim Ef 223cient incremental maintenance of data cubes In  pages 823\205833 2006  F  Li B C Ooi M T  250 Ozsu and S Wu Distributed data management using mapreduce In  2014  I S Mumick D Quass and B S Mumick Maintenance of data cubes and summary tables in a warehouse In  pages 100\205111 1997  A Nandi C Y u P  Bohannon and R Ramakrishnan Distrib uted cube materialization on holistic measures In  pages 183\205194 2011  L Neume yer  B Robbins A Nair  and A K esari S4 Distrib uted stream computing platform In  pages 170\205177 2010  C Olston B Reed U Sri v asta v a R K umar  and A T omkins Pig latin a not-so-foreign language for data processing In  pages 1099\2051110 2008  K Ser ge y and K Y ury  Applying map-reduce paradigm for parallel closed cube computation In  pages 62\20567 2009  M Stonebrak er  D J Abadi A Batkin X Chen M Cherniack M Ferreira E Lau A Lin S Madden E O\220Neil P O\220Neil A Rasin N Tran and S Zdonik C-store a column-oriented dbms In  pages 553\205564 2005  A Thusoo J S Sarma N Jain Z Shao P  Chakka S Anthon y  H Liu P Wyckoff and R Murthy Hive a warehousing solution over a mapreduce framework  2\(2 2009  P  V assiliadis and A Simitsis Near real time ETL In  volume 3 pages 1\20531 2009  C White Intelligent b usiness strate gies Real-time data w arehousing heats up  2012 SIGMOD VLDB NSDI OSDI SIGMOD SIGMOD Proc VLDB Endow In ICDE IEEE Trans Comput VLDB ACM Computing Survey SIGMOD ICDE ICDMW SIGMOD DBKDA VLDB PVLDB Annals of Information Systems DM Review 0    Processing Time \(s Time since the Creation of Data Cube \(day Baseline                  Updates Per Second \(K Number of Nodes Updates only                  Response Time for 1000 Updates\(s Number of Nodes Updates only                  0    20    40    60    80    100  10  20  30  40  50  60  70  Updates + OLAP                                    0    2    4    6    8    10  10  20  30  40  50  60  70  Updates + OLAP                                    Proc VLDB Endow 


  13    1  2   3   4   5   6   7   8  9  10  11   


and aeronautical engineering with degrees from Universitat Politecnica de Catalunya in Barcelona Spain and Supaero in Toulouse France He is a 2007 la Caixa fellow and received the Nortel Networks prize for academic excellence in 2002 Dr Bruce Cameron is a Lecturer in Engineering Systems at MIT and a consultant on platform strategies At MIT Dr Cameron ran the MIT Commonality study a 16 002rm investigation of platforming returns Dr Cameron's current clients include Fortune 500 002rms in high tech aerospace transportation and consumer goods Prior to MIT Bruce worked as an engagement manager at a management consultancy and as a system engineer at MDA Space Systems and has built hardware currently in orbit Dr Cameron received his undergraduate degree from the University of Toronto and graduate degrees from MIT Dr Edward F Crawley received an Sc.D in Aerospace Structures from MIT in 1981 His early research interests centered on structural dynamics aeroelasticity and the development of actively controlled and intelligent structures Recently Dr Crawleys research has focused on the domain of the architecture and design of complex systems From 1996 to 2003 he served as the Department Head of Aeronautics and Astronautics at MIT leading the strategic realignment of the department Dr Crawley is a Fellow of the AIAA and the Royal Aeronautical Society 050UK\051 and is a member of three national academies of engineering He is the author of numerous journal publications in the AIAA Journal the ASME Journal the Journal of Composite Materials and Acta Astronautica He received the NASA Public Service Medal Recently Prof Crawley was one of the ten members of the presidential committee led by Norman Augustine to study the future of human space\003ight in the US Bernard D Seery is the Assistant Director for Advanced Concepts in the Of\002ce of the Director at NASA's Goddard Space Flight Center 050GSFC\051 Responsibilities include assisting the Deputy Director for Science and Technology with development of new mission and measurement concepts strategic analysis strategy development and investment resources prioritization Prior assignments at NASA Headquarters included Deputy for Advanced Planning and Director of the Advanced Planning and Integration Of\002ce 050APIO\051 Division Director for Studies and Analysis in the Program Analysis and Evaluation 050PA&E\051 of\002ce and Deputy Associate Administrator 050DAA\051 in NASA's Code U Of\002ce of Biological and Physical Research 050OBPR\051 Previously Bernie was the Deputy Director of the Sciences and Exploration Directorate Code 600 at 050GSFC\051 Bernie graduated from Fair\002eld University in Connecticut in 1975 with a bachelors of science in physics with emphasis in nuclear physics He then attended the University of Arizona's School of Optical Sciences and obtained a masters degree in Optical Sciences specializing in nonlinear optical approaches to automated alignment and wavefront control of a large electrically-pumped CO2 laser fusion driver He completed all the course work for a PhD in Optical Sciences in 1979 with emphasis in laser physics and spectroscopy He has been a staff member in the Laser Fusion Division 050L1\051 at the Los Alamos National Laboratories 050LANL\051 managed by the University of California for the Department of Energy working on innovative infrared laser auto-alignment systems and infrared interferometry for target alignment for the HELIOS 10 kilojoule eight-beam carbon dioxide laser fusion system In 1979 he joined TRW's Space and Defense organization in Redondo Beach CA and designed and developed several high-power space lasers and sophisticated spacecraft electro-optics payloads He received the TRW Principal Investigators award for 8 consecutive years Dr Antonios A Seas is a Study Manager at the Advanced Concept and Formulation Of\002ce 050ACFO\051 of the NASA's Goddard Space Flight Center Prior to this assignment he was a member of the Lasers and Electro-Optics branch where he focused on optical communications and the development of laser systems for space applications Prior to joining NASA in 2005 he spent several years in the telecommunication industry developing long haul submarine 002ber optics systems and as an Assistant Professor at the Bronx Community College Antonios received his undergraduate and graduate degrees from the City College of New York and his doctoral degree from the Graduate Center of the City University of New York He is also a certi\002ed Project Management Professional 14 


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


