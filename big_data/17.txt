49 ANALOG CMOS MODELING OF INVERTEBRATE LEARNING C R Schneider and H C Card University of Manitoba, Canada Abstract Analog CMOS circuits which model synaptic habitua tion sensitization and classical conditioning found in the marine mollusc Aplysio are presented These circuits are expected to be useful in ANNs with higher-order synapses and learning rules that perform temporal association of mul tiple inputs Our investigation illustrates that biological synaptic learning in invertebrates involves more elaborate mechanisms than those found in most current ANN models I Introduction In previous studies we have described implementations of 
artificial neural networks ANNs using low-accuracy analog CMOS transconductance circuits 4,.5I These ICs implement conventional ANN architectures with Hebbian 7 and contrastive Hebbian 9 learning circuitry at each synapse We briefly outline our work with contrastive Heb bian ANNs in section II For the balance of this paper we take a slightly different approach to the implementation of ANNs rather than designing circuits to implement an ANN architecture we propose circuits which are suggested by the behavior of a biologicol neural network Our circuits model threa learning paradigms present in biological synapses habituation sensitization and classical conditioning Specifically 
we are modeling the behavior of the marine mollusc Aplysia which has been studied exten sively  see for example 1,2 It must be emphasized that although our circuits are more biologically plausible than most current ANNs they still present a highly simplified picture of biological neural networks More complex aspects of biological synapse operation such as those involving extinction and spontaneous recovery Z are omit ted from the circuits described below In doing so we achieve a balance between accurate modeling of neural biol ogy, and mainstream ANNs This work draws upon two of 
our previous investiga tions 3 in which CMOS circuits implementing habitua tion sensitization and classical conditioning with EEPROM weight storage are described, and 4 which deals with ana log CMOS Hebbian learning circuits using capacitive weight storage Other approaches to ANN implementation with learning circuitry at each synapse have been rehd recently including 6 Below we begin by describing our work with contrastive Hebbian ANNs Then in section IJI biological synaptic learning is reviewed Section W presents a simplified mathematical model of this learning bahavior, which is followed by a description of the proposed analog CMOS synaptic circuit as well as simulation results in 
section V 11 Contrastive Hebbian ANNs In this section we describe an analog CMOS implemen tation of a fully connected ANN with contrastive Hebbian learning 9 circuitry at each synapse Networks with contrastive Hebbian learning are intended for supervised learning applications, where a set of training pat terns is used for weight learning A key feature of contras tive Hebbian learning is that it can be used to train weights in networks with hidden neurom that is neurons whose activations are neither network inputs nor outputs The architecture of the ANN under consideration is a 
fully-connectad Hopfield-type arrangement of neurons and synapses in which there is a synaptic connection between each pair of neurons Thus a network of N neurons has N\(N-1 synapses and synaptic circuitry occupies the majority of siliwn chip area Manhattan contrastive Hebbian learning CHI is a two-phase weight learning process, governed by vi  f ccwi vj  AW,I  o sgn\(V,'V  VFV 1 2 I where V and are the activations of the i and j neu rons in 
the clamped 9 phase V and V are the activa tions of the i and j neurons in the unclamped phase and o is a small positive constant which determines the weight leaming rate I  Other synapsm Figure 1 Svsteni block diagram N  1 synoptic circuits ore connected to earh neuron circuit Fig 1 is a block diagram of the ANN system the cir cuit is an analog CMOS approximation 
to 1 and 2 Both neuron activations and synaptic weights may take on analog values in the range V,V Multipliers P3 and P are tran sconductance multipliers, whose inputs are voltages and out put is a c-t P and P are linear current to voltage convertas functioning as a resistor to VGW  Vss The comparator P7 outputs either Vdd or Vss depending on whether its input is above or below its threshold reference 


50 and Pa and Pp are conventional digital gates Lastly PI and P2 are Manhattan CHL circuits containing weight update circuitry and weight storage capcitors Synaptic weights are represented by the differential voltage The circuit of Fig 1 approximates equations 1 and 2 representing ideal network behavior as follows The synaptic weight W is represented by the differential vol tage Vu  VuN Then the product W,,V in 1 is imple mented by P3 as Ism  b\(VcL  VW and the summa tion operation is pedormed as current summation at the input to P P  Ps realize a variable gain neuron whose transfer function may be adjusted in a variety of ways, using the NGain control signals  Since we implement Manhattan leaning only the sign of neuron activations are required for learning P7  PS generate LV  a binary version of each neuron activation for learning v  v Figure 3 Simplified schematic diagram of the portion of Aplysia's NOUF system responsible for gill withdrawal reflex this section, biological synaptic function is described at the behavioral level for a description of the electro-chemical processes involved see I Fig 3 shows a simplified schematic diagram of the portion of Aplysia's lo5 neuron narvaus svstem reswnsible for its urotective bill withdrawal   _ I reflex The neural network connecting the gill Aplvsm respuatory organ siphon a small spout for expelling sea water mantle and tail allows the mollusc to withdraw its gill when a stlmulus is applied to the tail siphon or mantle In nature this reflex has obvious evolutionary advantages as it allows Aplysia to protect its sensitive glll at the first sign of attack Aplysio'3 response to tail mantle and siphon stimuh may be altered dramatically by what it has learned about these atlmuh in the past In Fig 3 the three sensory neurons SN SN2 and SN3 receive stimuli from the mantle siphon and tail respectively SN I and 5N 2 synapse directly with motor neuron MN S and S2 so siphon and mantle stlrnull can tngger gill withdrawal In addition SN synapses with the facilitating mterneuron FN which in turn synapses with synapres Si and S2 through I and F2 These synnpses on synapses I I and f play a cntical role Figure 2 Photomicrograph of 19 neuron 342 synapse net work 4.16 x 2.76mm2 1.2pm CMOS Network operation proceeds as follows First the net work inputs are set for a particular training pattern while output and hidden neurons remain free ie unclamped The network is allowed to settle to its unclamped phase minimum energy state LV and LV the binary learning activations generated by the i and jh neurons wtle and control signals LUPulSe and LUPu/seN are pulsed briefly The binary product LV,ZV determines whether a small quantity of charge is added to or removed from the storage capacitor connected to the unclamped learning circuit Next network outputs are set to training pattern values the network settles in the clamped phase qd learning control signals LCPuZse and LCPulseN are pulsed briefly As before the binary product LV,'LV detamines whether charge Is added to or removed from the clamped storage capacitor This two phase procedure is rspested for each training pattern Typically many passes through the entire set of training data is required to leam network weights Fig 2 is a photomicrograph of the multi-project die contain ing a 19 neuron 342 synapse test network 111 Basic Biological Synaptic Learning Kandel et a1 l in their study of the marine mollusc Aplysia have shown that chemical changes in individual synapses are responsible for three important typas of learn ing habituation sensitization and classical conditioning In in learning as described below For the purposes of neural modeling the pairs Si and F1 and S2 and F2 may be regarded as single ternary synapses \(synapses between three neurons in which the synaptic weight is determined by the interaction of two external signals This is the approach that we take in the following section Note that the schematic of Fig 3 is highly simplifiad in particular each neuron illus trated represents approximately 6 to 24 neurons operating in parallel Habituation may be defined as a decrease in the strength of a behavioral response that occurs when an ini tially novel eliciting stimulus is repeatedly presented l Kandel et a1 have shown that repeated mild tactile stimuli to the mantle cause the gill withdrawal reflex to habituate as the animal kns that the stimuli pose no danger This learning behavior has been traced to chemical changes in the synaptic connection between mantle sensory neurons and gill motor mns SJ the effective weight of the synapse SI is reduced The sensitization mechanism is somewhat more com plex Sensitizdwn is defined as the enhancement of an animal's reflex response as a result of the presentation of a strong or noxious stimulus l In the case of Aplysia noxious stimuli to the tail result in enhanced subsequent gill withdrawal in response to mild mantle stimuli Again the locus of learning is the synapse SI in this case through presynaptic facilitation from synapse F i The mechanism is as follows: tail sensory neuron firing SA causes the facili tating interneurons FA to fire which in turn cause F1 to 


51 alter the chemistry of SI such that the synaptic weight of SI is increased Not surprisingly sensitization can reverse the effects of habituation 223In classical conditioning an initially weak or ineffective conditioned stimulus CS becomes highly effective in producing a behavioral response after it has been paired in time with a strong unconditioned stimulus lis l Classical conditioning is a form of associative learning by which an organism learns a predictive relation ship between two stimuli Aplysia can be classically condi tioned by applying a mild tactile stimulus to the mantle CS followed-approximately second later by a strong stimulus to the tail US Again Fi plays an important role In classical conditiomng, recent activity in SI due to the CS results in activitydependent enhancement of presynaptic icilitation Thus classical conditioning uses the same mechanism as sensitization presynaptic facilitation the effect of which is enhanced by the arrival of the CS second before In classical conditioning the time between the CS and the US is critical if the i.5 arrives before the CS  no classical conditioning occurs This makes good sur vival sense since Aplvsin is concerned about learning when a mild stimulus CS predicts a potentially threatening one 13 In Kandel\222s experiments with Aplysia CScm siphon stimulus was used to differentiate between the effects of sensitization and classical conditioning As the above discussion shows classical conditioning is a form of associative leaining and is therefore related to Hebbian learning 7 and its variants However classical conditioning is noli commuralive because the IS must pre cede the US by a critical interval Kandel\222s investigation of synaptic learning shows that Aplpia uses learning rules which are much more elaborate than those used by most ANNs Aplysia employs non commutative timing-critical associative learning as well as two forms of non-associative learning habituation and sen sitization In the following section an abstraction of these aspects of learning in Aplvsia is presented Note that the above description of biological synaptic function omits many interesting aspects of learning in Aplysia see  for a more detailed discussion IV Model of Synaptic Learning As a starting point for the development of this learning model we use a standard ANN model with Hebbian learn ing described in 4 In this model both synaptic weights and neuron activations can take on values in the range V,V Network operation is governed by the following equations 3 v  f p,l VI  I  AV V  BW,l dt 4 where V and VI are the current activations of the i\223 and j\224\221 neurons W,l is the weighting factor determining the effect that the j\224 neuron has on the irh neuron\222s activation and A and B are small constants f  is a sigmoidal saturating non-linear function The current activation of the i neuron is computed from a weighted summation of the current activations of all neurons which synapse on neuron i 3 Network learning or adaptation is governed by 4 a common form of the Hebbian learning rule 7 The neural network model presented above differs from neuron biology in two important ways Neuron activations are represented as analog values in the model rather than as neuron firing rates Secondly biological neurons only have positive unipolar activations and synaptic weights whereas the above model allows both positive and negative bipolar activations and weights System behavior in the model of biological synaptic function proposed in the present paper is governed by the equations v  f Ew,lk v  5 dw  C d\(VI   D I VI I Wdlk  E I vk I w,lk 6 The ternary nature of these synapses is evident in 5 and 6 W is the weighting factor determining the effect that correlation between the activations of the j\223 and k neu rons will have on the activation of the id\221 neuron Hebbian learning as described by 4 may be regarded as a special case of 6 in which i  j Using the notation of the previ ous section the unconditioned stimulus 11s  VU  Vk the conditioned stimulus CS  Vcs  VI and 6 may be rewritten for motor neuron MN as it dt   Cd\(Vcs  D I Vcs I W E I VU I W 7 where the synaptic weight W is a shorthand notation for We  WMN and indices CS and US represent neurons SNI and FN respectively rather than their activations The term C d\(Vcs implements classical conditioning where d\(Vcs is a delayed time-averaged version of VCS representing the presynaptic facilitation function of F I in Fig 3 Note that 7 differs from Hebbian learning as it involves a correlation of two inputs rather than an input and an output activation Vcs must precede Vu by a critical interval for classical conditiomng to cause a change in synaptic weight W as in Aplvsia and the size and direction of the weight change are determined by the signs and mag nitudes of V and VUS as in the system described by 4 Thus the system can learn prediclive relationships between Vcs and VUS  Note that in addition to creating a delay the function d\(V is gated by Vu  0 such that classical conditioning will not occur when Vus precedes or coincides with VCS absence of reverse conditioning The second term in 7 represents habituatton Habitua tion occurs when V is non-zero and always causes W to decay towards zero This extension of habituation to the bipolar case makes intuitive sense since it causes non associative weight decay when V is active as in habitua tion in Aplysia The form of the expression is also appropriate in light of Mead\222s observation 223A great deal of inhibitory feedback in biological systems depends on activity in sensory input channels but does not depend on the sign of the input\224 SI Finally the third term in 7 E Vus I W is the sensiti zation term Vu activity causes an increase in the magni tude of the weight resulting in increased sensitivity to sub sequent VCS signals In general C  E  D so associative learning dom inates and sensitization and habituation result in smaller non-associative weight changes The role of habituation as a form of inhibitory negative feedback is apparent from 


m 7 Sensitization causes weight values to increase how ever the natural saturating behavior of any practical realiza tion of 7 will limit the effects of sensitization V Synaptic Learning Circuit The analog CMOS circuits which we use to implement our synaptic learning circuit are compact low-accuracy amplifiers, multipliers absolute value circuits etc 5 The decision to use analog rather than digital computation results in large silicon chip area savings at the expense of computational accuracy and repeatability However the biological machinery of Aplysia is even less precise so any neural network architecture that requires high accuracy com ponents is not biologically plausible and therefore not of interest in the present work The components which we are using are transconduc tmcc circuits where the input signals are voltages and the output is a current Many of these circuits are variants of the circuits used in SI but unlike in Mead's work the transistors in our circuits are operating above threshold VGS  vr Vdd CSBias TBm TCntl Flgure 4 Synaptic circuit Fig 4 is a bipolar synaptic learning circuit built from simple analog components This circuit approximates 7 lncorporating classical conditioning, habituation, and sensiti zation The synaptic weight is stored as the voltage W on capacitor CI and weight changes are governed by where IC IH and Is are respectively the classical condi tioning habituation and sensitization weight change com ponents PI  Pg M and R implement classical condition ing Inhibition of presynaptic facilitation prevention of reverse conditioning is achieved as follows when I VU I  Vmm PI turns on MI which turns off PZ by setting its bias current to zero As a result when 1 VU I  Vmm V has no effect on VF and therefore no classical conditioning can occur Conversely when I VUS 1  Vmm Pz is active and classical conditioning can occur Absolute value multipliers P5 and P6 approximate habi tuation and sensitization Note that W is connected to the negative input of Pg since habituation drives W towards zero Below two examples are presented which illustrate the learning behavior of this synaptic circuit For the pur poses of illustration higher than typical learning rates are used so that weight changes are evident over the relatively short interval of these simulations A Clsssical Conditioning I I t 1  0 2 04 1 VUSI  0 6 08   I  I         0 5 10 15 20 25 30 35 40 Tune us Figure 5 Classical conditioning and semitilation V precedes Vus by critical interval so classical condifioning occurs Fig 5 shows the synaptlc circuit's response to a typical classical conditioning event At I  3p.s a 2p.s conditioned stimulus Vcs pulse initiates a presynaptic facilitation pulse VF At t e7p.s the unconditioned stimulus V is presented resulting in an increase in the weight W as the correlation relation between V and Vu is learned The critical period for classical conditioning that is the time after V occurs in which V must be presented for condi tioning to occur is approximately 2 to 8p.s The shape of VF can ensily be changed by choosing different component values for the delay circuit P3 At t I 3Ws Vu is briefly pulsed negative The presynaptic facilitation signal VF has decayed to almost zero and thus no classical conditioning occurs However VUS does cause a slight increase in W through the sensiti zation mechanism Notice that despite Vus being negative at t I 3@U W increases since sensitization is independent of the sign of Vus The slow decay of W from 13 to 3Op is due to a small component mismatch in PI P5 and P6 B Reverse Conditioning 0.61      0 8 0 5 10 15 20 25 30 35 40 Time s Figure 6 Reverse condifioning Vus occurs before VCS so no classical Conditioning occurs In the example of Fig 6 Vus occurs before VCs and 


53 thus no classical conditioning can occur For 3p.1  t  5p.s the presence of Vu is preventing VCS from generating a V.G pulse, through the gating mechanism imple mented with Pi in Fig 4 At t  5bs Vu is set to zero and V begins to increase If a second Vu pulse were presented for instance at t  lops then classical condition ing would occur The small weight changes from t  2ps to 7b are due to sensitization and habituation During the interval 2ps  I  3w sensitization is occurring during the inter val 3ps  f  5p sensitization and habituation occur the rate of weight increase is reduced and during 511s  I  7p.s only habituation occurs the weight decays towards zero Habituation also occurs during 3p.y  I  5p.s in Fig 5 but the effect on W was smaller because the rate of habituation is determined by the product DW I Vcs I  Thus the rate of habituation is always a frac tion of the synaptic weight rather than a fixed value VI Implications for ANNS The development of our biology-motivated learning model raises a number of interesting questions including whcther the biological learning details presented in this paper are important in ANN models We began with a con ventional Hebbian model generalizing and extending it to incorporate the functions of synaptic learning found 111 Aplysia Thus Hebbian learning \(4 is a special case of 7 in which the delay of d is zero i  j and reverse wndi tioning is allowed The effect of non-zero delay in d together with inhi bition of reverse conditioning is that learning becomes non-commutative This 223symmetry breaking\224 would result in non-symmetric weights W,Ik  W,kI in the case of a fully-connected Hopfield-type network This type of non commutative learning could be incorporated into networks using contrastive Hebbian learning 9 again resulting in non-symmetric weights Further work is planned to investi gate this possibility The non-associative learning found in biological sys tems habituation and sensitization may also have a role to play in ANNs Their importance and how to go about incorporating them into an artificial system is less clear than with non-commutative learning because non-associative learning is not used in many ANNs However they are obviously important in biological neural networks as shown by the study of a variety of animals and therefore must be considered for inclusion in ANNs An important issue is whether the biological synaptic learning presented in this work depends upon a certain degree of 223hard-wiring\224 For example Aplysia has been 223wired\224 so that it can learn a predictive relationship between mild mantle stimuli and noxious tail stimuli If the neural network of Fig 3 were missing the facilitating inter neuron FN then neither sensitization nor classical condi tioning could occur There are two interpretations of this 223pre-wiring\224 it is evidence that the biology of Aplysia IS not of interest to ANN researchers because it represents a special case in which predetermined network architecture plays a major role or it is an indication that tailoring a net work architecture for a particular purpose is an essential part of neural networks whether natural or artificial Our ten dency is towards the latter view although the issue is far from settled VII Conclusion This work has demonstrated that biological details which are omitted from most current ANN models may be efficiently implemented with analog CMOS circuits Three types of lesmmg habituation sensitization and classical conditioning are incorporated in the synaptic learning cir cuit which we have developed Habituation and sensitiza tion are types of non-associative learning and are not often included in ANN models Classical conditioning is a form of associative learning related to Hebbian learning It differs from Hebbian learning in temporally correlating two inputs at ternary synapses synapses between three neurons rather than the input and output of binary synapses Classi cal conditioning is also more complex as it is non commutative resulting in a type of 223symmetry breaking\224 in the neural network system Our work shows that biological synaptic learning is substantively different from current ANN learning both in terms of learning paradigms employed and in terms of the extensive use of 223hard wiring\224 of connections in biological neural networks Further research is required to determine whether these aspects of neural biology are a critical part of information processing in biological and artificial neural networks Acknowledgements Discussions with Roland Schneider University of Man itoba and Geoffrey Hinton University of Toronto were very valuable in this work The financial support of NSERC and MICRONET are also gratefully acknowledged References l E R Kandel and J H Schwartz eds Principles of Neural Science 2nd edition Elsevier New York 1985 2 R D Hawkins and G. H Bower Computational Models of Learning in Simple Neural System Academic Press San Diego CA pp 65-108 1989 3 H C Card and W R Moore 223Silicon Models of Asso ciative Learning in Aplysia\224 Neural Nehvorks Vol 3 4 C Schneider H C Card CMOS Implementation of Analog Hebbian Synaptic Learning Circuits Proc 1991 Int. Joint Conf on Neural Nefworks Seattle to be pub lished 5 C Schneider Analog CMOS Circuits for Artificial Neural Nehvorks Ph.D Dissertation Dept of Electrical and Computer Engineering University of Manitoba 1991 6 J Alspector R B Allen and A Jayahmar 223Relaxa tion Networks for Large Supervised Learning ob lems\224 Proc NIPS-90 paper VLSI-6 in press 7 D 0 Hebb Organization of Behavior 1949 New York John Wiley 8 C A Mead Aiiolog VLSI and Neurol System 1988 Reading Addison-Wesley 9 Movellan J R 223Conrrastive Hebbian Learning in the Continuous Hopfield Model\224 Connectionist Models Proceedings of the 1990 Summer School D S Touretzky et al eds 1990, pp 10-17 pp 333-446 1990 


percentage is close to 0 these principal componenb can be discarded without any fwthpr data exploration   2 I4 is 6 16 2 6 10 6 b4 82 0 20 xM4&%mpc&4 BOX IWY Figure 8 The maxitnum minimum and average munber of nonzero PC correlations per principal component against the different percentages of nom PC correlations for all the principal components Another enmuraging aspect of using PCA is that the average number of itemset size is only 15 compared with 20 because some of the PC correlations are zems inherently. When the PC correlation threshold is set at 0.0 the recall of items is 100 Hence there may be some potential in saving processingspgd using less number of items V SUMMARY In this paper we have explored the use of finite mixture densities to model the consumer spending patterns Products hcught for specific social and/or cultural events were considaed to be the underlying driving forces of cross selli effects as multiple related items are needed for consumrs to participate into those events We applied the principal component analysis PCA to discover these events as principal components taking a simplistic view of the discovery process for this initial exploration We used a set of 10,000 tansaction data generated by Quest  121 to examine haw PCA discovered c-occurring items may relate to associatim rules mined using the Apriai algorithm Our initial understanding is that the frequent itemsets of association rules may relate to the CO-odng items discovered by PCA. These co-occuning items are thought to he the smng correlations between the principal component and the specific item Instead of using a threshold to define s-g cornlatiom we use a novel measure called the min max correlation to illushate graphically how PCA mining of patterns relate to the fiquent itemsets discovered by the Apriori algorithm Our initial study show that as the conelation threshold is increased the average, minimum and maximm confidenoe values of the itemsets discovered by the Aprim algorithm and the PCA converges to high confidence value \(around 80 However, this process would loose some high mfidence value rules Nevertheles certain high confidence and nontrivial association rules related to the itemsets remain Further work is nccessary to demonstrate the extend of the utility of PCA Aekoowledgement This research is supported in part by project funding for the postgraduate study Reference I Agrawal R T Imielinsld and A Swami Mining associationrules between sets of items in large database In Proceedings of fhe I993 Infenarional Conferrpnce on Management of Dora SIGMOD 93 pages 207-216 May 1993 2 Agrawal R and R Srikant Fasf ulgorifhmsfor mining arrocimion mla in large daatases In YLDB 94 September 1994 31 Russell G.J and A Petersen Analysis of cross category dependence in market basket selection Journd of Refai!ing 67\(3 367-392.2000 4 Meo R Theory of dependence values ACM Trans on Dofabase Sysremr 25\(3 380406,2000 5 Bayardo Jr R Ef\200iciently mining long patterns km databases In Proc I998 Infsnmional Confexnce on Management of Dofa SIGMOD 98 pages 85-93.1998 6 Bryan F.J Manly Mdtiwniufe Sfarirtiml Mefhds A Primer Chapman and Hall 1986 7 F Korn A Labrinidis Y Kotidis C Faloutsas A Kaplunwich and D Perkovic Quantifiable data mining using principal component analysis CS-TR-3754 and UMUCSTR-97-13 techniculreportr Febmuy 1997 SI Manning A.M and J.A Keane Induing load balancing and efficient data distribution prior to association Rule Discovery in a Parallel Ennvimment In Pro Europron Co&ence on Puralld Processing pages 1460-1463 1999 9 Cadez I.V Smyul P and Mannila H Probabilistic modeling of transaction data with applications of profiling, visualization and prediction In Proc ACM KDD Confereno pages 37-46.2001 IO Levin A.U ken T.K and Moody I.E Fast pruning using principal components Advances in Neural Information Processmg Systems 6 3542.1994 I11 AgganvaZ C.C On the effects of dimensionality reduction on high dimmsimal similarityearch In ACM PODS Cmfererce 2001 I21 Agrawal R M Mehta J Shafer R Srikant A Amhg and T Bollinger The Quest data mining system In Proc 2nd Int Con5 Knowledge Discovery and Dam Mining pages 244249,1996 I31 Han J M Kamber Data mining concepts and techniques Morgnnffiufmnnn Publishers 2001 


4 If large k-itemset is empty the algorithm termi nates Otherwise k  k  1 and the coordinator broadcasts large k-itemsets to all the processors and goto 2231\224 3.5 HPA with Extremely Large Itemset Duplication  HPA-ELD In case the size of candidate itemset is smaller than the available system memory HPA does not use the remaining free space However HPA-ELD does utilize the memory by copying some of the itemsets The itemsets are sorted based on their frequency of ap pearance HPA-ELD chooses the most frequently oc curring itemsets and copies them over the processors so that all the memory space is used which contributes to further reduce the communication among the pro cessor In HPA it is generally difficult to achieve a flat workload distribution If the transaction data is highly skewed that is, some of the itemsets appear very fre quently in the transaction data the processor which has such itemsets will receive a much larger amount of data than the others This might become a system bottleneck In real situations the skew of items is easily discovered In retail applications certain items such as milk and eggs appear more frequently than others HPA-ELD can handle this problem effectively since it treats the frequently occurring itemset entries in a special way HPA-ELD copies such frequently occurring item sets among the processors and counts the sup port-count locally like in NPA In the first phase when the processors generate the candidate k-itemset using the large \(k-1 if the sum of the support val ues for each large itemset exceeds the given threshold it is inserted in all the processor\222s hash table The re maining candidate itemsets are partitioned as in HPA The threshold is determined so that all of the available memory can be fully utilized using sort After reading all the transaction data all processor\222s support-count are gathered and checked whether it satisfies the min imum support condition or not Since most of the algorithm steps are equal to HPA we omit a detailed description of HPA-ELD 4 Performance Evaluation Figure 6 shows the architecture of Fujitsu APlOOODDV system, on which we have measured the performance of the proposed parallel algorithms for mining association rules NPA SPA HPA and HPA ELD APlOOODDV employs a shared-nothing archi tecture A 64 processor system was used where each processor, called cell, is a 25MHz SPARC with 16MB local memory and a 1GB local disk drive Each pro t15.14 t20.14 T-net 15 4 2048K 145MB 20 4 2048K 187MB Figure 6 Organization of the APlOOODDV system Name I It1 I 111 I ID1 I S ize tlO.14 I 10 I 4 I 2048K I lOOMB Table 4 Parameters of data sets cessor is connected to three independent networks T net B-net and S-net The communication between processors is done via a torus mesh network called the T-net and broadcast communication is done via the B-net In addition a special network for barrier syn chronization called the S-net is provided To evaluate the performance of the four algo rithms synthetic data emulating retail transactions is used where the generation procedure is based on the method described in 2 Table 4 shows the mean ing of the various parameters and the characteristics of the data set used in the experiments 4.1 Measurement of Execution Time Figure 7 shows the execution time of the four pro posed algorithms using three different data sets with varying minimum support values 16\(4 x 4 proces sors are used in these experiments Transaction data is evenly spread over the processor\222s local disks In these experiments, each parallel algorithm is adopted only for pass 2 the remaining passes are performed using NPA, since the single processor\222s memory can not hold the entire candidate itemsets only for pass 2 and if it fits NPA is most efficient HPA and HPA-ELD significantly outperforms SPA 25 


tlO.14 16 processors 1m 1 161 t 1 the number ofall the transactions 127  CpP 10 I 0 0.2 0.4 0.6 0.8 1 1.2 1.4 minimum supwrt  t15.14 16 processors 0 0.2 0.4 0.6 0.8 1 1.2 1.4 rrrrm SUDPOIT  t20.14 16 processors looOa 1 A SPA  0 0.2 0.4 0.6 0.8 1 1.2 1.4 U SUDWrt  Figure 7 Execution time varying minimum support value Since all transaction data is broadcast to all of the processors in SPA its communication costs are much larger in SPA than in HPA and HPA-ELD where the data is not broadcasted but transfered to just one pre cessor determined by a hash function In addition SPA transmits the transaction data while HPA and HPA ELD transmit the itemsets which further reduces the communication costs In NPA the execution time increases sharply when the minimum support becomes small Since the can didate itemsets becomes large for small minimum sup port the single processor's memory cannot hold the entire candidate itemsets NPA has to divide the can didate itemsets into fragments Processors have to scan the transaction data repetitively for each frag ment which significantly increases the execution time 4.2 Communication Cost Analysis Here we analyze the communication costs of each algorithm Since the size of the transaction data is usually much larger than that of the candidate item set we focus on the transaction data transfer In NPA the candidate itemsets are initially copied over all the processors which incurs processor communication In addition during the last phase of the processing, each processor sends the support count statistics to the co ordinator where the minimum support condition is ex amined This also incurs communications overhead But here we ignore such overhead and concentrate on the transaction data transfer for SPA and HPA in sec ond phase In SPA, each processor broadcasts all transaction data to all the other processors The total amount of communication data of SPA at pass IC can be expressed as follows p=l i=l N It x N  1 x ID1 1 where the number of items in i-th transaction of pth processor the number of pth Drocessor's transactions In HPA the itemsets of the transaction are trans mitted to the limited number of processors instead of broadcasting The number of candidates is dependent on the data synthesized by the generator The total 26 


amount of communication for HPA at pass IC can be expressed as follows CAN M p=l i=l the amount of the candidate itemset in bytes the size of main memory of a single pre cessor in bytes One transaction potentially generate t,p ck candi dates However in practice most of them are filtered out as is denoted by the parameter CY Since a is usually small4 MkSPA  MFPA Since it is difficult to derive a we measured the amount of data received by each processor Figure 8 shows the total amounts of received messages of SPA HPA and HPA-ELD where t15.14 transaction data was used with 0.4 minimum support As you can see in Figure 8 the amount of messages received of HPA is much smaller then that of SPA In HPA-ELD the amount of messages received is further reduced since a part of the candidate itemset is handled separately and the itemsets which corre spond to them are not transmitted but just locally processed SPA HPA HPA-ELD Figure 8 the amount of messages received pass 2 4.3 Search Cost Analysis In the second phase the hash table which consists of the candidate itemsets are probed for each transac tion itemset 41f the number of processors is very small and the number of items in transaction is large then McPA could be larger than MzPA With reasonable number of processors, this does not happen as you can see in Figure 8 We are currently doing experiments on mining association rules with item\222s classifica tion hierarchy, where combination of items becomes much larger than the ordinary mining association rules When ak increases McPA tends to increase as well we will report on this case in a future paper In NPA the number of probes at pass IC can be expressed as follows p=l i=l 11 ItlCk x lak x ID x N 4 In HPA and HPA-ELD the number of searches at pass IC can be expressed as follows p=l i=l E ltlck x lakl x 5 The search cost of HPA and HPA-ELD is always smaller than SPA It is apparent that SFPA  SfPA Not only the communication cost but also search cost also can be reduced significantly by employing hash based algorithms which is quite similar to the way in which the hash join algorithm works much better than nested loop algorithms. In NPA the search cost depends on the size of the candidate itemsets If the candidate itemset becomes too large SrPA could be larger than SfPA But if it fits SFPA N SZPA  SfPA that is the search cost is much smaller than SPA and almost equal to HPA Figure 9 shows the search cost of the three algorithms for each pass where the t15.14 data set is used under 16 processors with the minimum support 0.4 In the experimental results we have so far shown all passes except pass 2 adopts NPA algorithm We applied different algorithms only for pass 2 which is computationally heaviest part of 27 


the total processing However here in order to focus on the search cost of individual algorithm more clearly each algorithm is applied for all passes The cost of 500 400 300 200 100 0 1 2 3 4 oass number    Figure 9 the search cost of SPA NPA and HPA NPA changes drastically for pass 2 The search cost of NPA is highly dependent on the size of available main memory If memory is insufficient, NPA's performance deteriorates significantly due to the cost increase at pass 2 In Figure 9 the search cost of NPA is less than SPA However as we explained before it incurred a lot of additional 1/0 cost Therefore the total execution time of NPA is much longer than that of SPA 4.4 In this section the performance comparison be tween HPA and HPA-ELD is described In HPA-ELD we treat the most frequently appearing itemsets sepa rately In order to determine which itemset we should pick up we use the statistics accumulated during pass 1 As the number of pass increases the size of the candidate itemsets decreases Thus we focused on pass 2 The number of the candidate itemsets to be separated is adjusted so that sum of non-duplicated itemsets and duplicated itemsets would just fit in the available memory Figure 10 shows the execution time of HPA and HPA-ELD for t15.14 varying the minimum support value on a 16 processors system HPA-ELD is always faster than HPA The smaller the minimum support the larger the ratio of the difference between the execu tion times of the two algorithms becomes As the min imum support value decreases the number of candi date itemsets and the count of support increases The candidate itemsets which are frequently found cause large amounts of communication The performance of HPA is degraded by this high communications traffic Comparison of HPA and HPA-ELD  0 0.5 1 1.5 2 minimum supwrt  Figure 10 Execution time of HPA and HPA-ELD at pass 2 Figure 11 shows the number of probes in each pro cessor for HPA and HPA-ELD for t15.14 using a 16 processor system for pass 2 We picked up an exam ple which is highly skewed Horizontal axis denotes processor ID In HPA the distribution of the number 14 9 c g 12 g 10 s U  2 6 4t 2 Y 0 2 4 6 8 101214 processor ID Figure 11 The number of search of HPA and HPA ELD at pass 2 of probes is not flat Since each candidate itemset is allocated to just one processor the large amount of messages concentrate at a certain processor which has many candidate itemsets occurring frequently In HPA-ELD the number of probes is compara tively flat HPA-ELD handle certain candidate item sets separately thus reducing the influence of the data skew However as you can see in Figure 11 there still remain the deviation of the load amongst processors If we parallelize the mining over more than 64 proces sors we have to introduces more sophisticated load 28 


18001  balancing mechanism which requires further investi gation 4.5 Speedup Figure 12 shows the speedup ratio for pass 2 vary ing the number of processors used, 16 32 48 and 64 where the curve is normalized with the 16 processor execution time The minimum support value was set to 0.4 4.5 0.5 1 1 0 I 10 20 30 40 50 60 70 number of mxessors Figure 12 Speedup curve NPA HPA and HPA-ELD attain much higher lin earity than SPA HPA-ELD an extension of HPA for extremely large itemset decomposition further in creases the linearity HPA-ELD attains satisfactory speed up ratio This algorithm just focuses on the item distribution of the transaction file and picks up the extremely frequently occurring items Transferring such items could result in network hot spots HPA-ELD tries not to send such items but to process them locally. Such a small mod ification to the original HPA algorithm could improve the linearity substantially 4.6 Effect of increasing transaction Figure 13 shows the effect of increasing transac tion database sue as the number of transactions is increased from 256,000 to 2 million transactions We used the data set t15.14 The behavior of the results does not change with increased database size The minimum support value was set to 0.4 The num ber of processors is kept at 16 As shown each of the parallel algorithms attains linearity 5 Summary and related work In this paper we proposed four parallel algorithms for mining association rules A summary of the four database size Sizeup 0 I 0 500 loo0 1500 uxw amount of transaction thousands Figure 13 Sizeup curve algorithms is shown in Table 5 In NPA the candi date itemsets are just copied amongst all the proces sors Each processor works on the entire candidate itemsets NPA requires no data transfer when the supports are counted However in the case where the entire candidate itemsets do not fit within the mem ory of a single processor the candidate itemsets are divided and the supports are counted by scanning the transaction database repeatedly Thus Disk 1/0 cost of NPA is high PDM, proposed in 6 is the same as NPA which copies the candidate itemsets among all the processors Disk 1/0 for PDM should be also high The remaining three algorithms SPA HPA and HPA-ELD partition the candidate itemsets over the memory space of all the processors Because it better exploits the total system's memory, disk 1/0 cost is low SPA arbitrarily partitions the candidate itemsets equally among the processors Since each processor broadcasts its local transaction data to all other pro cessors the communication cost is high HPA and HPA-ELD partition the candidate itemsets using a hash function which eliminates the need for transac tion data broadcasting and can reduce the comparison workload significantly HPA-ELD detects frequently occurring itemsets and handles them separately which can reduce the influence of the workload skew 6 Conclusions Since mining association rules requires several scans of the transaction file its computational requirements are too large for a single processor to have a reasonable response time This motivates our research In this paper we proposed four different parallel algorithms for mining association rules on a shared nothing parallel machine and examined their viabil 29 


Table 5 characteristics of algorithms ity through implementation on a 64 node parallel ma chine the Fujitsu AP1000DDV If a single processor can hold all the candidate item sets parallelization is straightforward It is just suf ficient to partition the transaction over the proces sors and for each processor to process the allocated transaction data in parallel We named this algo rithm NPA However when we try to do large scale data mining against a very large transaction file the candidate itemsets become too large to fit within the main memory of a single processor In addition to the size of a transaction file a small minimum support also increases the size of the candidate itemsets As we decrease the minimum support computation time grows rapidly but in many cases we can discover more interesting association rules SPA HPA and HPA-ELD not only partition the transaction file but partition the candidate itemsets among all the processors We implemented these al gorithms on a shard-nothing parallel machine Per formance evaluations show that the best algorithm HPA-ELD attains good linearity on speedup by fully utilizing all the available memory space which is also effective for skew handling At present we are doing the parallelization of mining generalized association rules described in 9 which includes the taxonomy is-a hierarchy Each item belongs to its own class hierarchy In such mining associations between the higher class and the lower class are also examined Thus the candidate itemset space becomes much larger and its computation time also takes even longer than the naive single level association mining Parallel pro cessing is essential for such heavy mining processing Acknowledgments This research is partially supported as a priority research program by ministry of education We would like to thank the F\221ujitsu Parallel Computing Research Center for allowing us to use their APlOOODDV sys tems References l R.Agrawal T.Imielinski and ASwami 223Min ing Association Rules between Sets of Items in Large Databases\224 In Proc of the 1993 ACM SIGMOD International Conference on Manage ment of Data pp207-216 May 1993 2 R.Agrawal and RSrikant 223Fast Algorithms for Mining Association Rules\224 In Proc of the 20th International Conference on Very Large Data Bases pp.487-499 September 1994 3 J.S.Park M.-S.Chen and P.S.Yu 223An Effec tive Hash-Based Algorithm for Mining Associ ation Rules\224 In Proc of the 1995 ACM SIG MOD International Conference on the Manage ment of Data SIGMOD Record Vo1.24 pp.175 186 June 1995 4 H.Mannila H.Toivonen and A.I.Verkamo 223Ef ficient Algorithms for Discovering Association Rules\224 In KDD-94:AAAI Workshop on Knowl edge Discovery in Databases pp.181-192 July 1994 5 A.Savasere, E.Omiecinski and S.Navathe 223An Effective Algorithm for Mining Association Rules in Large Databases\224 In Proc of the 21th International Conference on Very Large Data Bases pp.432-444 September 1995 6 J.S.Park M.-S.Chen and P.S.Yu 223Efficient Parallel Data Mining for Association Rules\224 In Proc of the 4th International Conference on In formation and Knowledge Management pp.31 36 November 1995 7 T.Shintani and M.Kitsuregawa 223Considera tion on Parallelization of Database Mining\224 In Institute of Electronics Information and Com munication Engineering Japan SIG CPS Y95 88 Technical Report Vo1.95 No.47 pp.57-62 December 1995 8 T.Shimizu T.Horie and H.Ishihata 223Perfor mance Evaluation of the APlOOO Effects of message handling broadcast and barrier syn chronization on benchmark performance-\224  In S WO PP 22292 9.2 ARC 95 Information Processing Society of Japan Vo1.92 No.64 1992 9 R.Srikant and R.Agrawal 223Mining Generalized Association Rules\224 In Proc of the 21th Inter national Conference on Very Large Data Bases pp.407-419 September 1995 30 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


