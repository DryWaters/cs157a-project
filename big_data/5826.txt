DISCOVERING VISUAL-CONCEPTS OF ONLINE IMAGES FROM ASSOCIATIONAL IMAGE PATCHES Ya-Shin Chian a nd Wei-Guang Teng Department of En gineering Science National Cheng Kung University Tainan, Taiwan wgteng@mail.ncku.edu.tw  ABSTRACT With the advance of networking technologies, the widespread use of handheld devices not only enables instant access of required information but also shortens the distance among acquaintances. No wadays one can easily share their photos on the Internet right after the photos are 
taken. With an increasing amount of images, a challenging problem for computer algorithms is to discover the visualconcept embedded within an image. Instead of modeling this problem as a classification process which attempts to categorize images into different pre-defined classes, we propose in this work to start with identifying similar image patches. Specifically, we adopt the technique of mining association rules to construct evident relationships among image patches so as to discover identical visual-concepts from online images 1  INTRODUCTION As the technology of cons 
umer electronics and mobile computing advances, the usage of handheld devices is of increasing growth in recent years. Moreover, establishing social relationships and communicating with friends on the Internet also becomes much easier. These facts significantly change user habits in several aspects. One illustrative example is that many people take photos whenever they want and wherev er they want with their digital cameras or mobile phones. Then, these photos can be uploaded immediately to share among friends in their web albums or blog spaces Consequently, the Internet undoubtedly forms the largest archive containing a variety 
of image data It is usually a trivial task for human beings to "read images or more specifically to understand the scene embedded in an image within s econds. On the contrary, it is quite difficult for computer algorithms to reach the same capability. A crucial issue behind this is the so-called semant Specificall y, there is usuall y weak or  no correspondence between low-level image features, e.g colors, shapes and textur es, extracted by computer algorithms and high-level conceptual interpretations perceived by human users. To bridge this gap, many researchers have been working to propose several different 
approaches. It is thus easily understood that without a proper scheme, manually handling a large number of online images is obviously a labor-intensive and errorprone task In this work, we aim at di scovering visual-concepts from a collection of online images. Th is objective is similar to that of a scene detection task in capturing the "gist" of an image. In a typical process training examples are provided to help the recognition of objects or the overall scene contained in a new image. Also, recognition results are presented by classifying images into pre-defined scene 
categories. This process may work well in a static dataset but fails to learn new objects or scenes in a continuously growing image archive, e.g the Internet. Alternatively we propose to start with identifying the relationship between two image patches \(or regions\ from two different images. This help to consolidate the basis of discovering identical visual-concepts from online images, especially when a collection of the images are shared among community members, e.g., f riends, relatives or colleagues To estimate the similarity of two image patches with a comparison technique is a fundamental and core issue to 
many computer vision problems. In view of this, we propose to introduce the concept of visual words in this work. Note that visual words ar e not real words, but rather summarize local patches of the image to describe some details, i.e., a corner, texture or point [1 Furtherm or e the well-known data mining technique of association rules 2 p t e d to  i d en t i f y  ev id e n t relation sh ips of  i m ag e patches that can help to constitute a specific visualconcept, e.g., the Santa, reindeers and stockings imply a Christmas scene 
The rest of this paper is organized as follows Preliminaries and related works are generally reviewed in Section 2. Techniques of visual words and association rule mining used in our scheme to discover visual-concepts from online images are explored in Section 3. Empirical studies to show the feasibility of our proposed scheme are conducted in Section 4. Finally, this paper concludes with Section 5 2011 IEEE 15th International Symposium on Consumer Electronics 978-1-61284-842-6/11/$26.00 ©2011 IEEE 218 


2  PRELIMINARIES In most of current CBIR systems, images are represented by general features, e.g., color shape, and texture descriptors, and locally measured features   e.g SIFT and SURF descriptors. Specifically, local descriptors are invariant to cancel out accidental circumstances of the recording caused by differences in lighting, viewpoint or   Note that featur es extr act ed fr om i m ages ar e usually indexed beforehand to improve the efficiency of following retrieval pr  A typical image retrieval pr ocess starts as the user provides a query image, i.e., QBVE \(query by visual example\1 f i cally retrieval is performed by comparing the features of the query image against those of images in the database using a feature matching algorithm In general, a distance measure is used to calculate the similarity of two images in terms of various feature dimensions such as color texture, shape, and others Therefore, based on the similarities and relevance feedback returned from users ranked images are presented with a visualized user inter  As there are billions of imag es available on the Internet conventional approaches which store respective descriptors for each image are no longer appropriate. To solve this problem, image descr iptors are quantized into the çvisual words 4    9]. This co ncept is extended from the technique of text retrieval. Specifically significant words can be used to identify similar documents, whereas significant visual words can be used to identify similar images The storage requirements for features and the execution time required for evaluating image similarities can thus be significantly reduced as visual words are repre sentatives of all the features Some prior works propose to classify images into predefined semantic concepts based on the visual words contained in those images [5  Specificall y, a probability model can be utilized to assign a probability to each of the concept cate gories. Note that these concept categories may correspond to visual obviously attributes meaningful objects, and descr In the task of image retrieval, images can thus be ranked according to the probabilities of concept presence, i.e how likely an image is to represent the sp ecified concept In fact, the recognition and learning of object categories can be classified into several main approaches including parts and structure models discriminative methods, and combined recognition and segmentation [5   Provi ded a proper training process, the task of scene detection is able to be achieved with these techniques. Nevertheless, a significant limitation is that all concepts or scenes have to be specified in advance 3  DISCOVERING THE ASSOCIATION AMONG IMAGES The way relevant visual-wor ds are associated to form a visual-concept is explored in Section 3.1. Moreover, the proposed scheme is de vised in Section 3.2 3.1  Associating Relevant Image Patches It is easy for people to comprehend the meanings of a word and associate this word with other relevant ones. For example, the patterns çstaré, çd ayé, çApolloé, and çJava can imply the word suné, because the sun is a fixed star we have day time when the sun shows in the sky, Apollo is identified with the sun in Greek Mythology, and Java is a kind of programming language developed by Sun Microsystems. That is to say, this word association game is the connection and production of other words to a given word based on the noun phrase word association Similarly, many relevant objects are simultaneously contained in an image. Although object recognition is not a main target in this work, we use visual objects for illustrating purposes in th e following example. As shown in Figure 1, one may easily understand that the cooccurrence of bunnies, eggs, chicks and lilies implies the Easter scene  Figure 1 The Easter scene can be implied with associational image patches  In view of this, we propose to introduce the technique of association rule mining. In general, asso ciation rule mining is a widely-used approach for discovering significant regularities between items, i.e., frequent itemsets, in large scale custo mer transaction data recorded by usually a large retailing com  I n this work visual-words, i.e., significant image patches, are taken as items whereas an image is reg arded as a transaction when association rule mining is applied. For our purpose of identifying similar concepts or scenes from numerous images, association rule mining helps to discover frequently co-occurring visual words so as to ease the problem of scene detection. Unlike the approaches developed in prior works visual concepts to be learned are not required to be pre-defined as a limited number categories in our approach 3.2  Proposed Scheme For our purpose of discovering visual-concepts from online images, we extend the idea of frequent patterns in 219 


association rule mining to identify co-occurrences of visual words in a collection of images. Note that if some visual words frequ ently co-occur in more than a few images, these visual words together may indicate a specific scene    Visual Words Frequent Itemsets Images Internet Feature Quantization Feature Extraction Association Rule Mining Explorer Database Conceptual Groups  Figure 2 Proposed scheme for discovering visualconcepts of online images  To concrete the above concept, we propose a scheme whose flow is as shown in Figure 2. Specifically, our scheme can be divided into several parts to be illustrated as follows   Online Images: Images coll ected from web albums or blog spaces are taken as inputs of the proposed scheme. One can easily noti ce that these images are very likely to share common visual-concepts as they are shared among acquaintances of common interests   Explorer: Several processing steps are conducted in this part. First, features are extracted from each input image. Then, these features are quantized into visual words. Finally, the technique of mining association rules is applied   Database: In the database, images collected from the Internet, visual words quantized from features and the discovered f requent itemsets are stored   Conceptual Groups: Imag es containing similar visual-concepts are grouped together. This facilitates the execution of an image retrieval task  4  EMPIRICAL STUDIES AND DISCUSSIONS In our preliminary experiments, we firstly collect a set of 100 photos representing four different festival scenes, i.e St. Patrick Day, Easter, Halloween, and Christmas. Some example images of this dataset are shown in Figure 3. This image dataset is then used in our experimental process for evaluation purposes  Figure 3 Example images of the festival dataset Our experimental process is depicted in Figure 4. The upper part of Figure 4 shows the training process that all images are carefully processed through necessary steps Specifically, feature vectors are firstly extracted from an image. Then, visual words are formed after the feature quantization step. Frequent itemsets can thus be discovered by utilizing the technique of association rule mining. Finally, several conceptual groups can be obtained With the festival image dataset, about 20,000 features and 500 visual words are generat ed. Moreover, more than 450,000 frequent itemsets are obtained when the minimum support, i.e., co-occurrence frequency, is set to be 10 Also, the maximum length of discovered frequent itemsets is 5. In other words, there are usually up to five visual words co-occurring in images of a similar scene. Based on these results, two example con ceptual groups, i.e, the Christmas scene and the Easter scene, discovered by our approach are shown in Figu re 5\(a\ and Figure 5\(b respectively An example scenario of utilizing our proposed scheme for image retrieval is also shown in the lower part of Figure 4 Once a user takes and uploads a photo with his or her smart phone, digital camera or even tablets, similar processing steps can be done to identify corresponding visual words of this im age. With a proper matching mechanism, images containing similar visual-concepts can be retrieved even if these images are not so similar in terms of low-level features. To further illustrate the scenario as shown in the lower part of Figure 4, one can easily understand that the corresponding visual-concept is the Christmas. Several im age patches with strong associations, e.g., Christmas stock ings, candy canes, and gingerbread man, are found in the images belonging to the same conceptual group. By further devising a more interactive interface, human users may not only perform retrieval tasks but also help to annotate corresponding images 220 


 Feature Vectors Internet Visual Words Frequent Itemsets Conceptual Groups Images Images Query Query Results User Feature Extraction Feature Quantization Association Rule Mining Feature Vectors Visual Words Feature Extraction Feature Quantization   Figure 4 Our experimental process and an illustrative example a b  Figure 5 Example results of discovered conceptual groups: \(a\ the Christm as scene; and \(b\Easter scene 5  CONCLUSIONS As more and more people get used to sharing their photos on the Internet, the problem of image retrieval has attracted increasing research interests. In this work, we have proposed to discover visual-concepts embedded within numerous images. Instead of adopting the typical process of scene detection to categorize images into different pre-defined classes, we have d evised a scheme to start with identifying similar image patches. Specifically we have introduced the concept of visual words and the technique of mining association rules to construct evident relationships among image patches so as to discover identical visual-concepts from online images 6  ACKNOWLEDGMENT The authors are supported in part by the National Science Council, Taiwan, R.O.C. \(NSC98-2221-E-006-164-MY2 7  REFERENCES 1  M. Agrawal, K. Konolige, and M. Blas, çCenSurE: Center Surround Extremas for Realtime Feature Detection and Matching Proceedings of the 10th European Conference on Computer Vision pages 102-115, October, 2008 2  R. Agrawal, T. Imielinski, and A. Swami, çMining Association Rules Between Sets of Items in Large Databases Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data pages 207-216, May 1993 3  V. N. Gudivada, and V. V. Raghavan, çContent-Based Image Retrieval Systems IEEE Computer 28\(9 September 1995 4  S. Lazebnik, C. Schmid, and J. Ponce, "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories Proceedings of 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2\(14\, June 2006 5  L.-J. Li, R. Socher, and F.-F. Li, çTowards Total Scene Understanding: Classification, Annotation and Segmentation in an Automatic Framework Proceedings of the 2009 IEEE Conference on  Computer Vision and Pattern Recognition pages 2036-2043, June 2009 6  D. G. Lowe, çDistinctive Image Features from ScaleInvariant Keypoints  International Journal of Computer Vision 60\(2\mber 2004 7  D. Nister and H. Stewenius, çScalable Recognition with a Vocabulary Tree Proceedings of the 2006 IEEE Conference on  Computer Vision and Pattern Recognition  pages 2161 - 2168, June 2006 8  J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman Object Retrieval with Large Vocabularies and Fast Spatial Matching Proceedings of the 2007 IEEE Conference on Computer Vision and Pattern Recognition pages 1-8, 2007 9  J. Sivic and A. Zisserman, çVideo Google: A Text Retrieval Approach to Object Matching in Videos Proceedings of the Ninth IEEE International Conference on Computer Vision vol. 2, page 1470, October 2003 10  C. G. M. Snoek and A. W. M. Smeulders, "Visual-Concept Search Solved Computer 43\(6\e 2010 11  A. Torralba, R. Fergus, and W. T. Freeman, ç80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition IEEE Transactions on Pattern Analysis and Machine Intelligence 30\(11\958-1970 November 2008 12  N. Vasconcelos, çFrom Pixels to Semantic Spaces Advances in Content-Based Image Retrieval IEEE Computer 40\(7\uly 2007 13  R. Veltkamp and M. Tanase, "Content-Based Image Retrieval Systems: A Survey Technical Report UU-CS2000-34 October 2000 221 


               NRMSE Moving Average Exponential Smoothing Kalman Filter Fourier Transform Wavelet MK-Wavelet 0.0 0.2 0.4 0.6 0.8 1.0 normalised similarity value a Similarity All methods evaluated on all sinusoids               NRMSE Moving Average Exponential Smoothing Kalman Filter Fourier Transform Wavelet MK-Wavelet 0.0 0.2 0.4 0.6 0.8 1.0 normalised similarity value b Dissimilarity All methods evaluated on all random walks Figure 2 Method evaluation The similarity should be 0.99 but the dissimilarity as low as possible with minimal variance                                                                                                                                           Mersenne Twister MT19937 dev/random Figure 3 Using the wavelet method as a test of randomness Closer to 0.5 with less variance is better     0 20 40 60 80 100 noise 0.0 0.2 0.4 0.6 0.8 1.0 normalised similarity value NRMSE Moving Average  Exponential Smoothing  Kalman Filter  Fourier Transform  Wavelet MK-Wavelet a All methods evaluated on all sinusoids     0 20 40 60 80 100 noise 0.0 0.2 0.4 0.6 0.8 1.0 normalised similarity value b All methods evaluated on all random walks Figure 4 Stability evaluation of the methods under increasing amounts of noise The stability curve should stay constant regardless of the amount of noise This is due to the decomposition of the signal into its describing features where the noise is eectively stripped away until its inîuence on the scaling components is negligible 11 I n t erestingly  t he Minkowski distance seems to have the same inîuence as in study 1 and the MK-wavelet behaves as the lter methods Figure 4b shows the same approach on the random walk data set to evaluate if dissimilarity is found coherently under increasing amounts of noise The comparisons using normalised root mean squared error exponential smoothing and Kalman lter do not stabilise but instead degrade quickly They cannot cope with increasing amounts of noise 


and converge towards the expected value of 0.25 The output function of the Fourier transform stays at near zero for all data sets thus yielding a high similarity value again for the same properties as mentioned before Therefore the Fourier transform is unable to identify the random walks in the data sets correctly because it misinterprets the noise Only our wavelet method stays stable at 0.64 and is only mildly inîuenced by increasing amounts of local noise The result for the wavelet is almost equal to the one in gure 2b and also correlates with the Mersenne Twister result thus strengthening conìdence in its proper behaviour 3.3.4 Stud y 4  Execution time Finally another important issue is the large scale of the systems under study which requires a fast method Our baseline was that results should be attainable in under 5 seconds per comparison We experimented with other methods namely seasonal auto-regressive integrated moving average SARIMA models  E lman-st y le recurren t artiìcial neural networks  a nd longest c ommon s ubsequence  Unfortunately  S ARIMA mo del  tting a nd neural n etwork training were too time-consuming to pursue further Whereas the wavelet method evaluates 1 million events in under 2 seconds we were not able to t or learn sample sizes as small as 10000 events within several minutes using these methods The longest common subsequence method was worst with an execution time of more than 10 minutes for only 1000 events It was because of these timing results in early trials that we only focused on the methods described Note that we only used straightforward implementations for all the models and methods even though some higher performance variants exist in literature The full runtime evaluation is omitted due to lack of space but since all presented methods were considerably faster than our baseline a more detailed examination of runtime is not as interesting We will now move on to the second method that is used by our similarity measure 4 ASSOCIATION RULE METHOD In this section we explain association rules how we use them and evaluate their suitability to measure similarity 4.1 Association rules Agrawal et al d eìne asso ciation rule l earning as follows Let I   i 1 i n  be a set of n binary attributes called items Let D   t 1 t n  be a set of transactions called the database  Each transaction in D must be uniquely identiìable and contains a subset of the items in I An association rule is then deìned as an implication X  Y  X Y  I    X 012 Y    Such association rules therefore map the structural dependency between attributes X and dierent attributes Y  As a result the set of all association rules for a given database D corresponds to the internal structure of all transactions The most frequent and signiìcant association rules are selected via two indicators called the support and the conìdence  using the apriori algorithm The s upp ort i s t he minimum required percentage of existence for a given item set in the database For example if the item set  a b c  exists 5 times in a database of 8 item sets then its support is 5  8=0  625 The analyst speciìes the support at the beginning of the evaluation eectively selecting the minimum threshold for the inclusion of a rule In the second step the conìdence then states the probability threshold that multiple Y from dierent association rules have the same X  eectively selecting only item sets that are common in pairwise comparison Again the analyst speciìes the conìdence at the beginning of the evaluation The apriori algorithm then generates item sets breadth-ìrst where parts of the search tree are pruned via support and conìdence We chose to use the apriori algorithm with support and conìdence because it is the basic algorithm for generating frequent item sets There are also other association rule learning algorithms available for example the Eclat algorithm F P-gro w th algorithm 15 o r O PUS s earc h  E v e n t hough d ieren t a lgorithms m igh t yield d ieren t rules the choice of which rule learning algorithm to use is actually not important The method only works on rules and does not need to know how the rules were created so every analyst can choose a rule learning algorithm of their liking Via selection of dierent learning algorithms one can tailor the method to a speciìc use case but care has to be taken that comparative evaluations of multiple workloads always use the same learning algorithm 4.2 Method As a preprocessing step the apriori algorithm has to run on the descriptive attributes of the given workload events We suggest using support=0.01  i.e the apriori algorithm considers every rule that contributes to at least 1 of the original transaction database regardless of its size That should yield a granularity that is small enough for detailed studies while eliminating most of the one-time rules that occur at the lower values of support  0  01 We also suggest using conìdence=0.80  i.e the top 20 of common rules are considered the most important This threshold is suggested based on the Pareto principle i.e the 80Ö20 rule Once the appropriate rules have been generated the similarity measure m a can be calculated The workîow of the method is shown in algorithm 1 First the corresponding type of attribute has to be evaluated for each value in X and Y of the rule We call this attributes 1  attributes 2 collection a link  The reason to store the attributes only and not the value as well is that in dierent workloads from the same system the values can change e.g user U 12 in one workload sample corresponds to user U 37 in another workload sample However we are not interested in a particular value at this stage but the dependencies of their attributes By eliminating such possible value inconsistencies we focus only on the dependency link between the attributes These links between attributes are then sorted and stored in lists The second step uses the results hash table to count the occurrences of each link By using the sorted link as the key in the hash table every occurrence of dependency links can be accounted for This means that association rules attributes such as user 015 protocol and protocol 015 user are considered equal because both correspond to the same link  protocol user   The third step evaluates these counts of occurrences Evaluation starts with one workloadês association rules r 1  The maximum and minimum values for each link are evaluated against the corresponding link from the dependency list of the comparison rules r 2  The relative dierence in the number of occurrences is then stored e.g link  datatype project user   22 from r 1 and  datatype project user   12 from r 2 result in a re 


 Algorithm 1 Association rule similarity method Require association rules r 1  2 1 dependencies 1  2  list    2 for event  r 1  2 do 3 link 1  2  list    4 for value  event do 5 link 1  2   attributes 1  attributes 2   link 1  2   6 end for 7 dependencies 1  2  sort  link 1  2   dependencies 1  2 8 end for 9 result 1  2   10 for link 1  2  dependencies 1  2 do 11 if link 1  2  result 1  2 then 12 result 1  2  link 1  2  link 1  2 1 13 else 14 result 1  2  link 1  2  1 15 end if 16 end for 17 final  list    18 count  0 19 for link 1  results 1 do 20 if link 1  results 2 then 21 m 1  min results 1  link 1  results 2  link 1  22 m 1  max results 1  link 1  results 2  link 1  23 final  final  min m 1   max m 2  24 delete result 2  link 1 25 else 26 final  final  0 27 end if 28 count  count 1 29 end for 30 for link 1  sequence   results 1   count  do 31 final  final  0 32 end for 33 for link 2  results 2 do 34 final  final  0 35 end for lative dierence of 12  22  0  54 Thatspeciìclinkisthen removed from the dependency list of the comparison workload If the link is not found then a penalty of 0 is applied These 0s directly reduce the similarity value During this loop the number of evaluated links is counted The nal step penalises leftover rules that were not used in the previous step From the starting workload r 1 all leftover links are penalised by adding 0s to the result set as counted from the number of previously evaluated links Any leftover links from workload r 2 are also directly penalised by adding 0s to the result set The method nishes with a list of relative dierences and penalties The algorithm returns with the similarity measure m a  r 1 r 2   mean  final ycalculating the arithmetic mean over all elements in the multi-set 4.3 Evaluation We evaluate the association rule method with two synthetic data sets each having 5 descriptive attributes 4.3.1 Methodology Again each data set is split into 8 distinct segments that are compared to each other yielding a sample size of 28 comparisons For one data set the workloads are created with round-robin selection of 1024 distinct values for each attribute per segment repeated 2 16 times This creation guarantees that dependencies stay consistent and should result in m a  1 by the association rule method The other data set selects uniformly distributed random values between 0 and 1024 for each attribute repeated 2 16 times Random data provides the practical lower limit of the method Again the evaluation of the method is subject to random number generation as mentioned in section 3 4.3.2 Results As expected the association rules found in the roundrobin workload scored a similarity value of m a 1 The method thus is able to nd consistent repeatable behaviour perfectly On the random uniform workload though the method scored a similarity value of m a 0 Avalueof0 was slightly unexpected because we assumed uniform random data to produce at least some usable rules However the apriori algorithm did not consider any of the random attribute dependencies as frequent enough Once we reduced the number of distinct values from 1024 to only 16 the apriori algorithm was able to nd 3 to 5 rules This reduction resulted in a low similarity value of m a 0  13 However note that xed values for support and conìdence are responsible for this result As we see later this problem does not exist in large-scale workloads as the possible permutations of attributes and events produce a much larger sample space As we are not aware of any other method that automatically quantiìes the dependencies of association rules we have no ground truth or competitors against which we can compare our methodês eectiveness With a practical result of 0  13  m a  1 we are quite conìdent that the method is applicable to many workloads because it covers the possible range of similarity values to a high degree Nevertheless while the association rule measure on its own is not a good choice for a similarity measure it does play the key role in improving the results attainable by our wavelet method 5 COMPOUND MEASURE We combine the wavelet measure and the association rule measure into a compound measure and evaluate its accuracy on operational workloads and with a hypothesis test 5.1 Combining the two methods The combination is based on the concept of transfer functions  where outputs of one measure are weighted according to the outputs of another measure This was shown to be superior to simple linear combinations of measures  I n our case we deìne our transfer function to weight the coefìcients of the wavelet transform before the intermediary result is calculated The idea is that coecients that involve events with attributes from the association rule measure should be weighted according to the inîuence of the attribute Finding these events is trivial because the length of the wavelet is given and the involved attributes were already found via the association rule measure A simple match on event and attribute can thus trigger the weighting and use the weighting value of an attribute f attribute  final  m a  t 1 DWT X 1 w min l 1 l 2   f attribute t 2 DWT X 2 w min l 1 l 2   f attribute 5 These t 1 t 2 arethenusedinsteadof x 1 x 2 to nish the wavelet method calculation However we call this result then our compound similarity measure 0 m 1 


 Data set Events Attr Intvl Intvl AuverGrid 404176 7 12 months DAS-2 1124772 7 22 months Gridê5000 1020195 7 31 months LCG 188041 4 11 days NorduGrid 781370 5 38 months SHARCNET 1195242 4 13 months DQ2 965519 7 24 hours Table 1 Overview of the GWA and DQ2 workloads 5.2 Methodology We evaluate the accuracies of all methods against two scheduling metrics mean waiting time and mean execution time  The premise of this evaluation is as follows if input workloads are similar then the performance metrics that evaluate the output of the system that is executing the workloads should be similar as well An elegant way of doing that is to use scheduling metrics b ecause they can b e principal targets for performance optimisation in a system Two workloads that produce the same waiting times or the same execution times can be considered similar in performance evaluation Many more dierent scheduling metrics can be used if necessary e.g mean resource utilisation or link throughput but for this evaluation we narrow ourselves to mean waiting time  W andmeanexecutiontime  E  The objective is to minimise the error between a similarity measure and the output metrics For example two workloads that dier by 0.3 in their relative waiting times should also yield a similarity value of 0.3 The normalised errors between two workloads w 1 w 2 are thus error  W   similarity w 1 w 2      W w 1   W w 2    error  E   similarity w 1 w 2      E w 1   E w 2    6 We use all the available workloads provided by the Grid Workloads Archive GWA  as w e ll as a w orkload from DQ2  T he GW A w orkloads a re pro vided i n a common trace format for the following large-scale distributed systems AuverGrid Distributed ASCI Supercomputer 2 DAS2 Gridê5000 LHC Computing Grid LCG NorduGrid and SHARCNET These workloads all stem from scientiìc computational traces DQ2 workload stems from scientiìc distributed data management traces N ote t hat D Q2 uses LCG resources but its traces are sampled independently and separately from the LCG traces in the GWA Table 1 gives an overview of the contents of the data sets that store the workload It shows the number of events the number of attributes and the segments per data set DQ2 generates as many events in one third of a single day as the other systems in multiple years It is because of this imbalance that we decided to keep the number of events roughly equivalent and not the actual time spent in the workload Note that the actual number of events is not relevant because the association rule method will only consider the most important constituent events and attributes Additionally the original workload traces contained many dierent attributes but we are only using attributes with a cardinality greater than one i.e multiple dierent values of the attribute are used throughout the workload The inclusion of attributes that stay constant during the workload would yield no additional information to the attribute analysis i.e trivial association rules and such information could be modelled statically anyway The apriori algorithm thus takes care of selecting proper attributes and events and the analyst does not need to select sets of attributes manually Lastly we only use attributes that are part of the workload deìnition e.g submission time or user identiìcation  We are not using output attributes like completion time because they are not part of the workload they are the result of the workload being executed by the system We split each data set into distinct segments that we compare against each other thus yielding sample sizes between 55 and 703 comparisons depending on the number of intervals in each workload In total there are 1874 possible permutations Each workload segment is then separated into two distinct parts a time series signal for the submission time attribute and a list of attribute tuples from descriptive attributes The time series signal is created by aggregating the count of submission times in the workload into cells appropriate for the workload We chose hourly daily and monthly cells as indicated in table 1 with the corresponding number of segments We obtain all possible error  W and error  E by calculating all similarity measures as well as  W  E for all segments in the workloads from the GWA that have this information This includes AuverGrid DAS-2 Gridê5000 SHARCNET In DQ2 workload the  W  E are staging and transfer times respectively and are calculated in exactly the same way 5.3 Results Figure 5 shows the error results over all evaluated workloads Figure 5a focuses on the errors of all methods against  W  There the compound measure is more accurate than all competing methods The interesting observation is that both constituent methods have a higher error but their combination as a compound measure reduces error drastically Especially the association rule measure cannot be used on its own Against the best case i.e the moving average the improvement in accuracy is 12.2 Against the worst case i.e the discrete Fourier transform the improvement is 24.5 This shows how the inclusion of a method for descriptive attributes is highly relevant in addition to time-dependent attributes and should not be ignored The wavelet method however is responsible for reducing standard deviation between 4.6 and 10.4 showing the advantage of a multi-resolution approximation free approach Figure 5b focuses on the errors of all methods against  E  Here the improvement in accuracy for the compound method is only 3.8 against the best case i.e again the moving average and 20.8 against the worst case i.e the Kalman lter Also the improvement in standard deviation is slightly less with 0.3 to 7.6 It is interesting though that the ltering methods also have a slight advantage in this case We have the following explanation for these results The waiting time is largely dependent on the number of already existing events in a system Therefore the speciìcs of a new event are important for its proper scheduling e.g which resource to use These speciìcs are stored in the descriptive attributes and can be found by the association rule method Like a scheduler uses this additional information to make an informed choice where and when to execute the event so does the association rule method evaluate the importance of the attributes This is less obvious for the execution time 


                    NRMSE Moving Average Exp Smoothing Kalman Filter Fourier Transform Wavelet MK-Wavelet Association Rules C ompound Measure 0.0 0.2 0.4 0.6 0.8 1.0 normalised error a Similarity error against Mean Waiting Time                    NRMSE Moving Average Exp Smoothing Kalman Filter Fourier Transform Wavelet MK-Wavelet Association Rules Compound Measure 0.0 0.2 0.4 0.6 0.8 1.0 normalised error b Similarity error against Mean Execution Time  Figure 5 Normalised errors of all methods against mean waiting time and mean execution time Lower is better Method  p value  p   0  50  25 0  125 NRMSE 0.38 510 224 3 Mov Avg 0.22 1120 504 22 Exp Smooth 0.36 488 252 1 Kalman 0.30 693 360 12 Fourier 0.36 589 252 7 Wavelet 0.21 1412 522 65 MK-Wavelet 0.22 1344 504 68 Ass Rules 0.25 640 450 5 Compound 0.20 1510 558 182 Table 2 Two-sample Anderson-Darling test per method for all 1874 pairwise permutations of segments Results are the number of times the null hypothesis could not be rejected where scheduling already took place and the descriptive attributes are less useful However the timing of the event is more important which is properly modelled through the wavelet methods Consequently the wavelet methods have less error and less variance than all other methods in this case A wavelet-based method biased by a transfer function is therefore a good choice in both the waiting and execution time cases In the case of our scientiìc workloads the association rule measure only has a signiìcant impact when the descriptive attributes inîuence the system We can conìrm this for DQ2 workload from our own operational experience 5.4 Hypothesis test We complement the accuracy evaluation with a statistical hypothesis test The two-sample Anderson-Darling test can validate if the empirical distribution functions of two independent samples follow from the same distribution It is a non-parametric k-sample test that does not make assumptions about the distribution function and is therefore an ideal candidate for cases without ground truth Our null hypothesis is that both samples come from the same distribution i.e the similarity error is minimal We prepare all 1874 workload segments by normalising their time-dependent attribute to a relative start of zero Additionally we replace all values in the descriptive attributes into unique numerical representations for easy calculation of an attributeês empirical distribution Then we evaluate all segments with each available method and present the results in table 2 For each method we give the mean P-value of the test and the number of times we could not reject the null hypothesis for a given statistical signiìcance   We want this number to be as close as possible to the maximum of 1874 The higher the number the more often the particular method identiìed two segments as similar which corresponds to a lower error for the similarity measure The results conìrm our previous performance metrics evaluation At a signiìcance of  0  5 the results show four good candidates our compound measure the two wavelet methods and the moving average These results are also true for error  W and error  E in the metrics evaluation The same observation still holds at  0  25 but only at  0  125 we can see the clear advantage of the compound measure Whereas all other methods mostly reject the null hypothesis the compound measure still provides an acceptable result at about 11 with 182 non-rejects Again as in the metrics evaluation the association rule method by itself is seemingly useless but in combination with the wavelet method provides a signiìcant boost to error reduction 6 CONCLUSIONS The performance evaluation of a system strongly depends on the input workload If fundamentally dierent workloads are used for evaluations then conclusions drawn from the results are likely to be non-representative Therefore we propose a compound measure to quantify the similarity between two workloads This compound measure comprises two independent methods the rst one to analyse the timedependent attribute in the workload and the second one to analyse the descriptive attributes in the workload The rst method uses the discrete wavelet transform to derive and compare components that describe the periodic time and frequency behaviour of the time-dependent attribute The novel idea of this approach is that we take advantage of the property of the inverse discrete wavelet transform that guarantees that the original signal can be reconstructed from the scaled coecients This idea improves upon exist 


ing work because the approach is free from any assumptions on the structure of the attribute and does not have to rely on statistical approximations We evaluate the approach using two synthetic data sets to establish the upper and lower bounds of the covered similarity space We nd that we can cover the whole similarity space and that we are only constrained by the random number generation To validate the method against this constraint we investigate the speciìc inîuence of the random number generation by using the method as a test of randomness Additionally operational systems usually exhibit large amounts of noise in their workload therefore we validate the method against noise as well Our ndings show that it is highly resistant to noise whereas other commonly employed methods yield to the law of large numbers or fail completely Additionally the method can consistently identify dissimilar behaviour as well even though it is not as good at that task which is only partially true for other commonly employed methods The second method uses association rule analysis to identify and quantify the relationship between descriptive attributes of the workload The novel idea of this approach is to eliminate the use of values and instead focus on the attributes themselves and therefore rank the relative usage of the attribute That way the most important building blocks of the workload can be compared directly The algorithm is described and then evaluated using synthetic data sets to establish the upper and lower bounds of the covered similarity space This time we nd that we are only constrained by the amount of learnt rules which follows from the used rule learning algorithm At least 16 rules need to be available to cover the whole similarity space when the apriori algorithm is used The method itself however does not require a speciìc rule learning algorithm We then present our compound measure that can address problematic workload characteristics We use the transfer function concept to weight speciìc events in the waveletmethod based on relative attribute dependencies Then we conduct an empirical study to evaluate all methods on operational workload from seven large-scale distributed systems Two important characteristics stand out First the time and frequency behaviour is surprisingly well-modelled with a wavelet method Second the association rule by itself is seemingly useless However the inclusion of descriptive attributes improves accuracy when determining similarity of workloads We show that the compound measure improves upon existing work by evaluating it against two important scheduling metrics mean waiting time and mean execution time The analysis of descriptive attributes for the similarity in addition to the analysis of the time-dependent attribute yields a compound similarity measure that can improve accuracy by 24.5 At the same time the standard deviation can be reduced by 10.4 We conìrm these results with an independent statistical hypothesis test a twosample Anderson-Darling statistic and show the advantage of the compound measure even at higher signiìcance levels of  0  125 with only 11 dierence Our results strongly reassure our initial expectations that the compound measure is a good choice for large-scale and data-intensive systems that have to deal with enormous amounts of events and previously unknown dependencies in their workload Furthermore even though we only used scientiìc workloads in our evaluations we did not observe any constraint that would limit the use to scientiìc workloads 7 FUTURE WORK As a result of this work we can continue to evolve our simulation eort for DQ2 We will use the similarity measure to evaluate the results of our future workload models to speciìcally address problems like data transfer cycles distributed le caching or popularity-based deletion However the measure can still be improved If it is suspected that dependencies change over time then the rate of change needs to be investigated Extending the association rule measure with time-evolving graphs may prove to be appropriate for such cases Additionally we only used the Haar wavelet as suggested by previous work We are interested in investigating the inîuence of dierent types of wavelets on the analysis of non-periodic burst behaviour An implementation of the similarity measure including already pre-processed versions of the used workloads from the evaluation can be downloaded from our website  8 ACKNOWLEDGEMENTS We are grateful to the following teams for providing traces via the Grid Workloads Archive The AuverGrid team the DAS-2 team the Gridê5000 and OAR teams the HEP eScience Group at Imperial College London for the LCG traces the NorduGrid team and J Morton and C Chrush for the SHARCNET traces 9 REFERENCES 1 R A g r a w a l T I m i e l i  nski and A Swami Mining association rules between sets of items in large databases ACM SIGMOD Record  22\(2 June 1993  L  B ergroth H Hak o nen and T  R aita A surv ey of longest common subsequence algorithms In 7th International Symposium on String Processing Information Retrieval  page 39 A Coru na ES Sept 2000 IEEE Computer Society  C  Borgelt and R  K ruse Induction of asso ciation rules apriori implementation In 14th Conference on Computational Statistics  pages 395Ö400 Berlin DE 2002 Physica  M  B ranco Distributed data management for large scale applications  PhD thesis School of Electronics and Computer Science University of Southampton UK Nov 2009  M  B ranco E Zalusk a D de Roure M  L assnig a nd V Garonne Managing very large distributed data sets on a data grid Concurrency and Computation Practice and Experience  22\(11 Aug 2010  R  G  B ro wn D  E ddelbuettel a nd D Bauer Dieharder A random number test suite http://www.phy.duke.edu rgb/General/dieharder.php Oct 2009  L  C  C arrington M Laurenzano A Sna v e ly  R  L  Campbell Jr and L P Davis How well can simple metrics represent the performance of hpc applications In ACM/IEEE International Conference for High Performance Computing Networking Storage and Analysis  Seattle WA USA 2005 ACM  G  C asale N Mi a nd E Smirni C WS a mo del-driv e n scheduling policy for correlated workloads In ACM SIGMETRICS International conference on 


measurement and modeling of computer systems  pages 251Ö262 New York NY USA June 2010 ACM  K  P  C han a nd A W.-C F u Ecien t t ime s eries matching by wavelets In 15th International Conference on Data Engineering  pages 126Ö133 IEEE 1999  C Chatìeld The analysis of time series an introduction  CRC Press 1997  I Daub ec hies Ten lectures on wavelets  Society for Industrial and Applied Mathematics 1992  P  A Dinda a nd D R OêHallaron H ost l oad prediction using linear models Cluster Computing  3\(4 2000  D F r eedman and P  D iaconis On the h istogram as a density estimator L2 theory Probability Theory and Related Fields  57\(4 1981  A Graps An in tro duction to w a v e lets IEEE Computational Science  Engineering  2\(2 1995  J Han J P e i Y Yin and R  M ao Mining frequen t patterns without candidate generation A frequent-pattern tree approach Data Mining and Knowledge Discovery  8\(1 2004  T Ho eîer T  S c hneider a nd A Lumsdaine Characterizing the inîuence of system noise on large-scale applications by simulation In ACM/IEEE International Conference for High Performance Computing Networking Storage and Analysis New Orleans LA USA 2010 IEEE Computer Society  A Iosup H Li M  J an S  A no ep C  Dumitrescu L Wolters and D H J Epema The grid workloads archive Future Generation Computer Systems  24\(7 July 2008  A Kramp e  J  L epping a nd W Sieb en A h y brid markov chain model for workload on parallel computers In ACM International Symposium on High Performance Distributed Computing  pages 589Ö596 Chicago IL USA June 2010 ACM  M Lassnig C ERN P H-ADP DDMLAB public website http://cern.ch/ddmlab-public April 2011  M Lassnig T  F ahringer V  G aronne A  M olfetas and M Branco Stream monitoring in large-scale distributed concealed environments In 5th IEEE International Conference on e-Science  pages 156Ö163 Oxford UK Dec 2009 IEEE Computer Society  M Lassnig T  F ahringer V  G aronne A  M olfetas and M Branco Identiìcation modelling and prediction of non-periodic bursts in workloads In 10th IEEE/ACM International Symposium on Cluster Cloud and Grid Computing  pages 485Ö494 Melbourne AU May 2010 IEEE Computer Society  H Li W orkload dynamics on clusters and g rids The Journal of Supercomputing  47\(1 2009  H Li and M  Muskulus Analysis and m o d eling o f j ob arrivals in a production grid ACM SIGMETRICS Performance Evaluation Review  34\(4 Mar 2007  U Lublin and D  G  F eitelson T he w o rkload on parallel supercomputers modeling the characteristics of rigid jobs Journal of Parallel and Distributed Computing  63\(11 Nov 2003  D P  Mandic a nd J A Cham b e rs Recurrent Neural Networks for Prediction Learning Algorithms Architectures and Stability  Wiley 2001  M Matsumoto a nd T Nishim ura M ersenne Twister a 623-dimensionally equidistributed uniform pseudo-random number generator ACM Transactions on Modeling and Computer Simulation  8\(1 Jan 1998  T N Minh L W o lters and D  E p e ma A realistic integrated model of parallel system workloads In 10th IEEE/ACM International Conference on Cluster Cloud and Grid Computing  pages 464Ö473 Melbourne AU May 2010 IEEE Computer Society  J C Mogul Emergen t mis eha v ior v s complex software systems ACM SIGOPS Operating Systems Review  40\(4 Oct 2006  K Mohror a nd K L Kara v a nic Ev aluating similarity-based trace reduction techniques for scalable performance analysis In 22nd Annual International Conference on Supercomputing  page 55 ACM 2009  Q P a n L Zhang G Dai and H  Z hang T w o denoising methods by wavelet transform IEEE Transactions on Signal Processing  47\(12 Dec 1999  P  Ratn F  Mueller B  R  d e Supinski a nd M Sc h u lz Preserving time in large-scale communication traces In 22nd Annual International Conference on Supercomputing  pages 46Ö55 ACM 2008  F W Sc holz and M  A  S tephens K sample anderson-darling tests Journal of the American Statistical Association  82\(399 1978  Z R Struzik and A  S ieb e s Principles of Data Mining and Knowledge Discovery  volume 1704 of Lecture Notes in Computer Science  chapter The Haar Wavelet Transform in the Time Series Similarity Paradigm pages 12Ö22 Springer 1999  E Theresk a and G  R  G anger IR ONMo del Robust performance models in the wild In ACM SIGMETRICS International conference on measurement and modeling of computer systems  pages 253Ö264 Annapolis MD USA June 2008 ACM  F W a silewski P yW a v elets Discrete W a v e let Transform in Python http://www.pybytes.com/pywavelets May 2010  G I W e bb OPUS A n e cien t admissible algorithm for unordered search Journal of Artiìcial Intelligence Research  3:431Ö465 1995  M J Zaki S calable a lgorithms f or asso ciation m ining IEEE Transactions on Knowledge and Data Engineering  12\(3 May 2000 


association rules and decision trees on analysis of diabetes data from the DiabCare program in France stud health technol inform 2002;90:557-61 6] J.Mondelle Simeon and Rober, Hilderman Exploratory Quantitative Contrast Set Mining:A Discretization Approach, 19th IEEE International Conference on Tools with Artificial Intelligence - Vol.2 ICTAI 2007 7] D.Newman, J. S.Hettich, C.L.S. Blake, and C.J. Merz, UCI Repository of machine learning databases,Irvine, CA: University of California, Department of Information and Computer Science.1998 last accessed: 1/10/2009 8] J.Han, and M.Kamber, Data mining: Concepts and techniques, San Francisco: Morgan Kaufmann Publisher, pp.47- 94, 2006 9] Glenn J. Myatt  Making sense of data: A Practical Guide to Exploratory Data Analysisand   Data Mining:Wiley\(2007 10] G. Chen, AND T.Astebro,  How to deal with missing categorical data: Test of a simple Bayesian method, Organ. Res. Methods 6, 3 2003 11] R.Agrawal, T. Imielinski, & A. Swami, Database mining aperformance perspective, IEEE Transactions on Knowledge and Data Engineering, 5\(6 1993 Special issue on Learning and Discovery in Knowledge-Based Databases 12] R.Agrawal, T.Imielinski, & A.Swami,Mining association rules between sets of items in large databases, In Proc. ACM-SIGMOD int. conf. management of data \(SIGMOD93 USA \(pp. 207216 13] Ian H.Witten and Elbe Frank, Datamining Practical Machine Learning Tools and Techniques, Second Edition, Morgan Kaufmann, .San Fransisco, 2005 14] S.Brin, R. Motwani, J.D. Ullman,  & S.Tsur, Dynamic itemset counting andimplication rules for market basket data, Proceedings of the ACM SIGMODInternational Conference on Management of Data pp. 255-264, Tucson, AZ, May 1997,ACM Press 15] M.J.Zaki, S. Parthasarathy, M. Ogihara, & W.Li, W. New algorithms for fast discovery of association rules, Proceedings of the 3rd International Conference on KnowledgeDiscovery and Data Mining \(KDD 1997,AAAI Press 16] B.Liu, W. Hsu, & Y.Ma, Pruning and summarizing the discovered association, Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 125-134 San Diego, CA, August 1999 


17] J.Han, J.Pei, & Y.Yin, Y,  Mining frequent patterns without candidate generation, Proceedings of the ACM SIGMOD International Conference on Management of Data,  Dallas, TX, May 2000 18] Y.Li, & L.Sweeney, Adding semantics and rigor to association rule learning: the GenTree approach, Technical Report CMU ISRI 05-101 2005 19] M.Rangsipan, Structure-Based Rule Selection Framework for Association Rule Mining of Traffic Accident Data, CIS 2006: 231239                                 


           334 


21] S. Baker and S.K. Nayar, A Theory of Catadioptric Image Formation, IEEE International Conference on Computer Vision \(ICCV pp.35-42, Jan, 1998 22] S.K. Nayar, Catadioptric Omnidirectional Cameras, IEEE Conference on Computer Vision and Pattern Recognition \(CVPR 488, Jun, 1997 23] A.Victorino, La commande referencee capteur: une approche robuste au proble`me de navigation, localisation et cartographie simultanees pour un robot dinterieur. PhD thesis, LUniversite de Nice-Sophia Antipolis, Inria Sophia Antipolis, 2002 3524 


ec  d Fig. 5: Computation Performance Comparison Tab. 4: Computation Savings by TOP-MATA K Connect K Retail K Wap La12 50 58.35% 100 0.01% 200 0.83% 23.04 150 55.91% 400 2.65% 400 30.12% 45.38 250 53.61% 700 1.84% 800 20.03% 25.95 350 48.28% 1100 3.95% 1600 13.06% 27.89 450 43.12% 1400 1.48% 3200 6.14% 12.70 550 39.36% 1700 4.00% 6400 5.63% 7.11 Second, Fig. 5 shows the results of four data sets computed by TOP-MATA and TOP-DATA, respectively. As can be seen, in general, TOP-MATA shows a better performance than TOP-DATA. And as the increase of the ? value, the advantage tends to be even more impressive for these four data sets 4.3. The Computation Saving of TOP-MATA As can be seen in the Tab. 4, four data sets, enjoy signi?cant computation savings brought by TOP-MATA. We can conclude that the computation saving is a major factor for the performance of TOP-MATA. That is, compared with TOP-DATA, a higher computation saving implies a much better performance of TOP-MATA. Since this saving is more signi?cant as the increase of the items, TOP-MATA works better for large scale data sets with a large number of items 5. Conclusion In this paper, we studied the problem of searching for top? item pairs with the highest cosine values among all item pairs. Speci?cally, we provided a novel algorithm TOPMATA which employ a Max-First traversal strategy for ef?ciently performing top-? cosine similarity search. Extensive experimental results veri?ed the effectiveness of the algorithms, And TOP-MATA algorithm is superior to TOPDATA for large-scale data sets with multiple items Acknowledgment This research was partially supported by the National Natural Science Foundation of China \(NSFC No. 70901002 and the Ph.D. Programs Foundation of Ministry of Education of China \(No. 20091102120014 


REFERENCES 1] R. Agrawal, T. Imielinski, and A. Swami, Mining association rules between sets of items in large databases, in SIGMOD 1993 2] C. Alexander, Market Models: A Guide to Financial Data Analysis. John Wiley & Sons, 2001 3] W. Kuo, T.-K. Jensen, A. Butte, L. Ohno-Machado and I. Kohane, Analysis of matched mrna measurements from two different microarray technologies Bioinformatics, vol. 18, p. 405C412, 2002 4] H. Xiong, X. He, C. Ding, Y. Zhang, V. Kumar, and S. Holbrook, Identi?cation of functional modules in protein complexes via hyperclique pattern discovery in PSB, 2005 5] J. Han, H. Cheng, D. Xin, and X. Yan, Frequent pattern mining: Current status and future directions DMKD, vol. 15, no. 1, pp. 5586, 2007 6] P.-N. Tan, M. Steinbach, and V. Kumar, Introduction to Data Mining. Addison-Wesley, 2005 7] S. Brin, R. Motwani, and C. Silverstein, Beyond market basket: generalizing association rules to correlations, in SIGMOD 1997, Tucson, AZ, 1997, pp 265276 8] E. Omiecinski, Alternative interestmeasures formining associations, TKDE, vol. 15, pp. 5769, 2003 9] H. Xiong, S. Shekhar, P.-N. Tan, and V. Kumar Exploiting a support-based upper bound of pearsons correlation coef?cient for ef?ciently identifying strongly correlated pairs, in KDD 2004, 2004, pp 334343 10] I. Ilyas, V. Markl, P. Haas, P. Brown, and A. Aboulnaga, Cords: Automatic discovery of correlations and soft functional dependencies, in SIGMOD 2004 2004, pp. 647658 11] J. Zhang and J. Feigenbaum, Finding highly correlated pairs ef?ciently with powerful pruning, in CIKM 2006, 2006, pp. 152161 12] H. Xiong, W. Zhou, M. Brodie, and S. Ma, Top-k correlation computation, JOC, vol. 20, no. 4, pp 539552, 2008 13] S. Zhu, J. Wu, and G. Xia, Top-k cosine similarity interesting pairs search, in 


http://datamining.buaa.edu.cn/TopKCos.pdf 14] M. Zaki, Scalable algorithms for association mining, TKDE, vol. 12, pp. 372390, 2000 


enhance item-based collaborative filtering, in 2nd IASTED International Conference on Information and Knowledge Sharing, Scottsdale, Arizona, 2003 476 2010 10th International Conference on Intelligent Systems Design and Applications 


Basi Association Rles Basi c  Association R u les Association is basically connecting or tying up occurrences of Association is basically connecting or tying up occurrences of events Ol dib t f ilt t O n l y d escr ib e se t s o f s i mu lt aneous even t s Cannot describe patterns that iterate over time e g  itemset a  0  b  0  g    Eg If you sense higher data rates on the downlink than normal AND New Route generated Implies high chances of Intrusion AND New Route generated Implies high chances of Intrusion Associative IDS for NextGen Frameworks Dr S Dua LA Tech 20 


Enhanced Inte r transaction Association Rules Enhanced Inter transaction Association Rules Enhanced Inter transaction Association Rules Extension of association rules Conditional relationships at multiple different time steps e.g itemset a\(0 0 1 2 You sense Higher data rate than normal AND You see New Route g enerated AND 1 minute a g o you detected checksum gg error packets AND 2 minutes ago your encountered wrong checksum   Implies High Chance of Intrusion Enhanced Rules and Confidence Associative IDS for NextGen Frameworks Dr S Dua LA Tech 21 


Complex Spatio temporal Association Complex Spatio temporal Association Rules Further extension of inter transaction association rules Describe event durations e.g itemset a\(0,X j,Y k,Z Eg  You sense high data rates for X seconds AND new route generated j minutes ago task completed in Y AND new route generated j minutes ago task completed in Y seconds AND checksum error packets received k minutes ago for Z seconds High Chance of Intrusion With highest confidence level in association rules  association rules  Associative IDS for NextGen Frameworks Dr S Dua LA Tech 22 


DMITAR Al ith ARD DMITAR Al gor ith m  ARD Problem Domain Problem Statement and Challenges Aiti Miig bd IDS A ssoc i a ti ve Mi n i n g b ase d IDS  Introduction to data mining Association rule in data mining DMITAR Algorithm  ARD New research Associative IDS for NextGen Frameworks Dr S Dua LA Tech 23 


DMITAR Algorithm DMITAR Difference Matrix Based Inter Transaction Association Rule Miner developed in DMRL Uses vertical data format Differences of the transaction IDs are used to generate extended itemsets Windowless mechanism Associative IDS for NextGen Frameworks Dr S Dua LA Tech 24 


Deep into the Mechanism The DMITAR algorithm is based on lhilii comp l ex mat h emat i ca l assoc i at i ve formulation and proofs Four major parts Four major parts Frequent 1 itemset generation Frequent 2 itemset generation Frequent k itemset generation k>2 Spatio temporal rule formation Associative IDS for NextGen Frameworks Dr S Dua LA Tech 25 


DMITAR, Datasets Used Stock Data Stock Data Daily stock information provided by Yahoo finance Wth Dt W ea th er D a t a Provided by the US Department of Commerce and National Climactic Data Center for 700 locations across US Synthetic Data Provided by a CRU weather generator that uses a Markov chain model to generate simulated weather data for 11 UK sites Associative IDS for NextGen Frameworks Dr S Dua LA Tech 26 


DMITAR Results 1/5 Varying Support DMITAR Results 1/5 Stock Database Support FITI ITPMine PROWL DMITAR 14 6424.7s 132.39s 3.03s 5.556s 16 2348.9s 67.14s 2.14s 4.015s 18 861.92s 34.62s 1.55s 2.89s 20 334.51s 18.89s 1.12s 2.07s 22 143 84s 10 87s 0 87s 1 45s 22 143  84s 10  87s 0  87s 1  45s 24 63.62s 7.15s 0.671s 1.04s Weather Database Support FITI ITPMine PROWL DMITAR 14 36362.6s 893.1094s 5.843s 19.8281s 36362.6s 893.1094s 5.843s 19.8281s 16 11913.04s 378.2188s 3.8906s 13.4375s 18 4116s 170.3438s 2.75s 9.1406s 20 1507s 86.5781s 2.14s 6.203s 22 859.2813s 63.3438s 1.7969s 5.7656s 24 378.5313s 36.1875s 1.4375s 3.5625s Synthetic Dataset Support FITI ITPMine PROWL DMITAR 14 1651.6s 199.843s 3.1406s 17.015s 16 574 32 119 32 2 0938 10 875 16 574  32 s 119  32 s 2  0938 s 10  875 s 18 416.109s 95.31s 1.6094s 7.39s 20 370.25s 83.31s 1.453s 5.8438s 22 265.78s 66.3438s 1.3594s 4.75s 24 133.96s 43.0781s 0.9219s 3.5781s 


DMITAR Rlt  DMITAR  R esu lt s 2  5 Rules Formed Associative IDS for NextGen Frameworks Dr S Dua LA Tech 28 


DMITAR Rl  Varying Maxspan DMITAR  R esu l ts 3  5 Varying Maxspan Associative IDS for NextGen Frameworks Dr S Dua LA Tech 29 


DMITAR Res lts 4/5 Vig Di i DMITAR  Res u lts  4/5 V ary i n g Di mens i ons Associative IDS for NextGen Frameworks Dr S Dua LA Tech 30 


DMITAR Rl  Varying Number of Transactions DMITAR  R esu l ts 5  5 Varying Number of Transactions Associative IDS for NextGen Frameworks Dr S Dua LA Tech 31 


N/C t Rh N ew C urren t R esearc h Problem Domain Problem Statement and Challenges Associative Mining based IDS Associative Mining based IDS Introduction to data mining ii lid ii Assoc i at i on ru l e i n d ata m i n i ng DMITAR Algorithm  ARD h New Researc h Associative IDS for NextGen Frameworks Dr S Dua LA Tech 32 


Further Research Further Research Objectives of Our Intrusion Detection System Development Objectives of Our Intrusion Detection System Development 1 Refine and scale the DMITAR algorithm to suit our application 2 Develop methods for dynamically altering the sensor parameters Our Focus is Securing the NextGen Frameworks with New Technology Technology Associative IDS for NextGen Frameworks Dr S Dua LA Tech 33 


Simulated Sensing Environment Simulated Sensing Environment Screenshots of Data C ollected from S ynthetic Sensors Screenshots of Data C ollected from S ynthetic Sensors Simulated in Our Laboratory Three Steps Slides Collect data Collect data from all sources  all attributes  Select Select the source and their hierarchical attributes attributes to be monitored and Select Select the source and their hierarchical attributes attributes to be monitored  and Sample Sample them at different rates different rates and process them Associative IDS for NextGen Frameworks Dr S Dua LA Tech 34 


Data Collection Simultaneous collection of data screen from ENTITIES aircrafts Associative IDS for NextGen Frameworks Dr S Dua LA Tech 35 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Parameter Selection Selection of an ENTITY/aircraft and monitoring its parameters and sensor readings Associative IDS for NextGen Frameworks Dr S Dua LA Tech 36 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Sensor Sampling Selecting one sensor/probe and reading it at different resolutions or sampling rates Associative IDS for NextGen Frameworks Dr S Dua LA Tech 37 Source PRTG Network Monitor software screen shot Demo info www.paessler.com 


Si l ti Nt k Si mu l a ti on on N e t wor k Most scenarios can be simulated on a computer or computer network network  The aircrafts and sensors are simulated on a computer network bllb b y ana l ogica l su b stitutions Sensors provide information at different rates Sensors provide information at different rates Need Based Sensor Prioritization NSP and Dynamic Sensing Rate Sampling Associative IDS for NextGen Frameworks Dr S Dua LA Tech 38 


Vulnerability Search Scan 39 Sample UQA script with Nmap performed in DMRL Associative IDS for NextGen Frameworks Dr S Dua LA Tech 39 Source Nmap screenshot with a pearl script  Find Namp on Nmap.org 


Modality Aspect Modality Aspect A Multimodal distribution is a continuous probability distribution with two or more modes of underlying data Mltil d i M u lti p l e mo d es i n our model Associative IDS for NextGen Frameworks Dr S Dua LA Tech 40 Source http://en.wikipedia.org/wiki/File:Bimodal bivariate small.png 


Multi Modality Modality Fusion 41 SENSITIVE  UNCLASSIFIED For Official Use Only Associative IDS for NextGen Frameworks Dr S Dua LA Tech 41 


Emphasis Emphasis Our approach emphasizes on pre empting the attack Our intent is NOT to perform an autopsy to discover attacks Instead we aim to detect and prevent in attacks in real time Associative IDS for NextGen Frameworks Dr S Dua LA Tech 42 


Techniques for Discriminative Rules Techniques for Discriminative Rules Resolution Analysis Features Analyzed at Different Depths Anti Monotonic Principle Modality Aspect Treading into Unexplored Feature spaces Associative IDS for NextGen Frameworks Dr S Dua LA Tech 43 


Representative Outcomes Representative Outcomes Illustration of the Final Analysis Our method shifts between modalities and sampling rates for optimization The blue parallelograms are actual intrusions 44 The blue parallelograms are actual intrusions The Red Green Blue Plots are response of system Associative IDS for NextGen Frameworks Dr S Dua LA Tech 44 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


