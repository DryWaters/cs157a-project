Learning Associations of Conjuncted Fuzzy Sets for Data Prediction Hanlin Goh Joo-Hwee Lim Member IEEE  and Chai Quek Member IEEE Abstract  Fuzzy Associative Conjuncted Maps FASCOM is a fuzzy neural network that represents information by conjuncting fuzzy sets and associates them through a combination of unsupervised and supervised learning The network rst quantizes input and output feature maps using fuzzy sets They are subsequently conjuncted to form antecedents and consequences and associated to form fuzzy if-then rules These associations are learnt through a learning process consisting of three consecutive phases First an unsupervised phase initializes based on information density the fuzzy membership functions that partition each feature map Next a supervised Hebbian learning phase encodes synaptic weights of the input-output as 
sociations Finally a supervised error reduction phase ne-tunes the ne-tunes the network and discovers the varying inîuence of an input dimension across output feature space FASCOM was benchmarked against other prominent architectures using data taken from three nonlinear data estimation tasks and a realworld road trafìc density prediction problem The promising results compiled show signiìcant improvements over the stateof-the-art for all four data prediction tasks I I NTRODUCTION Neural networks and fuzzy systems represent key methodologies in soft computing Neural Netw orks tak e inspiration from brain functions and connectionist structures to be highly capable in learning and adaptation while fuzzy systems mimic human-like knowledge representation and reasoning Each methodology possess its own share of drawbacks On one hand neural networks are typically black boxes and have a problem of being opaque On the 
other hand fuzzy systems face design problems such as determining membership functions as well as identifying and inferring fuzzy rules By integrating neural networks with fuzzy logic the architectureês transparency can signiìcantly improve Conversely design problems faced by fuzzy systems may be alleviated through brain-inspired techniques such as self organization Thus the hybridization of neural networks with fuzzy systems results in powerful tools for machine learning and logic inference because their strengths are complimentary and are hence able to maximize the desirable properties of both methodologies Such hybrid intelligent systems are widely known as neuro-fuzzy systems or fuzzy neural networks the name used henceforth Fuzzy neural networks are generally grouped into two distinctively different paradigms based on how they represent fuzzy if-then rules The rst paradigm is called the linguistic H Goh and J.-H Lim are with the Computer Vision and Image Under 
standing Department Institute for Infocomm Research A*STAR Agency for Science Technology and Research 21 Heng Mui Keng Terrace Singapore 119613 email  hlgoh,joohwee  i2r.a-star.edu.sg C Quek is with the Centre for Computational Intelligence School of Computer Engineering Nanyang Technological University Block N4 Nanyang Avenue Singapore 639798 email ashcquek@ntu.edu.sg fuzzy model e.g Mamdani model whereby both the antecedent and consequence are represented using fuzzy sets The second is known as the precise fuzzy model e.g TSK model  in which the antecedent is a fuzzy set while the consequence is expressed as linear equations  Linguistic fuzzy models e x cel i n interpretability while lacking in accuracy while the inverse is true for precise fuzzy models see 8 for discussion This paper proposes the Fuzzy Associative Conjuncted Maps 
FASCOM architecture which is a variant of a fuzzy linguistic model whereby a fuzzy if-then rule R k is represented in the form of R k if x 1 is A k 1 and  and x I is A k,I then y 1 is C k 1 and  and y S is C k,S 1 where 
X  x 1 x I  T and Y  y 1 y S  T are the input and output vectors respectively A k,i i  1 I  and C k,s s  1 S  represent linguistic labels of input and output linguistic variables while I and S are the number of antecedents and consequences respectively Each rule is 
weighted by v k denoting the strength of the R k  The current research direction in fuzzy neural networks is to learn modify and infer fuzzy rules based on past experience This is achieved using either unsupervised and/or supervised learning techniques to identify learn and adjust fuzzy if-then rules In unsupervised learning approaches  algorithms are used to identify fuzzy rules before applying neural network techniques to adjust rules without the need for a priori outputs However due to the heavy reliance on the training data non-representative data may lead to ill deìned rules and hence producing an inaccurate models For supervised learning approaches 14 rules are identiìed by mapping input-output data pairs However a drawback is that the semantics of the fuzzy neural network remains opaque contradicting the original conceived objec 
tive of combining fuzzy logic with neural networks FASCOM is encoded through a learning process consisting of one unsupervised learning phase followed by two consecutive supervised learning phases The initial phase involves the unsupervised initialization of membership functions of fuzzy sets of data dimensions This is followed by a supervised Hebbian learning phase to determine the synaptic weights between associated nodes in the network The nal supervised phase ne-tunes the network by reducing the error produced by the system The entire learning process consisting of the three learning phases will be described in detail in Section III and the contributions of the individual phases to the accuracy of data prediction is discussed in V-A.4 1515 978-1-4244-1821-3/08/$25.00 c  2008 IEEE 


The following contributions of the proposed architecture are described in this paper 1 Improving the precision of output prediction by using uniform information density topology over one with a proportional distribution of fuzzy sets in feature space 2 Using a novel supervised error reduction algorithm to ne-tune the network and discover the varying inîuence of an input dimension across output space 3 Improvements in accuracy over the state-of-the-art for three Nakanishiês nonlinear estimation tasks as well as a a real-world road trafìc density prediction problem II T HE FASCOM A RCHITECTURE The FASCOM architecture Fig 1 has six layers with each layer performing a speciìc fuzzy operation The inputs and outputs are vectors X  x 1 x i x I  T and Y   y 1 y s y S  T  where I and S denote the number of input and output linguistic variables respectively In layer 1 input linguistic node IF i  represents the i th input linguistic variable of input x i  In layer 2 input label node IL i,j  represents the j th linguistic label of the i th input linguistic variable IL i,j nodes corresponding to the same input x i are grouped in input map IM i  Layer 3 consists of antecedent nodes A k,l grouped into input conjuncted maps ICM k  A k,l represents either a single antecedent that is mapped from an IM  or multiple antecedents which are a conjunction of two or more IL i,j nodes from different IM  In layer 4 consequence node C q,p represents either a single consequence or multiple consequences and nodes are organized into output conjuncted maps  Layer 5 consists of output maps OM s  each containing output label nodes OL s,r  with each node symbolizing the r th linguistic label of the s th output variable In layer 6 output linguistic nodes OD s denotes the s th output linguistic variable of output y s                                         Fig 1 Six-layered structure of the Fuzzy Associative Conjuncted Maps FASCOM architecture 1516 2008 International Joint Conference on Neural Networks IJCNN 2008 


In the hybrid associative memory every ICM k is connected to every OCM q via a  ICM k  OCM q  association each representing a fully connected two-layered heteroassociative network In a  ICM k  OCM q  network when A k,l is linked with C q,p  a fuzzy if-then rule is formed between them i.e if A k,l then C q,p  Memory between two nodes A k,l and C q,p is stored as synaptic weight w k,l  q,p  while modulatory weight m k  q,p signiìes the connection strength between ICM k and C q,p  As a result a rule is weighted by resultant weight v k,l  q,p  which is computed as v k,l  q,p  m k  q,p  w k,l  q,p  2 As a convention the output nodes and maps are denoted using Z with the subscripts specifying its origin For example Z IF i represents the output of node IF i and Z IM i denotes the output vector of nodes in IM i  All outputs from a layer are propagated to the corresponding inputs at the next layer with unity link-weight  1  0 where a connection exists except for the connections between layers 3 and 4 where they are weighted by v k,l  q,p  A Layer 1 Input Fuzziìer A IF i node in layer 1 fuzziìes the input into a fuzzy membership function is given by Z IF i   i  x i  3 where  i     0  1 is the membership function of IF i If a fuzzy input is presented to a node in this layer the node simply redirects it as the output of the node i.e Z IF i  x i  Two fuzzifying functions were identiìed Gaussian G     and Laplacian of Gaussian LoG     The LoG    function produces similar effects as lateral inhibition in the biological neural circuitry w hich is used to increase signal discrimination by improving feature contrast B Layer 2 Single Antecedent In layer 2 a IL i,j node computes the fuzzy subsethood measure between  i,j    and Z IF i  Adopting the minimum T-norm operator for the intersection operation Z IL i,j can be approximated as 4 where Z  IF i  0  1 and Z  IF i    1  0 are positive and negative components of Z IF i respectively and  i,j    is the membership function of input label IL i,j  C Layer 3 Multiple Antecedent In layer 3 output Z A k,l    1  1 is known as the ring strength  resulting from the conjunction of IL i,j nodes Using the algebraic product T-norm operator Z A k,l is given by Z A k,l  I  i 1 J i  j 1  IL i,j  A k,l 1 Z IL i,j 5 where  IL i,j  A k,l is a link weight of 1 for a connection between IL i,j and A k,l  and is otherwise 0 D Layer 4 Multiple Consequence For layer 4 output Z C q,p    1  1 of C q,p is its activation level  C q,p  which is obtained through the recalling process to be explained in Section IV and given by Z C q,p   C q,p  6 E Layer 5 Single Consequence In layer 5 Z OL s,r    1  1 is the maximum activation level of connected C q,p nodes deìned as Z OL s,r max x  q,y  p   C x,y  OL s,r  Z C x,y  7 where  C q,p  OL s,r is a link weight of 1 if C q,p is connected to OL s,r  and is 0 otherwise F Layer 6 Output Defuzziìer Finally layer 6 defuzziìes the fuzzy outputs from layer 5 to produce crisp outputs The center of area defuzziìcation scheme s COA is applied for every OD s as follows Z OD s  s COA  R s  r 1 Z OL s,r  8 where  R s r 1 Z OL s,r is the aggregated output membership function III P HASES IN THE L EARNING P ROCESS The learning process Fig 2 consists of three consecutive phases 1 unsupervised membership function initialization 2 supervised Hebbian learning and 3 supervised error reduction During learning the output layers 4 to 6 are functionally similar to input layers 1 to 3 which are fuzziìcation for layer 6 Section II-A subsethood calculation for layer 5 Section II-B and conjunction formation for layer 4 Section II-C For clarity an alternate symbol   Z is used to represent an output obtained via this reverse propagation process    Fig 2 Phases in the learning process highlighted in gray Z IL i,j  max x  i min Z  IF i  x   i,j  x   max x  i min  Z  IF i  x    i,j  x  max x  i   i,j  x  4 2008 International Joint Conference on Neural Networks IJCNN 2008 1517 


A Unsupervised Membership Function Initialization A linguistic label for a particular linguistic variable is represented by a neuron in a feature map In FASCOM feature maps may be scalar or cyclic while member ship functions of neurons may be discrete rectangular or Gaussian The rst two result in crisp or classical sets while the third results in fuzzy sets The number of neurons in a feature map is xed according to the number of labels of a linguistic variable The initialization of membership functions involves a three-stages First the centroids of membership functions of these neurons are then placed evenly across the feature space represented by the feature map Next a membership function i.e discrete rectangular or Gaussian is applied for each neuron Finally uniform information density equalization is performed The third stage of equalization based on information density results in a disproportion in neuron allocation This is inspired by biological sensory maps whereby more neurons are allocated to areas with higher usage or sensitivity requirements  In this case F ASCOM allocates neurons to optimize information density of the training data across feature space Fig 3d This can be achieved by rst forming a histogram of data points and smoothing it using a Gaussian smoothing function Fig 3a This is followed by equalizing the histogram Fig 3b and mapping the equalized scale to the feature map Fig 3c With more neurons allocated to the areas with higher information concentration the output will be more responsive and precise  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 i N Data histogram Smoothed histogram a Step 1  2 Insert and smooth data 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 1 2 3 4 5 i N  Equalized histogram  b Step 3 Histogram equalization   0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1 i   i  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1 i   i  c Step 4 Mapping of equalized scale to feature map 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1 i   i  d Final neuron allocation Fig 3 Uniform information density allocation of neurons B Supervised Hebbian Learning The second stage of the learning process is a hybrid of Hebbian learning whereby the simultaneous stimulation of two connected neurons results in an increase of synaptic efìcacy between them Assuming neurons i and j are respective A k,l and C q,p nodes in ICM k and OCM q  then the change in synaptic weight  w i  j between i and j can be computed based on their ring strengths Z i and   Z j as follows  w i  j   0 if  Z i  0 and   Z j  0    w i  j  max  Z i   Z j otherwise  9 where   w i  j  max is the maximum possible absolute change in synaptic weight  w i  j is aggregated with the current w i  j to form the new synaptic weight as in w i  j  w i  j  w i  j  10   w i  j  max is determined by the mechanisms of synaptic plasticity forgetting and long-term facilitation result in the following concepts 1 The effects of both synaptic plasticity and forgetting are approximated as constants through time Through mutual cancellation the approximated combined effect is a DC component 2 The effect of long-term facilitation is initially high but decays as the memory strengthened When the memory is strong the effect of facilitation is insigniìcant 3 The initial effect of long-term facilitation is very much greater than that of the resultant effect of synaptic plasticity and forgetting A decaying sigmoid function is used to describe the effects of these three concepts deìned by   w i  j  max   k LTF 1+exp  a   w i  j  c   k LTF  k DC 11 where k LTF is the initial effect of long-term facilitation k DC is the resultant DC component from synaptic plasticity and forgetting while a and c deìne the slope of the function The process of modifying synaptic weights is performed for all training samples and may result in memories being stored to be strong like long-term memory or weak like shortterm memory C Supervised Error Reduction After Hebbian learning the supervised error reduction phase is performed to ne-tune the network The varying inîuences of an input dimension across output feature space is discovered stored as modulatory weights m k  q,p  nonneg  Table I lists the symbols used to explain the algorithm shown in Algorithm 1 Modulatory weights are rst initialized to 1  0 line 1 Temporary modulatory weights are then assigned for every training input-output pair n line 4 Next memory is recalled using the process described in Section IV line 5 and an initial error is obtained for each output node line 6 Next using only one input conjuncted map ICM k in each iteration of k  memory is again recalled line 8 and a new error for each output node is computed line 9 The difference between the new and initial errors is computed line 10 and based on this the temporary modulatory weights for the training instance n are modiìed line 11 After processing all ICM s for all training samples the modulatory weights are updated line 14 and the entire 1518 2008 International Joint Conference on Neural Networks IJCNN 2008 


process is repeated until a terminating condition is met The algorithm terminates when 1 the maximum number of epoch is reached or 2 the total change in error falls below a low threshold i.e  Q q 1  P q p 1  E C q,p  0  TABLE I L IST OF S YMBOLS U SED IN S UPERVISED E RROR R EDUCTION P HASE  Symbol Description m k  q,p  Modulatory weight between ICM k and C q,p u  n  k  q,p  Temporary value of m k  q,p for the n th training input-output pair U mod  Factor that updates temporary modulatory weights u  n  k  q,p N  No of training input-output pairs Z L3  Z 1 L3 Z  N  L3   Training input vector   Z L4    Z 1 L4    Z  N  L4   Training output vector Z  n  L3  Z  n  ICM 1 Z  n  ICM K   n th training input vector   Z  n  L4    Z  n  OCM 1    Z  n  OCM Q   n th training output vector  L4   OCM 1  OCM Q   Propagated output of layer 4  OCM q   C q 1  C q,P q   Propagated output of OCM q  C q,p  Activation level of C q,p E init C q,p  Initial squared-error for C q,p E new C q,p  New squared-error for C q,p  E C q,p  Squared-error change for C q,p begin S UPERVISED E RROR R EDUCTION A LGORITHM 1 initialize m k  q,p 1  k  K  q  Q  p  P q 2 while not stable do 3 for all training vectors n  1 N  do 4 u  n  k  q,p  m k  q,p  k  K  q  Q  p  P q 5 with all ICM  compute  6 E init C q,p   C q,p    Z  n  C q,p  2  q  Q  p  P q 7 for all ICM k  k  K do 8 with only ICM k  compute  9 E new C q,p   C q,p    Z  n  C q,p  2  q  Q  p  P q 10  E C q,p  E new C q,p E init C q,p  q  Q  p  P q 11 u  n  k  q,p  U mod  u  n  k  q,p  q  Q  p  P q 12 end for k  1 K  13 end for n  1 N  14 m k  q,p  1 N N  n 1  u  n  k  q,p   k  K  q  Q  p  P q 15 end while end S UPERVISED E RROR R EDUCTION A LGORITHM Algorithm 1 Supervised error reduction algorithm U mod as deìned in 12 determines the magnitude of change in u  n  k  q,p  If the error decreases when using only ICM k  it means that ICM k is important for output prediction and u  n  k  q,p is strengthened When the opposite occurs ICM k is insigniìcant and u  n  k  q,p should be reduced The magnitude of change in u  n  k  q,p also depends on the number of IM preceding ICM k  and the current epoch U mod  2 1+exp  d  ep   E C q,p  12 where    1 is a parameter for the initial learning rate d   1  if   0  1 1    otherwise    I 1 2  I  i 1   IM i  ICM k  and ep 1    max  2  where  and  max are current and maximum epochs IV R ECALLING P ROCESS In the recalling process Fig 4 based on the input presented to layer 1 of the encoded network input functions of layers 1 to 3 are performed Sections II-A II-B and IIC The memory recall process then occurs in the hybrid associative memory Finally the output functions of layers 4 to 6 are performed Sections II-D II-E and II-F and the estimated output is recalled Here we explain the signaling processes of neurons within the hybrid associative memory   Fig 4 The recalling process highlighted in gray In the hybrid associative memory i is a signal transmitting A k,l node from layer 3 and j is a C q,p node from layer 4 that receives inputs from N neurons in layer 3 Based on ring strength Z i  i transmits a signal deìned by a i   0 for d  min  Z i  d  min  k prop  Z i otherwise  13 where a i is the output of i  k prop  0  1 is a propagation factor to improve network stability and d  min  0  1 and d  min    1  0 are predeìned positive and negative thresholds Nodes j in layer 4 then receive these output signals and update their activation levels  j asin  j  N  i 1 v i  j  a i 14 where v i  j is the resultant weight between i and j  V D ATA P REDICTION E XPERIMENTS Four data prediction tasks were used to benchmark our architecture against other existing architectures They include three separate sets of data from Nakanishiês nonlinear estimation tasks and real-w orld dataset for predicting the density of highway trafìc 2008 International Joint Conference on Neural Networks IJCNN 2008 1519 


TABLE II B ENCHMARKING R ESULTS ON N ONLINEAR E STIMATION FOR THE N AKANISHI E STIMATION T ASKS  Architecture Example A Example B Example C MSE R MSE R MSE R FASCOM 0  181 0  929 8  170  10 3 0  999 13  930 0  953 Hebb-RR 0  185 0  911 2  423  10 4 0  998 15  138 0  947 SVM 0  258 0  876 2  423  10 5 0  993 29  510 0  925 RSPOP 0  383 0  856 2  124  10 5 0  983 24  859 0  922 DENFIS 0  411 0  805 5  240  10 4 0  995 69  824 0  810 POP-CRI 0  270 0  877 5  630  10 5 0  946 76  221 0  733 ANFIS 0  286 0  853 2  969  10 6 0  780 38  062 0  875 EFuNN 0  566 0  720 7  247  10 5 0  946 72  541 0  756 A Nakanishiês Nonlinear Estimation Tasks The Nakanishiês dataset consists of three e xamples of real-world nonlinear estimation tasks The tasks for data prediction are namely 1 a nonlinear system 2 the human operation of a chemical plant and 3 the daily stock price of a stock in a stock market Based on these three experiments FASCOMês performance based on data prediction accuracy was benchmarked against Hebb-RR SVM 26 RSPOP 8 DENFIS 28 POP-CRI ANFIS 30 31 and EFuNN 32 T w o performance measures are used for this evaluation namely the mean squared error MSE and the Pearson productmoment correlation coefìcient R 1 Example A Nonlinear System FASCOM was used to model a nonlinear system given by y 1 x  2 1  x 1  5 2  2 1  x 1 x 2  5 15 The dataset consists of four input variables  x 1  x 2  x 3  x 4  and one output variable  y  whereby only x 1  x 2 are useful and x 3  x 4 are irrelevant The mean of modulatory weights discovered was computed across the output feature space It was found that the input conjuncted maps ICM s representing x 3  x 4 and x 3 012 x 4 exerted no inîuence on the outcome of y and could be discarded This is similar to the results obtained by 24  F o r this e xample task F A SCOM w a s most accurate when compared to other benchmarked architectures 2 Example B Chemical Plant Operation This example involves the human operation of a chemical plant whereby ve inputs representing monomer concentration  x 1  charge of monomer concentration  x 2  monomer ow rate  x 3  and local temperatures inside the plant  x 4  x 5  are used to estimate the set point for monomer ow rate  y  Similar to F ASCOM discarded x 2  x 4 and x 5 As a comparison discarded x 1  x 2 and x 5  while discarded all but x 3  Also we deduce that the set point for monomer ow rate  y  depends mainly on the combination of monomer ow rate  x 3  and monomer concentration  x 1  i.e x 1 012 x 3  For this example with a MSE of 8  170  10 3 and near perfect R value FASCOMês performance was signiìcantly better than the state-of-the-art 3 Example C Stock Price Forecasting The prediction of the price of a stock y for this example is performed with the ten inputs which are the past and present moving averages over a middle period  x 1  x 2  past and present separation ratios with respect to moving average over both short and middle periods  x 3  x 4  x 8  x 10  present change of moving average over both short and long periods  x 5  x 9  and past and present price changes  x 6  x 7  The supervised error reduction algorithm signiìcantly reduced inputs corresponding to x 1  x 2  x 3  x 5  x 6  x 8 and x 9  discarded inputs x 1  x 2  x 3  x 6  x 7  x 9 and x 10  discarded x 1  x 2  x 3  x 6 and x 10  and discarded only x 2 and x 5  Interestingly all methods did not discard x 4  which means the present separation ratio with respect to moving average over a middle period  x 4  is a critical component to predict stock prices Similar to the previous two examples FASCOM outperformed all other benchmarked architectures 4 Discussion The three tasks were also used to analyze the effects of three different experimental initializations of the learning process 1 using all three phases in the learning process 2 omitting membership function initialization phase 1 3 omitting error reduction phase 3 For each task each initializationês MSE was computed and normalized with respect to the highest MSE amongst the three initializations For all three tasks the comparison between the three initializations indicates that the inclusion of both phases 1 and 3 produced a MSE that is signiìcantly lower than when either one was omitted Fig 5 This shows that both phases 1 and 3 are crucial in improving the accuracy of data prediction and their existence within the learning process is justiìed Task A Task B Task C 0 0.2 0.4 0.6 0.8 1 Normalized M S E With phase 1, 2 & 3 Omit phase 1 Omit phase 3 Fig 5 Comparison between the three different experimental initializations 1520 2008 International Joint Conference on Neural Networks IJCNN 2008 


B Highway Trafìc Density Prediction The raw trafìc data 33 w a s collected for three straight lanes and two exit lanes at site 29 located at exit 5 along the east bound direction of the Pan Island Expressway PIE in Singapore using loop detectors embedded beneath the road surface Fig 6 Data spanning a period of six days from September 5 to 10 1996 for the three straight lanes i.e lanes 1 to 3 were considered for this experiment The dataset has four input attributes representing the normalized time and the trafìc density of each of the three lanes           Fig 6 Photograph of site 29 where the trafìc data was collected The purpose of this experiment is to model the trafìc ow trend and subsequently produce predictions of the trafìc density of each lane at time t    where  5  15  30  45  60 min is the prediction time interval Three cross-validation groups CV1 CV2 and CV3 of training and test sets were used for evaluation purposes The mean squared error MSE and the Pearson product-moment correlation coefìcient R were computed for each predictions run From the example in Fig 7 we observe that the prediction accuracy decreases as the time interval  increases To evaluate the accuracy of prediction the Avg MSE indicator was computed by taking the average MSE across all 45 prediction runs 3 lanes 5 time intervals and 3 crossvalidation groups Also the Var indicator reîecting the consistency of predictions over different time intervals across the three lanes was computed by taking the change in the mean of R from  5 min to  60 min expressed as a percentage of the former This was then averaged across all three lanes to produce Avg Var The results of trafìc density prediction was compared to Hebb-RR SVM 26 RSPOP 8 POP-CRI 29 DENFIS GeSoFNN 34 and EFuNN 32 F ASCOM signiìcantly outperformed all other architectures based on the results as shown in Fig 8 with the best combination of Avg MSE  0  098  and Avg Var  19  0  as compared to other architectures The results indicates that the output prediction by FASCOM are both highly accurate and consistent over different time intervals This is desirable 0 100 200 300 400 500 600 700 800 0 1 2 3 4 5  Time instance Normalized density Actual  Predicted  R=0.904   MSE=0.082 a Prediction at  5 min 0 100 200 300 400 500 600 700 800 0 1 2 3 4 5  Time instance Normalized density Actual  Predicted  R=0.886   MSE=0.099 b Prediction at  15 min 0 100 200 300 400 500 600 700 800 0 1 2 3 4 5  Time instance Normalized density Actual  Predicted  R=0.873   MSE=0.108 c Prediction at  30 min 0 100 200 300 400 500 600 700 800 0 1 2 3 4 5  Time instance Normalized density Actual  Predicted  R=0.839   MSE=0.135 d Prediction at  45 min 0 100 200 300 400 500 600 700 800 0 1 2 3 4 5  Time instance Normalized density Actual  Predicted  R=0.816   MSE=0.152 e Prediction at  60 min Fig 7 Trafìc density prediction of lane 1 using CV1 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0.22 15 20 25 30 35 40 Avg MSE Avg Var  0  143  19  1 RSPOP POP CRI 0  189  33  2 EFuNN FA SCO M 0  098  19  0 Hebb RR 0  114  29  6 0  150  33  2 DENFIS SVM 0  120  39  6 GenSoFNN 0  164  17  4 0  167  21  9 Fig 8 FASCOM outperforms other benchmarked architectures for highway trafìc density prediction 2008 International Joint Conference on Neural Networks IJCNN 2008 1521 


VI C ONCLUSIONS This paper proposed the Fuzzy Associative Conjuncted Maps FASCOM fuzzy neural network which represents information using conjuncted fuzzy sets and learns associations between them through three consecutive unsupervised and supervised phases in the learning process During the rst phase of unsupervised membership function initialization neurons are allocated to optimize information density For the supervised learning phase the single pass Hebbian learning performed is based on the memory mechanisms of synaptic plasticity forgetting and long-term facilitation Finally the nal supervised phase involves ne-tuning the network using the supervised error reduction algorithm which uses an error comparison to determine the inîuence of an input dimension across output space In the series of experiments performed we showed that each of the phases contributed to the overall performance of the architecture We were also able to demonstrate FASCOMês effectiveness in performing nonlinear data prediction on a variety of real-world problems of different natures and consisting of noisy data such as nonlinear system modeling chemical plant operation stock price forecasting and trafìc density prediction For every experiment FASCOM produced the best data prediction accuracy when benchmarked against existing architectures R EFERENCES  L A Zadeh Fuzzy logic neural netw orks and soft computing  Communications of ACM  vol 37 no 3 pp 77-84 1994  E H Mamdani and S Assilian  A n e xperiment in linguistic synthesis with a fuzzy logic controller Intl Jnl of Man-Machine Studies  vol 7 no 1 pp 1-13 1975  T  T akagi and M Sugeno Deri v ation of fuzzy control rules from human operatorês control actions Proc IFAC Symp Fuzzy Information Knowledge Representation and Decision Analysis  pp 55-60 1983  T  T akagi and M Sugeno Fuzzy identiìcation of systems and its applications to modelling and control IEEE Trans Syst Man Cybern  vol 15 no 1 pp 116-132 1985  M Sugeno and G T  Kang Structure identiìcation of fuzzy model  Fuzzy Sets and Systems  vol 28 pp 15-33 1988  S Guillaume Designing fuzzy inference systems from data An interpretability-oriented review IEEE Trans Fuzzy Syst  vol 9 no 3 pp 426-443 2001  J Casillas O Cordn F  Herrera and L Magdalena Interpretability issues in fuzzy modeling  Berlin Springer-Verlag 2003  K K Ang and C Quek RSPOP Rough set-based pseudo outer product fuzzy rule identiìcation algorithm Neural Computation  vol 17 pp 205-243 2005  C T  Lin and C S Lee Neural-netw ork-based fuzzy logic control and decision system IEEE Trans on Computers  vol 40 no 12 pp 13201336 1991  I Hayashi H Nomura H Y amasaki and N W akami Construction of fuzzy inference rules by NDF and NDFL Intl Jnl of Approximate Reasoning  vol 6 no 2 pp 241-266 1992  R R Y a ger  Modeling and formulating fuzzy kno wledge bases using neural networks Neural Networks  vol 7 no 8 pp 1273-1283 1994  C Quek and W  L T ung  A no v e l approach to the deri v ation of fuzzy membership functions using the Falcon-MART architecture Pattern Recognition Letters  vol 22 no 9 pp 941-958 2001  M Lee S Y  Lee and C H P ark  A ne w neuro-fuzzy identiìcation model of nonlinear dynamic systems Intl Jnl of Approximate Reasoning  vol 10 pp 30-44 1994  H Ishib uchi H T anaka and H Okada Interpolation of fuzzy if-then rules by neural networks Intl Jnl of Approximate Reasoning  vol 10 pp 3-27 1994  E R Kandel J H Schw artz and T  M Jessell Principles of neural science  4th ed New York McGraw-Hill Health Professions Division 2000  D O Hebb The organization of behavior a neuropsychological theory  New York Wiley 1949  N V  Swindale Ho w dif ferent feature spaces may be represented in cortical maps Network Computation in Neural Systems  vol 15 pp 217-242 2004  J Ro v amo V  V irsu and R Nsnen Cortical magniìcation f actors predicts the photopic contrast sensitivity of peripheral vision Nature  vol 271 pp 54-6 1978  S W  K u f er  J  G  Nicholls and A R Martin From neuron to brain a cellular approach to the function of the nervous system  2nd ed Sunderland Mass Sinauer Associates 1984  N Sug a Cortical computation maps for auditory imaging  Neural Networks  vol 3 pp 3-21 1990  P  Azzopardi and A Co we y  The o v errepresentation of the fo v e a and adjacent retina in the striate cortex and dorsal lateral geniculate nucleus of the macaque monkey Neuroscience  vol 72 pp 627-639 1996  A Das Plasticity in adult sensory corte x a r e vie w   Network Computation in Neural Systems  pp R33-R76 1997  M D Plumble y  Do cortical maps adapt to optimize information density Network Computation in Neural Systems  vol 10 pp 4158 1999  H Nakanishi I B T u rksen and M Sugeno  A re vie w and comparison of six reasoning methods Fuzzy Sets and Systems  vol 57 no 3 pp 257-294 1993  G K T an Feasibility of predicting congestion states with neural networks Technical Report School of Civil and Environmental Engineering Nanyang Technological University Singapore 1997  V  V apnik The Nature of Statistical Learning Theory  Springer-Verlag 1995  F  Liu C Quek and G S Ng  A no v e l generic Hebbian orderingbased fuzzy rule base reduction approach to mamdani neuro-fuzzy system Neural Computation  vol 19 pp 1656-1680 2007  N Kasabo v and Q Song DENFIS Dynamic e v olving neural-fuzzy inference system and its application for time-series prediction IEEE Trans Fuzzy Syst  vol 10 no 2 pp 144-154 2002  K K Ang C Quek and M P asquier  POPFNN-CRI\(S Pseudo outer product-based fuzzy neural network using the compositional rule of inference and singleton fuzziìer IEEE Trans Syst Man Cybern B Cybern  vol 33 no 6 pp 838-849 2003  J S Jang  ANFIS Adapti v e-netw ork-based fuzzy inference system  IEEE Trans Syst Man and Cybern  vol 23 no 3 pp 665-685 1993  S L Chiu Fuzzy model identiìcation based on cluster estimation  Journal of Intelligent and Fuzzy Systems  vol 2 no 3 pp 267-278 1994  N Kasabo v  Ev olving fuzzy neural netw orks for supervised  unsupervised online IEEE Trans Syst Man and Cybern B Cybern  vol 31 no 6 pp 902-918 2001  C Quek M P asquier  and B Lim POP-TRAFFIC A N o v el Fuzzy Neural Approach to Road Trafìc Analysis and Prediction IEEE Trans Intell Transp Syst  vol 7 no 2 pp 133-146 2006  W  L T ung and C Quek GenSoFNN a generic self-or ganizing fuzzy neural network IEEE Trans Neural Netw  vol 13 no 5 pp 10751086 2002 1522 2008 International Joint Conference on Neural Networks IJCNN 2008 


If in Fig. 4 there is at least one T, then DCAR6 holds DCAR1 through DCAR6 forms a complement lattice shown in Fig. 5 In Fig. 5, the lower rule implies the upper rule That is, if DCARj is reachable from DCARi via an ascending path, and DCARi holds, then DCARj holds Because DCAR1 through DCAR6 satisfies Fig 5, their algorithms can be merged into one algorithm called connective determination algorithm, shown in Fig. 6 Suppose cf 1 80%, cf 2 75%. In Fig. 4, for the column of C1, there are M*cf 1 5*80%=4 elements whose values are T \(namely, S1, S2, S3, S5 Therefore, DCAR2: course\(Cno 004 1  student\(Sno 003 1  study\(Sno, Cno\olds. From Fig. 5, we know that DCAR3 and DCAR6 also hold. In Fig. 4, there are at least N*cf 2 4*75%=3 columns which have value T \(namely, in the column of C1 there is S1, in the column of C2 there is S1, in the column of C3 there is S2, in the column of C4 there is S5 therefore DCAR5: course\(Cno 003 1  student\(Sno 004 1  study\(Sno, Cno  VI. CONCLUDING REMARKS 1\ Double-connective association rule mining is different from single-connective association rule mining. The former mines the association among the primary keys of the two entity tables and the primary key of the binary relationship table. The latter mines the association between frequent item sets 2\. 4 is different from data cubes in data warehouses. The elements in Fig. 4 are T or F. The elements in the data cubes are data 3\The differences between double-connective association rule and database query are that, first, the query information in databases are predeterminate while the information to be mined by double-connective association rule is not predeterminate, it is implied. Secondly, database query needs to write SQL statements, while double-connective association rule mining is automatic. Thirdly, the information obtained by database query is quantitative, while the information obtained by double-connective association rule mining is qualitative such as ìfor manyî, ìthere are some  REFERENCES 1 Ji a w ei H a n   M i ch eli n e K a m b er   D a t a  M i n i n g C onc ep t s  a nd Techniques, Higher Education Press, Beijing, 2001, Morgan Kaufmann Publishers, 2000 2 A  G  Ha m i lt on  L o gi c for M a th em a t i c ia ns R evi s ed E d i t i o n   Cambridge University Press, 1988, Tsinghua University Press Beijing, 2003 3 X unw e i Z h o u   Br ie f I ntr o du c t io n  to  Mu t u al l y I nve r s is tic Logicî, 1999 European Summer Meeting of the Association for Symbolic Logic, Utrecht, The Netherlands, August 1-6 1999 4 u n w ei Zh ou F i r s t leve l exp l i c i t m u lt ip le i ndu ct i v e compositionî, 2005 Spring Meeting of the Association for Symbolic Logic, The Westin St. Francis Hotel, San Francisco CA. USA, March 25-26, 2005 5 A b rah a m S i lb ers c ha t z  Hen r y  F  Kort h  S S u da rs ha n Dat a b a s e  System Concepts \(Fourth Edition\, Higher Education Press Beijing, 2002, McGraw-Hill Companies, 2002  
279 
279 


support pruning gene expression classifier with an accurate and compact fuzzy rule base for microarray data analysis Biosystems vol 85 computationally challenging As training set sizes increase it is likely that these difficulties will also increase VI RELATED WORK While operating on a microarray dataset current CAR 1 2 3 4 and other pattern/rule 20 21 mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets Hence they are generally quite computationally expensive for datasets containing many training samples or genes as the case may be BSTC is explicitly related to CAR-based classifiers but requires no expensive CAR mining BSTC is also related to decision tree-based classifiers such as random forest 19 and C4.5 family 9 methods It is possible to represent any consistent set of boolean association rules as a decision tree and vice versa However it is generally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data 1 2 6 7 11 BSTC is explicitly related to and motivated by CAR-based methods To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here Perhaps the work closest to utilizing 100 BARs is the TOPRULES 22 miner TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci Hence TOP-RULES discovers all 100 confident CARs in a dataset However the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER 23 and so generally isn't polynomial time Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules 24 25 Once obtained fuzzy rules can be used for classification in a manner analogous to CARs However the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM 25 VII CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with preclassification CAR mining see Tables IV and VI we developed a novel method which considers a larger subset of CAR-related boolean association rules BARs These rules can be compactly captured in a Boolean Structure Table BST which can then be used to produce a BST classifier called BSTC Comparison to the current leading CAR classifier RCBT on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining see Section VB Hence BSTC extends generalized CAR based methods to larger datasets then previously practical Furthermore unlike other association rule-based classifiers BSTC easily generalizes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse then CAR-based methods after all exponential time CAR mining is completed O SlS CGl versus O Si CGi As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists ACKNOWLEDGM[ENTS We thank Anthony K.H Tung and Xin Xu for sending us their discretized microarray data files and Top-k/RCBT executables This research was supported in part by NSF grant DMS-0510203 NIH grant I-U54-DA021519-OlAf and by the Michigan Technology Tri-Corridor grant GR687 Any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies REFERENCES 1 G Cong K L Tan A K H Tung and X Xu Mining top-k covering rule Mining SDM 2002 5 R Agrawal T Imielinski and A Swami Mining associations between sets of items Y Ma Integrating classification and association rule mining KDD 1998 11 T McIntosh and S Chawla On discovery of maximal confident rules without pp 43-52 1999 24 S Vinterbo E Kim and L Ohno-Machado Small fuzzy and interpretable pp 165-176 2006 1071 pp 207-216 1993 6 G Dong pp 273-297 t995 9 pp 5-32 2001 20 W Li J R Quinlan Bagging boosting and c4.5 AAAI vol 1 V Vapnik Support-vector networks the best strategies for mining frequent closed itemsets KDD 2003 4 M Zaki and C Hsiao Charm L Wong Identifying good diagnostic genes or gene expression data SIGMOD 2005 2 G Cong A K H Tung X Xu F Pan and J Yang Farmer Finding interesting rule gene expression data by using the gene expression based classifiers BioiiiJcrmatics vol 21 l and Inrelligent Systenis IFIS 1993 16 Available at http://sdmc.i2r.a-star.edu.sg/rp 17 The dprep package http:/cran r-project org/doclpackages dprep pdfI 18 C Chang and C Lin Libsvm a library for support vector machines 2007 Online Available www.csie.ntu.edu.tw cjlin/papers/libsvm.pdf 19 L Breiimnan Random forests Maclh Learn vol 45 no 1 M Chen and H L Huang Interpretable X Zhang 7 J Li and pp 725-734 2002 8 C Cortes and Mac hine Learming vol 20 no 3 in microarray data SIGKDD Worikshop on Dtra Mining in Bioinfrrnatics BIOKDD 2005 12 R Agrawal and R Srikant Fast algorithms for mining association rules VLDB pp 1964-1970 2005 25 L Wong and J Li Caep Classification by aggregating emerging patterns Proc 2nd Iat Coif Discovery Scieice DS 1999 gene groups from pp 487-499 t994 13 Available ot http://www-personal umich edu/o markiwen 14 R Motwani and P Raghavan Randomized Algoriitlms Caim-bridge University Press 1995 15 S Sudarsky Fuzzy satisfiability Intl Conf on Industrial Fuzzy Contri J Han and J Pei Cmar Accurate and efficient classification based on multiple class-association rules ICDM 2001 21 F Rioult J F Boulicaut B Cremilleux and J Besson Using groups for groups in microarray datasets SIGMOD 2004 3 concept of emerging patterns BioinformJotics vol 18 transposition for pattern discovery from microarray data DMKD pp 73-79 2003 22 J Li X Zhang G Dong K Ramamohanarao and Q Sun Efficient mining of high confidence association rules without S Y Ho C H Hsieh H pp 725-730 1996 10 B Liu W Hsu and support thresholds Principles f Drata Mining aind Knowledge Discovery PKDD pp 406 411 1999 23 G Dong and J Li Efficient mining of emerging patterns discovering trends and differences KDD J Wang J Han and J Pei Closet Searching for An efficient algorithm for closed association rule mining Proc oJ the 2nd SIAM Int Con on Data in large databases SIGMOD 


