Polarity Identiìcation of Sentiment Words based on Emoticons Shuigui Huang 
 Wenwen Han  Xirong Que and Wendong Wang State Key Laboratory of Networking and Switching Technology Beijing University of Posts and Telecommunications hsguidemail@gmail.com hww860810@gmail.com rongqx@bupt.edu.cn wdwang@bupt.edu.cn 
Abstract 
002 002 002 002 002 
The orientation of sentiment words plays an important role in the sentiment analysis but existing methods have difìculty in classifying the orientation of Chinese words especially for the newly emerged words in Internet Most approaches are mining the association between sentiment words and seed 
words using the big corpora and manually labeled seed words with deìnite orientation But less work has ever focused on the efìcient seed words selection As we observed emoticons which are widely used on social network because of the simplicity and visualization are good indicators for sentiment orientation Thus this paper proposes the sentiment word model based on emoticons which built orientation model of sentiment words with the orientation of emoticons and train the model with the SVM classiìer Meanwhile this work proposes a high efìcient way to automatically classify the orientation of emoticons Experiments show the precision rate of emoticon classiìcation could reach 93.6 and that of sentiment words classiìcation could be 81.5 
Keywords 
sentiment analysis emoticon sentiment words emoticon based model similarity SVM 
I I NTRODUCTION Sentiment is used to analysis the users opinions expressed in positive or negative comments It has become one of the major research topics in the subjective information processing Using sentiment analysis users opinions could be classiìed as having positive orientation or negative  3 which beneìts to companies and go v ernment to know peopleês emotion The fact that sentiment words are strong predictors of subjecti mak es them be the important role to sentiment analysis Sentiment words are words that convey positive or negative sentiment orientation which could be adjective adverb and idiom etc The w 3 cate gory sentiment words as having positive orientation or negative orientation 
There are two main methods to classify the orientation of sentiment words one is based on 6 7 and the other is based on 8 9 The one based on corpora is to compare the association of sentiment words with positive seed words and negative seed words concretely the sentiment words are positive if the sentiment words are more associated with positive seed words otherwise negative But this method has a problem it didnêt tell us the selection of seed words Whereas seed words selection is not only inevitable but also a difìcult task in sentiment classiìcation because different seed words usually yield results of diverse quality The other based on dictionary is to use the synonymous and antonyms of seed sentiment words with deìnite orientation to 
predict the polarity of sentiment words Particularly the synonymous of positive seed words and the antonyms of negative seed words have positive orientation while the synonymous of negative seed words and the antonyms of positive seed words have negative orientation However this method could not be used to classify the newly emerged words which didnêt exist in the dictionary The drawbacks encourage us to seek polarity assignment method which could deal with both the seed words selection problem and new merged words problem Emoticons are widely used in the social network Weibo for instance to express users personal emotion which are a set of dynamic images and are portrayed like peopleês expression For example  Suzhou has an explosion 
 Obviously emoticons can convey users positive and negative sentiment clearly and visually and the orientation is usually deìnite which make them suitable to serve as annotated seed opinion words The w also points out that emoticons can convey strong sentiment They help the users to express their mood when post statuses The w uses the orientation of emoticons to represent the orientation of statuses For example   is considered as a positive status because   is a positive emoticon while 
Life is wonderful 
 
 is considered as a negative status because   is a negative emoticon As we observed sentiment words appearing with emoticons have nearly the same orientation of emoticons For example above Wonderful and   are both positive and terrible and   are both negative The ndings inspire us to use emoticons in guiding the recognition of polarities Our paper is based on the intuition that 
Fatso is terrible 
 The differences between our work and the previous work are 1 use the emoticons to build the model for sentiment words 2 nd an effective method to compute the orientation of emoticons As indicated above our approach is performed in three steps 1 Classify the orientation of emotions into positive and negative classes 2 Construct a dimension-reduced vector space model for emoticon-based word sentiment classiìcation and principle component extraction is used to reduce dimensions 3 Train the classiìer with SVM algorithm and optimized the parameters via 10-fold cross validation 
the orientation of sentiment words share the same orientation of emoticons when they co-occur 
2013 Ninth International Conference on Computational Intelligence and Security 978-1-4799-2548-3/13 $31.00 © 2013 IEEE DOI 10.1109/CIS.2013.35 134 
2013 Ninth International Conference on Computational Intelligence and Security 978-1-4799-2548-3/13 $31.00 © 2013 IEEE DOI 10.1109/CIS.2013.35 134 
2013 Ninth International Conference on Computational Intelligence and Security 978-1-4799-2548-3/13 $31.00 © 2013 IEEE DOI 10.1109/CIS.2013.35 134 
2013 Ninth International Conference on Computational Intelligence and Security 978-1-4799-2548-3/13 $31.00 © 2013 IEEE DOI 10.1109/CIS.2013.35 134 
2013 Ninth International Conference on Computational Intelligence and Security 978-1-4799-2548-3/13 $31.00 © 2013 IEEE DOI 10.1109/CIS.2013.35 134 
2013 Ninth International Conference on Computational Intelligence and Security 978-1-4799-2548-3/13 $31.00 © 2013 IEEE DOI 10.1109/CIS.2013.35 134 


       002 
ne ne i i i j i j i i i ne i i 
1 2 3 1 2 3 1 2 
V V 
e e 
and or but and or A Classifying the Orientation of Emoticons Deìnition 3.1 Deìnition 3.2 Deìnition 3.3 Deìnition 3.4 Deìnition 3.5 Manually tagging Automatically annotation 
EOC ne EOC EOC e e e  e e e e  e n e e i   ne n e e e e i j   ne n e e n e EOC e e  e EOC ind af f e e 
1 where af f  e i  002 1 e i is positive  1 e i is negative e i 002 EOC 2 This paper uses a method of both manually tagging and automatically annotation to classify the orientation of emoticons 1 
II R ELATED W ORK Our work is related to orientation identiìcation for sentiment words The existing work for orientation classiìcation of sentiment words could be categorized into two approaches corpora-based approaches and dictionary-based approaches Our work falls into the category of corpora-based approaches The w predicts orientation of sentiment w ords by detecting pairs of such words conjoined by conjunctions such as       in a large document set The underlying intuition is that the orientation of conjoined sentiment words are subject to some linguistic constraints Two sentiment words are usually the same orientation when they are conjoined by   while the two sentiment words are the opposite orientation when they are conjoined by   The weakness of this method is that it relies on the conjunction relations and needs a large amount of manually tagged training data In the w the orientation of a sentiment w ord is calculated as the point mutual information between the given word and the set of manually labelled positive words minus the point mutual information between the given word and the set of manually labelled negative words The mutual information is estimated by issuing queries to a search engine and noting the number of hits If the result is a positive value then the sentiment word has the positive orientation If the result is a negative value then the sentiment word has the negative orientation The underlying intuition is that the positive word is more likely to associate with positive words otherwise the negative word is more likely to associate with negative words The weakness of this method is that it is hard to select the proper positive and negative seed words in Chinese And also their work requires additional access to the Web Search Engine which is time consuming The w 9 tak es adv antage of W ordNet to predict the orientations of sentiment words The underlying intuition is that in WordNet the synonymous words have the same orientation while the antonymous words have the opposite orientations In the w a w ords netw ork is constructed by connecting pairs of synonymous words in WordNet The orientation of a word is decided by its shortest path to the positive sentiment seed words and the negative sentiment seed words The w determines the orientation of w ords based on glosses in an online dictionary The classiìer is trained on glosses of an unknown word to categorize the sentiment words as positive or negative The weakness of this method is that for dictionary-based methods they are unable to classify the orientation of sentiment words that are not in the dictionary for instance the newly cyberwords III E MOTICON BASED S ENTIMENT W ORD M ODEL Fig 1 gives the framework to classify the orientation of sentiment words Recall that the task of classifying sentiment words orientation is telling whether the orientation of sentiment words is positive or negative this paper performs this task in three main steps 1 classifying the orientation of emoticons 2 constructing the vector space model for sentiment words 3 training the model with the SVM Fig 1 Proposed Words Classiìcation Approach by Using Emoticons To use emoticons in guiding the word sentiment classiìcation above all we need to obtain a set of annotated emoticons with known semantic orientation In this section this preprocess work is done with small proportion of manually labeling and large part of automatic annotation Before handling the classiìcation of emoticons we deìne some symbols rst represents the set of emoticons contained in the corpora and is the size of the set Then  where represents the emoticon represents the count of the emoticon occurring in the corpora  represents the count of cooccurrence between emoticon and in the corpora  Especially represents the co-occurrence vector of the emoticon e with each emoticon in  then is represents the orientation of emoticon  That is ind  The emoticons whose orientation are obvious are easy to label so we can label them manually For the emoticons whose orientation are not that obvious the automatically annotation is used We tag positive emoticons with 1 and negative ones with 1 resulting in 116 emoticons 2  For the emoticons whose orientation are not obvious to label we use an unsupervised learning method to label them We automatically annotated 524 emoticons in this phase 
   1 2    1 2       
V e  n  e e 1  n  e e 2    n  e e ne  
135 
135 
135 
135 
135 
135 


        1 2   1 2       
pe PE ne NE ne j j j sword 1 sword nw T k i 1 002 i,i ne j 1 002 j,j k k 
003 003 004 005 V 006 007 010 010 
             V e 1   V e 1              U   A 
1 2 1 2 1 1 1  1 2  2 
ind af f e S e pe ind af f pe S e ne ind af f ne S e 1 e 2 cos  V e 2 f sword e 1  f sword e f sword e n sword e ind af f e r 
PE NE ind af f e e S e e e e EOC EOC p EOC n n sword e j sword e j n sword e j j    ne sword j th e j f sword e j j    ne f sword e j SW SW n SW SW sword   sword nw  sword   sword nw SW nw  nw nw  ne ne  ne k k<ne k r r k r k k nw 003 k r k ne 
002 002 
3 4 Experiments in Section IV show that this method can classify the orientation of emoticons with high accuracy With this method we can classify the set 5 And 6 In this model two problems are solved selecting the features and extracting V sword    V 7 Secondly SVD is applied to A 8 where U 9 What 10 B k 
The intuition of automatically annotation of the emoticon orientation is if different emoticons appear in the same status it is the usual case that the emoticons express the same orientation because the attitude of writers within a short message is usually stable Suppose the set of emoticons that manually tagged as positive is  and the set of negative emoticons is  The orientation estimation measure of an unlabelled emoticon could be formulated as 3 calculates the orientation difference between and seed emoticons means the similarity of two emoticons and  as formulated in 4 which measures the weight of each seed emoticons in predicting the orientation of a new emoticon Here V e follows the deìnition in 1 into positive emoticons and negative emoticons  which will be used to build sentiment word model B Construct the Model for Sentiment Words After classifying the orientation of emoticons we describe the task of constructing the model for sentiment words We propose an emoticon-based sentiment word model ECWM In this model the orientation of sentiment word is represented by the co-occurrence information of sentiment words with emoticons and the orientation of emoticons We describe some deìnitions rst Deìnition 3.6 means the sentiment word and the emoticon are co-occurred in statuses  Deìnition 3.7 V sword represents the feature vector of the sentiment word  The vector component of V sword is the association between the sentiment word and the emoticon  whose value is   then V sword is computed as 6 s principal components 1 Selecting the Features We use emoticons as features because their clear and deìnite orientation make them more appropriate to serve as features than ordinary words Moreover its broad usage on the Internet could contribute largely to the coverage of messages Hence emoticons are used to construct VSM model of sentiment words 2 Extracting the Principal Components There are hundreds of thousands of emoticons in a large corpora which makes the vector V sword a high dimensional vector Hence there is a need to extract the principal components In fact some emoticons appear rarely while some are redundant which increases the computation workload and decrease the accuracy of the model Singular value decomposition\(SVD is used to extract the principal components of the The method can be depicted as the following four steps Firstly suppose the set of sentiment words is  the size of is  that is  and represents the sentiment word The feature matrix is constructed for the set  which we called A  A  to decompose A into a product of three matrices as follows A is column orthogonal form 002 is a rectangular diagonal matrix with nonnegative real numbers on the diagonal and V is column orthogonal form The diagonal entry 002 i,i of 002 is the singular value of matrix A  Thirdly let 002 k be the diagonal matrix formed from the top singular values where  and the top singular values are 002  002    002 k,k  Then we deìne to be means is the rate of information that the top singular values contain We make equal to 99 to let the top singular values depict 99 information of the original matrix Finally when selecting the top singular values to form the diagonal matrix 002 k  and let U k and V k be the matrices produced by selecting the corresponding columns from U and V  then extracting the k principal components from original matrix could be done by following B is a matrix and the information rate it contains is  Remember the feature num is  which is much smaller than that of original feature num  C Train the Vector Space Model of Sentiment Words The last step is to train the model using the machine learning method In this paper SVM is chosen which is widely used in machine learning and demonstrated effective We also chose the radial basis function\(RBF kernel as the kernel function For training the model is used LibSVM is an efìcient SVM implements package for Java language There are two parameters we should set manually and 10-fold cross validation is used to nd the best parameters 
   V e 2  V e 1    V e 2     002  V  V 
136 
136 
136 
136 
136 
136 


010 003 p 002 Pw 003 n 002 Nw 
            
p n p k n k rest k k p i k test test correct w correct test 
i i 
1 1 2 1 2 
 100    
A Build the Corpora B The Accuracy of Orientation Classiìcation of Emoticons angry disgusting joyful sadness angry disgusting sadness joyful C Experiment of Classifying Sentiment Words Orientation Filter the noun and verb Filter the words with weak sentiment strength and ambiguous meanings Filter the words that are not in the corpora 1 PMI+W 
ET ET accuracy ET ET ET p f avg f p T T T f T T Pw Nw A word word word word 
num of emoticons classiìed correctly num of total tagged emoticons 11 To avoid the uneven distribution of emoticons in the corpora we use different training emoticons and testing emoticons to make multiple trials and synthesis the average accuracy rate of these trials as the nal accuracy rate We randomly select k positive emoticons 13 
IV E XPERIMENTS AND D ISCUSSION The Weibo statuses were crawled by Xiaoxin crawler There are totally 12,543,164 statuses from date time August the 18th 2009 to December the 27th 2011 To make sure the quality of statuses improper statuses are ltered out including emoticonfree statuses and too short statues\(less than 4 words resulting in 3,921,794 statuses and 640 different kinds of emoticons In addition we indexed all ltered statuses to improve efìciency of the approach using Lucene 4.0\(http://lucene.apache.org To validate accuracy of orientation classiìcation of emoticons the manually labeled emoticons set is used which contains 121 emoticons In the set emoticons are classiìed into four different sentiment categories including       and   The sentiment categories in our paper are positive and negative So the mapping is built       are classiìed as negative while   is classiìed as positive After reclassifying positive emoticons set contains 58 emoticons and negative emoticons set contains 63 emoticons The method of validating for 3 is following The is used to measure the performance of the algorithm which is deìned as 11 and k negative emoticons as the training set with remaining emoticons as the testing set Here we run times for each separate k Deìning the accuracy rate of each trial for each value k as just as 11 says and we use the average accuracy rate to measure the p trials which is calculated as 12 12 Fig.2 shows the average accuracy of the algorithm To measure the average accuracy we use different number of seed emoticons Here k is in the range 5 with step 5 From Fig.2 we can see that the average accuracy is high When we use 5 seed emoticons the average accuracy rate is 86.2 and the average accuracy rate can reach to 90.9 when we use 10 seed emoticons Moreover when we use more seed emoticons the average accuracy is enlarged consequently To validate the accuracy of sentiment words orientation classiìcation the labeled sentiment words from the Dalian University of Technology Information Retrieval laboratory are used\(http://ir.dlut.edu.cn There are 27,466 sentiment words in this ontology These sentiment words are Fig 2 The Average Accuracy with the Number of Seed Emoticons tagged with three features the sentiment strength orientation and part-of-speech But not all the words are proper for the experiments We lter the ontology to get 1500 words The lter rules are 1  Intuitively most sentiment words are adjective adverb and prepositional phrase While lots of the noun and verb sentiment words in the ontology are not exactly sentiment words So we lter them out 2  Sentiment words with deìnitely orientation are considered to be more proper than those weak sentiment ones So we just keep the sentiment words with sentiment strength value equal to or larger than 5 3  Most words in the ontology are too ofìcial to use in the Internet language So we only keep the words that are in our crawled corpora Suppose is the set of 1500 sentiment words All the sentiment words in are labeled as either positive or negative Suppose the correctly classiìed words set is  then the accuracy of this approach is  To validate the effectiveness of the algorithm two baselines are used is short for PMI with word seeds This approach is an effective approach to classify the orientation of sentiment words which uses the point mutual information to compute the association of two sentiment words It rst tags two seed sets one set is which contains the positive seed sentiment words and the other set is which contains the negative seed sentiment words The orientation of a given word is calculated from the strength of its association with the positive seed words minus the strength of its association with the negative seed words just as 13 shows is the measure of association between and  which is the point mutual information of two words In 7 positi v e w ords and 7 ne g ati v e w ords are selected as the seed words But the author didnêt tell how to select the 14 seed words The rules we selected are 1 seed words should appear frequently in the corpora 2 seed words should have deìnitely orientation The selected seed words are listing as Table I 
accuracy PMI sword A sword p A sword n 
137 
137 
137 
137 
137 
137 


Positive seeds Negative seeds Word n\(Word Word n\(word Positive seeds Negative seeds Emoticon n\(Emoticon Emoticon n\(Emoticon Approaches Accuracy Learning type Feature number 
002\003 
002 
002 003\004 
TABLE I S EED S ENTIMENT W ORDS U SED IN joyful 113,991 poor 82,401 happy 83,250 wronged 36,677 happiness 54,095 contempt 27,627 wonderful 27,894 002 002 despair 25,027 002\003 beautiful 23,362 002 sad 14,089 002 002 handsome 20,450 002 002 disgusting 9,629 002 002 ne 18,128 003 002 catty 9,400 TABLE II S EED E MOTICONS USED IN 515,559 272,282 271,247 126,016 262,008 92,197 234,699 85,064 162,469 82,401 151,573 72,998 145,506 69,151 TABLE III T HE A CCURACY OF D IFFERENT A LGORITHMS PMI+W 68.5 unsupervised 14 words PMI+E 78.7 unsupervised 14 emoticons ECWM 81.5 supervised 80  vol 2 no 1-2 pp 1Ö135 Jan 2008  M Hu and B Liu Mining and summarizing customer re vie ws   August 2004  P  D T urne y  Thumbs up or thumbs do wn semantic orientation applied to unsupervised classiìcation of reviews in  ser ACL 02 Stroudsburg PA USA Association for Computational Linguistics 2002 pp 417Ö424  V  Hatzi v assiloglou and J M W iebe Ef fects of adjecti v e orientation and gradability on sentence subjectivity ser COLING 00 2000  P  D T urne y and M L Littman Measuring praise and criticism Inference of semantic orientation from association  vol 21 pp 315Ö346 October 2003  Hatzi v assilogou V asileios and K R McK eo wn Predicting the semantic orientation of adjectives  pp 174Ö181 1997  B P ang and L Lee  A sentimental education Sentiment analysis using subjectivity summarization based on minimum cuts in  Association for Computational Linguistics 2004 p 271  J Kamps M Marx R J Mokk en and M De Rijk e Using w ordnet to measure semantic orientations of adjectives 2004  S.-M Kim and E Ho vy  Determining the sentiment of opinions  in  ser COLING 04 2004  S A OKI and T  Uni v ersity   A method for automatically generating the emotional vectors of emoticons using weblog articles  2011  J Zhao L Dong J W u and K Xu Moodlens An emoticon-based sentiment analysis system for chinese tweets  August 2012  A Esuli and F  Sebastiani Determining the semantic orientation of terms through gloss classiìcation in  ser CIKM 05 New York NY USA ACM 2005 pp 617Ö624  I Jollif fe  Wiley Online Library 2005  C.-C Chang and C.-J Lin LIBSVM A library for support vector machines  vol 2 pp 27:1Ö27:27 2011 software available at http://www.csie.ntu.edu.tw cjlin/libsvm  L Xu H Lin and Y  P an The construction of the emotional v ocab ulary    vol 27\(2 pp 180Ö185 2008 
2 PMI+E PMI+E PMI+W PMI+W ECWM PMI+W PMI+E ECWM PMI+E 
002\003 
is short for PMI with emoticon seeds Itês similar with  But what we select emoticons as the seeds That means the orientation of a given word is calculated from the strength of its association with a set of positive emoticons minus the strength of its association with a set of negative emoticons The rules that we select the seed emoticons are the same as the approach of the  The seed emoticons are listing as Table II The overall result is showed as Table III For the result of  the feature number of original feature vector V sword is 640 After the reduction of the features the feature number of principal components of V sword is 80 which is much less than the original features What means that the original vector does have lots of redundant features Table III shows that emoticons are better features The accuracy of is just 68.5 compared to that of  which is 10.2 higher and is 2.8 higher than that of  The seeds play the key role In fact the selection of seeds is the process of human intervention and sentiment words are more widely used in terms of usage We have a large range to select the seed sentiment words but the range of emoticons are much more limited So the error that selection of emoticons brings is smaller V C ONCLUSION This work aims to classify the polarity of sentiment words The way we did it is using the emoticons First we proposed an efìcient approach to classify the polarity of emoticons Based on the polarity of the emoticons as well as the association between the sentiment word and emoticons we build the sentiment wordês model through the emoticons which is called ECWM Experimental results demonstrate that ECWM was a good model to represent the orientation of sentiment words For the future work more data would be crawled to capture the rich co-occurrence between emoticons and sentiment words Moreover the polarity of sentiment words would be used to classify the orientation of statuses in Weibo A CKNOWLEDGMENT This work was partially supported by the Fundamental Research Funds for the Central Universities the National Natural Science Foundation of China under Grant No 61271041 National Basic Research Program of China 973 Program under Grant No 2009CB320504 R EFERENCES  B P ang and L Lee Opinion mining and sentiment analysis  
002 003 002 
002 003 
PMI+W PMI+E Found Trends Inf Retr KDD Proceedings of the 40th Annual Meeting on Association for Computational Linguistics ACM Transactions on Information Systems Proceedings of ACL Proceedings of the 42nd annual meeting on Association for Computational Linguistics Proceedings of the 20th international conference on Computational Linguistics ACACOS KDD Proceedings of the 14th ACM international conference on Information and knowledge management Principal component analysis ACM Transactions on Intelligent Systems and Technology Journal of The China Society For Scientiìc and Technical Information 
002 003 
138 
138 
138 
138 
138 
138 


side sends a certain number of key-value pairs starting from the beginning of map output le instead of sending the entire map output le at once While receiving these key-value pairs from all map locations a ReduceTask now merges all these data to build up a Priority Queue It then keeps extracting the key-value pairs from the Priority Queue in sorted order and puts these data in a rst in rst out structure named as DataToReduceQueue As each map output le is already sorted in the mapper side the merger in the ReduceTask can only extract the data from Priority Queue until the point when the number of key-value pairs from a particular map decreases to zero At that point it needs to get next set of key-value pairs from that particular map task to resume extracting from Priority Queue For faster response in TaskTracker side we propose an efìcient caching mechanism for the intermediate data residing in map output les stored in local disk User can enable caching in TaskTracker side through a conìguration parameter  The following is a new component in the TaskTracker side for caching  MapOutputPrefetcher is a daemon threadpool which caches intermediate map output as soon as it gets available After nishing a map task one of the daemons starts to fetch the data from this map output and caches it in PrefetchCache The novel feature for this cache is that it can adjust caching based on data availability and necessity It can also prioritize which data to cache more frequently based on the demand from the ReduceTasks Depending on heap size availability it can limit the amount of data to be cached in PrefetchCache Even with caching cache misses may occur as ReduceTasks requests may arrive faster than caching In that case TaskTracker fetches data directly from disk itself without waiting for caching But after disk fetch it requests MapOutputPrefetcher to cache this particular map output data with more priority so that successive requests for this output le can be served from the cache Intermediate data pre-fetching and caching plays an important role for achieving better performance with respect to job execution time For large number of ReduceTasks more requests for map output les arrive to a single TaskTracker which can swamp out the I/O bandwidth in TaskTracker side So an efìcient cache implemented here can alleviate such bottleneck and provide better performance In Section IV-D we show how our caching mechanism provides signiìcant performance improvement Figure 3 shows our design which enhances performance by efìciently overlapping shufîe merge and reduce operations in ReduceTask In the default design the merge process starts immediately with shufîe However there exists an implicit barrier for the start of reduce operation the reduce operations can only be started after all the merge operations have completed In our design we start reduce operation as soon as the rst merge completes In this way we can achieve maximum beneìt by introducing pipelining between merge and reduce stages Here we distinguish our proposed RDMA-based design for MapReduce with that of Hadoop-A Some major differences are as follows 1  with RDMA-based communication in the shufîe stage we redesign the merge mechanism to get more overlapping of the reduce stage with the shufîe and merge stages In order to respond to the key-value pairs requested from the reducer as soon as possible we design the intermediate data pre-fetching and caching mechanism in Hadoop TaskTrackers On the other hand although Hadoop-A has proposed a DataEngine tool for intermediate data access on the disk of the mapper side DataEngine doesnêt provide data caching to decrease the disk access based on the paper In section IV we compare our design with Hadoop-A The better performance of our design is a result of the data pre-fetching and caching mechanism 2  HadoopA has implemented a separate merge algorithm and the DataEngine tool in native C and has provided a plug-in based approach to provide RDMA operations on both TaskTracker and ReduceTask side However in order to provide minimal code changes in the existing Hadoop codebase our implementation for merge algorithm still makes use of the existing Merge operation with minimal code changes And the data pre-fetching and caching mechanism is implemented as a daemon in TaskTracker side which does not require any separate tools 3  Between these two RDMA based implementations our design has more user level exibility in terms of conìguration and tuning We provide a number of conìguration parameters such as RDMA packet size enabling of caching number of key value pairs transmitted in each packet etc to give user the exibility and options of choosing the best conìguration for a particular workload Tuning of these parameters can also play a major role on achieving better performance in terms of job execution time Section IV-C shows one of these examples where better parameter tuning achieves better beneìts On the other hand we donêt nd the similar interfaces in Hadoop-A implementation 
3  MapOutputPrefetcher 4  C Differences with respect to Hadoop-A Data Pre-fetching and Caching JAVA Implementation vs C Implementation Conìguration and Tuning Interfaces 
Intermediate Data Pre-fetching and Caching Overlap of Shufîe Merge and Reduce 
mapred.local.caching.enabled 
1913 


017\012\037\007\034\022\020\027.\007\026$\012\016\034\010\012\027 017.*4\004\007\013\012\016\027.\007\026$\012\016\034\010\012\027 
We have used an Intel Westmere cluster for our evaluations This cluster consists of compute nodes with Intel Westmere series of processors using Xeon Dual quad-core processor nodes operating at 2.67 GHz with 12GB RAM and 160GB HDD Each node is equipped with MT26428 QDR ConnectX HCAs 32 Gbps data rate with PCIEx Gen2 interfaces The nodes are interconnected using a Mellanox QDR switch Each node runs Red Hat Enterprise Linux Server release 6.1 Santiago at kernel version 2.6.32131 with OpenFabrics version 1.5.3 This cluster also has dedicated storage nodes with the same conìguration but with 24GB of RAM each Additionally eight of the storage nodes are equipped with two 1TB HDD each Four of the storage nodes also have Chelsio T320 10GbE Dual Port Adapters with TCP Ofîoad capabilities In the gures presented in this section we have mentioned OSU-IB to indicate our RDMA-based design of MapReduce and Hadoop-A to indicate the design in 32 Gbps indicates InìniBand QDR card speed We have performed this experiment in 10 GigE IPoIB 32 Gbps with vanilla Hadoop and Hadoop-A and compared the results with our design For this experiment we have found that the optimal HDFS block-size for 10 GigE IPoIB 32 Gbps and our design is 256 MB whereas it is 128 MB for Hadoop-A We have used TeraGen to generate the input data for TeraSort Figure 4 shows the job execution times of the TeraSort benchmark in a four-DataNode cluster In the experiments for Figure 4\(a we show performance results with single and dual HDDs for each interconnect For single HDD 30 GB sort size our design reduces the job execution time by 9 over Hadoop-A 32 Gbps 35 over IPoIB 32 Gbps and 38 over 10 GigE Compared with IPoIB and 10 GigE our design uses native IB verbs communication for the data shufîe which is much better than the socket based communication on IPoIB and 10 GigE Although HadoopA also uses native IB verbs communication the data prefetching and caching mechanism of our design improves the performance On the other hand if two HDDs are used per node our design improves the execution time by 13 over HadoopA 32 Gbps 38 over IPoIB 32 Gbps and 43 over 10 GigE for the same sort size For 40 GB sort size our design achieves an improvement of 17 48 and 51 over Hadoop-A 32 Gbps IPoIB 32 Gbps and 10 GigE respectively When multiple HDDs are used per node the performance bottleneck of the local disk read and write bandwidth is alleviated Compared to Hadoop-A our design can utilize the improved bandwidth more efìciently to overlap the data shufîe merge and reduce further as illustrated in section III-B We have performed similar experiments with the TeraSort benchmark using eight DataNodes In this case we varied the sort size from 60 GB to 100 GB As shown in Figure 4\(b our design reduces the job execution time 21 over Hadoop-A 32 Gbps for 100 GB sort size with single 
Figure 3 Overlapping of different processes in MapReduce workîow IV P ERFORMANCE E VALUATION In this section we present the detailed performance evaluations of our RDMA-based design of Hadoop MapReduce and its impact on different Hadoop benchmarks We compare the performance of our design with socket based interconnects 10 GigE and IPoIB and Hadoop-A W e ha v e performed the experiments on different storage platforms single/multiple HDDs or SSD per node in order to illustrate the effect of I/O on our design In this study we perform the following set of experiments 1 Evaluation with the TeraSort benchmark and 2 Evaluation with the Sort benchmark These benchmarks are described in section II For each of the benchmarks we have identiìed the optimal values of HDFS block-size for different interconnects as well as for Hadoop-A and our design Additionally in our experimental setup we have also determined that four is the maximum number of map and reduce tasks that can be run simultaneously to achieve the optimal performance by a TaskTracker In all our experiments we have used Hadoop 0.20.2 Hadoop-A and JDK 1.7.0 
002\003\004 005\006\007\010\010\011\012 002\012\013\014\012 015\012\016\007\017\012 020\021\004\011\022\017\022\023\024\025\003\013\013\022\012\013 002\003\004 005\006\007\010\010\011\012 002\012\013\014\012 015\012\016\007\017\012 020\021\004\011\022\017\022\023\024\025\003\013\013\022\012\013 
A Experimental Setup B Evaluation with the TeraSort Benchmark 
1914 


      
0 500 1,000 1,500 2,000 2,500 40 30 20 Job Executiion Time \(sec Sort Size \(GB 10GigEä1disk 10GigEä2disks IPoIBä1disk \(32Gbps IPoIBä2disks \(32Gbps HadoopAäIBä1disk \(32Gbps HadoopAäIBä2disks \(32Gbps OSUäIBä1disk \(32Gbps OSUäIBä2disks \(32Gbps 0 500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 100 80 60 Job Executiion Time \(sec Sort Size \(GB 1GigEä1disk 1GigEä2disks IPoIBä1disk \(32Gbps IPoIBä2disks \(32Gbps HadoopAäIBä1disk \(32Gbps HadoopAäIBä2disks \(32Gbps OSUäIBä1disk \(32Gbps OSUäIBä2disks \(32Gbps 0 500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 200GBä24nodes 100GBä12nodes Job Executiion Time \(sec Sort Size 1GigE IPoIB \(32Gbps HadoopAäIB \(32Gbps OSUäIB \(32Gbps 0 50 100 150 200 250 300 350 400 450 500 20 15 10 5 Job Executiion Time \(sec Sort Size \(GB 1GigE IPoIB \(32Gbps HadoopAäIB \(32Gbps OSUäIB \(32Gbps 
C Evaluation with the Sort Benchmark 
a Total job execution times in 4 nodes cluster          b Total job execution times in 8 nodes cluster Figure 4 TeraSort benchmark evaluation HDD From Figure 4\(b we observe that with two HDDs per node our design achieves an improvement of 31 over Hadoop-A 32 Gbps          Figure 5 TeraSort benchmark evaluation with larger sort size We have also evaluated TeraSort with larger sized cluster In this case we have varied the sort size from 100 GB to 200 GB with 12 and 24 compute nodes in the cluster respectively Figure 5 shows these results For 100 GB sort size we achieve 41 beneìt over IPoIB 32Gbps and 7 beneìt over Hadoop-A 32Gbps For 200 GB sort size also we achieve similar beneìts From Figure 4 and Figure 5 we observe that Hadoop-A performs better with respect to IPoIB in a bigger cluster with the same sort size whereas our implementation achieves better performance in both cases compared to Hadoop-A As in our setup storage nodes have twice as much memory as compute nodes our implementation has more beneìts in storage nodes compared to those in compute nodes This clearly depicts the efìciency of the caching mechanism implemented in our design Figure 7 Sort benchmark evaluation with SSD Figure 6\(a shows the performance results of the Sort benchmark using four DataNodes For this we have used four compute nodes in our cluster Each DataNode has single HDD per node In this case our design reduces the job execution time by 26 over IPoIB 32 Gbps and 38 over Hadoop-A for 20 GB sort From Figure 6\(b we observe that with eight DataNodes our design can achieve an improvement of 27 over IPoIB 32 Gbps and 32 over Hadoop-A Compared with the TeraSort benchmark the difference in the Sort benchmark is the variable size of the key-value pairs In Sort the combined length of key-value pairs can be as large as 20,000 bytes From the results we can observe that Hadoop-A performs worse than IPoIB even after conìguring all the tunable parameters with optimum values as mentioned in Hadoop-A release It re v eals that only substituting the socket based communication with the native IB verbs some applications such as the Sort benchmark cannot get better performance due to the inefìciency in number of keyvalue pairs transferred each time that also affects proper overlapping between all the stages Our design with the efìcient caching mechanism can get better performance in 
We perform the regular Sort benchmark in 1 GigE IPoIB 32 Gbps with vanilla Hadoop and Hadoop-A and compare the results with our design For this experiment the optimal value of HDFS block-size is 64 MB We have used RandomWriter to generate the input data for the Sort benchmark            
1915 


        
0 200 400 600 800 1,000 1,200 1,400 20 15 10 5 Job Executiion Time \(sec Sort Size \(GB 1GigE IPoIB \(32Gbps HadoopAäIB \(32Gbps OSUäIB \(32Gbps 0 200 400 600 800 1,000 1,200 1,400 40 35 30 25 Job Executiion Time \(sec Sort Size \(GB 1GigE IPoIB \(32Gbps HadoopAäIB \(32Gbps OSUäIB \(32Gbps 0 100 200 300 400 500 600 20 15 10 5 Job Executiion Time \(sec Sort Size \(GB IPoIB OSUäIB \(Without Caching Enabled OSUäIB \(With Caching Enabled 
D Beneìts of Caching 
a Total job execution times in 4 nodes cluster         b Total job execution times in 8 nodes cluster Figure 6 Sort benchmark evaluation both these cases as it considers the size of the key-value pair before the transfer We also evaluate Sort benchmark using SSD as HDFS data stores Figure 7 shows the comparison for this evaluation In this case our design achieves a beneìt of 22 over Hadoop-A 32Gbps and 46 over IPoIB 32Gbps in job execution time for 15 GB sort Figure 8 Effect of Caching Mechanism Figure 8 shows the performance comparison between caching enabled and caching disabled in our RDMA-based design for Sort benchmark We perform this experiment using SSD as HDFS data store In this case enabling caching with our design can enhance performance by 18.39 over caching disabled in the same design for 20 GB sort size It reveals that for big workload as in sort an efìcient caching mechanism can signiìcantly improve the performance for an RDMA-based design using a high performance interconnect V R ELATED W ORK Many studies have paid attention to improve the performance of MapReduce in recent years As mentioned before the most analogous one is Hadoop-A which pro vides a new merge method to Hadoop MapReduce framework by utilizing RDMA over InìniBand Our work has some major enhancements and differences with respect to their work along pre-fetching caching codebase modiìcation etc We have discussed these detail in section III-C In the authors ha v e proposed techniques of prefetching and pre-shufîing into MapReduce From their results the techniques can improve the overall performance of Hadoop We have focused on implementing an efìcient key-value pairs pre-fetching and caching mechanism inside TaskTracker which is responsible for fast data shufîe when the mappers are yet to be completed Such a design can help reduce the overhead in the reducer side when the shufîe and merge procedures run in an overlapped manner The research has demonstrated that there is an impressi v e space for performance improvement in Hadoop MapReduce compared with traditional HPC technologies such as MPI Our earlier work 7 re v ealed that SSD can reduce the I/O cost and make the overheads involved in datatransmission over the network prominent In this paper we have conducted experiments with multiple HDDs and SSDs per node to lessen the I/O bottleneck when studying the effect of communication over different interconnects VI C ONCLUSION In this paper we have presented an RDMA-based design of MapReduce over InìniBand We have also proposed efìcient pre-fetching and caching mechanisms for retrieving of the intermediate data Our performance evaluation shows that we achieve 21 beneìt in terms of execution time over Hadoop-A for 100 GB TeraSort For regular Sort benchmark our design outperforms Hadoop-A by 32 for 40 GB sort We also observe an improvement of 22 over Hadoop-A for the same sort size using SSD In future we plan to extend 
In our design we have implemented efìcient map output pre-fetching and caching mechanism We have also provided a conìguration parameter to enable/disable the caching In this experiment we have evaluated the performance improvement that we can get through caching enabled        
1916 


our design to handle faster recovery in case of task failures We will also evaluate our design on larger clusters with a range of applications R EFERENCES  Y  W ang X Que W  Y u D Goldenber g and D Sehgal Hadoop Acceleration through Network Levitated Merge in  ser SC 11 2011  Apache Hadoop http://hadoop.apache.or g  J Dean and S Ghema w at MapReduce Simpliìed Data Processing on Large Clusters in  2004  K Shv achk o H K uang S Radia and R Chansler  The Hadoop Distributed File System in  2010  J Appa v oo A W aterland D Da Silv a V  Uhlig B Rosenburg E Van Hensbergen J Stoess R Wisniewski and U Steinberg Providing A Cloud Network Infrastructure on A Supercomputer in  ser HPDC 10 New York NY USA ACM 2010 pp 385Ö394  Greenplum Analytics W orkbench http://www.greenplum.com/news/greenplum-analyticsworkbench  S Sur  H W ang J Huang X Ouyang and D K Panda Can High Performance Interconnects Beneìt Hadoop Distributed File System in  Atlanta GA 2010  J Jose H Subramoni M Luo M Zhang J Huang M W Rahman N S Islam X Ouyang H Wang S Sur and D K Panda Memcached Design on High Performance RDMA Capable Interconnects in  Sept 2011  J Huang X Ouyang J Jose M W  Rahman H W ang M Luo H Subramoni C Murthy and D K Panda High-Performance Design of HBase with RDMA over InìniBand in   N S Islam M W  Rahman J Jose R Rajachandrasekar H Wang H Subramoni C Murthy and D K Panda High Performance RDMA-based Design of HDFS over InìniBand in  November 2012  J Jose M Luo S Sur  and D K P a nda Unifying UPC and MPI Runtimes Experience with MVAPICH in  Oct 2010  Hadoop Map Reduce The Apache Hadoop Project  http://hadoop.apache.org/mapreduce  Sort http://wiki.apache.or g/hadoop/Sort  RandomWriter http://wiki.apache.or g/hadoop RandomWriter  Inìniband T rade Association http://www inìnibandta org  OpenF abrics Alliance http://www openf abrics.or g  P  Balaji H V  Shah and D K P anda Sockets vs RDMA Interface over 10-Gigabit Networks An In-depth analysis of the Memory Trafìc Bottleneck in  2004  RDMA Consortium  Architectural Speciìcations for RDMA over TCP/IP http://www.rdmaconsortium.org  B Fitzpatrick Distrib uted Caching with Memcached   vol 2004 pp 5 August 2004  A v ailable http://portal.acm.or g/citation.cfm id=1012889.1012894  Apache HBase The Apache Hadoop Project  http://hbase.apache.org  MV APICH2 MPI o v er InìniBand 10GigE/iW ARP and RoCE http://mvapich.cse.ohio-state.edu  Mellanox T echnologies Unstructured Data Accelerator http://www.mellanox.com/page/products dyn product family=144  S Seo I Jang K W oo I Kim J.-S Kim and S Maeng HPMR Prefetching and Pre-shufîing in Shared MapReduce Computation Environment in  Sep 2009 pp 1Ö8  X Lu B W ang L Zha and Z Xu Can MPI Beneìt Hadoop and MapReduce Applications in  2011 
Proceedings of 2011 International Conference for High Performance Computing Networking Storage and Analysis Operating Systems Design and Implementation OSDI IEEE 26th Symposium on Mass Storage Systems and Technologies MSST Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing Workshop on Micro Architectural Support for Virtualization Data Center Computing and Clouds in Conjunction with MICRO 2010 International Conference on Parallel Processing ICPP IEEE International Parallel and Distributed Processing Symposium IPDPSê12 The International Conference for High Performance Computing Networking Storage and Analysis SC Fourth Conference on Partitioned Global Address Space Programming Model PGAS Workshop on Remote Direct Memory Access RDMA Applications Implementations and Technologies RAIT in conjunction with IEEE Cluster Linux Journal Cluster Computing and Workshops 2009 CLUSTER 09 IEEE International Conference on IEEE 40th International Conference on Parallel Processing Workshops ICPPW 
1917 


A Global Solution COVERAGE North and South America EMEA and Asia White lines are flights in the masFlight platform from February 8, 2013 Yellow pins are weather stations feeding hour ly data to our platform Maps from Google Earth / masFlight masFlight tracks flights, airports and weather around the world  Global daily flight information capture  82,000 flights  350 airlines  1700 airports  Integrated weather data for 6,000 stations  Match weather to delays  Validate block forecasts at granular level  Add weather analytics to IRROPS review and scenario planning 


Example 1: Proposed FAA Tower Closures masFlight used big-data to link airport operations across three large data sets  Current and historical airline schedules  Raw Aircraft Situation Display to Industry \(ASDI\AA  Enhanced Traffic Management System Counts \(ETMS\Airport operations counts by type \(commercial, freight, etc TOWER CLOSINGS Dots indicate closures; Red dots have scheduled service Based on scheduled service March 1 7, 20 13; scheduled service includes scheduled charter flights, cargo flig hts, and passenger flights Dots  indicate  closures  Red  dots  have  scheduled  service Bas ed  o n sc h edu l ed  se rvi ce  M a r c h 1  7, 2013; scheduled se rvi ce includ es scheduled c harter fli g hts car g o fli g hts a nd passen g er fli g hts Findings: Proposed Tower Closings  From schedules database: 55 airports with scheduled passenger airline service  14 EAS Airports  From ASDI & ETMS: 10,600 weekly flights on a flight plan \(ex. VFR and local traffic  6,500 Part 91/125 weekly flights  4,100 Part 135/121 weekly flights  


Example 1: Big-Data Analytics Applied to ASDI and ETMS To Analyze Operations TOWER CLOSINGS  26 44 24 23 11 10 6 2 1 2 Up to 5 5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45 Count of Airports Average Number of Daily Operations with a Flight Plan Filed Distribution of Airports By Average Number of ìDailyî Impacted Flights Airports Affected by Tower Closures Source: ASDI radar data ñ Part 91 151 flying and Part 135/121 flying March 1-7, 2013; masFlight analysis Note: Average ìdailyì operations based on 5-day week 


Example 2: Aviation Safety Causal Factor For example, consider the following ASRS report \(ACN 1031837 Departing IAH in a 737-800 at about 17,000 FT, 11 m iles behind a 737-900 on the Junction departure over CUZZZ Intersection. Smooth air with wind on the nose bearing 275 degrees at 18 KTS We were suddenly in moderate chop which lasted 4 or 5 seconds then stopped and then resumed for another 4 or 5 seconds with a significant amount of ri ght rollingÖ I selected a max rate climb mode in the FMC in order to climb above the wake and flight path of the leading -900 We asked ATC for the type ahead of us and reported the wake encounter. The 900 was about 3,300 FT higher than we were  Synopsis  B737-800 First Officer reported wake encounter from preceding B737-900 with resultant roll and moderate chop What causal factors can be identified from this narrative that could be applied to future predictive applications CAUSAL FACTORS Data-mining algorithms can mine the text of safety reports to obtain specific data that can be used to analyze causal factors  


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators ñ Data Element Methods ñ Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Todayís cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlightís data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlightís hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlightís method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





