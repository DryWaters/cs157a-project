Solving Cross-selling Problems with Ensemble Learning: A Case Study   Xinjian Guo, Yilong Yin 002 Guangtong Zhou and Cailing Dong School of Computer Science and Technol ogy, Shandong University, Jinan, 250101, China Email: ylyin@sdu.edu.cn      002 Corresponding author: Yilong Yin, Email: ylyin@sdu.edu.cn  Abstract  This paper shows our solution to PAKDD Competition 2007 as a case study of cross-selling problems. Following a brief description of the data mining task, we discuss several difficulties to be confronted with in the task from the view of data mining. Then, we show how to do the data preprocessing. In the solution we proposed, to weaken class imbalance of the modeling dataset externally, we combine under-sampling and over-sampling techniques. Besides, we adjust the parameters of each base learner internally to solve cost-sensitivity. Next  we get an ensemble of base learners to achieve a better predicting performance. Experimental results on prediction dataset of real world, provided by PAKDD Competition 2007 show that our solution is effective and efficient with its AUC value 60.73  1. Introduction  Cross-selling is becoming a more and more important and fashionable selling style in recent years  Cross-selling can be categorized to two classes customers based cross-selling and business based cross-selling. Most of research concentrates on the former [1-3  Custo m e r s b a se d c r o ss-se lli ng d i sc o v e r s  business demand of each customer by mining association rules, according to different customers However, there are few successful cross-selling models based on business to guide cross-selling by mining customers’ datasets of some type of business. In the later class, customers from the same dataset have the same type of business. This paper concentrates on predicting possible business based cross-selling The cross-selling problem of PAKDD competition 2007 is described as follows. The company currently has a customer base of credit card customers as well as a customer base of home loan \(mortgage\stomers Both of these products have been on the market for many years, although for some reason the overlap between these two customer bases is currently very small. The company would like to make use of this opportunity to cross-sell home loans to its credit card customers, but the small size of the overlap presents a challenge when trying to develop an effective scoring model to predict potential cross-sell take-ups. The data mining task is to produce a score for each customer in the prediction dataset, indicating a credit card customer's propensity to take up a home loan with the company \(the higher the score, the higher the propensity The reminder of the paper is organized as follows Section 2 describes the difficulties to address the task briefly. Section 3 shows how to pre-process the modeling dataset and prediction dataset. Section 4 presents our solution. Section 5 provides some representative evaluation results by PAKDD Competition 2007 and section 6 concludes the work  2. Analysis and Understanding to Crossselling Problems  Based on  our understanding to the dataset provided there are several difficulties to be dealt with in the cross-selling task from data mining aspect  2.1. Nominal Attributes and Missing Values  In both of the modeling and prediction datasets there are many nominal attributes which cannot be used, when modeling, by the majority of the most nonprobability based algorithms directly, such as SVM, knearest neighbors, and so on. Besides, there are many missing values in several variables. Simply removing all the samples with missing values will lose too much information. Then, how to pre-process the dataset seems an interesting and complex challenge  2.2. Class Imbalance  
2008 International Conference on Advanced Computer Theory and Engineering 978-0-7695-3489-3/08 $25.00 © 2008 IEEE DOI 10.1109/ICACTE.2008.86 128 
2008 International Conference on Advanced Computer Theory and Engineering 978-0-7695-3489-3/08 $25.00 © 2008 IEEE DOI 10.1109/ICACTE.2008.86 128 


In the modeling dataset, the ratio between the number of credit card customers and that of home loan customers is 40,000:700, so severe class imbalance exists in the modeling dataset. It is known that class imbalance makes traditional learners behave badly especially for minority class prediction. However minority class always causes our more attention in most cases. Then, how to deal with such class imbalance effectively  2.3. Cost-sensitivity  From the potential interest of the consumer finance company, it may be anticipated that all potential home loan customers are scored high, because a higher score represents a greater possibility to become a home loan customer, who will bring profit to the company, and in the same time, credit card customers are scored as low as possible, so that they will be paid less attention in the cross-selling process. However, a predictor cannot always predict correctly, and may producing a high score for a credit card customer and a low score for a home loan customer, but the costs are quite different because mis-producing high scores for credit card customers will introduce some cost, such as advertising fees, but mis-producing low scores for home loan customers will introduce profit loss. And profit loss may be hundreds or thousands times of advertising fees regarding to average cost for one customer. In the cost sense, we would produce high scores for dozens of credit card customers rather than producing a low score for a home loan customer. This suggests that the task is cost-sensitive. But the cost information is unknown then, how to deal with such cost-sensitivity  2.4. Scoring  In the modeling dataset, target flags for all modeling samples are given, whose value may be 0 or 1, so it may be easy to do classifying. But the task is to produce a score for each customer in the prediction dataset, not to simply predict a target flag. In decisionmaking of cross-selling, predicting each sample with a score, which implies the possibility a credit card customer will became a home loan customer, gives decision maker more freedom than providing a predicted label simply. Then, how to produce a score for each customer in the prediction dataset  3. Data Pre-processing  Table 1 Statistical results of missing values in the datasets provided  Modeling dataset Prediction dataset Count Perc. Count Perc AMEX_CARD 3707 9.09 721 9.01 DINERS_CARD 3884 9.52 760 9.50 VISA_CARD 2455 6.02 477 5.96 MASTER_CARD 2937 7.20 570 7.13 RETAIL_CARDS 3693 9.05 724 9.05 DISP_INCOME_CODE 28742 70.45 5672 70.9 CUSTOMER_SEGMENT 1740 4.26 338 4.23 By statistical methods, we observed that missing values mainly exists in seven attributes in the modeling dataset 1 as well as the prediction dataset, as shown in Table 1. The left column is the attributes with missing values and two columns in the middle and the two columns in the right are counts and proportions of samples with missing values in corresponding attributes for modeling dataset and prediction dataset separately. Considering specialization of the seven attributes, we use corresponding filling methods for different attributes. By the data dictionary, we find that there are close relationships among the attribute TOTAL_NBR_CREDIT_CARDS and the five attributes related to credit cards \(i.e., AMEX_CARD DINERS_CARD, VISA_CARD, MASTER_CARD and RETAIL_ CARDS\here may be some cases in the following, which cause missing values in the attributes. Some customers might only mark corresponding attribute with ‘Y’ in the five credit cards when he or she had some kind of card, while did not mark the attribute with ‘N’ in the five credit cards when he or she did not have such kind of card. In order to keep the attribute TOTAL_NBR_CREDIT_CARDS consistent with the attributes related to five cards, we propose the method, as shown in Table 2, to deal with missing values in the five attributes. To make the best use of samples in the modeling dataset Group Mean Imputation a s been adopt ed t o f i l l  missing values in the two attributes DISP_INCOME_CODE and CUSTOMER_SEGMENT. We also use Group Mean to transform nominal attributes to numerical attributes, and finally we rescale all the attribute values of the modeling dataset and the prediction dataset to the interval [0, 1    Table 2 Method to fill missing values in the five attributes related to credit cards    1 Besides, there is only one missing value in the attribute CHQ_ACCT_IND and ANNUAL_INCOME_RANGE in modeling dataset separately. We f ill the two missing values manually on the base of Group Mean  
129 
129 


 For each sample of modeling dataset and prediction dataset do  if TOTAL_NBR_CREDIT_CARDS value The number of “Y” in these five cards then  complete the missing value in the five cards with char “N else replace TOTAL_NBR_CREDIT_CARDS value with the number of “Y” in these five cards complete the missing values in the five cards with char “N end for  4. Solution  We propose an ensemble method to solve the crossselling task. The pseudo-code of the solution is shown in Table 3, where the sub-algorithms SMOT d Bootstrap [6 h o w n i n T a bl es 4 an d 5  respectively  Table 3 Ensemble method to the crossselling task  Ensemble of base learners  Input D:  Modeling dataset  L:  Number of base learners Process  1   D  the positive subset\(i.e. home loan customers\ of D  2   D  the negative subset\(i.e. credit card customers D  3   SMOTE D  SMOTE N K D     4  for i 1  L  do  5    i D  Bootstrap N D    6       SMOTE i i D D D  7  Training base learner i with i D on the base of AUC  8  end for 9    L i x SVMi x v 1      Output    x h a score of sample x  1  if    x v Threshold then  2         1     1   x v j j x y probabilit x v x h  base learner j 1    x  3  else 4        1     1   x v j j x y probabilit x v x h  base learner j 1     x    Re-sampling  As analyzed in section 2, the concerned task is high class-imbalanced. In the modeling dataset, the number of samples of the negative class \(i.e., credit card customers class\ is about 57 times more than that of the positive class \(i.e., home loan customers class\ this case, standard machine learning algorithms tend to be overwhelmed by the majority class and ignore the minority class since traditional classifiers seeking an accurate performance over a full range of instances. To deal with such class-imbalance, we re-sample the modeling dataset by combining over-sampling and under-sampling methods. First, we over-sample the positive class to a moderate extent, then under-sample negative class to the similar si  SMOTE [1 an ov er-s a m pl i n g  m e t h od, h a s bee n  shown better than random over-sampling with replacement, for it can avoid over-fitting. In the solution, we adopt SMOTE to over-sample the positive class, as shown in the step 3 of Table 3. And we set the number of nearest neighbors K for each a sample in the positive class to be 2. That’s we want to over-sample the positive class from 700 samples to 2100 samples The detail process of the SMOTE algorithm is shown in Table 4. Readers who are familiar with it can skip it over  Table 4 The SMOTE algorithm  SMOTE  Input   D the positive subset of D  K number of nearest neighbors for a sample in  D  N number of samples in  D  Process  1  erpolated D int  null 2  for   N  3  i K i x x  1  the K nearest neighbor instances of the i-th instance i x in  D 4  for  i j x i K i x x  1  5  i j D  positive samples randomly 
130 
130 


interpolated between i x and i j x  6   erpolated D int  i j erpolated D D  int  7  end for 8  end for Output positive set after interpolated erpolated D D int     In order to keep over-sampled positive class and under-sampled negative class with similar size, that is to get a balanced distribution, we propose Bootstrap to under-sample the negative class, as shown in the step 5 of Table 3. In particular, we set the size of the negative class N to be 2100 after the bootstrap process. The detail process of the Bootstrap algorithm is shown in Table 5  Table 5 The Bootstrap algorithm  Bootstrap  Input   D the negative subset of D  N size of the positive class after Bootstrapped Process  1  for i  N   2   x  randomly choose a sample from  D  with replacement 3    D    x D   4  end for Output negative set after Bootstrapped  D   In the step 6 of Table 3, we unite size-matched oversampled positive class with L under-sampled negative classes individually as training datasets for base learners  4.2. Base Learners Selection  SVM \(support vector machine\as established itself as a successful approach for various machine learning tasks. One of its advantages is that it maximizes the margin between two classes, Besides, SVM enable a considerably easier parame terization when compared to other learning machines like for example multi-layer perception neural networks [8  So w e  se l e c t SV M  1 0    bas e learn e r, as s h o w n i n th e s t ep 7 of T a ble 3  As analyzed in section 2, the concerned task is also cost-sensitive, while the relative misclassification costs function are unknown. To deal with cost-sensitivity in the task we adjust the cost parameter C of SVM internally t h e proces s of trai n i ng S V M w e  select RBF \(radial base function\ 11 t h e k e rn el  function of SVM In the proposed method, we set the number of base learners L i.e. the number of SVMs\o be 20. Note that in the first for loop \(i.e. the steps 4 to 8\ Table 3, we obtain L different under-sampled negative classes through L iterations. Farther, we obtain L  different training sets by uniting the L under-sampled negative classes with the same over-sampled positive class. Then, in the step 7 of Table 3, we train L SVMs on the L training datasets  4.3. Parameters selection  The preprocessed modeling dataset is treated as testing dataset when selecting parameters for SVM and the threshold of ensemble. We use the area under the ROC curve \(AUC\or evaluation, as is adopted by the organizing committee when evaluating submitted entries in PAKDD Competition 2007 In order to get better performance for base learners we test the two parameters C and  for the first base learner \(i.e., SVM 1 where C varies from 1 to 1001 step by 50 and  varies from 10 to 30 step by 0.5, as shown in Fig.1 Fig.2 shows that there is a climax when the parameter C is at 5 regardless of the parameter  and that the AUC value will rise quite gently certainly when the threshold of the AUC value is 0.98. So we train SVM 1 when C equals 5 and  equals 21. Optimal parameters for the other nineteen support vector machines can be obtained in the same way when the threshold of the AUC value is 0.98  Fig 1 Relation between the AUC value and the parameters for SVM 1  
131 
131 


Fig 2 Feature of the relation   4.4. Ensemble  Ensemble learning as one of the most research directions in machine learning has received much attention [13 t h e s t ep 9 of T a bl e 3  w e i m prov e  prediction performance with an ensemble of base learners. The difficulty of scoring, as analyzed in section 2, is tackled in the segment. In ensemble, we classify a sample by threshold voting and then compute its score on average according to the classification result. The detail process is as follows When scoring, we adjust the threshold variable v  For a sample, if it is classified to the positive class by no less base learners than v we will classify it to the positive class, and its score is the average of probabilities 2 of the base learners who classify it to the positive class; Otherwise, it will be classified to the negative class, and its score is the average of positive probabilities of base learners who classify it to negative class  Fig 3 Relation between the threshold v and accuracy Fig.3 reflects that when v is 15, the true positive fraction achieves the highest value, and the false negative achieves a higher value. Considering producing a low score for a potential home loan customer is with a higher cost than producing a high    2 In LIB_SVM, SVM can provide two probabilities for each sample: one is the probability of the sample classified to the positive class, and the other is that of the sample classified to the negative class. The sum of the two probabilities is 1 score for a credit card customer, as analyzed in section 2, we prefer to misclassify negative samples other than positives when we must make a choice alternatively, so we set the threshold v to be 15. The accuracies of the two classes for different thresholds are showed in Fig.3  5. Evaluation  Prediction results have been reported on dataset of real world provided by PAKDD competition 2007 with AUC as the evaluation metric 3 Some representative modeling algorithms and their AUC values are shown in shown in Table 6. It is shown that our solution is effective with its AUC value 60.73%. It is evident that TreeNet + Logistic Regression” and “MLP + n-Tuple Classifier” are much better than the other algorithms The grand champion team of the competition propose TreeNet + Logistic Regression” to solve the crossselling task with the AUC value is 70.01%. And our solution works better than “Neural Network Decision Tree” and “Predicted Apriori Association Rules + Nearest Neighbors  Table 6 Predicting results of some representative algorithms  Algorithms AUC TreeNet + Logistic Regression 70.01 MLP + n-Tuple Classifier 69.62 Our solution 60.73 Structural Resonance in Multi-dimensional Data 56.35 Neural Network 55.96 Random Forest 55.16 Decision Tree 55.53 Tree Boosting 52.42 Customer Growth Model Clustering Support Vector Machine 52.24 Predicted Apriori Association Rules Nearest Neighbors 50.06  6. Conclusion  The solution to cross-selling problems proposed in the paper is an ensemble method based on majority voting, whose essence can be interpreted as follows Since we train twenty different base learners with various training datasets, we can get twenty groups of different support vectors, each of which establish a    3 The competition result can be seen from http://lamda.nju.edu.cn/conf/pakdd07/dmc07/results.htm, and the ID of our team is P060  
132 
132 


classification hyperplane in a 40-dimension space 4  Ensembling base learners with the threshold v equals 15 means combining at least 15 hyperplanes to separate a group of subspaces for positive class from the 40-dimension space. These combined hyperplanes in the 40-dimension space  works as a piecewise linear classifier in a 2-dimension space in some sense. The only difference is these combined hyper-planes are not static, but dynamic when classifying a given sample Formally, let N L L L    2 1 represent the N hyperplanes. For a sample x if x in the subspace v L L L    2 1 where  j L      2 1 N L L L and threshold predefined v    our algorithm will classify it to the positive class Otherwise, it will be classified to the negative class  In the paper, we proposes an ensemble method to solve cross-selling problems. The task is of class imbalance and cost-sensitivity. We combine oversampling and under-sampling methods to solve the class imbalance problem, and solve cost-sensitivity by adjusting the parameters of base learners. At last, we adopt majority voting to get an ensemble of base learners. Experiment on prediction dataset provided by PAKDD Competition 2007 shows our solution is effective and efficient Though we a dopt SVM as the base learner and it has been shown effective, there may be more suitable algorithms to solve cross-selling problems, since our solution is a wrapper ensemble method. So exploring more effective and efficient methods for the task seems to be interesting. Besides majority voting, more advanced ensemble strategies are anticipated to get a better performance  Under a deeper analysis to the dataset, it is found that there are several redundant and irrelevant variables in cross-selling datasets, so effective variables selection will improve performance of the solution proposed in this paper. Since the task is to predict a score for each customer, logic regression and learning to rank methods can be anticipated to exhibit better performances in the task, and it is implied from the competition result in some sense. In the view of decision making, learning to rank methods may be a better solution compared to classification and regression methods, since the decision makers have more freedom to the quantity or the proportion of cross-selling customers according to their ranks  7. References  1 S  B. L i B  H. S u n a n d  R. W ilc ox  C ross-se lling  sequentially ordered products: an application to consumer    4 The modeling dataset is 40 dimensions, so it is a 40-dimension space banking services Journal of Marketing Research 42\(2  2004, pp.233-239 2 R  C  W  W ong A  W  C  Fu, K  W a ng D a t a  m i ning  f o r  inventory item selection with cross-selling considerations Data Mining and Knowledge Discovery, 11\(1 2005, pp.81112 3 B Bh ask e r, H.H P a rk  J P a rk an d H S. Kim   P ro d u c t  recommendations for cross-selling in electronic business”, In Proceedings of the Nineteenth Australian Joint Conference on Artificial Intelligence \(AUS-AI’06 Lecture Notes in Computer Science \(LNCS\ 4304, Springer, 2006 pp. 10421047 4 J Sc he f f e r  D e a ling w ith m i s s ing da ta    Research Leters of  Information and  Mathematical Science, vol.3 2002 pp.153-160 5 R  A k ba ni, S. K w e k a nd N  J a pk ow ic z  A ppl y i ng  support vector machines to imbalanced datasets”, In Proceedings of the Fifteenth European Conference on Machine Learning \(ECML2004 Lecture Notes in Artificial Intelligence, pp.39-50  Daviso n and D  V Hin k ley  Bootstrap Methods and Their Application Cambridge University Press A.C.,1997 7 S. L e s s m a nn S olv i ng im ba la nc e d c l a s s i f i c a tion Problems with Support Vector Machines”, In Proceedings of the International Conference on Artificial Intelligence ICAI’04 pp.214-220 8 S. F. C r one S  L e s s m a nn, a n d R  Sta h l boc k   E m p ir ic a l  Comparison and Evaluation of Classifier Performance for Data Mining in Customer Relationship Management”, In Proceedings of the IEEE 2004 International Joint Conference on Neural Networks \(IJCNN2004 2004, pp.443448 9 T h e L A MD A e r T e a m P red i ctin g Fu tu re Cu st o m ers v i a  Ensembling Gradually Expanded Trees presented at PAKDD’06 Competition 2006 10  Z.Q. Bia n a n d X  G  Zha n g   Pattern Recognition   Tsinghua Publish House, Beijing, 2000, pp.299-303. \(in Chinese 11 Y   Xia o a n d C  Z. H a n   M e t ho d of R e duc ing Fa ls e  Positive Alerts Based on Support Vector Machine in Intrusion Detection Computer Engineering  Vol.32 No.17 2006, pp.25-27  1 N  V  Ch aw la L  O Hall  K W  Bo wyer an d W  P   Kegelmeyer, “SMOTE: Synthetic Minority Oversampling Technique Journal of Artificial Intelligence Research, 16 2002 pp. 321-357 1 Z  H Zh ou a n d W Tan g  C l u st er er e n sem b l e   Knowledge-Based Systems 19 \(1 2006,  pp.77-83 14 H. Lappalainen, J. W. Miskin, “Ensemble Learning Advances in Independent Component Analysis M. Girolami Ed., Berlin, Springer Verlag Scientific Publishers, 2000 pp.75-92 15 A  Krog h  P  S o llic h  S ta tist i c a l m e c h a n ic s of e n se m b le  learning Physical Review E  55\(1 1997, pp.811-825    
133 
133 


 Kluwer Academic Publishers Springer, New York 1st edition, 2001 14  S c h e f f e r   T   F i n d i n g  A s s o c i a t i o n  Ru l e s  t h a t  T r a de Support Optimally Against Confidence th The Elements of Statistical Learning self_care_guide/Urogenital/Postate%20Cancer.pdf  Accessed, 25 August, 2008 11  A g r a w a l   R  T   I m i e l i n s k i     A   S w a m i   M i n i n g  association rules between sets of items in large databases, In Proceedings of the 1993 ACM SIGMOD international conference on Management of data  The Netherlands 42 2001 61-95  Ordonez C Association rule discovery with the train and test approach for heart disease predictio n 207\226 216 12 001 13  H a s t i e   T    R  T i b s h i r a n i     J  H   F r i e d m a n   Proceedings of the 5th European Conference on Principles and Practice of Knowlege Discovery in Databases\(PKDD'01 IEEE Transactions on Information Technology in Biomedicine, 10\(2\, 2006. 334 \226 343 001 Freiburg, Germany : SpringerVerlag, 2001. 424-435 15  F l a c h   P  A     L a c h i c h e   N   Co n f i r m a t i o n g u i d e d  discovery of first-order rules with Tertius 10  P h a r m a c y   h t t p    w w w  p h a r m a c y  g o v  m y    


 7. Conclusions  In this paper we have proposed an intelligent and efficient technique to reassess the distances between dynamic XML documents when one or all of the initially clustered documents have changed. After the changes, the initial clustering solution might become obsolete - the distances between clustered XML documents might have changed more or less depending on the degree of modifications \(insert update, delete\hich have been applied. Re-running full pair-wise comparisons on the entire set of modified documents is not a viable option, because of the large number of redundant operations involved Our proposed technique allows the user to reassess the pair-wise XML document distances, not by fully comparing each new pair of versions in the clustering solution, but by determining the effect of the temporal changes on the previously known distances between them. This approach is both time and I/O effective, as the number of operations involved in distance reassessing is greatly reduced  References  1  Beringer, J. and H\374llermeier, E., Online clustering of parallel data streams Data and Knowledge Engineering 58\(2\,  2006, 180-204 2  Catania, B. and Maddalena A., A Clustering Approach for XML Linked Documents, Proceedings of the 13th International Workshop on Database and Expert Systems Applications \(DEXA\22202\, IEEE 2002 3  Chen, M.S., Han, J. and Yu, P., Data Mining: An Overview from Database Perspective, IEEE Transactions on Knowledge and Data Engineering vol. 8, 1996, 866-883 4  Cormen, T., Leiserson, C. and Rivest, R Introduction to algorithms, MIT Press, 1990 5  Costa, G., Manco, G., Ortale, R. and Tagarelli, A., A tree-based Approach to Clustering XML documents by Structure, PAKDD 2004, LNAI 3202, 137-148 Springer 2004 6  Dalamagas, T., Cheng, T., Winkel, K.J. and Sellis, T 2004, Clustering XML documents by Structure SETN 2004, LNAI 3025, 112-121, Springer 2004 7  Ester, M., Kriegel, H.P., Sander, J., Wimmer,M. and Xu, X., Incremental Clustering for Mining in a Data Warehousing Environment, Proc.of the 24 th VLDB Conference, New York, USA, 1998 8  Garofalakis, M., Rastogi, R., Seshadri, S. And Shim K., Data Mining and the Web: Past, Present and Future Proceedings of WIDM 99 Kansas, US, ACM 1999 9  Mignet, L., Barbosa, D. and Veltri, P., The XML web : a first study, In Proceedings of the 12 th  International Conference on WWW, 500-510 2003   Nayak, R., Xu, S., XCLS: A Fast and Effective Clustering Algorithm for Heterogeneous XML Documents, In Proceedings of the 10 th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, Singapore, LNCS 3918, 2006   Rusu, L.I., Rahayu, W. and Taniar, D., A methodology for Building XML Data Warehouses International Journal of Data warehousing Mining, 1\(2 67-92, 2005   Rusu, L.I., Rahayu, W. and Taniar D.,  Maintaining Versions of Dynamic XML Documents, In Proceedings of the 6th International Conference on Web Information Systems Engineering, New York NY, USA, November 20-22, 2005, LNCS 3806   Rusu, L.I., Rahayu, W. and Taniar, D., Warehousing Dynamic XML Documents, In Proceedings of the 8 th  International Conference on Data Warehousing and Knowledge Discovery \(DaWaK 2006 LNCS 4081 Springer, 175-184, 2006   Shen, Y. and Wang, B., Clustering Schemaless XML documents, CoopIS / DOA/ODBASE 2003, LNCS 2888, 767-784, Springer 2003   Yoon, J. P., Raghavan, V., Chakilam, V., and Kerschberg, L., BitCube: A Three-Dimensional Bitmap Indexing for XML Documents J. Intel. Inf Syst 17, 2-3 \(Dec. 2001\, 241-254   XML data repository, online at http www.cs.washington.edu / research / projects / xmltk xmldata  
456 
456 


5 Related Work There exists extensive previous work on both the mining of software repositories and on the use of clustering algorithms in software engineering This discussion focuses on the most similar and recent work in the area of software evolution Mining Software Repositories Our technique was partially inspired by the work of Zimmermann et al and Y ing et al 17 on the mining of association rules in change history As described in Section 1 we sought to expand the technique to be able to recommend larger but less precise clusters of elements to guide program navigation Bouktif et al also investigated how to recommend cochanges in software development As opposed to the work cited above Bouktif et al used change patterns instead of association rules Also their approach does not attempt to reconstruct transactions and can consider associated 002les that were changed in different transactions ChangeDistiller is a tool to classify changes in a transaction into 002ne-grained operations e.g addition of a method declaration and determines how strongly the change impacts other source code entities Our approach uses similar repository analysis techniques but is focused on providing task-related information as opposed to an overall assessment of a system's evolution Finally repository mining can also be used to detect aspects in the code In this conte xt aspects are recurring sets of changed elements that exhibit a regular structure Aspects differ from the clusters we detect in the regular structure they exhibit which may not necessarily align with the code that is investigated as part of change tasks Clustering Analysis The classical application of clustering for reverse engineering involves grouping software entities based on an analysis of various relations between pairs of entities of a given version of the system Despite its long and rich history  e xperimentation with this approach continues to this day For example Andreopoulos et al combined static and dynamic information K uhn et al used a te xtual similarity measure as the clustering relation and Christl et al used clustering to assist iterative semi-automated reverse engineering The main dif ferences b e tween most clusteringbased reverse engineering techniques and the subject of our investigation is that the entities we cluster are transactions rather than software entities in a single version of a system For this reason our analysis is based strictly on the evolving parts of the system Both Kothari et al and V an ya et al 15 recently reported on their use of clustering to study the evolution of software systems The idea of using change clusters is the same in both works and ours but the purpose of the work is different Kothari et al use change clusters to uncover the types of changes that happened e.g feature addition maintenance etc during the history of a software system Vanya et al use change clusters which they call evolutionary clusters to guide the partitioning of a system that would increase the likelihood that the parts of the system would evolve independently In contrast we cluster transactions based on overlapping elements not 002les to recommend clusters to support program navigation as opposed to architectural-level assessment of the system Finally Hassan and Holt evaluated on 002ve open source systems the performance of several methods to indicate elements that should be modi\002ed together This study found that using historical co-change information as opposed to using simple static analysis or code layout offered the best results in terms of recall and precision The authors then tried to improve the results using 002ltering heuristics and found that keeping only the most frequently cochanged entities yielded the best results As opposed to our approach the evaluated 002ltering heuristics were only applied on entities recovered using association rules and not using clustering techniques The focus of their study was also more speci\002c as they recommend program elements that were strictly changed  as opposed to recommending elements that might be inspected by developers 6 Conclusion Developers often need to discover code that has been navigated in the past We investigated to what extent we can bene\002t from change clusters to guide program navigation We de\002ned change clusters as groups of elements that were part of transactions or change sets that had elements in common Our analysis of close to 12 years of software change data for a total of seven different open-source systems revealed that less than 12 of the changes we studied could have bene\002ted from change clusters We conclude that further efforts should thus focus on maximizing the quality of the match between the current task and past transactions rather than 002nding many potential matches Our study has already helped us in this goal by providing reliable evidence of the effectiveness of some 002ltering heuristics and useful insights for the development of additional heuristics Acknowledgments The authors thank Emily Hill and Jos  e Correa for their advice on the statistical tests and the anonymous reviewers for their helpful suggestions This work was supported by NSERC 
25 
25 
25 
25 
25 


References  B Andreopoulos A An V  Tzerpos and X W ang Multiple layer clustering of large software systems In Proc 12th Working Conf on Reverse Engineering  pages 79–88 2005  S Bouktif Y G Gu  eh  eneuc and G Antoniol Extracting change-patterns from cvs repositories In Proc 13th Working Conf on Reverse Engineering  pages 221–230 2006  S Breu and T  Zimmermann Mining aspects from v ersion history In Proc 21st IEEE/ACM Int'l Conf on Automated Software Engineering  pages 221–230 2006  A Christl R K oschk e and M.-A Store y  Equipping the re\003exion method with automated clustering In Proc 12th Working Conf on Reverse Engineering  pages 89–98 2005  D 020 Cubrani  c G C Murphy J Singer and K S Booth Hipikat A project memory for software development IEEE Transactions on Software Engineering  31\(6 465 2005  B Fluri and H C Gall Classifyi ng change types for qualifying change couplings In Proc 14th IEEE Int'l Conf on Program Comprehension  pages 35–45 2006  A E Hassan and R C Holt Replaying de v elopment history to assess the effectiveness of change propagation tools Empirical Software Engineering  11\(3 2006  D H Hutchens and V  R Basili System s tructure analysis Clustering with data bindings IEEE Transactions on Software Engineering  11\(8 1985  D Janzen and K De V older Na vig ating and querying code without getting lost In Proc 2nd Int'l Conf on AspectOriented Software Development  pages 178–187 2003  J K ot hari T  Denton A Shok ouf andeh S Mancoridis and A E Hassan Studying the evolution of software systems using change clusters In Proc 14th IEEE Int'l Conf on Program Comprehension  pages 46–55 2006  A K uhn S Ducasse and T  G  021rba Enriching reverse engineering with semantic clustering In Proc 12th Working Conf on Reverse Engineering  pages 133–142 2005  M P  Robillard T opology analysis of softw are dependencies ACM Transactions on Software Engineering and Methodology  2008 To appear  M P  Robillard and P  Mangg ala Reusing program in v estigation knowledge for code understanding In Proc 16th IEEE Int'l Conf on Program Comprehension  pages 202 211 2008  J Sillito G Murph y  and K De V older Questions programmers ask during software evolution tasks In Proc 14th ACM SIGSOFT Int'l Symposium on the Foundations of Software Engineering  pages 23–34 2006  A V an ya L Ho\003and S Klusener  P  v an de Laar and H van Vliet Assessing software archives with evolutionary clusters In Proc 16th IEEE Int'l Conf on Program Comprehension  pages 192–201 2008  N W ilde and M C Scully  Softw are reconnaissance Mapping program features to code Software Maintenance Research and Practice  7:49–62 1995  A T  Y ing G C Murph y  R Ng and M C Chu-Carroll Predicting source code changes by mining change history IEEE Transactions on Software Engineering  30\(9 586 2004  A Zeller  The future of programming en vironments Integration synergy and assistance In Proceedings of the 29th International Conference on Software Engineering The Future of Software Engineering  pages 316–325 2007  T  Zimmermann and P  W eißgerber  Preprocessing C VS data for 002ne-grained analysis In Proc 1st Int'l Workshop on Mining Software Repositories  pages 2–6 May 2004  T  Zimmermann P  W eißgerber  S Diehl and A Zeller  Mining version histories to guide software changes In Proc 26th ACM/IEEE Int'l Conf on Software Engineering  pages 563–572 2004 A Clustering Algorithm This algorithm is not sensitive to whether a given program element exists or not in a given version of a program For example if method m exists in one version it is considered a valid program element even if it is removed in a later version In the rest of this section we use the term program element to refer to the uniquely identifying representation of the element e.g a Java fully-quali\002ed name Let T be a transaction modeled as a set of program elements changed together during the history of a software system Let T be a sequence of transactions In this algorithm a cluster is also modeled as a set of elements 1 Input  T  A sequence of transactions 2 Parameter  M IN O VERLAP  A positive non-zero value indicating the minimum overlap between two transactions in a cluster 3 Var  C  A set of clusters initially empty 4 for all T i 2 T do 5 MaxOverlap  0 6 MaxIndex  000 1 7 for all C j 2 C do 8 if j C j  T i j  MaxOverlap then 9 MaxOverlap  j C j  T i j 10 MaxIndex  j 11 end if 12 end for 13 if MaxIndex   0  MaxOverlap 025 M IN O VERLAP  then 14 C MaxIndex   C MaxIndex  T i  15 else 16 NewCluster  T i 17 C  C  f NewCluster g 18 end if 19 end for 20 return C B Systems Analyzed System home pages last veri\002ed 7 May 2008 Ant ant.apache.org Azureus azureus.sourceforge.net Hibernate www.hibernate.org JDT-Core www.eclipse.org/jdt/core JDT-UI www.eclipse.org/jdt/ui Spring springframework.org Xerces xerces.apache.org 
26 
26 
26 
26 
26 


