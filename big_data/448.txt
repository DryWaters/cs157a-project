Optimized Disjunctive Association Rules via Sampling J Elble 001 Department of Economics University of Rochester Rochester NY Email jelble@troi.cc.rochester.edu C Heeren 201 Department of omputer Science University of Illinois at Urbana-Champaign Urbana IL Email heeren@cs.uiuc.edu L Pitt 202 Department of omputer Science University of Illinois at Urbana-Champaign Urbana IL Email pitt@uiuc.edu Abstract The problem of 223nding optimized support association rules for a ingle numerical attribute where the optimized region is a union of k disjoint intervals from the range of 
the attribute is investigated The 223rst polynomial time algorithm for the problem of 223nding uch a region maximizing support and meeting a minimum cumulative con\223dence threshold is given Because the algorithm is not practical an ostensibly easier more constrained version of the problem is considered Experiments demonstrate that the best extant lgorithm for the constrained version has signi\223cant performance degradation on both a synthetic model of patterned data nd on real world data sets Running the lgorithm on a mall random sample is proposed as a means of obtaining near optimal results with high probability Theoretical bounds on su\001cient sample size to achieve a given performance level are proved and rapid convergence on synthetic and real-world data is validated experimentally 
1 Optimized Association Rules The search for meaningful patterns in large datasets is one of the main foci of data mining research A wellinvestigated type f pattern is the association rule introduced in a r ule o f t he form X 1 X 2 X s 001 C  meaning in essence that 215when X 1 X s all hold about a datum then C tends to hold also\216 Often data is so-called market-basket\216 data and X i and C are  
001 Some of the work was performed at the National Center for Supercomputing Applications supported in part by NSF grant ACI 96-19019 201 Supported n part by SF grant IIS-9907483 202 Supported n part by SF grant IIS-9907483 boolean variables indicating the presence of some item in a customer\220s order Such a rule is useful when it has su\001cient support and con\223dence  The support of a rule is the number or percentage of records for which the antecedent X 1 X s holds The con\223dence is a measure of the implication\220s validity 205 the percentage of 
time the consequent C holds given that the antecdent holds The literature is rich with work on how to e\002ectively 223nd such rules and variants survey in The idea f optimized association rules was introduced by Fukuda et l Consider a s ingle l arge relation The motivation is that in many settings a user is interested in a speci\223c attribute c of the data as a consequent in an association rule e.g c corresponds to 215good credit risk\216 as well as a collection of antecedent attributes X i whose values are likely to be predictive of c  The goal is to 223nd one or more in 
stantiations of the attributes X i that will result in rules satisfying constraints n support and con\223dence For example in a credit-history database X 1 002 single  married  divorced  might represent marital status X 2 and X 3 might be numeric attributes representing income and age respectively and c 002 0  1  might represent creditworthiness based on past history r by expert judgement A discovered rule in the existing database such as X 
1 married 003 X 2  50 K 003 X 3 002 30  60 001 c 1 might be useful in making decisions about new cases Let D be a large relation For any S 004 D  de\223ne 200 The support of S sup S  is just  S   200 The set S    s 002 S  s.c 1   200 The con\223dence 
of S conf S  is sup S    sup S  200 If S 1 S k are subsets of D thenthe cumulative support and cumulative con\223dence of the sets are respectively the support and con\223dence of the union 005 k i 1 S i  Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


Results vary based on the form of the set S Fukuda et al 223 rst d e\223ned t he problem and c onsidered t he case that S is speci\223ed by a single numeric attribute and later generalized the problem to that of two numeric attributes 4 3 E\001cien t a lgorithms w ere g iv en for maximizing the cumulative support given a minimum con\223dence threshold for the dual problem of maximizing the cumulative con\223dence given a minimum support threshold and for maximizing the 215gain\216 of a rule Rastogi  Shim generalized the problem to allow unions of categorical attributes  and o f o ne or two quantitative attributes  Z elenk o ga v e a p olynomial time algorithm for unions of categorical attributes W ijsen a nd Meersman o\002er an in v e stigation into the complexity of variants of the problem  To better understand our results and their relationship to past work we need just a couple more de\223nitions Let D be a data set of cardinality n witheachdatum d containing a single real-valued attribute d.r and a boolean 215consequent\216 attribute d.c Withoutlossof generality assume that each lue d.r 002 1  2 n   re\224ecting he possibility of t most n distinct values An interval I is just a pair  a b ith a 006 b  and represents the set D  a b   d 002 D  a 006 d.r 006 b   Given a data set D as described above a value minconf 002 0   a nd a p ositiv e i n t eger k the maxsupport-min-cumulative-con\223dence problem is to 223nd a collection I  I 1 I 2 I k of k intervals with cumulative con\223dence conf I  is t least minconf and so that the cumulative support sup I  is maximized Alternatively the max-support-min-independent-con\223dence problemisto\223ndacollection I of k intervals so that each interval of I has con\223dence at least minconf and so that the cumulative support sup I  is maximized Our contributions are s follows 200 We provide the 223rst polynomial time algorithm for the max-support-min-cumulative-con\223dence problem for a single numeric attribute 200 For the max-support-min-independent-con\223dence problem we analyze the performance of an algorithm of Rastogi  Shim which s shown to scale well on random unpatterned data We test the lgorithm on several rld data sets and 223nd that the algorithm does not scale as well but rather behaves as it does on a proposed synthetic model of random patterned data 200 We propose sampling as a preprocessing phase for any lgorithm addressing either the independent or the cumulative con\223dence problem and derive theoretical bounds on the sample size su\001cient to achieve near-optimal performance Because the bounds are independent f the size of the riginal data set dramatic speedups re ssible 200 Experimental results demonstrate the utility of the sampling approach a sample of size less than 500 s su\001cient in ll cases to obtain a solution within 5 of optimal n both real world and on synthetic data models 2 Maximum support meeting minimum cumulative con\336dence In a reduction from the N P-hard W eigh ted Set Cover problem to the max-support-min-cumulativecon\223dence problem is given showing that this latter problem is P-hard The reduction translates weights from the set cover problem directly into support and con\223dence values for the max-support problem rather than creating a data set D that realizes these values Because the NP-hardness relies on very large values of support and con\223dence that cannot be realized by a polynomially-sized database it does not neccessarily apply to the problem in which a database is given as part of the input But it is exactly this latter problem that is of interest to us it is only natural to allow an algorithm for mining a database to at least scan the data spending time polynomial in n   D  and k The NP-hardness result of  do es not p reclude t he existence of an algorithm that takes time poly n k  We give just such an algorithhm admittedly impractical due to the degree of the polynomial Note that a trivial exhaustive algorithm solves the problem in time O  n 2 k  The support and con\223dence for the O  001 n 2 k 002  O  n 2 k  k tuples can be computed and the optimal solution selected However we would like an algorithm that runs in time polynomial in k not exponential We describe a O  n 5 k  algorithm leaving its improvement as an open challenge We use dynamic programming in a manner similar to that of taking a dv an tage of a b ound on con\223dence c as was done in  F or ev ery i n t erv a l  i j  n n data points and every number of disjunctions l the maximum support l interval on  i j  e xceeding m inimum con\223dence has con\223dence tally c where c is in the range 1 n  F o r e v e ry p o ssible con\223dence l ev el c and for every interval  i j  and f or ev ery d isjunct s ize l  store the largest rt region exceeding minconf  001  together with its support Call that value S  i j  l,c  To solve the problem we want S 1 b  k,n\001  Consider S  i j   1 c  From Fukuda et al  t hese n 3 values can each be computed in O  n Forthe general case subsequent values of l can be computed in terms of previous values for each interval and con\223dence level Speci\223cally notice that every solution for l intervals on  i j  o f c on\223dence c can be partitioned into a disjoint union of an optimal solution for 1 interval on  i m  w ith c on\223dence c 001  and an optimal solution for l 212 1 ls on  m 1 j  w ith c on\223dence c 212 c 001  for some choice of m and c 001  The task then is to 223nd thebestchoiceof m c 001 002 1 n  W e kno w of no other way o do this than searching through all their possible values Thus is the s et of maxim u m s upport among the n 2 unions of the form  S  i m   1 c 001  005 S  m 1 j  l 212 1 c 212 c 001    where 1 006 m c 001 006 n  Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


The total running time f the algorithm is O  n 5 k  which corresponds to O  n 2  update time for n 3 k tabulated values 3 Maximum support meeting minimum independent con\336dence 3.1 The RS Algorithm Rastogi and Shim  attac k a more tractable v e rsion of the problem in which a disjoint union is found consisting entirely of intervals each meeting the minimum con\223dence constraint Because of the dditional constraint hey are able to obtain a relatively fast algorithm denoted 215RS\216 whose performance will be of interest in our discussion of the e\001cacy of sampling The S algorithm like Fukuda et al\220s 4 3 a ssumes initially that the data has been pre-bucketed so that the input consists of n buckets b 1 b n whereeach bucket corresponds to a particular discretized value of the quantitative attribute\220s range The problem is to 223nd k intervals each of which contains some contiguous collection of buckets Each bucket may be thought of as an indivisible unit corresponding to a weighted point with weight equal to the support of the bucket and with 215con\223dence\216 value c arealnumberinthe interval 0   a s o pp osed to the set  0  1  Theyassume also that the buckets have been sorted in order of increasing value of the attribute The RS algorithm has three phases 1 Preprocess the data by merging all adjacent buckets that have con\223dence at least minconf here is never any reason to include one of these without its su\001ciently high con\223dence neighbors they will all be in the same interval 2 Find in linear time ll 215partition points\216 A partition point is one that is not contained in any interval of con\223dence t least minconf Note that no interval in the solution to he problem can contain any of these partition points 3 Solve the k interval problem separately on the subproblems between the partition points and merge the solutions to obtain a global choice of the k best intervals Suppose that the n original buckets are divided into m subgroups of n 1 n 2 n m buckets separated by m 212 1 r more partition points with 003 i n i  b  Then if b max max  n i  is the number of buckets in the largest subproblem lgorithm RS takes time O  b 2 max mk  mk 2  The 223rst term is for solving the m subproblems and the second term is the cost f combining the m local solutions into one lobal solution If there are many partition points the data can splinter into a linear number f constant-sized sets In this case the run time will be dominated by the second term mk 2  nd will grow only linearly with the size of the data set as shown in  O n t he other h and if there are few partition points the data will not lend itself to the divide-and-conquer pproach since b max will be large  O  n  In this case the run time will be dominated by the 223rst term and will increase quadratically with the size f the data set The question is 215Where between hese two extremes will the lgorithm\220s performance fall in practice 216 In order to empirically test the utility of the preprocessing and divide and conquer approach in t he algorithm was run on synthetic data Binned data was created as follows For each b i 1 006 i 006 n  a support value s i and con\223dence value c i were chosen to represent s i data points c i 267 s i of which corresponded to data points x satisfying the consequent requirement x.c 1 The values s i were chosen uniformly in 0  2  n  and then normalized so that the sum of all s i was 1 The con\223dence values c i were chosen uniformly in 0   A s a consequence here is no correlation between he con\223dence values in bucket b i and b i 1  The net result of using random data was that the partitioning algorithm performed very well because as noted by the authors long stretches of high con\223dence were unlikely The algorithm was able to handle problems with up to 100,000 initial buckets in less than 15 seconds Even with this much data b max s no more han 20 In other rds linear growth was bserved It can be shown that if minconf  5 with probability approaching 1 exponentially quickly the random data fragments into constant-sized chunks separated by partition points explaining the dramatic improvement o\002ered by the partitioning algorithm On the other hand if minconf  5 212 002 thenwithprobability pproaching 1 exponentially fast the single interval encompassing all of the random data is an optimal solution and a constant ime lgorithm su\001ces choose the entire data set It is only when minconf is close to  5 the mean con\223dence that the algorithm\220s performance degrades as observed empirically in  3.2 Our Data Models Described While the RS algorithm was shown to provide dramatic speedups this was only for a data model corresponding to uniform random data It is reasonable to assert that real rld data sets of interest uld be less random than the arti\223cial data generated in  I t i s precisely the randomness of their data model that creates the fortuitous partitioning whose result is speedy run times The rue utility f ny method can nly be demonstrated empirically by performance analysis n many di\002erent instances of real-world data On the other hand the availability of parameterized data models may be used to advantage to gain insight into how an algorithm\220s performance depends on various data characteristics Toward this end to better understand how the S lgorithm performs we develop a model of random patterned data and carefully examine the role played by various parameters controlling the data disProceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


 0 0.2 0.4 0.6 0.8 1 1.2 0 100 200 300 400 500 600 domain labels confidence Series1 Figure  Example f patterned data tribution and the e\002ect on the run time of the algorithm We also consider the behavior of the algorithm on real world data selected from a census database prediction of marital status from income and from forestry data prediction f round cover type from elevation For the patterned synthetic data and for the real rld data sets we 223nd that he dramatic improvement observed on unpatterned random data is not typical and that the behavior of the RS algorithm on the real world data more closely matches our patterned data model for which quadratic runtime dominates We conclude that such adverse data is not rare 1 Random patterned data Our synthetic data was generated according to the following method Each bucket received support as in W e assumed ho w e v e r that con\223dence w ould be a function of he numeric attribute value i.e the bucket number so that low con\223dence buckets would tend to cluster as uld high con\223dence buckets We generated a simple triangular wave-form with rying numbers of peaks and lleys A typical peak had con\223dence values rising from 2 to 8 and then falling back to 2 Call this multipeak piecewise linear function function f  We randomly generated the con\223dence for bucket i to be a binomially distributed ratio with mean f  i  and N  20 trials Figure 1 shows a typical example data set enerated in this manner We do not mean to suggest that this synthetic model is 215the right\216 one indeed we do not believe such a thing exists The point is to consider how the lgorithm performs when parameters such as the size and frequency of peaks change empirically validating the 1 o pecial e\001ort was made to 223nd hese data sets hey were selected due to their availability their size and the presence of a quantitative attribute with many possible values that was likely o be predictive of an associated categorical attribute 0 0.2 0.4 0.6 0.8 1 1.2 2500 3000 3500 4000 4500 5000 domain labels - income confidence s ingle census data Figure  Census data tradeo\002 suggested between the two terms in the complexity bounds for the divide and conquer algorithm We generated data with n  2000 4000 8000 and 16000 buckets distinct domain lues for the quantitative attribute This was done in two ways r 215\223xed peaks\216 data the number of peaks for these datasets were respectively n 50 n 100 n 200 and n 400 thus keeping the number of peaks constant t 40 and hence the peak width increasing from 50 to 400 For 215\223xed peak width\216 data the number of peaks was set at n 200 for each data set so that the peak width was 223xed at 200 and the number of peaks varied from 0 to 80 Because each valley was likely o contain at least one partition point and the neighborhood around each peak was unlikely to do so this e\002ectively allowed control of the parameters b max here the peak width and m here the number of peaks in the RS algorithm Real-world data In addition to the patterned synthetic data we extracted a real world data set from the 1999 census data for the Los Angeles/Long Beach area The total-familyincome and marital-status attributes were projected from a database f 88443 households with positive income levels The marital-status ttribute s summarized by a boolean attribute whose lues were 0 for a married head of household and 1 otherwise Finally the data was bucketed ccording to the total-familyincome so that one record existed for every value in the income domain A record consisted of an income level together with a tally of the number of households with that income nd a tally of the number f unmarried heads f household The largest 223nal data set consisted of 11990 records This same procedure was followed n smaller census data sets to achieve smaller domains for our tests Figure 2 shows a small portion of the census data set As a simple test point for scienti\223c data in this conProceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


 0 0.2 0.4 0.6 0.8 1 1.2 0 100000 200000 300000 400000 500000 600000 domain labels \(n=1978 elevations confidence l odgepole pine forestry data Figure 3 Forestry data text we tested the RS algorithm on the forestry data available from the UCI KDD archive at http://kdd.ics.uci.edu We used a similar procedure to that of the census data to extract the elevation and forest cover type for 581,012 soil samples The tion and cover type attributes were projected from the set f bservations The cover type attribute s condensed into a single lean attribute whose lue was 1 if the forest cover was 215lodgepole pine\216 and 0 otherwise Finally the data was bucketed according to unique elevations Each record in the 223nal data set consisted of an elevation followed by a tally of the number of observations that were taken at that elevation and another tally of the number of samples at that elevation classi\223ed as cover type 215lodgepole pine\216 The 223nal data set had only 1978 elements tions Figure 3 shows the entire forestry data set 3.3 Results of RS lgorithm on patterned and census data We ran 2 the RS algorithm on each patterned data set for k  5 intervals We 223x the value of k sincewe are most interested in characteristics of the data and how hose characteristics 002ect running time r each setting of the parameters each experiment was run 30 times and the average run times were recorded s expected because there were stretches of correlated con\223dence within each peak there were sequences of buck2 The experiments presented in this paper were implemented in Perl and performed on an iBook laptop The goal of our investigation was not to determine the maximum problem sizes that can be handled by the algorithm based on the limits of current technology but rather to see how well the algorithm scales with various parameters Our esults are o\001 by a 223xed constant factor when compared to implementations with faster hardware and software Our run times are consistently 50 times slower than that of the implementation used by Rastogi  Shim      0 50 100 150 200 250 300 350 400 450 0 4000 8000 12000 16000 domain size \(n run t ime  s  fixed peak width n/200  fixed peaks \(40  uniform random data  census data  forestry data Figure  ize s run ime minconf  65 k=5 ets that neither merged together in the initial preprocessing phase nor resulted in a partition point Figure 4 gives the running time for 223nding 5 intervals as a function of the domain size We expect the most salient parameter to be the peak width  b max  As discussed earlier this lue is a small constant for the uniform random data so run time scales linearly with a small slope as the amount of data increases For the 223xed peak-width data  n 200 we also have a linear increase in running time but with a larger coe\001cient as b max is likely to be near 200 However when the number of peaks is 223xed the number of buckets b max within each peak grows with he amount of data and the algorithm exhibits its characteristic quadratic increase in running time Also shown are the running times for the census datasets Our admittedly subjective evaluation is that this data behaves more like 223xed-peak data hence exhibits quadratic running time Also striking is the amount of time required for 223nding he best 5 intervals on the fewer than 2000 records of the forestry data set Referring back to Figure 3 we see that the data has a single peak nd it appears that not much can be gained by partitioning the peak into 223ve interls so as to exclude some small amount of 215uncon\223dent\216 data We suspect that the algorithm spends a lot of time 223ghting the law of diminishing returns This view is supported by our sampling results in the next section where most of the gain in support can be obtained by king at relatively few records Another view is given in Figure 5 where for a 223xed data size runtime is plotted against the number of peaks in the synthetic data set The census and uniform random lines are plotted for comparison and do not vary s the number of peaks increases for a 223xed data size the number of domain lues within a peak decreases  b max  and the running time decreases quadratically consistent with the complexity bounds Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


   0 10 20 30 40 50 60 70 80 90 0 50 100 number of peaks run t ime  s  synthetic peaked data  uniform random data, unknown # of peaks  census data unknown # of peaks Figure  Shape s run time n=4000 minconf=0.65 k=5    0.00 100.00 200.00 300.00 400.00 500.00 600.00 700.00 0.5 0.6 0.7 0.8 0.9 1 minconf runtime  s  census data n=3888 uniform random data  peaked data, 20 peaks Figure 6 Minconf vs run time n=4000 k=3 of the RS algorithm Finally Figure 6 shows the e\002ects of varying the demanded minimum con\223dence while holding the other parameters 223xed Rastogi  Shim noted that as minconf approached the mean con\223dence 5 for their uniform random data the runtime increased dramatically This phenomenon holds s well for our synthetic patterned data as well as for the census data which had mean 63 explaining why the curve is shifted It seems reasonable that for any data set there would be many possible feasible intervals with con\223dence close to that of the mean so setting minconf near the mean is inviting an algorithm to consider perhaps far more possibilities than is practical with perhaps only nominal gain in support 4 Sampling We consider sampling as an alternative means for e\001ciently handling data with large domains and for which the divide and conquer algorithm o\002ers no signi\223cant speed up While a worst-case quadratic or any polynomial time algorithm is theoretically acceptable in data mining pplications it is often impractical to implement an algorithm which requires more than a 223xed number one say of scans of the data Absent a linear time exact algorithm we turn to sampling to reduce the problem size and when we run the S lgorithm on a sample-driven coarsening of the data thereby reduce the computation time Our data summarization technique is similar to that employed by Fukuda et al Let D denote the original data  D   n andlet B denote a coarsening of D  Create B as follows 200 Select a random sample of buckets S from D accordingtosupport  S   s  This can be done in linear time using the reservoir sampling of  200 If sample is unsorted sort ccording to the numerical attribute of interest in time O  s log s  200 Partition the original data into a new set of buckets B  whose bucket boundaries are hose in S  Thus B consists of the buckets in S  together with a new bucket for every interval between sampled buckets  B 006 2 s 1.Thisrequirestime O  n log s  if the data is unsorted O  n  otherwise The burning question is how many examples do we need to assure that the support of an optimal solution on our sample is likely o deviate only slightly from the support of an optimal solution on the whole data set We choose a dense enough sample so that with high probability any l containing more than some small user speci\223ed mount f data is likely to be sampled nd thus when the actual data is compiled into the new buckets no interval\220s support deviates from its original support by more than this small amount Then this new set f bucketed data is used as input to algorithm RS The choice of parameters gives the user a tradeo\002 between the error hat she is willing to tolerate and the amount of time required by the RS algorithm on the sample De\336nition 1 An 002 signi\223cant interval I  is n interval with sup I  002  We se a sample f su\001cient size from D to assure with high probability that a bucket is sampled for every 002 signi\223cant interval in D Insodoing,weassure whp that the buckets representing the accumulation of data between sampled buckets have weight no more than 002  Call he resulting coarsened data B an 002 coarsening of D  Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


Lemma 2 Given an independent uniform sample S of size s from a relation R containing n rows if s 007 max 4  002 log 2 003  16 002 log 13 002  then Pr  b an 002 signi\223cant interval I with I 004 S  t   003  Proof llows from the fact that the VC-dimension of the class of intervals is 2 and the su\001cient sample size unds given by Blumer et l Consider any interval I on the original data D and notice that the support of I canbeestimatedonthe bucketed data B by selecting the largest l I 001  I 001 004 I  so that the endpoints of interval I 001 fall on bucket boundaries Then sup I  212 sup I 001   2 002 since the 002 error can occur at each of he 2 endpoints ow let I  005 I k be a k interval and let I 001  005 I 001 k where I 001 k 004 I k  are the intervals of largest support whose endpoints fall on sampled bucket boundaries as in the single interval case Then because there are k intervals sup I  212 sup I 001   2 k\002 Thuswehave Lemma 3 Given a data set D and B an 001 2 k coarsening of D  for any k interval I on D and I 001 on B with I 001 argmax i 002 I sup i   sup I  212 sup I 001  002  We have assured that with probability more than 1 212 003  the support of any k interval can be computed on our coarsened data set with error no more han 002  That is for association rule F  A 002 I 001 C sup A 002 I  can be approximated by some interval I 001 on the set of buckets B so that sup A 002 I  212 sup A 002 I 001  002  Notice that the argument holds for the condition A 002 I 003 C as well That is there exists an interval I 001 002 B such that sup A 002 I 003 C  212 sup A 002 I 001 003 C   2 002 Thisobservation aids in analyzing the error in estimating con\223dence Finally we are in a position to bound the error incurred by running the RS algorithm on our coarsened data set B  The optimal solution to the problem is a measure of support There are two types of error in support we can incur First we may overlook small intervals We have bounded the magnitude f this type of error by 002  Second algorithm RS searches among intervals exceeding the minimum con\223dence threshold for the optimal solution Danger lies in the case we fail to consider some su\001ciently con\223dent interval because we underestimate the con\223dence of that interval on the bucketed data If an interval is actually con\223dent and we cannot detect it we are at risk of tossing out large chunks of support Instead of quantifying this neglected support we create a new slightly lower con\223dence bound that these de\223cient intervals will almost certainly meet and so they will be considered by the algorithm for inclusion in the optimal solution to the problem In the next lemma we show that any con\223dent interval on the data set D can be pproximated by an interval of slightly lower con\223dence on the coarsened data set B  ence the theoretical results show that for a suitably smaller con\223dence threshold which necessarily depends on he optimal support with high probability only the 223rst kind of error where small intervals re overlooked can occur Lemma 4 For any k interval I on D let k interval I 001 004 I be the largest subinterval of I on B  Suppose the minimum con\223dence threshold is 001  and that sup I  212 sup I 001  002  Then with probability greater than 1 212 003  conf I  007 001 001 conf I 001  007 001 212 002 1 212 001  sup I 001   The proof which is largely lgebraic is ilable from the authors Here\220s an intuitive explanation The fact that the original region meets minimum con\223dence is an indication that the average con\223dence over the region is no less than the threshold Since the sampled portion f the region is de\223cient the unsampled region must have excess con\223dence How de\223cient can the con\223dence of the sample be At most the unsampled region has 1 212 001  002 excess where the 1 212 001 risesbecause we are measuring excess above 001  This excess in terms of average con\223dence for the sample must be normalized by the support of the sample In e\002ect we are redistributing the excess across a region of size sup I 001  The previous lemmas combine to give the following bounds demonstrating that small error  002  in total support can be achieved on a sample if a suitably smaller value of con\223dence depending on the support is chosen Our empirical results demonstrate however that this reduction in the minconf 001 is not necessary in practice Theorem 5 Let RS  D 001  denote the maximum support k interval exceeding on\223dence 001 on data set D Let B   b 1 b 2 b m  be an 001 2 k coarsening of D  Then sup RS  D 001  212 sup RS  B 001 212 002 1 212 001  sup RS  D 001   002 These nalytic results imply a reasonable practical approach to sampling with the only complication arising from the fact that the error in con\223dence depends on the support of the interval of interest The sampling bounds are independent of the size of the domain and thus tradeo\002s are simply between sample size and accuracy In the next section we demonstrate that convergence to optimal solutions occur surprisingly quickly across di\002erent data sets The bounds we obtained are signi\223cantly di\002erent than those given by Toivonen and Zaki e t a l 11 for example In both of these papers a goal is to estimate the support of an itemset so as to determine with high probability whether or not it quali\223es as 215frequent\216 by meeting a minimum support threshold n this context dependence n the error parameter 002 is quadratic that is there is a factor of 1 001 2  whereas our bound improves this by relying nly linearly on 1 001 The Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


key di\002erence is that we are using sampling not to obtain uniformly good estimates f the support f intervals but rather to select admissible endpoints of intervals The actual support f these intervals re computed exactly by a linear scan f the original data So the error induced by sampling is not from inaccurate estimation of support but rather solely from the necessity of representing an arbitrary interval using only 215admissible\216 intervals with endpoints in the sample RS Algorithm run on sampled data Our sampling experiments were conducted as follows for a data set D  and for sample size s and for 30 repetitions a sample of s buckets was randomly selected from D and the optimized rt set was found on the sample The maximum support discovered among the 30 runs is compared to optimal We performed the following experiments using our synthetic nd real rld data models 200 223xed peak width  n 200 varied data size n 002  2000  4000  8000  16000   200 223xed data size  n  4000 varied number of peaks p 002 n 400 n 200 n 100 n 50   200 223xed number of peaks  p  40 varied data size n 002 2000  4000  8000  16000   200 Census varied n 002 1836  3888  6661  11990   200 Forestry 223xed n  1978 All of our experiments demonstrate that convergence to the optimal support is quite rapid for all data sizes and independent of the variability in our synthetic data The results are remarkably consistent with more than 95 convergence occurring for all sample sizes larger than 500 The same results are observed for the census data as well as for the forest cover data We show here a typical convergence graph from our experiments  Real Data 0.90 0.92 0.94 0.96 0.98 1.00 1.02 0 500 1000 1500 2000 sample size fraction of optimal 1836 records, census 3888 records, census 6661 records, census 11990 records, census forestry data These experimental results re consistent with the theoretical results derived in the previous section Indeed the theory promises that convergence to optimal for a particular level of minconf 001 willoccur,givenarelaxation in 001  Our experiments indicate that such a relaxation is unnecessary When viewed together he theoretical nd experimental results demonstrated here 002er sampling s a reasonable approach to data reduction in the case of optimized support association rule 223nding The theoretical results suggest the appropriate tradeo\002s between accuracy and sample size to be considered by the practitioner References  R  A gra w a l T Imielinski and A N Sw ami Mining association rules between sets of items in large databases In P Buneman and S Jajodia editors Proceedingsofthe 1993 ACM SIGMOD International Conference on Management of Data  pages 207\226216 Washington D.C 26\226 28 1993  A  B lumer A Ehrenfeuc h t  D  H aussler and M K W armuth Learnability and the Vapnik-Chervonenkis dimension Journal of he ACM JACM  36\(4 October 1989  T  F ukuda Y Morimoto S  Morishita and T kuyama Data mining using two-dimensional optimized association rules scheme algorithms and visualization In Proceedings of the 1996 ACM SIGMOD international conference on Management of data  pages 13\22623 ACM Press 1996  T  F ukuda Y Morimoto S  Morishita and T Tokuyama Mining optimized association rules for numeric attributes n Proceedings of the 336fteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems  pages 182\226191 ACM Press 1996  J  H ipp U G 250 untzer and G Nakhaeizadeh Algorithms for association rule mining 226 a general survey and comparison SIGKDD Explorations  2\(1 July 2000  R  R astogi and K Shim Mining optimized supp ort rules for numeric attributes Information Systems  26\(6 444 2001  R  R astogi and K  Shim Mining optimized asso ciation rules with categorical and numeric attributes Knowledge and ata Engineering  14\(1 2002  H  T oiv onen Sampling l arge databases for asso ciation rules n T  Vijayaraman A P Buchmann C Mohan and N L Sarda editors In Proc 1996 Int Conf Very Large Data Bases  pages 134\226145 Morgan Kaufman 09 1996  J  S  V itter Random sampling with a r eserv oir ACM Transactions on Mathematical Software  11\(1 Mar 1985  J Wijsen and R Meersman On the c omplexit y o f m ining quantitative association rules Data Mining and Knowledge Discovery  2\(3 1998  M J Zaki S  P arthasarath y  W  L i and M Ogihara Evaluation of sampling for data mining of association rules n 7th International Workshop on Research Issues in Data Engineering RIDE\32597  Birmingham UK Apr 1997  D Zelenk o Optimizing disjunctiv e asso ciation rules I n Proe.ofPKDD\32599,LectureNotesinComputerScience LNAI 1704  pages 204\226213 Springer-Verlag 1999 Proceedings of the Third IEEE Internati onal Conference on Data Mining \(ICDM\22203 0-7695-1978-4/03 $ 17.00 \251 2003 IEEE 


of the query expression without ha ving the global view of the in ten tion There is a big c hance that the enco ded pro cedure ma y not b e the b est w a y to compute the rules dep ending on the database instance F urthermore as w e understand it their prop osals require p oten tially large n um ber of name generation for relations and attributes The names that are needed are usually database dep enden t and th us p ossibly cannot b e gathered at query time An additional pro cess needs to b e completed to gather those v ariables b efore actual computations can b egin 5  9 Optimization Issues While it w as in tellectually c hallenging to dev elop a declarativ e expression for asso ciation rule mining from deductiv e databases there are sev eral op en issues with great promises for resolution In the w orst case the least xp oin tneedsto generate n 2 tuples in the rst pass alone when the database size is n  Theoretically  this can happ en only when eac h transaction in the database pro duces an in tersection no de and when they are not related b y subset-sup erset relationship In the second pass w e need to do n 4 computations and so on The question no w is can w e a v oid generating and p erhaps scanning some of these com binations as they will not lead to useful in tersections F or example the no de b 0 3 in gure 11 is redundan t A signican t dierence with apriori lik e systems is that our system generates all the item sets top do wn in the lattice without taking their candidacy as a large item set in to consideration Apriori on the other hand do es not generate an y no de if their subsets are not large item sets themselv es and thereb y prunes a large set of no des Optimization tec hniques that exploit this so called an ti-monotonicit y prop ert y of item set lattices similar to apriori could mak e all the dierence in our setup The k ey issue w ould b e ho ww e push the selection threshold minim um supp ort inside the top do wn computation of the no des in the lattice in our metho d F or the momen t and for the sak e of this discussion let us consider a higher supp ort threshold of 60 for the database T of gure 9 No w the l-en v elop e will b e the one sho wn in ligh ter dashed lines in gure 11 and the no des under this line will b e the large item sets Notice that no ww eha v eto discard no des ad 2 0 and d 0 2 to o This raises the question is it p ossible to utilize the supp ort and condence thresholds pro vided in the query and prune candidates for in tersection an y further Ideas similar to magic sets transformation 3  24 ma y be b orro w ed to address this issue The only problem is that pruning of an y no de dep ends on its supp ort coun t whic h ma y come at a later stage By then all no des ma y already ha v e b een computed and th us pushing selection conditions inside aggregate op erator ma y b ecome non-trivial Sp ecial data structures and indexes ma y also aid in dev eloping faster metho ds to compute ecien t interse ction joins that w e ha v e utilized in this pap er W e lea v e these questions as op en issues that should be tak en up in the future F ortunately though there has been a v ast b o dy of researc h in optimizing Datalog programs including recursiv e programs suc h as the one w e ha v e used in this pap er and hence the new questions and researc h 5 Recall that their prop osal requires one to express the mining problem to the system using sev eral queries and up date statemen ts that utilizes information ab out the database con ten ts to ac hiev e its functionalit y  c hallenges that this prop osal raises for declarativ e mining ma y exploit some of these adv ances Needless to emphasize a declarativ e metho d preferably a formal one is desirable b ecause once w e understand the functioning of the system w e will then be able to select appropriate pro cedures dep ending on the instances to compute the seman tics of the program whic hw e kno wis in tended once w e establish the equiv alence of declarativ e and pro cedural seman tics of the system F ortunately  w e ha v e n umerous pro cedural metho ds for computing asso ciation rules whic h complemen t eac h other in terms of sp eed and database instances In fact that is what declarativ e systems or declarativit y buy us  a c hoice for the most ecien t and accurate pro cessing p ossible 10 Conclusion Our primary goal for this pap er has b een to demonstrate that mining asso ciation rules from an y rst-order kno wledge base is p ossible in a declarativ ew a y  without help from an y sp ecial to ols or mac hinery  and that w e can no wha v ea v ery in tuitiv e and simple program to do so W eha v esho wn that it is indeed p ossible to mine declarativ ekno wledge b y exploiting the existing mac hinery supp orted b ycon temp orary inference engines in programming languages e.g Prolog or kno wledge base systems e.g RelationLog XSB LD L  CORAL All w e require is that the engine b e able to supp ort set v alued terms grouping aggregate functions and set relational op erators for comparison functionalities whic hmostofthesesystemscurren tly supp ort W e ha v e also demonstrated that our formalism is grounded on a more mathematical foundation with formal prop erties on whic h the seman tics of the R ULES system rely  W e ha v e also raised sev eral op en issues related to eciency and query optimization whic h should b e our next order of business As future researc h w e plan to dev elop optimization tec hniques for mining queries that require non-trivial lo ok ahead and pruning tec hniques in aggregate functions The dev elopmen ts presen ted here also ha v e other signican t implications F or example it is no w p ossible to compute c hi square rules 4 using the building blo c ks pro vided b y our system Declarativ e computation of c hi square rules to our kno wledge has nev er b een attempted for the man y pro cedural concepts the computation of c hi square metho d relies on In a separate w ork 2 w e sho w that the coun ting metho d prop osed in this pap er can be eectiv ely utilized to generate the exp ectations needed in order to compute suc h rules rather easily  These are some of the issues w e plan to address in the near future The motiv ation imp ortance and the need for in tegrating data mining tec hnology with relational databases has b een addressed in sev eral articles suc h as 12  13 They con vincingly argue that without suc h in tegration data mining tec hnology ma y not nd itself in a viable p osition in the y ears to come T o b e a successful and feasible to ol for the analysis of business data in relational databases suc htec hnology m ust b e made a v ailable as part of database engines and as part of its declarativ e query language Our prop osal for declarativ e mining bears merit since it sheds ligh t on ho w rst order databases can be mined in a declarativ e and pro cedure indep enden t w a y so that the optimization issues can b e delegated to the underlying database engine Once suc h argumen ts are accepted sev eral systems 9 


related issues b ecome prime candidates for immediate atten tion F or example traditionally database systems supp orted declarativ e querying without the necessit y to care ab out the pro ceduralit y of the queries In this pap er w eha v e actually demonstrated that asso ciation rule mining can b e view ed as a Datalog query  It is immediate that a direct mapping from the Datalog expressions presen ted in this pap er to SQL can be dev elop ed with no problem at all W e can then rely on ecien t database pro cessing of the query in an optimized fashion Hence w ecomeclose to the essence of the visions expressed b y the leading database researc hers and practioners 12  References 1 Rak esh Agra w al and Ramakrishnan Srik an t F ast algorithms for mining asso ciation rules in large databases In VLDB  pages 487{499 1994 2 Anon ymous A declarativ e metho d for mining c hisquare rules from deductiv e databases T ec hnical rep ort Departmen t of Computer Science Anon ymous Univ ersit y USA F ebruary 2001 3 C Beeri and R Ramakrishnan On the po w er of magic In Pr o c e e dings of the 6th A CM Symp osium on Principles of Datab ase Systems  pages 269{283 1987 4 Sergey Brin Ra jeev Mot w ani and Craig Silv erstein Bey ond mark et bask ets Generalizing asso ciation rules to correlations In Pr o c A CM SIGMOD  pages 265 276 1997 5 D Chimen ti et al The LD L system protot yp e IEEE Journal on Data and Know le dge Engine ering  2\(1 90 1990 6 Jia w ei Han Jian P ei and Yiw en Yin Mining frequen t patterns without candidate generation In Pr o c A CM SIGMOD  pages 1{12 2000 7 Marcel Holsheimer Martin L Kersten Heikki Mannila and Hann uT oiv onen A p ersp ectiv e on databases and data mining In Pr o c of the sixth A CM SIGKDD Intl Conf  pages 150{155 Mon treal Queb ec 1995 8 Flip Korn Alexandros Labrinidis Y annis Kotidis and Christos F aloutsos Ratio rules A new paradigm for fast quan tiable data mining In Pr o c of 24th VLDB  pages 582{593 1998 9 Brian Len t Arun N Sw ami and Jennifer Widom Clustering asso ciation rules In Pr o c of the 3th ICDE  pages 220{231 1997 10 Mengc hi Liu Relationlog At yp ed extension to datalog with sets and tuples In John Llo yd editor Pr oc e e dings of the 12th International L o gic Pr o gr amming Symp osium  pages 83{97 P ortland Oregon Decem ber 1995 MIT Press 11 Rosa Meo Giusepp e Psaila and Stefano Ceri An extension to SQL for mining asso ciation rules Data Mining and Know le dge Disc overy  2\(2 1998 12 Amir Netz Sura jit Chaudh uri Je Bernhardt and Usama M F a yy ad In tegration of data mining with database tec hnology  In Pr o c e e dings of 26th VLDB  pages 719{722 2000 13 Amir Netz Sura jit Chaudh uri Usama M F a yy ad and Je Bernhardt In tegrating data mining with SQL databases In IEEE ICDE  2001 14 Ra ymond T Ng Laks V S Lakshmanan Jia w ei Han and Alex P ang Exploratory mining and pruning optimizations of constrained asso ciation rules In Pr o c A CM SIGMOD  pages 13{24 1998 15 Jong So o P ark Ming-Sy an Chen and Philip S Y u An eectiv e hash based algorithm for mining asso ciation rules In Pr o c A CM SIGMOD  pages 175{186 1995 16 Karthic k Ra jamani Alan Co x Bala Iy er and A tul Chadha Ecien t mining for asso ciation rules with relational database systems In Pr o c e e dings of the International Datab ase Engine ering and Applic ations Symp osium  pages 148{155 1999 17 R Ramakrishnan D Sriv asta v a and S Sudarshan CORAL  Con trol Relations and Logic In Pr o c of 18th VLDB Confer enc e  pages 238{250 1992 18 Konstan tinos F Sagonas T errance Swift and Da vid Scott W arren XSB as an ecien t deductiv e database engine In Pr o c of the A CM SIGMOD Intl Conf  pages 442{453 1994 19 Sunita Sara w agi Shib y Thomas and Rak esh Agra w al In tegrating mining with relational database systems Alternativ es and implications In Pr o c A CM SIGMOD  pages 343{354 1998 20 Ashok a Sa v asere Edw ard Omiecinski and Shamk an tB Nav athe An ecien t algorithm for mining asso ciation rules in large databases In Pr o c of 21th VLDB  pages 432{444 1995 21 Pradeep Sheno y  Ja y an t R Haritsa S Sudarshan Gaura v Bhalotia Ma y ank Ba w a and Dev a vrat Shah T urb o-c harging v ertical mining of large databases In A CM SIGMOD  pages 22{33 2000 22 Abraham Silb ersc hatz Henry F Korth and S Sudarshan Datab ase System Conc epts  McGra w-Hill third edition 1996 23 Shib y Thomas and Sunita Sara w agi Mining generalized asso ciation rules and sequen tial patterns using SQL queries In KDD  pages 344{348 1998 24 J D Ullman Principles of Datab ase and Know le dgeb ase Systems Part I II  Computer Science Press 1988 25 Mohammed J Zaki Generating non-redundan t association rules In Pr o c of the 6th A CM SIGKDD Intl Conf  Boston MA August 2000 1 0 


OM OM 006 OD8 01 012 014 016 018 02 022 False alarm demity Figure 9 Percentage of tracks lost within 200 seconds using three-scan assignment with PD  0.9 TI  O.ls Figure 11 T2  1.9s and T  Is ij  20 and 0  0.1 24 1 22  20  E fls 0  8l 16 0 n 14  12  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 T1/12 PD Average track life of three-scan assignment with PD varying TI  0-ls T2  1.9s T  Is X  0.02 ij LO and   0.1 mareuvenng index Figure 12 Percentage of lost tracks of 4-D assipment in 200 seconds with maneuvering index varying X  0.01 Ti  0.1 T2  1.9s and T  IS PD  0.98 Figure 10 Percentage of lost tracks of 4-D assignment in 200 SeoDllCls with TI and T2 varying PD  0.98 X  0.02 q 20 and 0  0.1 4-1607 


Figure 13 Average gate size for Kalman filter Figure is relative as compared to nq and curves are parametrized by ij/r with unit-time between each pair of samples 1.2 Iy I 1.1 0.5 I A CRLB for he unifm samiina I  0.4 0.35 d 3 03 i7 3 0.25 0 0.M 0.04 0.06 008 0.1 0.12 0.14 0.16 0.18 0.2 False A!am DemW V I    Figure 14 CramerRao Lower Boundfor Mean Square Error of uniform and nonuniform sampling schemes with Ti  O.ls T2  1.9s T  IS PD  0.9 ij  5 and U  0.25 1 unifon sampling r-ls ked i non-uniform sampling loge inlewi I ti non-uniform sampling shod interva I 0.9 0.8 I Figure 15 MSE comparison of three-scan assignment with Ti and T2 varying I'D  1 X  0.01 ij  20 and U  0.1 4-1608 


Plenary Panel Session 30 XML Databases   Moderator: Michael Carey, IBM Almaden Research Center USA Panelists Adam Bosworth, Microsoft Corporation USA David De Witt University of Wisconsin-Madison, USA Alon Levy University of Washington USA Bruce Lindsay IBM Almaden Research Center USA Jennifer Widom Stanford University USA Demo Session 1 Web Query Optimizer  661 V Zadorozhny L Bright L Raschid T Urhan and M Vidal ReQueSS: Relational Querying of Semi-structured Data  664 R Sunderraman The IDEAL Approach to Internet-Based Negotiation for E-Business  666 J Hammer C Huang Y Huang C Pluempitiwiriyawej M Lee H Li L Wang Y Liu and S Su READY A High Performance Event Notification Service  668 R Gruber B Krishnamurthy, and E Panagos A Multimedia Information Server with Mixed Workload Scheduling  670 G Nerjes DISIMA An Object-Oriented Approach to Developing an Image Database System  672 V Oria T Ozsu P Iglinski B Xu and L Cheng Demo Session 2 The Collaboration Management Infrastructure  677 H Schuster D Baker A Cichocki D Georgakopoulos and M Rusinkiewicz Assisting the Integration of Taxonomic Data The LITCHI Toolkit  679 I Sutherland J Robinson S Brandt A Jones S Embury W Gray R White and F Bisby TheaterLoc: Using Information Integration Technology to Rapidly Build Virtual Applications  681 G. Barish Y.4 Chen D Dipasquo, C Knoblock S Minton I Muslea and C Shahabi Lineage Tracing in a Data Warehousing System  683 Y Cui and J Widom xiii 


The Mentor-Lite Prototype A Light-Weight Workflow Management System  685 J Weissenfels M Gillmann 0 Roth, G Shegalov and W Wonner Location Prediction and Queries for Tracking Moving Objects  687 0 Wolfson B Xu and S Chamberlain Semiorder Database for Complex Activity Recognition in Multi-Sensory Environments  689 S Bhonsle A Gupta S Santini and R Jain Tutorial 1 Web Information Retrieval  693 M Henzinger Tutorial 2 Mobile and Wireless Database Access for Pervasive Computing  694 P Chrysanthis and E Pitoura Tutorial 3 Data Mining with Decision Trees  696 J Gehrke Tutorial 4 Directories Managing Data for Networked Applications  697 D Srivastava Tutorial 5 Indexing High-Dimensional Spaces Database Support for Next Decade\222s Applications  698 S Berchtold and D Keim xiv 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


