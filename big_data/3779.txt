FUZZY NEURAL PATTERN RECOGNITION M.Enge1 M.Leclercq C Pradels UFR Sciences University of Rouen 76821 Mont Saint Aigan Cedex France CNRS G.R Commande Symbolique et neuromimetique phone 35.14.65.89 Abstract  The common techniques of neural networks seem to can be apply perfectly to the classification and to the pattern recognition but the learning can sometime be long and the complexity of the final network very big An association between the neural networks and the fuzzy way seems feasible and looks to can make significantly easier the learning period and to allow also to simplify its structure The fuzzy logic expresses the knowledge in an explicit manner by its rules of inferences, while the neural networks assimilate that knowledge in 
the weight during the learning the knowledge is so here implicit In merging these two approaching we can introduce the connectionnist techniques in the fuzzy logic operating in the best way for every underlying analogies between these two approaching It is that last possibility that is handled in that work We can by that association build a neural network of simple structure where we meet the stage of the fuzzy reasoning The work here described tries in a first time to identify a paraboloid by a simple structure in the aim to well understand the developed association in the aim to apply it subsequently to the recognition of characters I~monucrrox Fuzzy Logic is used in a lot of fields Controllers pattern 
recognition  since its apparition in 1974 Some applications have even been marketed in Japan in particular in controlled appliances But its used in pattern recognition remains modest results not being very promoted At the opposite, neural networks NNs make the object of a lot of researches They bring up a different approach than linguistic or algorithc systems Their two main qualities are their parallel structure which offer a good noise resistivity and their ability to learn from examples NNs have nowadays a large field of applications from model and control systems prediction to pattern recognition Since little time, researchers work about NNs and Fuzzy Logic association  In fact both tq to reproduce the human reasoning There are several 
ways to include their own qualities  We can introduce Fuzzy Logic in the elaboration of the network or put connections into fuzz rules but it could be possible to make a fuzzy-neural network This is this thud solution that we've study The target of this article is to suggest a feasible association for pattern recognition and more specifically for characters recognition Beyond NNs we 11 tackle in a first time, the Fuzq Logic linguistic to display in a second time a fuzzy-neural network A thud part consists of testing this association by applications in particular by the recognition of a paraboloid and by a type characters recognition approach THE YEXJRX NETWORKS Introduction The target 
of NNs is at once to understand and then to try to reproduce the human brain working whch always intrigues a lot of researchers by its incredible quickness and its fabulous ability to work in spite of the lack of explicit instructions. Then we've tried to fax 35.14.66.18 reproduce the mechanics of human brain apprenticeshp thanks to an artificial network  There are different productions of NNs distributed in two basic groups  layers networks and loops networks We'll do not here study these different types We 11 just describe in part layers networks and more especially multi-layers networks which will be used for our association bfulti-luyers network The multi-layers network is characterized by a neural distribution in layers We distinguish three 
sorts of layers  The input layer which transmits only data froin input to neural of the first hidden layer One or more hidden layers which extract characteristics from input vector which square with the treatment of the inpattemation The output layer which give the answer of the network Each neural is connected either to the nearer layer then we talk about simple layers networks or to any layer located downstream these networks are then called not simple layers networks The connections between neural could also be done from a layer to itself but they almost look like loops networks connections i-lpprenticeship The big problem of 
NNs is the apprenticeship of weights ai because all the knowledge is memorized in the synaptic coefficients which are fixed during the apprenticeship step  We distinguish two sorts of apprenticeship  The supervised apprenticeship where input output associations are presented to the network by a teacher and the not-supervised apprenticeship where we donY know outputs in relation to inputs. There are different learning methods the minor square method, the retropropagation of gradient, the Windrow-Hoff method  whch target is to minimize a criterion \(often called cost function by weights adjustment We won't describe here these methods THE FUZZY LOGIC Introduction The human reasoning bases generally on imperfect knowledge  either because we have a doubt about their validity then they are uncertain perhaps she I1 come tomorrow 
or because we meet a difficult to express them clearly, then they are inexplicit she 11 come tomorrow but I don't how when Probabilistic methods grapple to uncertain events when the Fuzzy Logic is a concept whlch allows to treat the human inexplicit reasoning Fuzzy sets theory permits to generalize events from natural causes appealing to mathematic tools It presents at least 2 interests  It allows to modelize inexplicit or vague knowledge  It is the only mean to treat at the same time numeric and symbolic knowledge Here is a scheme see figure 1 representing the general methodolom for the Fuzzy Logic utilization  nz~+~ld Inference Wuzzfiatimk Figure 1 0-7803-1328-3/94$03.000 1994 IEEE 1275 


Piczzi$cution 221This is a stage of transition from a numerical to a symbolic field  This stage is indispensable as soon as we want to manipulate precise or not physical data thanks to the fuzzy set theory It allows for one thmg to determinate the belonging function to a fuzzy set Fuzzy set Here is the definition of a fuzq set according to Given a set X countable or not, and x an element of X then a ZADEH  fuzzy set A member of X is a set of couples such as equation  1  vs EX   x  dx  1 1 with dx the belonging function which takes data in the interval O 1 1 Ifthis interval is reduced to the I value then the set 2224 is an ordinmy set Belonging Function It determinates 3 parts   One area where referential elements belong completely to a fuzzy set  a cc\(x  One area where elements are excluded 3 cc\(x  0  One fwzy area \(called transition area  where belonging to the set is estimated at different degrees  3 O<cc\(x The belonging function is the belonging degree of s to the fuzzy set. Its part consists in precising numerically the meaning of a fuzzy set expressed in principle in linguistic pattem This is the transition of numerical to symbolic field There are dffaent belonging hctions such as function in trapezium pattem  triangular function  Gaussian function \(continually derivable Pattem and parameters of different belonging functions must be adjusted in particular case to treat Inference It consists in treating the inpattemation by fmy rules Fuzq rules They reveal a liilk between elementaq fuzzy propositions  223The error IS big andpositive then the control is zero\223   They pattem the mference systems core used in control and they relate the reasoning a hunan could make about an imperfectly defined knowledge A fuvy rule breaks up in 2 parts  The premises  The place where one or several fuzzy propositions are set out The conclusion or the consequence  The answer A rule describes a causal llnk between the premises of data si and the conclusion thlnks to fuzzy implication operators Fuzz\implication operaror.s There are different operators of fwq implication  T-nomi T-conorm Minimum of Mandani and Zadeh Maximum of 1,ukasiewicz and Giles product of Goguen   The 2 operators usually used in the applications of the fuzzy sets theory are the Max and Minus functions  Nevertheless T-norm functions could to be used as conjunction operators instead of minus functions, while T-cononns could take place of the Max. in disjunctions I 3fuzzrfication Unfuzzification is the operation which allows to get through a linguistic datum pattem perception, a numerical variable physically applicable  this is the opposite step of fuzzification There are several feasible methods including the Max and the center of gravity. When the conclusion of the rule is distinct specific and sure \(that is when all of the fuzzy rules facing one to another have a no-fuzzy conclusion unfuzzification is then implicit Example  If the outside temperature is high efuzzy proposition Then turn off the heating distinct specific and 3 The fuzzification will give the belonging degree which allows to obtain the consequence thanks to fuzzy rules The unkification will give the numerical result sure decision Association We can represent thls analyze in the pattem as follow re 65  Figure 2 The first hdden layer neural represent different areas of the Fuzzy Logic Then the different outputs of th~s layer give us the belonging degree correspondent to the different areas. We introduce in the second hidden layer, rules of the Fuzzy Logic inference And then we obtain the rule IF x AND IF y THEN Currsequence The output layer will give us the result wluch could be obtained by unfuzzification The scheme above see figure3 hghlight the mass parallelism between the fuzzy reasoning and the coilnectionnist perceptioii We recopze NNs in the weights incorporation by apprenticeship  Figure 3 We can add more or less predefined weights which give a tipping between exDlicit knowledpe Fuzzy Logic The weights are all predefined and implicit reasoning Neural Networks  Weights are established b,y apprenticeship Conclusion This association seems to bring about the apprenticeship which is thus less complex \(some weights being preestablished and about the comprehension of the knowledge course in NNs raising gradually the degree of complexity we could see the evolution of the reasoning APPLICATIONS In a first time we have test this fuzzy-neural network to identify a paraboloid R~co~~rrros OF A CLRVE  represented by the equation 2 y  s12  x22 2 This network will be composed of  two input xl and s.2 such as   1 I 1 and  1 z 1 1276 


 a output y  05 This application which gives a simple structure will allow us to well understand the Fuzzy-neural association for a subsequent implementation for characters recognition Specification First we must built the fuzzy-neural network correspondent at IS example  That's why we 11 follow Fuzzy Logic reasoning Fuzzification It's the matter to break up the sets composed by x1 and x2 in fuzzy areas x1 and x2 have the same probability on their set 1;+1 So there's no reason to privilege a part rather than another  That's why each area 1;+1 is decomposed in equal parts We have cut up the area in 5 equal parts  courses  the Zero area  center 0  the Big Positive area  center l  the Middle Positive  area  center 0.5  the Middle Negative  area  center \(-0.5  the "Big Negative  area  center 1  There are several n~ers to represent areas in Fuzzy Logic We have choose the Gaussian function which eliminates the problem of derivability important for weights apprenticeship of intersection of disymmetical area _._ and which presents the advantage to have no distinct cutting up at the area limits at the ends The equation of a Gaussian is like the model as follow see equation 3 y  exp[-\(an  by 3 and b a.m I h\(E with a ak The symmetrical curve of Gauss hction is then represented by mi  center of the fuzzy area i considered  localization of three parameters m whch correspond at  the area each parameter mi of the 5 fuzzy areas have been fixed as  m  I c3 center of the Big Negative area ml  0.5 c9 center of the Middle Negative area m2  0 a center of the "Zero area m3  0.5 c9 center of the Middle Positive" area m4  1 c3 center of the "Big Positive area E  residual va lue allowed at the  distance from the Gaussian center We've chosen F 0.1 for   0.5  Then E and mC will be constant because all the areas are identical bi  a.mi bi is then fixed for each area a being a constant The output of a neural in the frst hidden layer must answer to equation 4  output-neural j  exp[-\(a.input-neural j  bj y  4 That well corresponds to a belonging degree of an input x i to ttUs area A neural making in general two tasks well specified l.A weighted sum 2.A processing will be represented by the next pattern  with equation 5 G  exp 2'1  Figure 4 The weights a and b seefigure 4 are here preestablished So it won't be weights apprenticeship between the input layer and the first hidden one. We see that each neural of the first hidden layer will have 2 input  xi and 1 We have then created 5 areas by input data that we 11 connected two by two this which corresponds at the creation of 25 neural 5x5 in the second hidden layer This one corresponds to the fuzzy mference rule  If  AND  ITEN  Inference  we had to choose a rule of inference adaptable to our problem. That's why we have executed a reasoning which gave us the type of decision to take in contact with the different areas The 5 Gaussian functions which determine the belonging degree are V~IY displayed into the space Then a point of the space will belong to several Gaussian functions but the more even to insignificant way We d try to eliminate these residues That's why  at once the belonging degree will be added to each component Then the insignificant data will be deleted thanks to the Max.hction see equation 6 If we standardize this equation we recognize the rule of inference of Lukasiewic such as equation 7 7 9 In fact there are several possibilities We could take a T-conorm for example but this case seems to suit to our problem because   It's simple and easily computable  It fits perfectly itself to neural case   weighted input sums oequation 8  following by the f function o equation 9 For the moment, we restrict to this inference rule Now we incorporate this method to the network The activation of the neural which implements the Max function of Lukasiewicz is realized by a weighted sum corresponding of the equation 8 The processing is then represented by the hction f equation 9 The implementation of this rule could be represented by the next scheme seefigure 3 Figure 5 The synaptic weights are preestablished and equals to 1 1 and 1 seefigure 4 There won't be weights learning between the first and the second hidden layer Moreover, we notice each neural of th~s hidden layer have three inputs Unjiuzzijication That's the fkzification opposite operation that is a symbolic variable will be transpattem in a numerical language In general this function comply at the end of the processing so it's situated at the output layer The two more standard methods use either the Maximum or the Barycenter function Taking the last one permits to take all the receipted inpatternations into account whereas the Maximum method just take the more important inpattemation into consideration. That's why we have choose to take the center of gravity method As there is just one output see equation 2 the unfuification hold only one neural We reach then the final network seefigure 2 The difference between Fuzzy Logic and NNs is the add of weights between the different links The output y will take different 1277 


inpatternations into account \(barycenter but with various degrees thanks to the weights Which data to give to these synapses? That's the part of the apprenticeshp Apprenticeship  The apprenticeship consists in determining the weights in order that the network respond correctly to our problem two inputs XI  x2 and one output y such as y resolve the equation 2 We'll use a supervised method which the target is to minimize a cost function by weights progressive adjustment The weights raised during the transition between the input layer and the fist hidden layer and between the fist hidden layer and the second one are preestablished by the Fuzzy Logic rules Apprenticeship is situated between the last hdden layer and the output neural The first hidden layer represents fuzzy areas set As there are two inputs a representation of these areas could be done in a two dimensions space Each parcel will correspond to an intersection between a Gaussian for the 1 input and another for the x2 one. The second hidden layer executes at once the sum of the equation 8 which is equivalent to a bumbs field 3D Gaussian function filed down by the Max function We notice here there will have intersections between the different bumbs The barycenter method for unfuification will allow to take the different areas superposition into account Weights will modify the height of the bumbs so as to draw closer in the representation of a parabolofd Th~s 3D representation allows us to estimate the value of different weights in particular at the ends and in the center so that the bumbs field tends towards a paraboloid The weights of bumbs center in l,l 1,-l l,l 1,-1 will approximately be equal to 2  They 11 probably be slightly below on account of the barycenter method which will take the neighboring bumbs into consideration In the same way, the weights of the central bumb m=O will be nearer to zero We conipared these weights with different apprenticeship methods Comparison of different apprenticeship method At once we tq to measure the error E between the NNs output and the wished one see equation IO We assign it a cost function J we'll try to minimize by an apprenticeshp algorithm E  wished-output  Nns-output 10 The target of this part is to determinate the best leanling algorithm for our me In fact no method seems to prevail We have then tested the different methods we know In a fust time we have been interested by the lowers square method The cost function is equal to the equation I I J=\(d-y E  square Error this equation IS equivalent to equation I 0 11 The minimization of this criterion corresponds to try to fmd the weights value which cancels out the cost derivative. We then come up against a inversion problem of the matrix contained the different second hidden layer output values Another prestige method used for the NNs apprenticeship is the Gradient retropropagation method It minimizes the same function than Lower Square method at about coefficient The difference is situated in level of the weights modification  The Lower Square method tries to minimize the function for the weights set so that the Gradient retropropagation method works on the w-eights taken individually The Windrow Hoff method is another learning method of the same kind of the Gradient retropropagation one which more to minimize the error takes the previous weights variation into account. That's why this method is commonly called method of the retropropagation of the gradient of the moment The advantage using this method instead of the Gradient retropropagation method is because between two weights modifications, the inputs change In order to visualize their advantages and their draw backs  we have tested these two methods simultaneously Analysis of different apprenticeship methods results After the network has been studied we wonder about the real advantage of the baryccnter method during the unfuification In fact this method don't make a linear interpolation between the different bumbs" but rather add a non-linearity into our processes We have think that a simple weighted sum would accurately smoothed our field to bumbs We have then testing these two cases Moreover the quick convergence of OUT network induced us to question ourselves about the initialization of weights In order to check the good working for each learning method we've voluntarily decided to initialize the weights at a random value contained between 0 and 10 Th~s interval has judiciously been chosen to check the convergence known that the final values set of different weights is contained approximately between 0 and 2 The superior value of the interval has been chosen for not increase too much the duration of the apprenticeship An idea as regard size of results is the same for most of the parameters So we can't draw any conclusion regarding the number of iterations We notice that however the method weights values are practically identical The error variation is because every the weights haven't yet reached their ended value The barycenter method using Windrow Hoff always gives a middle error bellow 10 6 It seems the best method of apprenticeshp It would probably increase the base of recognition to rule on the recognition level Possible Im rovements Our filzzy-neural network gives good results  Ixvel f not recognition reach about to Nevertheless we tried to fmd feasible mutations to improve our system Trend to the NNs One advantage of NNs is a good immunity to failures thanks to the repartition of the knowledge In our network we lost th~s benefice It happens then naturally to add connections between the different layers But the only feasible links adjunctions could intelligently be done between two hidden layers This mutation entails necessary the modifiable weights number increasing The Retropropagation of Gradient method for the connections adjustment must be extended to the previous layer Then the correspondent equation must be generalized But this correction requires the utilization of the derivative of the function of the second hidden layer Now the function Maximum not being continually derivable on its studied set, it needs being smooth by an equivalent but derivable function As hs function is necessarily contained into the O;1 set a good approximation consists in using the sigmoid function shifted fonvard right beforehand The problem as our opinion is that the add of links lnhibits in part the fuzzy reasoning  we tend then towards a nearly neural network. The association won't be there interesting except for the network construction Rather than adding connections which weigh down the network we could modify the weights preestablished by the Fuvy Logic We can  Modify the weights situated between the two hidden layers That would consist in giving more or less importance to some fuzzy areas In fact that entails filing down to different ficld of bumbs by the Max function which data won't belonging to 10 1  set. We can suppose that the second hidden layer will tend touards already to a approximation crude of the paraboloi'd or rather she'll carry out a final curve adjustment estimated by the output layer weights A draw back then appears  That's the introduction of not linearity into the network In fact this weights modification will be done individually on each bumb and it will have then discontinuities in level of the bumbs intersection Modify the input layer weights. That would consist in adjusting the coefficients a and b of the tiaussian function of the fwzification This correction allows then possibly another cutting out of the working interval in fuzzy areas this whch will be in part equivalent touse a scale factor Ihs improvement is the only one solution proposed which don't hit the fuzzy constituting of the network Modify either the 1278 


input layer weights and the weights situated between the two hidden layers, which would well strengthen eighter the fuzzy side and the neural side of our network We could modify too the number of fuzzy areas of the first hidden layer It will be interesting analysis obtained results In fact the number of bumbs 25 seems slight to describe all the paraboloid function by a well smooth curve the field will always be a little granular\More the number of bumbs is more w-e could act on the curve In adding under fuzzy sets, we would then reduce the error A comparative study of these diverse improvements and possibly of their association will be profitable Conclusion We've obtained very good results middle Level of no-recognition about  middle learning error of IO6 for a number of iteration contained between 4 and 50  The quick apprenticeship  I5 minutes maximum allows UT to test different methods. The results show the fuzzy-neural association is positive not only by the reached results but also by the 3D-representation of the network RECOGNlTION TO CHARACTERS Introduction The recognition of characters is the subject for many researches fl particular in level of handwritten characters The association neural-fuzzy seems to bring about good results for the form recognition and could then be applied in case of characters Nevertheless this recognition is very powerful by connectionist methods although the results are less interesting with the Fwq Logic In fact, the learning of classical methods of neural networks could sometime be long and the complexity of final network very big It must be then study the interest of a so network for the recognition of characters. At first these tests will be applied to a mono-fount mono-size recogrution to simpli@ the problem Yetwork's structure To determinate a network it is necessary at once to do a study of that we want obtain tj outputs of network of that we have as inputs then on the stnicture of network network's outputs It must be in less as so many outputs as wished classes the different letters of the alphabet and added eventually the figures We shall add another class of rejection for the subjects which have not been recognized A class could be active output=l or passive output But one among the advantages of neural networks is that a class could be more or less active. Each outputs of network will be then contained between 0 and 1 It must be then another processing to decide the valid class when several are actives In case of equality, we shall choose another class that we shall call confused class \(the network does not know to take a decision\We can also complicate the method  IF several classes have nearby outputs c a term to make explicit may-be with the Fwzy Logic  to the maximal value THEN to do another processing to differentiate the different classes The processing whch was differentiating the confused classes could be made by the device of other neuro-fuzzy networks which would function in a parallel way with the principal network Our system will be then composed of several cells in parallel each of them would be of simple composition and would make a specific processing. Each of networks could then have in output a boundary number of classes containing several characters An interconnection between them in a decision's cell would allow then to select the opportune character That would require less outputs for each networks the cell of decision treating then the interconnection thanks to the Fuzzy Logic IF AND THEN _ Network's input In order to build the network, it is necessary to define the inputs exactly we shall enter the characters that we want recognize. That is certainly the heart of problem to the recognition because it is necessaq to define: The number of inputs which will constitute our network The perception of used characters The difficulty comes Gom the nearly freedom of choice We must research the criterion which seems to be the best adapted in problem so that the system obtain a good recognition ratio The alone obligation comes from subsequent processing of data by the fuzzy methodology IF  AND   THEN  It must be then defined inputs from which we can calculate the degree of belonging to a set In a first time we have decide to not use a preprocessing of data and then to work directly with the pixels of picture In order to use the previously done work we have wanted use networks with two inputs It comes then of course to mind that these two inputs could express the coordinates of a pixel of the letter; Each letter will be then described by a succession of lighted" pixels The work is then made in series" in the aim to use only that two inputs. The rule of inference could be  IF coordx AND coordY THEN select ion the classes XYZ Rut a problem comes then How to memorize the information It could easily be made by the device of a counter by class which counts the times when number of that the class is active we shall choose then the one whose the counter will be maximal  with a eventual processing for two classes having the same probability Cf previous paragraph inconveniences appear here The time of processing  it is necessary to read pixel after pixel all the whole picture image even if we treat only the lighted" pixels The network has only that two inputs But there are as many ways as pixels constituting each letters That would render the network very complex in spite of its two inputs. We must then: treat the picture in a parallel way  3 to decrease the number of ways\cut it in areas and not in pixels   to decrease the time of processing We can then calculate the coordinates of barycenter of pixels of the letter to recognize for each areas. Each area would have two inputs corresponding to the coordinates xl and x2 of barycenter The processing will then be made by pixel in each area but as the sub-networks will be parallel, the time of processing Mill be so decreased The problem is to determinate the number of necessary and sufficient cuts to differentiate the characters one from the others eventuallv using fuzzy areas that is with intersections\to cut each areas in fwzy subsets The pixels of the neighborhood of outline of the picture do not mostly correspond to the character to recopze The cut-out of the picture must then be thmner around the center of the picture To determinate the number of divisions it is firstly necessary to analyze the set of characters  classes to recognize Another method that the we could use for the recognition of characters would consist to use a preprocessing which build an histogram of the picture Each class will be then the recognition of a curve representing the hstogram We should not use then the Fuzzy Logic for the preprocessing but we recognize very heavily the recognition of curves done in the previous chapter. It must then as many networks as characters to recognize. We could also use these two methods in parallel. We have then in input  The barycenter of each area  medium position The number of lighted pixels in each area  densit The set of these characteristics must allow to distinguish in a fine way all of the classes General structure The association of inputs and outputs gives us the nexT schemes A preprocessing by hstogranx A fwzy preprocessing when we have 1279 


This second method seem more complex but it is not obvious that the number of networks could be more considerable than with the first method It does a processing by fragment of each character then that the first method studies the picture in its set We could then envisage the processing of characters of different sizes the position of barycenter could be relative and of different fonts if 9 fragments between 10 recognize 1 character, even if the tenth select ion the rejection class then the character will be identified It seems then that the second method would be more attractive We must however make tests to verify these results and to test the neural fuzzy association in the recognition of characters Conclusion One of advantages of the neural-fuzz\association for the form recogrution will be the decomposition in sub-classes In fact the fuzq rule JF  AND  THEN  allows to determinate different classes thanks of characteristics IF caracteristicl AND caracteristic2 THEN Class X We can then envisage to elaborate the neuro-fuzq network in a rest of classes of more and more specialized Ilus method have the advantages to obtain a series of simples networks to know approximately the localization of the knowledge  Even if we smooth and add connections in each of sub-networks CONCLUSION The two advantages of neuro-fuzzy network were The simplification of the learning which is progressively established t layer after layer The approximate localization of the knowledge As it appears that the best method of learning is a method whch adapt itself perfectly to the connectionist networks \(without Fuzzy Logic The advantage in this field does not seem evident so many more, the learning in the neural networks is without big problems In an other part, the knowledge is deflected at the time of adding new others connections. We don't know more then where she is situated Two obvious advantages would be The decomposition in sub-classes \(for the recogrution of character The not-complexity of network  a more memory, bigger quickness   Tlus study could bring about an interest in front of the use of a such network to the recognition of handwitten characters BIBLIOGRXPHY Mr GLORENNEC Les reseaux nacro-jlous holutlfs  un pont entre leflou et le neuronal GR CSN,CNRS 92 France Mr 1,AMOTTE Lejlou  quelques de$nitions et applications Logique floue et reseaux de neurones ecole d'hiver de St Sorlin d'Arves.1993 Apprentissage et reseaux de neurones Pour la Science n"l81 Nov 92 Melle LEGENTIRE Reconstitution de mesurandes a l'aide d'un reseuu de neurones niulticouche appliquee a un systeme de mesure depression afibres optiques Rapport de DESS AU de Sept. 92 Melle PAKKF,R Reconnaissance des caracteres alphanumeriques par une analyse structurelle hdterarchique These de Mars 85 Mme BELLON  Mr BOSC  Mr PWE  Le boum drrjlou au Jupon  GR CSN,CNRS 92 France Mr D17BOIS  Typologie et utilisation de regles jloues  GR CSN,CNRS 92 France h4r GLORENNEC  Mr BARET  Mr BRUNET  Application of neuro,fuzzi networks to identification and control of nonlinear dvnamical xvstems La lettre du club logique floue n"2 Fev 92 Fuzzy rules for guiding reinforcmrenf leaming La lettre du club logique floue n"2 Fev N Wakami S Araki H Nomura Recent Applications of Fuzzy L'ogic to Home Appliances IEEE  93 pp155-160 D.T Pham D Karaboga Design of Neuromorphic Fuzz Controllers IEEE International Conference on System and Cybemetics 1993 pp103-107 P Freidel Mier techniques neuronales et conception Jloue Electronique #34, 1-94, pp73-76 Mr HERENJI  Mr KHEDKAR 92 1 1280 


An Extended Object-Oriented Database Approach to Networked Multimedia Applications  259 H Ishikawa K Kato, M Ono N Yoshizawa K Kubota and A Kanaya I Session 8 Video and Image Databases I Chair: Ming-Syan Chen National Taiwan University E Hwang V.S Subrahmanian and B Prabhakaran K.L Liu P Sistla C Yu and N Rishe Distributed Video Presentations  268 Query Processing in a Video Retrieval System  276 SEMCOG A Hybrid Object-based Image Database System and Its Modeling Language and Query  284 W.-S Li and K.S Candan I Panel Session 2 I Data Warehousing Lessons from Experience  294 Chair Patrick O\222Neil University of Massachusetts, Boston Panelists Richard Winter Winter Consulting Clark French Sybase I Dan Crowley Informix Software William McKenna Red Brick Systems I Industry Session 5 The Web and Databases 1 Chair Alex Buchmann T.H Darmstadt A Gupta A Gupta V Harinarayan and A Rajaraman Junglee Integrating Data of All Shapes and Sizes  296 Virtual Database Technology  297 Data-Intensive Intra  Internet Applications  Experiences Using Java and CORBA in the World Wide Web  302 J Sellentin and B Mitschang I Panel Session 3 I Migrating Legacy Databases and Applications  314 Chair Bhavani Thiraisingham MITRE Corporation Panelists Sandra Heiler GTE Laboratories Arnon Rosenthal MITRE Corporation Susan Malaika IBM Santa Teresa Lab Others TBD I Session 9 Architectural Issues in Data Mining Systems I Chair Lois Delcambre Oregon Graduate Center R Meo G Psaila and S Ceri A Tightly-Coupled Architecture for Data Mining  316  Vlll 


A Distribution-Based Clustering Algorithm for Mining in Large Spatial Databases  324 X Xu M Ester H.-P. Kriegel and J Sander Session 10 Cooperation and Workflow Management Chair Niki Pissinou University of Southwestern Louisiana M Kamath and K Ramamritham L Liu and C Pu H. Naacke G Gardarin, and A Tomasic Failure Handling and Coordinated Execution of Concurrent Workflows  334 Methodical Restructuring of Complex Workflow Activities  342 Leveraging Mediator Cost Models with Heterogeneous Data Sources  351 Session 11 Access Structures Chair: Thomas Mueck University of Wien A Henrich J Goldstein R Ramakrishnan and U Shaft P.M Aoki The LSDh-tree An Access Structure for Feature Vectors  362 Compressing Relations and Indexes  370 Generalizing usearch\224 in Generalized Search Trees  380 I Session 12 Association Rules and Dependencies Chair X Hu Sybase Inc Efficient Discovery of Functional and Approximate Dependencies Using Partitions  392 Y Huhtala J Karkkainen P. Porkka and H Toivonen C.C Aggarwal and P.S Yu B Ozden S Ramaswamy and A Silberschatz Online Generation of Association Rules  402 Cyclic Association Rules  412 lsession 13 Concurrency Control I Chair VGay Atluri, Rutgers University Asynchronous Version Advancement in a Distributed Three Version Database  424 H.V Jagadish I.S Mumick and M Rabinovich A.J Bernstein D.S Gerstl, W.-H hung and P.M Lewis K Chakrabarti and S Mehrotra Design and Performance of an Assertional Concurrency Control System  436 Dynamic Granular Locking Approach to Phantom Protection in R-trees  446 ix 


I Plenary Panel Session J Future Directions in Database Research  456 Chair Surajit Chaudhuri Microsoft Corporation Panelists Hector Garcia-Molina Stanford University Hank Korth, Bell Laboratories Guy Lohman IBM Almaden Research Center David Lomet Microsoft Research David Maier Oregon Graduate Institute I Session 14 Query Processing in Spatial Databases I Chair Sharma Chakravarthy University of Florida Processing Incremental Multidimensional Range Queries in a Direct Manipulation Visual Query Environment  458 High Dimensional Similarity Joins Algorithms and Performance Evaluation  466 S Hibino and E Rundensteiner N Koudas and K.C Sevcik Y Theodoridis E Stefanakis and T Sellis Cost Models for Join Queries in Spatial Databases  476 Mining Association Rules Anti-Skew Algorithms  486 J.-L Lin and M.H Dunham Mining for Strong Negative Associations in a Large Database of Customer Transactions  494 A Savasere E Omiecinski and S Navathe Mining Optimized Association Rules with Categorical and Numeric Attributes  503 R Rastogi and K Shim Chair: Anoop Singhal AT&T Laboratories S Venkataraman J.F Naughton and M Livny Remote Load-Sensitive Caching for Multi-Server Database Systems  514 DB-MAN A Distributed Database System Based on Database Migration in ATM Networks  522 T Hara K Harumoto M Tsukamoto and S Nishio S Banerjee and P.K Chrysanthis Network Latency Optimizations in Distributed Database Systems  532 I Session 17 Visualization of Multimedia Data I Chair Tiziana Catarci, Universita di Roma 223La Sapienza\224 W Chang D Murthy A Zhang and T.F Syeda-Mahmood Global Integration of Visual Databases  542 X 


The Alps at Your Fingertips Virtual Reality and Geoinformation Systeps  550 R Pajarola l Ohler P Stucki K Szabo and P Widmayer C Baral G. Gonzalez and T.C Son Design and Implementation of Display Specifications for Multimedia Answers  558 1 Session 18 Management of Objects I Chair: Arbee Chen National Tsing Hua University P Boncz A.N Wilschut, and M.L. Kersten C Zou B Salzberg, and R Ladin 0 Wolfson S Chamberlain S Dao L Jiang, and G. Mendei Flattening an Object Algebra to Provide Performance  568 Back to the Future Dynamic Hierarchical Clustering  578 Cost and Imprecision in Modeling the Position of Moving Objects  588 ROL A Prototype for Deductive and Object-Oriented Databases  598 A Graphical Editor for the Conceptual Design of Business Rules  599 The Active HYpermedia Delivery System AHYDS using the M Liu W Yu M Guo and R Shan P Lang W Obermair W Kraus and T Thalhammer PHASME Application-Oriented DBMS  600 F Andres and K. Ono S Chakravarthy and R Le S Mudumbai K Shah A Sheth K Parasuraman and C Bertram ECA Rule Support for Distributed Heterogeneous Environments  601 ZEBRA Image Access System  602 Author Index  603 xi 


11  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  data bindings domain-specific metric for assessing module interrelationship  interface errors errors arising out of interfacing software modules  Porter90 Predicting software  faults 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 From decision trees to association rules  Classifiers  223this and this\224 goes with that \223class\224  conclusions \(RHS\ limited to one class attribute  target very well defined  Association rules  223this and this\224 goes with \223that and that\224  conclusions \(RHS\ may be any number of attributes  But no overlap LHS and RHS  target wide open  Treatment learning  223this and this\224 goes with \223less bad and more good\224  223less\224,  \223more\224: compared to baseline  223bad\224, \223good\224: weighted classes Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


12  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Association rule learning  www.amazon.com  Customers who bought this book also bought  The Naked Sun by Isaac Asimov  The Caves of Steel by Isaac Asimov  I, Robot by Isaac Asimov  Robots and Empire by Isaac Asimov 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Support and confidence  Examples = D , containing items I  1: Bread, Milk 2: Beer Diaper Bread, Eggs 3: Beer Coke, Diaper Milk 4: Beer Bread, Diaper Milk 5: Coke, Bread, Diaper Milk  LHS  RHS = {Diaper,Milk  Beer  Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4  Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  Support-based pruningreject rules with s < mins  Check support before checking confidence Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


13  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Example of supportbased pruning 4 Bread 1 Eggs 4 Diaper 3 Beer 4 Milk 2 Coke Count 1Item 3 Beer,Diaper 3 Milk, Diaper 2 Milk,Beer 3 Bread, Diaper 2 Bread,Beer 3 Bread,Milk Count 2Item 2 Milk, Diaper Beer 3 Bread,Milk Diaper Count 3Item Support-based pruning 225 Min support =3 Ignore subsets of items of size N 225 only if N-1 support > min-support Without pruning 6 C 1  6 C 2  6 C 3 41 With pruning: 6 + 6 + 2 = 14 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Classifiers versus Association rules \(again  Classifiers  Assume entire example set can fit into RAM  Association rule learners  can handle very big data sets  Agraw  t he APRIORI alg o r i t h m   very large data sets  10,000,000 examples  843MB Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


14  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 The Data Mining Desiderata Bradley  Require one scan \(or less\ of the database if possible  On-line \223anytime\224 behavior  223best\224 is always available, with status information on progress, expected remaining time, etc. provided  Suspendable, stoppable, resumable  incremental  progress saved to resume a stopped job  Ability to incrementally incorporate additional data with existing models efficiently  Work within confines of a given limited RAM buffer  Ooops, good-bye traditional classifiers e.g. C4.5  Argued against by some  223Memory is cheap\224: [W A R2 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Treatment learning sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,          none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots outlook temp humidity wind hours on course A good attribute range 225 More frequent in good that bad 225 Weighted by 223distance\224good to bad 225 Normalized by total count 225 Summed for all good/bad class pairs Lots  none Lots  some Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


15  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 sunny, 85 86 false none \(2 1 2 sunny, 80 90 true none sunny, 72 95 false none rain 65 70 true,           none rain, 71 96 true none rain 70  false some \(2 2 4 rain, 68 80 false,  some rain, 75 80 false some sunny,      69 70 false lots    \(2 3 8 sunny,      75 70 true lots overcast,     83  false lots overcast,     64  true lots overcast,     72  true lots overcast,     81 75 false lots 0 1 2 3 attribute ranges with deltaf 4-2024681 conf1 225 treatments 002 attribute.range.conf1 > X 225 treatments|=N 225TAR2 = O\(2 N  225 fails for large N outlook temp humidity wind hours on course Conf1  outlo o k overc a s t   1 0   82  40    84  40   4 0 0  Lots  none Lots  some 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Treatments for golf 0 1 2 3 4 none some lots I f outl ook o verc as t Th en l o t s o f go l f  4 4  0 Least monitor watch the humidityalert if rising over 90 Least change pick a vacation location with overcast weather I f h u m i d i t y  90  97 Th en l o t s o f go l f  1 4  0 1 2 3 none some lots 0 1 2 3 4 5 6 none some lots If n o ch an ge Th en l o t s o f go l f  6 6 3 5  3  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


16  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 6.7 <= RM < 9.8 And 12.6 <= Ptratio 15.9 BEST ACTION 0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39 WORST ACTION BASELINE 500 examples  of bad--, bad, ok, good Stop staring at the scenery and tell me where to steer or what to dodge 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Require overall require2 require3 require5 require4     action1 action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 action2 fault2 fault3 fault1 JPL requirements Feather&Menzie Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


17  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study 99 proposed actions for deep space satellite design; 2 99 10 30 options Each row is one project plan action1, action2, action3,  \205   Cost,    Benefit 1 Y              Y             N,        \205   23200,  250 2           N              N             Y ,       \205   11400,  150 205..       \205             \205            \205        \205   \205         \205 Learnt 225 Do 16 225 Don\222t do 14 225 Ignore 66 options 225 c.f. genetic algorithms Each dot  is one randomly generated project plan 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Pr of tampering 0.02 Pr of fire 0.01 Pr of smoke  given [fi  0.90 Pr of smoke  given [fi  0.01 Pr of report given [exodus=ye 0.75 Pr of report given [exodus=no 0.01 Pr of exodus given [alarm=yes 0.88 Pr of exodus given [alarm=no 0.001 etc tampering fire alarm smoke exodus run away report hello, operator I want to report a fire 0.02 0.01 Use Bayesian analysis to update probabilities given new information Use Bayesian analysis to update probabilities given new information Bayesian Tuning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


18  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 tampering fire alarm smoke NO exodus report YES 0.50 was 0.02 0.03 was 0.01 Q1: What if there is a report, but no smoke Q1: What if there is a report, but no smoke Q2: What if there is a report, and smoke Q2: What if there is a report, and smoke tampering fire alarm smoke YES exodus 0.03 was 0.02 0.97 was 0.01 report YES Example from : [Poole98   p37 1 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl code/acp/bnet_t1.pl Bayesian Tuning 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Non-na\357ve model bayesian network Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


19  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Low testing effort EXPLAINS 1\ some observed operational defects  and 2\ low pre-release defects 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Ancestors  ancestor\(X,Y\:-parent\(X,Y  ancestor\(X,Y\:-parent\(X,Z\ancestor\(Z,Y  Lists  member\(X,[X|Z   member\(X,[Y|Z me mb er X Z   append X X   append\([X|X Y s X Z s  a ppe nd X s Ys Z s  Example Example action action hypothesis hypothesis p\(b,[b add clause p\(X,Y   specialize p\(X,[V p\(x,[a specialize p\(X,[X p\(b,[a add clause p\(X,[X p\(X,[V p\(X W Inductive Logic Programming Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


20  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 1. TRAINS GOING EAST 2. TRAINS GOING WEST 1 2 3 4 5 1 2 3 4 5 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ILP representation  Example eastbound\(t1  Background theory car\(t1,c1\      car\(t1,c2\       car\(t1,c3\.      car\(t1,c4 rectangle\(c1\  rectangle\(c2\     rectangle\(c3\.   rectangle\(c4 short\(c1\      long\(c2\.          short\(c3\       long\(c4 none\(c1\.        none\(c2\.          peaked\(c3\.      none\(c4 two_wheels\(c1\  three_wheels\(c2\two_wheels\(c3\two_wheels\(c4 load\(c1,l1\.     load\(c2,l2\       load\(c3,l3\    load\(c4,l4 circle\(l1\      hexagon\(l2\       triangle\(l3\    rectangle\(l4 one_load\(l1\  one_load\(l2\.      one_load\(l3\    three_loads\(l4  Output ne\(C Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


21  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting Correctness Almei NewID CN2 C4.5 C4.5_rule FOIL Accuracy 52 54 66 68 73 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 FOIL\222s best rule high\(A executable\(A,B maximum_statement_nesting_depth\(A,C lines_of_comments\(A,B commentsdivsize\(A,E n1\(A,F n2\(A,G less_or_equal\(E,F not less_or_equal\(B,G C <> 4 C <> 43 less_or_equal\(C,D High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


22  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Inside  some learners  neural nets  genetic algorithms  decision tree learners  association rule learners  treatment learners  bayesian tuning  inductive logic programming 225 sub-symbolic locally guided descent symbolic, global search 225 recursive diversity reduction 225 this goes with that CLASS 225 this goes with that 225 asses 225 a little model goes a long way 225 Horn clauses  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case studies predicting effort \(45 predicting faults \(51 model-based ML \(54 early lifecycle project planning \(60 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


23  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study How can we estimate earlier in the life cycle  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting development times in months\Srinivasan95 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


24  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Bayes for effort Chulani99  The COCOMO-II project  Open-source software cost estimation  Reuse vs effort XH : multiple product lines VH : across product lines H : across program N : across project L  : none  Regression over data from 83 software projects  Regression conflicted with \223Delphi values\224  Tune regression values using Delphi expectations 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 Low N H VH XH Delphi Regression Adjusted Da ta   reus e low e rs effo r t Ex pe ct e d  reus e incre a se  effo r t    251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 COCOMO-II \(1998\COCOMO-II \(1997 Pred\(30 Pred\(25 Pred\(20 Pred\(X 52 49 46 83 projects 63 59 54 161 projects 7561 68 55 63 48 161 projectsbased on Bayesian 161 projectsbased on Delphi Percentage of estimated effort within X of actual Conclusion data + delphi tuning\a Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


25  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Count the wi dge ts in the I n te r f ace to es ti m a te e f f o r t  Labels Edit Boxes Grid Boxes Check Boxes Buttons 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Neural Network Subsystem Pred\(25 MARE Buyer Admin 80 17.6 Buyer Client 80 14.6 Distribution Server 20 96.7 Supplier Client 90 12.2  12 Different Widgets Counted and associated with effort Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


26  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Case study: Predicting software 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Predicting software  faults Khoshgoftaar99 Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Whi c h d o g s di d not ba r k  225 42 attri b ute s  in dat a s e t 225 Only 6 in the l e arnt th e o ry Diffe re nt attri b ute s than b e fore 225 223c au se s f a u l t 224  do m a in s pec i f i c 225 Me thod for fin d ing fa ult s  gen e r a l Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


27  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Issue of generality  Specific conclusions may not apply to general projects  Proposal one  Intra-project learning  Lessons should generalize across the same developer methodology, application and tool set  Proposal two  Inter-project learning  Need larger training set  COCOMOII uses 161 projects  Note: two = N * one Khoshgoft good bad Tia bad good  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML Bratko89,Pearc Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


28  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Model-based ML simple e.g sum\(X,  Y Z sum   sum   sum\(0 0 0 sum 0  sum 0  sum\(0   sum\(0   sum  Any sum  Any if X >0 X\222=      if X < 0 0 if X= 0  switch\(State,Volts,Amps switch\(on,       0,     Any switch\(off,      Any,   0 blub\(Mode,Light,Volts,Amps bulb\(blown,dark, Any 0 bulb\(ok,     light   bulb\(ok,    light   bulb\(ok,    dark 0 0 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 A qualitative circuit go  :tell\('circ.data'\ go1, told go1 :functor\(X,circuit,9\ forall\(X, example\(X example\(circuit\(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3\classification\(B1,B2,B3,Class format\('~a,~a,~a,~a,~a,~a,~a~n Sw1,Sw2,Sw3,L1,L2,L3,Class  classification\(B1, B2, B3,Class needs 2 our of three bulbs working classification\( ok, ok, B3,   good classification\( ok, B2, ok,   good classification\( B1, ok, ok,   good classification\( B1, B2, B3,   bad Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


29  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Results from > 700 examples circ.names good,bad switch1: on, off switch2: on, off switch3: on, off bulb1: light, dark bulb2: light, dark bulb3: light, dark Command line c4.5 -f circ -m 2 W a t c hing bulb1 tells us th e rest Insight f ul  Or dull W a t c hing bulb1 tells us th e rest Insight f ul  Or dull 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 More Model-based ML Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


30  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ca n we r e v i s i t thos e warranti e s   Run 1 35,000 tions  Learn 1  Run 2 if Sw2c=off then 3264 tions  Learn 2  Run 2 if Sw2c=off n then 648 tions  Learn 3 Ca n\222t clos e  Sw3c warranty issu es No b u d g e t  for e x p e ns i v e ha rd wa r e 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 3 \223tunings\224 5 SLOC guesstimates 150,000 runs Treatments for software projects Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


31  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 flex=1 pmat=3 sced=2 rest anything from kc1 150,000 runs 150,000 runs Treatments for software projects \(ii 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 pmat=2 acap=2 sced=2 rest anything from kc1 30,000 runs 30,000 runs Treatments for software projects \(iii Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


32  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 ons discussion \(64 downloads \(69 further reading \(71 references \(72 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Will you try ML  Have we motivated you  Will you rush home and do ML on your data  Clearly  ML algorithms work  Caution  you may find it harder than you think Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


33  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Many ways to learn numerous case studies but there is still a problem Theme Learning is a solved problem \(sort of Data collecting and modeling is not 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Be warned match your ML goals to your software process level Project metrics coarse-grain conclusions Product metrics product learning Process metrics process learning Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


34  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Also, match your tool to task Task ML Tool Assembly line robot deciding what to reject Decision tree learner Repair robot trying to do the least to fix the rejected parts Treatment learner Predicting the life of a robot Neural Network Optimizing the assembly line Genetic Algorithm If clustering when no classes iation rule learning If simple background knowledge Bayesian If complex relational background knowledge ILP 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Have we learnt enough  Not yet  But wait Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


35  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost = $0  WEKA  E.g. http://www.cs.waikato.ac.nz/~ml/weka/: ML in JAVA 003 decision tree inducers,rule learners, naive Bayes, decision tables locally weighted regression  GDB_Net  http://nas.cl.uh.edu/boetticher/gdb_net.zip  TAR2  http://www.ece.ubc.ca/twiki/bin/view/Softeng/TreatmentLearner  APRIORI  http://fuzzy.cs.uni-magd eburg.de/~borgelt/apriori/apriori.html#download  And many others  E.g. ML  A public domain \223C\224 library of common algorithms  Naive Bayes, ID3, MC4 , Decision Tables ,   Holte's OneR CN2,\205  http://www.sgi.com/tech/mlc/utils.html 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Cost > $0  C4.5  Comes with the book Quinlan  C5.0  http://www.rulequest.com/download.html  Microsoft SQL SERVER 2000\231  Comes with numerous machine learning tools  Proprietary algorithms  Etc  223data mining\224 \223commercial software\224 in Google  3,340 links  223data mining consultancy\224 in Google  850 links Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


36  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 Further reading  Mendonca  great rev i e w art i cl e on ML  Large list of available tools  All the things you can do with a decision tree [Menzies0  Treatment learning: [Menzies01a  Michalski\222s excellent survey of ML types [Michalski  Neural nets: [Boetticher01  Special issue SEKE journal, knowledge discovery Morasca99  Inductive logic programming [Bergadano95,Cohen95  Come by IJCAI 2011 and I\222ll tell you all about it\222s applications  Genetic algorithms: [Goldberg8  Bayesian learning [Cheeseman88 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Agrawal  Agrawal, R., and T.Imeilinski and A.Swami \223Mining Association Rules between Sets of Items in Large Databases,\224 Proceedings of the 1993 ACM SIGMOD Conference Washington DC, USA  Bergadan  Bergadano, F., and D.Gunetti Inductive Logic Programming: From Machine Learning to Software Engineering The MIT Press, 1995  B  Berry, M. J. A., and G., Linoff Data Mining For Marketing, Sales, and Customer Support John Wiley Sons, Inc., New York, 1997  Boetticher01  Boetticher, G., "An Assessment of Metric Contribution in the Construction of a Neural Network-Based Effort Estimator Second International Workshop on Soft Computing Applied to Software Engineering  Enschade, NL, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Boetticher01  Boetticher, G., "Using Machine Learning to Predict Project Effort: Empirical Case Studies in Data-Starved Domains First International Workshop on Model-based Requirements Engineering San Diego, 2001 Available from http://nas.cl.uh.edu/boetticher/publications.html  Bradley  Bradley, P., U. Fayyad, and C. Reina. \223Scaling clustering algorithms to large databases\224. In KDD'98  B  Bratko, I., I. Mozetic, and N. Lavrac KARDIO: a Study in Deep and Qualitative Knowledge for Expert Systems MIT Press, 1989  Breim  Breiman, L., J. Friedman, R. Olshen, C. Stone, \223Classification and Regression Trees,\224 Wadsworth International Group, 1984 Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


37  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Burgess  Burgess, C.J., and Martin Lefley. \223Can genetic programming improve software effort estimation? A comparative evaluation,\224 Information and Software Technology er 2001  Cheesem  P. Cheeseman, D. Freeman, J. Kelly, M. Self, J. Stutz, and W. Taylor. \223Autoclass: a bayesian classification system,\224 In Proceedings of the Fifth International Conference on Machine Learning  Morgan Kaufman, 1988  Chulani  S.Chulani,  B. Boehm, and B. Steece 223Bayesian analysis of empirical software engineering cost models,\224 IEEE Transaction on Software Engineering 25\(4\ly/August  1999  Cohe  W. W. Cohen, \223Inductive specification recovery: Understanding software by learning  from example behaviors,\224 Automated Software Engineering 2:107-129, 1995  DeJon  DeJong, K.A., and Spears, W.M. "An Analysis of the Interacting Roles of Population Size and Crossover in Genetic Algorithms Proc. First Workshop Parallel Problem Solving from Nature  Springer-Verlag, Berlin, 1990  Dietteric  Dietterich, T. G., \223Machine Learning  Research: Four Current Directions,\224 AI Magazine 18 \(4\97 Pp. 97-136. Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz  s  Feather, M.S., and T. Menzies: \223Converging on the Optimal Attainment of Requirements IEEE Joint Conference On Requirements Engineering  ICRE'02 and  RE'02 9-13th September, University of Essen, Germany, 2002. Available from http://tim.menzies.com/pdf/02re02.pdf 251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02 References  Fenton00  Fenton, N., and  M. Neil \223Software Metrics: A Roadmap,\224 International Conference on Software Engineering, 2000. Available from http://www.dcs.qmul.ac.uk/~norman/papers/metrics_roadmap.pdf  Goldberg  Goldberg, D.E Genetic Algorithms in Search, Optimization, and Machine Learning Addison-Wesley Reading, Massachusetts, 1989  Khoshgoftaar  Khoshgoftaar, T.M., and E.B. Allen. \223Model software quality with classification trees,\224 in H. Pham, editor 223Recent Advances in Reliability and Quality  Engineering\224, World Scientific, 1999  Mendonc  Mendonca, M., and N.L. Sunderhaft, \223Mining Software Engineering Data: A Survey,\224 A DACS State-ofthe-Art Report September 1999. Available from http://www.dacs.dtic.mil/techs/datamining  Menzie  Menzies, T., \223Practical Machine Learning for Software Engineering and Knowledge Engineering,\224 ftware Engineering and Knowledge Engineering volume 1, 2001\vailable from http://tim.menzies.com/pdf/00ml.pdf  Menzies01a  Menzies, T., and Y. Hu, \223Reusing models for requirements engineering,\224 First International Workshop on Model-based Requirements Engineering 2001. Available from http://tim.menzies.com/pdf/01reusere.pdf  Menzies01b  Menzies, T., and Y. Hu, \223Constraining discussions in requirements engineering,\224 First International Workshop on Model-based Requirements Engineering San Diego, 2001. Available from http://tim.menzies.com/pdf/01lesstalk.pdf  Menzie  Menzies. T., and J. Kiper, \223Better reasoning about software engineering activities,\224 Automated Software Engineering 2001. Available from http://tim.menzies.com/pdf/01ml4re.pdf Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


38  251 2002 Tim Menzies, Gary D. Boetticher Page  SEW-27 Tutorials '02  Michalski90   Michalski, R.S., \223Toward a unified theory of learning,\224  In B.G. Buchanan and D.C. Wilkins, editors 223Reading in Knowledge  Acquisition and Learning\224, pages 7--38. Morgan Kaufmann, 1993  Mitchell  Mitchell, T Machine Learning McGraw-Hill, 1997  Morasca99  Morasca, S., and Gunther Ruhe, Guest editors' introduction of the Special issue on \223Knowledge Discovery from Software Engineering Data,\224 International Journal of Software Engineering and Knowledge Engineering October, 1999  Pearce  Pearce, D., \223The induction of fault diagnosis systems from qualitative models,\224 in Proc. AAAI-88 1988  Poole9  Poole, D. L.,  A. K. Mackworth, and R. G. Goebel Computational Intelligence: A Logical Approach  Oxford University Press, New York, 1998  Porter9  Porter, A.A., and R.W. Selby  \223Empirically guided software development using metric-based classification trees,\224 IEEE Software Pp. 46-54, March 1990  Quinla  Quinlan, R C4.5: Programs for Machine Learning Morgan Kaufman, 1992  Srinivasa  Srinivasan, K., and D. Fisher,  \223Machine learning approaches to estimating software development effort,\224 IEEE Transactions on Software Engi neering Pp. 126-137, February 1995  Tian9  Tian, J., and M.V. Zelkowitz 223Complexity measure evaluation and selection,\224 IEEE Transactions on Software Engineering 21\(8\p. 641-649,  August 1995  Webb0  Webb, G., \223Efficient search for association rules,\224 Proceeding of KDD-2000 Boston, MA,  2000  Zhang0  Zhang, Du, \223Applying Machine Learning Algorithms in Software Development,\224 Modelling Software System Structures in a fastly moving scenario Santa Margherita Ligure, Italy, 2000 References Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop \226 Tutorial Notes \(SEW\22202 0-7695-1854-0/02 $17.00 \251 2002 IEEE 


