Frequent Pairs in Data Streams Exploiting Parallelism and Skew Andrea Campagna 002  Konstantin Kutzkov 002  Rasmus Pagh 002 002 IT University of Copenhagen Copenhagen Denmark Email acam,konk,pagh@itu.dk Abstract We introduce the Pair Streaming Engine PairSE that detects frequent pairs in a data stream of transactions Our algorithm nds the most frequent pairs with high probability and gives tight bounds on their frequency It is particularly space efìcient for skewed distribution of pair supports conìrmed for several real-world datasets Additionally the algorithm parallelizes easily which opens up for real-time processing of large transactions Unlike previous algorithms we make no assumptions on the order of arrival of transactions and pairs Our algorithm builds upon approaches for frequent items mining in data streams We show how to efìciently scale these approaches to handle large transactions We report experimental results showcasing precision and recall of our method In particular we nd that often our method achieves excellent precision returning identical upper and lower bounds on the supports of the most frequent pairs Keywords data stream association rule parallel sharednothing algorithm I I NTRODUCTION A fundamental tasks in knowledge discovery in databases is the mining of high quality association rules from transactional databases over a set of items The pioneering Apriori  algorithm proposed about tw o decades ago has pa v ed the way for many important contributions to the problem Algorithms with much better space and time complexity have since been proposed  and sho wn to ef ciently handle large amounts of data In this work we concentrate on discovering frequent pairs  or 2-itemsets in a high speed stream of transactions Our algorithm can be generalized in a straightforward way to k itemsets but the analysis becomes more complex Also it has been observed that already the case of 2-itemsets captures the main challenge of frequent itemset mining  the initial candidate set generation especially for the large 2-itemsets is the key issue to improve the performance of data mining  A Mining data streams Classical approaches such as Apriori and FPgrowth require se v eral passes o v er the transactional database and thus it is necessary to have access to a storage system containing the database As observed by Manku and Motwani this requirement is not practical for man y real life applications where we want to mine frequent patterns 10 100 1000 10000 100000 Pair rank 10 100 1000 10000 100000 1 x 10 6 Pair frequency 
 
 
 
 
  Webdocs Accidents  Pubmed  Pumsb  Nytimes Pumsb_star  Mushroom  Retail  Kosarak Figure 1 The frequency distribution for the most frequent pairs on doublylogarithmic scale All start off with a straight line in only one pass from a high speed stream of transactions Since this seminal work many researchers have considered the special requirements of data stream association mining We refer the reader to the survey of Jiang and Gruenwald for an introduction to this area and references to many central works We restrict our attention to the fundamental case of mining frequent pairs over the entire stream landmark model in the classiìcation of This problem is fundamentally different from mining over a sliding window as the challenge is to use far less space than the size of the stream and keep up with the data stream in real-time We present algorithms that succeed with high probability and return upper and lower bounds on the number of occurrences rather than precise counts For example in the webdocs dataset there are around 700 million distinct pairs of items and keeping all their counts in a hash table would require at least 8 GB of memory In contrast we obtain accurate results using a sketch data structure of a few megabytes that ts in L2 cache B Pair similarity distribution Skewed distributions are common for real-life datasets We conjectured that the frequency of the pairs for many data sets will adhere to a power law or more precisely to a Zipìan distribution While it is well-known that this is true for single items in many data sets it is not obvious that this assumption holds for streams of pairs generated from a stream of transactions For this we computed the exact count 
2011 11th IEEE International Conference on Data Mining Workshops 978-0-7695-4409-0/11 $26.00 © 2011 IEEE DOI 10.1109/ICDMW.2011.87 145 


of the most signiìcant pairs for several well-studied data sets using Borgeltês Apriori implementation 1  and plotted them in decreasing order Figure 1 shows the supports of the most frequent pairs for our datasets In all cases we see that the curve starts of as approximately a straight line The length of this line varies from one data set to another from a few hundred pairs to hundreds of thousands Observe also that in all cases where the curve deviates from a line it drops below i.e the distribution is dominated by a powerlaw distribution This motivates the design and analysis of algorithms aimed at efìcient frequent pairs mining for pairs following Zipìan distribution C Related work Heuristic algorithms Manku and Motwani rst recognized the necessity for efìcient algorithms targeted at frequent itemsets in transaction streams They generalized their S TICKY S AMPLING algorithm to a heuristic for transaction streaming and showed empirically that it reliably estimates the frequency of the most frequent itemsets on several benchmarks The basic idea is to process the data set in memory-sized chunks mining each chunk for frequent itemsets to determine which itemsets should be counted in the next chunk However this method is vulnerable to large itemsets that are temporarily frequent An itemset of size k that is frequent in a chunk will have all its subsets counted in the following chunk using space 2 k  For this reason it does not seem suitable for general use Reduction to the single-item case Another approach to mining of frequent pairs mentioned but dismissed in  is to reduce the problem to that of mining frequent items which is well-studied in a data stream context For a transaction T 002  n   1 n   this approach generates all 002  T  2 003 002  T  2  pairs occurring in T and feeds the resulting stream S  where the items of S are the pairs generated into a frequent items algorithm Let F 2 denote the length of the stream generated in this way It is known that using space s one can compute the frequency of items which are in fact pairs in our case with an additive error of F 2 s   This means that all pairs with frequency above F 2 s can be reported with computed upper and lower bounds on the frequency that differ by at most F 2 s  While this is optimal over a worst-case data stream where all pairs occur with frequency about F 2 s  some methods notably the S PACE S AV I N G algorithm ha v e been observ ed to produce even tighter bounds on the highest frequencies in practice However to our best knowledge S PACE S AVING and related algorithms have never been experimentally investigated in the context of nding frequent itemsets Frequent items algorithms aim for using small time per item and as a 1 http://www.borgelt.net/apriori.html matter of fact the best methods use constant time per item therefore the time usage for the whole stream is O  F 2   The most space-efìcient methods do not parallelize efìciently as they rely on a single data structure any part of which may be updated for a particular transaction In contrast we show how to parallelize efìciently without any need for shared memory Muthukrishan and Cormode considered nding frequent items space-efìciently in a stream that is highly skewed Zipìan distribution with parameter greater than 1 In this case they are able to improve the space needed to identify the most frequent items However looking at the stream of all pairs none of the data sets that we considered exhibited large enough skew for their result to apply Algorithms for random streams Yu et al presented another algorithm for transaction stream mining The main idea in their approach is to keep a list of potentially frequent itemsets and to update the list in a clever way when advancing the stream They show also theoretical bounds for the quality of their estimates In order to derive these bounds however they need to assume that transactions are generated independently at random by some process and their analysis crucially depends on the Chernoff bounds that become applicable because of this assumption Campagna and Pagh mak e the ar guably weak er  but still questionable assumption that the order of the transactions in the stream is random Again this enables them to utilize Chernoff bounds in analogy with a frequent item mining algorithm by Charikar et al It is already clear from the experiments of that such optimistic assumptions do not even approximately hold for many data sets For both schemes 16 it is easy to nd an ordering of essentially any transaction stream that breaks the randomness assumption and makes it perform much worse than the theoretical bounds D Our contribution The main contribution of the present work is a randomized algorithm that returns with high probability a correct estimate of the frequency of the most frequent pairs We build upon well-known streaming algorithms and show how to extend them to transaction streaming The complexity as well as the quality of the output is determined by the Zipìan distribution parameters and the space allowed The space usage is a user-deìned parameter We show through extensive experiments on real and synthetic datasets that our algorithm achieves very good estimates and scales particularly well when parallelized on several cores II N OTATION The transaction stream is denoted by S  T 1   T m where T i 002  n  A subset p   i j 003  n  is called a pair The set of pairs is denoted by P  while the number of distinct pairs 
146 


occurring in the stream S is represented using d 004 002 n 2 003  Furthermore the number of frequent pairs by f  where the meaning of frequent will be speciìed in the given context The support of a pair p is the number of transactions containing p  sup p   T j  p 002 T j   1 004 j 004 m  A hash function h  P\005  k  for k 006 N is t wise independent if and only if Pr h  p 1  c 1 007 h  p 2  c 2 007∑∑∑\007 h  p t  c t  k  t for distinct pairs p i  1 004 i 004 t  and c i 006  k   Zipìan distribution with parameters C and z is deìned as f i  C/i z for the frequency f i of the i  th most frequent pair III O UR APPROACH A Background and intuition Before formally describing our algorithm let us give some technical background and intuition An algorithm detecting the frequent items in an item stream can be generalized in a straightforward way in order to nd frequent pairs in a stream of transactions Simply generate all subsets of size 2 of each transaction and treat them as items In particular the two well known algorithms C OUNT S KETCH and S PACE S AVING can be generalised as described In C OUNT S KETCH  e v ery item i is hashed by a hash function h  n  005  k  to a bucket B containing a counter c B  Upon arrival of an item i the corresponding counter is updated by a uniform sign hash function s  i  evaluating i to either 1 or  1  After processing the stream the frequency of a given item i can be estimated as c B  s  i  where B  h  i   The intuition is that the contribution from other items will cancel out Both h and s are pairwise independent and this is sufìcient to show that for an appropriate number of buckets the algorithm produces good estimates where the error is measured with respect to the 2  norm of the vector of item frequencies For skewed distribution of the stream frequencies this gives high quality estimates of the heaviest pairs One can amplify the probability for correct estimates by working with more than one hash functions Upon a query for the frequency of a given item C OUNT S KETCH returns the median of the estimates i.e the counters in the buckets the item hashes to The S PACE S AVING algorithm of fers upper and lo wer frequency bounds rather than an unbiased estimator It keeps a list of 002 triples  item j  count j  overestimation j   1 004 j 004 002  If not all 002 slots are already full it inserts a new triple as  i 1  0 for an arriving item i  The 002 triples are sorted according to their count value Once a new item arrives it checks if it is already in the list If yes it increases the corresponding counter by 1 and updates the order in the list Otherwise it replaces the last triple  item 002  count 002  overestimation 002  with a new triple  item new  count 002 1  count 002  where item new is the newly arrived item The intuition behind is that heavy items will get early on the pull positions and wonêt be evicted from the list until the end thus for skewed data we will get an accurate estimation However this relies on the assumption that the most frequent pair is also frequent in an early preìx of the stream so this will not be true in the worst case Our algorithm can be seen as a twofold reìnement of the above direct approach 1 In order to address the issue of having a quadratic number of pairs in each transaction hence a quadratic number of hash values to produce we use parallelism In this way we are able to distribute the computation among several cores in a way such that each core efìciently computes the pairs hashing to a given subset of the hash table 2 Assuming Zipìan distribution we want to use the fact that the most frequent pairs will not collide and thus we keep track of the most frequent pair hashing to a given bucket We will use an important property of S PACE S AVING namely that in a stream of items an item having relative frequency at least 1  2 will end up in the rst position of the S PACE S AVING data structure B Our algorithm The skeleton of our algorithm is the following  Hash each pair to a bucket  Keep track of the most frequent pair in each bucket  Return an estimate of the frequency of the most frequent pair for each bucket In the parallel version each processor keeps track of an interval of the hash table and the total space remains xed Thus we are in a shared nothing model with no need for a shared memory  the only requirement is that each processor sees the input stream It is well-known that this kind of parallel algorithms scales extremely well compared to algorithms that rely on interprocess communication or shared data structures Even for the largest data sets that we looked at it is feasible to keep the entire hash table in L2 cache of the involved processors on a large workstation resulting in extremely fast processing A crucial property is that most frequent pairs do not collide and thus we obtain high quality estimates on their frequency We combine two different ways for estimating the frequency of the heaviest pairs based on the C OUNT S KETCH and S PACE S AVING algorithms In particular we use a distribution hash function h  n    n  005 1 k  1  to split the set of pairs into k parts and use a S PACE S AVING sketch on each part The size k of the hash table and the size of the S PACE S AVING sketch determines the accuracy of the sketch Parallelizing processing of pairs Na  vely we could just iterate through all pairs of each transaction T t  but we would like an algorithm that runs in linear time when the number of pairs hashing to  i j  is small This will allow us to split the task of computing the sketch among several cores all the 
147 


way to the point where each core processes a transaction in linear time In other words given sufìcient parallelism we can handle a given data rate even if the transactions are huge Lemma 1 Let h  P\005 0 k  1  be a pairwise independent hash function Given a transaction T of size t and a subset L\002 0 k  1  we can construct P T L  the set of pairs occurring in T hashing to a value in L  in time O  P T L   t   In order to improve the algorithmês accuracy we may run several copies of the algorithm in parallel and report the median At the end a pair is reported frequent if it has won in at least t 2 of its corresponding S PACE S AVING data structures Our experimental results will be for a single run so the reported accuracy can be improved at the cost of time and space The second estimate of the algorithm is based on the C OUNT S KETCH algorithm by Charikar Chen and FarachColton Here we ha v e a counter serving as an unbiased estimator for the frequency of the heaviest pair where unbiased means that the estimate does not depend on the order of arrival of pairs As in the original C OUNT S KETCH algorithm we will work with an additional pairwise independent hash functions the sign function s  P\005 1  1   With each bucket B we associate a counter c B  The counter serves the same purpose as in the original algorithm Upon arriving of a new pair p we update the corresponding bucket we abuse notation and denote it as h  p   as follows c h  p   c h  p   s  p   The intuition is that the heaviest pair will contribute with the same sign and contributions from other pairs will cancel out At the end the algorithm returns s  p   c h  p  as estimated frequency for the pair where p is the rst pair in the S PACE S AVING data structure As we show in the next section if sup p  m 2  then a high-quality estimation of p s frequency is returned In order to reduce the error we can work again with several independent hash functions and report the median of the results IV E XPERIMENTS Table I summarizes the data sets that we use for experiments In all cases we use the order in which the transactions are given as the stream order We worked with two implementations a simple Python implementation and a cache-optimized Java implementation that was 10Ö20 times faster In both cases we used the built-in random number generator of the language to store hash values in a table A Accuracy of results Our rst set of experiments shows results on the precision of the counts obtained by PairSE using a S PACE S AVING data structure of size 2 The accuracy is of course inîuenced by Dataset  of pairs  F 2   of distinct pairs Mushroom 22  4  10 5 3  65  10 3 Pumsb 1360  10 5 536  10 3 Pumsb star 638  10 5 485  10 3 Kosarak 3130  10 5 33100  10 3 Retail 80  7  10 5 3600  10 3 Accidents 187  10 5 47  3  10 3 Webdocs 2  0  10 11  7  10 10 Nytimes 1  0  10 10  5  10 8 Pubmed 1  6  10 10  6  10 8 Wikipedia 5  17  10 11  5  8  10 9 Table I I NFORMATION ON DATA SETS FOR OUR EXPERIMENTS N YTIMES AND P UBMED ARE TAKEN FROM THE UCI M ACHINE L EARNING R EPOSITORY B AG OF W ORDS DATA SET  T HE WIKIPEDIA DATASET HAS BEEN CRAFTED ACCORDING TO WHAT IS DESCRIBED IN 18 P AGE  F OR THE LAST THREE DATA SETS THE NUMBER OF DISTINCT PAIRS WAS ESTIMATED USING A HASHING TECHNIQUE FROM  T HE DATASETS P UMSB  AND P UMSB STAR WERE PREPARED BY R OBERTO B AYARDO FROM THE UCI DATASETS AND PUMBS K OSARAK CONTAINS  ANONYMIZED  CLICK STREAM DATA OF A HUNGARIAN ON LINE NEWS PORTAL  PROVIDED BY F ERENC B ODON R ETAIL CONTAINS THE  ANONYMIZED  RETAIL MARKET BASKET DATA FROM A B ELGIAN RETAIL STORE B RIJS ET AL  1999 A CCIDENTS CONTAINS  ANONYMIZED  TRAFFIC ACCIDENT DATA G EURTSETAL  2003 the amount of space used as well as the number of pairs you are interested in reporting We made one experiment xing the space usage and looking at results for pairs of decreasing rank computed exactly and one that varies the space usage and considers the top-100 pairs Fixed space usage In practice it may be hard to foresee how much space will be needed for a particular stream so probably one will tend to use as much space as feasible with respect to running time ensure in-cache hash table or what amount of memory can be made available on the system A consequence of this will be even more precise results The results of our experiments on the Nytimes data set can be seen in Figures 2 and 3 The former zooms in on the zone where the lower and upper bounds computed by S PACE S AVING are very accurate Varying space usage We now investigate what happens to the quality of results when the space usage of PairSE is pushed to and beyond its limits For this we chose to work with 3 representative data sets namely Mushroom Retail and Accidents for decreasing space usage plotting the ratio between the upper and lower bounds for the top-100 pairs returned by our algorithm This is shown in Figure 4 and we can see how the transition between very good and very poor quality is fairly fast B Count-Sketch Estimates The result of the unbiased C OUNT S KETCH estimator for the Kosarak dataset with 50000 buckets and 2 pairs per bucket is presented in Figure 5 We ran the algorithm 11 times and for each pair reported at least 6 times we return the median of its estimates The plot shows the ratio of our 
148 


0 2000 4000 6000 8000 10000 True rank 0 2000 4000 6000 8000 10000 Rank of lower bound Figure 2 Top frequent pairs for Nytimes and their rank according to the frequency lower bound computed by PairSE using 10 6 buckets As can be seen recall is initially high but decreases with the support 1 10 100 1000 10000 Actual rank 0.01 0.1 1 Upper and lower bounds, relative to true value Figure 3 Upper and lower bounds for Nytimes computed by PairSE using 10 6 buckets Values are normalized by dividing by true support Upper bounds shadow lower bounds exact bounds are visible only as a red dot with no blue dot below As can be seen upper bounds are generally tighter than lower bounds 10 2 10 3 10 4 10 5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 number of buckets Estimate quality for top 100 pairs Retail Accidents Mushroom Figure 4 Average ratio of lower and upper bound for top-100 pairs for three representative data sets as function of number of buckets As can be seen there is a quick transition from poor to excellent precision 0 500 1000 1500 2000 2500 3000 0 0.5 1 1.5 2 2.5 Rank of pairs Ratio of estimate and exact count Figure 5 Ratio of estimates and true count for the top 3000 pairs of Kosarak All top 1400 pairs are reported by our algorithm and for most of the pairs the estimates are within of factor 2 estimates and the exact count of the 3000 pairs with highest support in the dataset Not reported pairs have ratio 0 C Performance and scalability Experiments have been carried out in order to verify how the algorithm scales in terms of time when parallel computations are used We ran the algorithm on various datasets using several different number of cores In this way it has been possible to highlight the parallel nature of the algorithm hence its capability of being very time efìcient when many cores are at hand Table II reports some of the results we obtained The machine we used is described in the caption of the table For large datasets such as nytimes with a high number of pairs our simple Java implementation processed almost 100 million pairs per second on an 8core Mac Pro The throughput of a competing hash table solution can be upper bounded by the number of updates of random memory locations possible disregarding time for hash function computation and other overheads On the Mac Pro the number of such updates per second was estimated to around 50 millon per second when updating a 1 GB table using 8 cores This means that we are at least a factor of 2 faster than any implementation based on a large shared data structure Acknowledgement We wish to thank Blue Martini Software for contributing the KDD Cup 2000 data R EFERENCES  R Agra w al and R Srikant F ast algorithms for mining association rules in large databases in VLDB  1994 pp 487 499  S Brin R Motw ani J D Ullman and S Tsur  Dynamic itemset counting and implication rules for market basket data in SIGMOD Conference  ACM Press 1997 pp 255 264 
149 


Dataset  of cores ms on  cores ms 1 core Kosarak 8 1551 4 1586 2881 2 1997 Webdocs 8 299153 4 357679 891565 2 482111 Nytimes 8 443119 4 524553 1313698 2 689058 Wikipedia 8 27526403 4 35397110 93477243 2 53795313 Table II E XPERIMENTS RAN ON AN I NTEL X EON E5570 2.93 GH Z EQUIPPED WITH 23 GB OF RAM THE OS IS GNU/L INUX  KERNEL VERSION 2.6.18 T IMES ARE GIVEN IN MILLISECONDS  ms  T HE NUMBER OF BUCKETS IS 003 2 20   J Han J Pei Y  Y in and R Mao Mining frequent patterns without candidate generation A frequent-pattern tree approach Data Min Knowl Discov  vol 8 no 1 pp 53 87 2004  J S P ark M.-S Chen and P  S Y u  An ef fecti v e hash-based algorithm for mining association rules SIGMOD Record  vol 24 no 2 pp 175Ö186 Jun 1995  A Sa v asere E Omiecinski and S B Na v athe  An ef cient algorithm for mining association rules in large databases in VLDB  1995 pp 432Ö444  G S Manku and R Motw ani  Approximate frequenc y counts over data streams in VLDB 02  Morgan Kaufmann Publishers 2002 pp 346Ö357  N Jiang and L Gruenw ald Research issues in data stream association rule mining SIGMOD Record  vol 35 no 1 pp 14Ö19 2006  Y  Zhu and D Shasha Statstream Statistical monitoring of thousands of data streams in real time in VLDB  2002 pp 358Ö369  C Bor gelt Recursion pruning for the apriori algorithm  in IEEE ICDM Workshop on Frequent Itemset Mining Implementations  2004  A Campagna and R P agh On nding similar items in a stream of transactions in ICDM Workshops  2010 pp 121 128  J Misra and D Gries Finding repeated elements  Sci Comput Program  vol 2 no 2 pp 143Ö152 1982  E D Demaine A L  opez-Ortiz and J I Munro Frequency estimation of internet packet streams with limited space in ESA 2002  2002 pp 348Ö360  R M Karp S Shenk er  and C H P apadimitriou  A simple algorithm for nding frequent elements in streams and bags ACM Trans Database Syst  vol 28 pp 51Ö55 2003  A Metw ally  D Agra w al and A E Abbadi Ef cient computation of frequent and top-k elements in data streams in ICDT  pp 398Ö412  G Cormode and S Muthukrishnan Summarizing and mining skewed data streams in SIAM International Conference on Data Mining  2005  J X Y u Z Chong H Lu Z Zhang and A Zhou  A false negative approach to mining frequent itemsets from high speed transactional data streams Inf Sci  vol 176 no 14 pp 1986Ö2015 2006  M Charikar  K Chen and M F arach-Colton Finding frequent items in data streams Theor Comput Sci  vol 312 no 1 pp 3Ö15 2004  E Ab usland and M Mark o vics Implementing and e v aluating a sampling-based approach to association mining on mapreduce Masterês thesis IT University of Copenhagen 2011  R R Amossen A Campagna and R P agh Better size estimation for sparse matrix products in APPROXÖRANDOM 2010  ser Lecture Notes in Computer Science vol 6302 Springer 2010 pp 406Ö419 
150 


 286 


T. Filali Ansary, F. Fratani, E. Garcia, G. Lavou, D. Lichau, F. Preteux J. Ricard, B. Savage, J.P. Vandeborre, T. Zaharia. SEMANTIC-3D COMPRESSION, INDEXATION ET TATOUAGE DE DONNES 3D Rseau National de Recherche en Tlcommunications \(RNRT 2002 4] T.Zaharia F.Prteux, Descripteurs de forme : Etude compare des approches 3D et 2D/3D 3D versus 2D/3D Shape Descriptors: A Comparative study 5] T.F.Ansary J.P.Vandeborre M.Daoudi, Recherche de modles 3D de pices mcaniques base sur les moments de Zernike 6] A. Khothanzad, Y. H. Hong, Invariant image recognition by Zernike moments, IEEE Trans. Pattern Anal. Match. Intell.,12 \(5 1990 7] Agrawal R., Imielinski T., Swani A. \(1993 between sets of items in large databases. In : Proceedings of the ACM SIGMOD Conference on Management of Data, Washington DC, USA 8] Hbrail G., Lechevallier Y. \(2003 In : Govaert G. Analyse des donnes. Ed. Lavoisier, Paris, pp 323-355 9] T.F.Ansary J.P.Vandeborre M.Daoudi, une approche baysinne pour lindexation de modles 3D base sur les vues caractristiques 10] Ansary, T. F.   Daoudi, M.   Vandeborre, J.-P. A Bayesian 3-D Search Engine Using Adaptive Views Clustering, IEEE Transactions on Multimedia, 2007 11] Ansary, T.F.   Vandeborre, J.-P.   Mahmoudi, S.   Daoudi, M. A Bayesian framework for 3D models retrieval based on characteristic views, 3D Data Processing, Visualization and Transmission, 2004 3DPVT 2004. Proceedings. 2nd International Symposium Publication Date: 6-9 Sept. 2004 12] Agrawal R., Srikant R., Fast algorithms for mining association rules in larges databases. In Proceeding of the 20th international conference on Very Large Dada Bases \(VLDB94 September 1994 13] U. Fayyad, G.Piatetsky-Shapiro, and Padhraic Smyth, From Data Mining toKnowledge Discovery in Databases, American Association for Artificial Intelligence. All rights reserved. 0738-4602-1996 14] S.Lallich, O.Teytaud,  valuation et validation de l'intrt des rgles d'association 15] Osada, R., Funkhouser, T., Chazelle, B. et Dobkin, D. \(\( Matching 3D Models with Shape Distributions International Conference on Shape Modeling & Applications \(SMI 01 pages 154168. IEEE Computer Society,Washington, DC, Etat-Unis 2001 16] W.Y. Kim et Y.S. Kim. A region-based shape descriptor using Zernike 


moments. Signal Processing : Image Communication, 16 :95100, 2000 


And put forward that we could use confidence, category homoplasy and relevancy strength to improve the quality of feature extension modes. We also verified that confidence category homoplasy and relevancy strength are effective through our experiments. In the same time we have drawn the following conclusions: \(1 relationships for short-text can improve their classification performance; \(2 effectiveness of information in the feature extension mode library we should choose the suitable thresholds; \(3 information is too small to meet the demand of short-text feature extension. So we should find out a perfect method which can increase information coverage in the feature extension mode library for short-text classification; \(4 extension library for short-text extension effectively, i.e., choosing a perfect feature extension strategy is also our further work ACKNOWLEDGMENT The research is supported in part by the National Natural Science Foundation of China under grant number 60703010 the Nature Science Foundation of Chongqing province in China under grant number CSTC, 2009BB2079, and the Scientific Research Foundation for the Returned Overseas Chinese Scholars of Ministry of Education of China under grant number [2007] 1109 REFERENCES 1] Fabrizio Sebastiani.Machine Learning in Automated Text Categorization, A.ACM Computing Surveys, C.2002.34\(1 2] Fan Xing-hua,Wang peng. Chinese Short-Text Classification in TwoStep, J.Journal of DaLian Maritime Universtiy, 2008,11\(2 3] Zelikovitz S. and Hirsh H. Improving Short Text Classification Using Unlabeled Background Knowledge to Assess Document Similarity C. In: Proceedings of ICML-2002, 2002, 1183-1190 4] Wang Xi-wei,Fan Xing-hua and Zhao Jun. A Method for Chinese Short Text Classification Based on Feature Extension, J.Journal of Computer Applications,2009,29\(3 5] JIAWEI HAN,JIAN PEI ,YIWEN YIN, BUNYING MAO.Ming Frequent Patterns without Candidate Generation:A Frequent-Pattern Tree.Data Mining and Knowledge Discovery,2004,8:53-87 6] Liu Fei. Huang Xuan-qing and Wu Li-de.Approach for Extracting Thematic Terms Based on Association Rule, J.Computer Engineering,2008\(4 7] Xinhua Fan, Jianyun Nie. Link Distribution Dependency Model for 


Document Retrieval, C.Journal of Information and Computational Science6:3\(2009  90 


shows that proposed post mining of association rule mining technique for missing sensor data estimation is an area worth to explore REFERENCES 1] Agrawal, R., & Imielinski, T., & Swami, A., "Mining association rules between sets of items in massive databases", International Conference on Management of Data, 1993 2] Austin, F. I., "Austin Freeway ITS Data Archive", Retrieved January 2003 from http://austindata.tamu.eduidefauIt.asp 3] Bastide, Y., & Pasquier, N., & Taouil, R, & Stumme, G., & Lakhal L., "Mining minimal non-redundant association rules using frequent closed itemsets", First International Conference on Computational Logic, 2000 4] Cool, A. L., "A review of methods for dealing with missing data The Annual Meeting of the Southwest Educational Research Association, 2000 5] Deshpande, A., & Guestrin C., & Madden, S., "Using probabilistic models for data management in acquisitional environments", The Conference on Innovative Data Systems Research, 2005 6] Halatchev, M., & Gruenwald, L., "Estimating missing values in related sensor data streams", International Conference on Management of Data, 2005 7] Iannacchione, V. G., "Weighted sequential hot deck imputation macros", Proceedings of the SAS Users Group International Conference, 1982 8] Nan Jiang, "Discovering Association Rules in Data Streams Based On Closed Pattern Mining", SIGMOD Ph.D. Workshop on Innovative Database Research, 2007 9] Li, Y., & Liu, Z. T., & Chen, L., & Cheng, W., & Xie, C.H Extracting minimal non-redundant association rules from QCIL The 4th International Conference on Computer and Information Technology, 2004 10] Little, R 1. A., & Rubin, D. B., "Statistical analysis with missing data", New York: John Wiley and Sons, 1987 II] McLachlan, G., & Thriyambakam, K., "The EM algorithm and extensions", New York: John Wiley & Sons, 1997 12] Mitchell, T., "Machine Learning", McGraw Hill, 1997 13] Papadimitriou, S., & Sun, 1., & Faloutsos, C., "Streaming pattern discovery in multiple time-series", The International Conference on Very Large Databases, 2005 14] Rubin, D., "Multiple imputations for nonresponce in surveys", New York: John Wiley & Sons, 1987 


15] Shafer, 1., "Model-Based Imputations of Census Short-Form Items In Proceedings of the Annual Research Conference, 1995 16] Taouil, R., & Pasquier, N., & Bastide, Y., & Lakhal, L., "Mining bases for association rules using closed sets", International Conference on Data Engineering, 2000 17] Wilkinson & The AP A Task Force on Statistical Inference, 1999 18] Zaki, M. 1., Hsiao, C. 1., "Efficient algorithms for mining closed itemsets and their lattice structure", IEEE Transactions on Knowledge and Data Engineering, 2005 V5-106 


General Chair f!!\f  Organizing Chairs  f!!\f  f$% \f!!\f  Organizing Co-chairs f    f  f\f   f\f\f   f*!\f!\f.\f  f f  Program Committee Chairs  f\f\f   f!!\f  Publication Chair 0   


200 250 300  The size of dataset/10,000 R es po ns e tim e S    a 0 50 100 150 200  The size of dataset/10,000 R es po ns e tim e S    b 0 10 20 30 40 50 


60  The size of dataset/30,000 R es po ns e tim e S    c Fig. 9 The scalability of our algorithm compared with FP-growth  Paper [12] proposed a way to reduce times of scanning transaction database to reduce the cost of I/O IV. CONCLUSIONS AND FUTURE WORK This paper first discusses the theory of foundations and association rules and presents an association rules mining algorithm, namely, FP-growth algorithm. And then we propose an improved algorithm IFP-growth based on many association rules mining algorithms. At last we implement the algorithm we propose and compare it with algorithm FPgrowth algorithm. The experimental evaluation demonstrates its scalability is much better than algorithm FP-growth 177 Now, lets forecast something we want to do someday Firstly, we would parallelize our algorithm, because data mining needs massive computation, and a parallelable environment could high improve the performance of the algorithm; Secondly, we would apply our algorithm on much more datasets and study the run performance; At last, we would study the performance when the algorithm deal with other kinds of association rules  REFERENCES 1] S. Sumathi and S. N. Sivanandam. Introduction to Data Mining and its Applications, Springer, 2006 2] V. J. Hodge, J. Austin, A survey of outlier detection 


methodologies, Artificial Intelligence Review, 2004, 22 85-126 3] Han, J. and M. Kamber. Data Mining: Concepts and Techniques. Morgan Kaufmann, San. Francisco, 2000 4] Jianchao Han, Mohsen Beheshti. Discovering Both Positive and Negative Fuzzy Association Rules in Large Transaction Databases, Journal of Advanced Computational Intelligence and Intelligent Informatics 2006, 10\(3 5] Jiuyong Li, Hong Shen, Rodney Topor. Mining Informative Rule Set for Prediction. Journal of Intelligent Information Systems, 2004, 22\(2 6] Jianchao Han, and Mohsen Beheshti. Discovering Both Positive and Negative Fuzzy Association Rules in Large Transaction Databases. Journal of Advanced Computational Intelligence, 2006, 10\(3 7] Doug Burdick, Manuel Calimlim, Jason Flannick Johannes Gehrke, Tomi Yiu. MAFIA: A Maximal Frequent Itemset Algorithm. IEEE Transactions on Knowledge and Data Engineering, 2005, 17\(11 1504 8] Assaf Schuster, Ran Wolff, Dan Trock. A highperformance distributed algorithm for mining association rules. Knowledge and Information Systems, 2005, 7\(4 458-475 9] Mohammed J. Zaki. Mining Non-Redundant Association Rules. 2004, 9\(3 10] J.Han, J.Pei, Y.Yin, Mining frequent patterns without candidate generation, Proceedings ACM SIGMOD 2000 Dallas, TX, May 2000: 1-12 11] P.Viola, M.Jones. Rapid Object Detection Using A Boosted Cascade of Simple Features. Proc. IEEE Conf. on Computer Vision and Pattern Recognition, 2001 12] Anthony K. H. Tung, Hongjun Lu, Jiawei Han, Ling FengJan. Efficient Mining of Intertransaction Association Rules. 2003, 154\(1 178 


For each vertex b in g form j forests body\(a, g, i s.t. bodyAnt\(a, g, i a, g, i with itemsets Ant\(b b and each subset of itemsets Ant\(b b in P\(a, g, j Assign to each leaf l of trees bodyAnt\(a, g, i bodyCons\(a, g, i a fresh variable Vm,M, m, M = size\(itemset\(l Assign to each leaf l of tree headAnt\(a, g, j the variable assigned to itemset l in some leaf of some tree bodyCons\(a, g, i TABLE II.  EXPERIMENTAL DATA Conf. #rules #pruned #dftrs PtC 0.5 6604 2985 1114 0.6 2697 2081 25 0.75 1867 1606 10 0.8 1266 1176 0 0.95 892 866 1 0.98 705 699 1 DSP 0.5 2473 1168 268 0.6 1696 869 64 0.75 1509 844 89 0.8 1290 1030 29 0.95 1032 889 15 0.98 759 723 1 Arry 0.5 770 492 82 0.6 520 353 60 0.75 472 327 39 0.8 408 287 22 0.95 361 255 25 0.98 314 243 30  Our induction algorithm has been launched for each combination of thresholds. Our scheme eliminates all redundant rules in the sense of [25, 31], i.e. those association rules that are not in the covers. All the meta-rule deductive schemes implicitly included in [25] and [31] are induced by our method. The percentage of pruning, thus, outperforms [25 


The results produced for k=3, support 0.25 and confidences between 0.7 and 0.99 are shown in Fig. 3, in terms of pruning percentage \(vertical axis when applied to low confidences \(from 0.7 to 0.9 The percentage of pruning achieved diminishes as the confidence is superior to 0.9. Nevertheless, the pruning is effective with confidence of 0.99 in the majority of cases Pruning at Support = 0.25 0,00 5,00 10,00 15,00 20,00 25,00 30,00 35,00 40,00 45,00 50,00 0,7 0,8 0,9 0,95 0,99 Confidence P ru n in g L e v e l Case 1 Case 2 Case 3  Figure 3.  Pruning experiences at support 0.25  V. DISCUSSION AND CHALLENGES It is important to discuss the technique presented here with focus on the purpose the technique pursues:  to produce semantic recommendation The reader should have noticed that the algorithm presented 


relies strongly on "choice". For instance, the algorithm chooses ears in the graph to form an order for elimination, and the choice is arbitrary. This strategy is essential to maintain low complexity \(polynomial practical. Nevertheless, a warned reader may conclude that this arbitrary choice implies that there are many compactions to produce and therefore the approach as a whole does not show to produce an optimal solution. And the reader is right in this conclusion. Since the goal is compaction, the search for an optimal solution can be bypassed provided a substantial level of pruning is achieved To complete the whole view, we describe how web service descriptions are complemented with the association rules as recommendations. In effect, under our scheme, the document describing the web service is augmented with a set of OWL/RDF/S triples that only incorporate the non-pruned rules with the format of Example 1, that is, the set ARmin of the compaction program obtained by our algorithm, together with the thresholds applied to the mining process and a registered URI of a registered description service. The assumptions and defeaters are not added to the web service description. If the associations encoded in the triples are not sufficient for the client \(a search engine, for instance widening of the response to the description service identified by the given URI, and then the assumptions and defeaters are produced. The reasoning task required for deriving all the implicitly published rules is client responsibility Notice that, under this scheme, the actual rules that appear as members of the set initial ARmin set are irrelevant; the only important issue is the size of the set The developed scheme also supports an extension of the algorithm that admits the assignment of priorities to rules and to itemsets, in order to allow the user to produce a more controlled program as output. Nonetheless, the importance of the extension has not been already tested, and therefore it is beyond the subject of the present paper It would be also interesting to design a scheme that supports queries where the client provides an itemset class and values for support and confidence and the engine produces a maximal class of inferred associated itemsets as a response. This scheme is also under development, so we have not discussed this aspect here 


VI. CONCLUSION In this paper, we have presented a defeasible logic framework for managing associations that helps in reducing the number of rules found in a set of discovered associations. We have presented an induction algorithm for inducing programs in our logic, made of assumption schemas, a reduced set of association rules and a set of counter-arguments to conclusions called defeaters, guaranteeing that every pruned rule can be effectively inferred from the output. Our approach outperform those of [17], because all reduction compactions presented there can be expressed and induced in our framework, and several other patterns, particular to the given datasets, can also be found. In addition, since a set of definite clauses can be obtained from the induced programs, the knowledge obtained can be modularly inserted in a richer inference engine Abduction can be also attempted, asking for justifications that explain the presence of certain association in the dataset The framework presented can be extended in several ways Admitting defeaters to appear in the head of assumption, to define user interest Admitting arithmetic expressions within assumptions for adjustment in pruning Admitting set formation patterns as itemset constants Extending the scope, to cover temporal association rules REFERENCES 1]  R. Agrawal, and R. Srikant: Fast algorithms for mining association rules In Proc. Intl Conf. Very Large Databases. \(1994 2]  A. V. Aho, J. E. Hopcroft, J. Ullman. The design and analysis of computer algorithms, Addison-Wesley, 1974 3]  G. Antoniou, D. Billington, G. Governatori, M. J. Maher, A. Rock: A Family of Defeasible Reasoning Logics and its Implementation. ECAI 2000: 459-463 4]  G. Antoniou, D. Billington, G. Governatori, M. J. Maher: Representation results for defeasible logic. ACM Trans. Comput. Log. 2\(2 2001 5]  A. Basel, A. Mahafzah, M. Al-Badarneh: A new sampling technique for association rule mining, Journal of Information Science, Vol. 35, No. 3 358-376 \(2009 6]  R. Bayardo and R. Agrawal: Mining the Most Interesting Rules. In Proc of the Fifth ACMSIGKDD Intl Conf. on Knowledge Discovery and Data Mining, 145-154, \(1999 


7]  R. Bayardo, R. Agrawal, and D. Gunopulos: Constraint-based Rule Mining in Large, Dense Databases. Data Mining and Knowledge Discovery Journal, Vol. 4, Num-bers 2/3, 217-240. \(2000 8]  A. Berrado, G. Runger: Using metarules to organize and group discovered association rules. Data Mining and Knowledge Discovery Vol 14, Issue 3. \(2007 9]  S. Brin, R. Motwani, J. Ullman, and S. Tsur: Dynamic itemset counting and implication rules for market basket analysis. In Proc. ACMSIGMOD Intl Conf. Management of Data. \(1997 10] L. Cristofor and D.Simovici: Generating an nformative Cover for Association Rules. In ICDM 2002, Maebashi City, Japan. \(2002 11] Y. Fu and J. Han: Meta-rule Guided Mining of association rules in relational databases. In Proc. Intl Workshop on Knowledge Discovery and Deductive and Object-Oriented Databases. \(1995 12] B. Goethals, E. Hoekx, J. Van den Bussche: Mining tree queries in a graph. KDD: 61-69. \(2005 13] G. Governatori, D. H. Pham, S. Raboczi, A. Newman and S. Takur: On Extending RuleML for Modal Defeasible Logic. RuleML, LNCS 5321 89-103. \(2008  14] G. Governatori and A. Stranieri. Towards the application of association rules for defeasible rules discovery In Legal Knowledge and Information Systems, JURIX, IOS Press, 63-75. \(2001 15] J. Han, J. Pei and Y. Yin: Mining frequent patterns without candidate generation. In Proc. ACM-SIGMOD Intl Conf. Management of Data 2000 16] C. Hbert, B. Crmilleux: Optimized Rule Mining Through a Unified Framework for Interestingness Measures. DaWaK: LNCS 4081, 238247. \(2006 17] E. Hoekx, J. Van den Bussche: Mining for Tree-Query Associations in a Graph. ICDM 2006: 254-264 18] R. Huebner: Diversity-Based Interestingness Measures For Association Rule Mining. Proceedings of ASBBS Volume 16 Number 1, \(2009 19] B. Johnston, Guido Governatori: An algorithm for the induction of defeasible logic theories from databases. Proceedings of the 14th Australasian Database Conference, 75-83. \(2003 20] P. Kazienko: Mining Indirect Association Rules For Web Recommendation. Int. J. Appl. Math. Comput. Sci., Vol. 19, No. 1, 165 186. \(2009 21] M. Klemettinen, H. Mannila, P. Ronkainen, H. Toivonen, and A Verkamo: Finding interesting rules from large sets of discovered association rules. In Proc. 3rd Intl Conf. on Information and Knowledge 


Management. \(1994 22] M. J. Maher, A. Rock, G. Antoniou, D. Billington, T. Miller: Efficient Defeasible Reasoning Systems. International Journal on Artificial Intelligence Tools 10\(4 2001 23] C. Marinica, F. Guillet, and H. Briand: Post-Processing of Discovered Association Rules Using Ontologies. The Second International Workshop on Domain Driven Data Mining, Pisa, Italy \(2008 24] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal: Closed sets based discovery of small covers for association rules. In Proc. BDA'99 Conference, 361-381 \(1999 25] N. Pasquier, R. Taouil, I. Bastide, G. Stume, and  L. Lakhal: Generating a Condensed Representation for Association Rules. In Journal of Intelligent Information Systems, 24:1, 29-60 \(2005 26] P. Pothipruk, G. Governatori: ALE Defeasible Description Logic Australian Conference on Artificial Intelligence.  110-119 \(2006 27] J. Sandvig, B. Mobasher Robustness of collaborative recommendation based on association rule mining, Proceedings of the ACM Conference on Recommender Systems \(2007 28] W. Shen, K. Ong, B. Mitbander, and C. Zaniolo: Metaqueries for data mining. In Fayaad, U. et al. Eds. Advances in Knowledge Discovery and Data Mining. \(1996 29] I. Song, G. Governatori: Nested Rules in Defeasible Logic. RuleML LNCS 3791, 204-208 \(2005 30] H. Toivonen, M. Klemettinen, P. Ronkainer, K. Hatonen, and H Mannila: Pruning and grouping discovered association rules. In ECML Workshop on Statistics, Machine Learning and KDD. \(1995 31] M. Zaki: Generating Non-Redundant Association Rules. In Proc. of the Sixth ACMSIGKDD Intl Conf. on Knowledge Discovery and Data Mining, 34-43, \(2000 32] w3c. OWL Ontology Web Language Reference. In http://www.w3.org/TR/2004/REC-owl-ref-20040210 33] w3c. RDF/XML Syntax Specification. In: http://www.w3.org/TR/rdfsyntax-grammar 34] w3c. RDF Schema. In: http://www.w3.org/TR/rdf-schema      


 8   2  3\f            8  D    F  \b 1 8 & #J      b 1  1  4    2  


4 1    9  E 1  2 4 1    9 1   4      8 2  8 1  D 1        1 1  b 


     b b b b b  K            8          2 D 9   F  \b 1 8 ,+J  9 


     b 1     1 2  9 1  12 L 1   9  8       1  2      2   


     b b b b b  K            2  0 \b f  b\f      9       


  8 2   E 1   1     M13 31L 1    b  8E 1   1 #3\b?### 1  1     E 1   1 \b?###3        


1   1   b 1  2 2 18 2     8              1    2 \b 1    2  


    2          2   1 L 2 1   1   L 2 2    2 1  2        


    8  2H D \b A             2  2H D \b A 2 \f 3%\f  f   4%\f f !  , \f\b  C    2    2 


 6    3 1      253 6   1 L 2    6   1         f\b3\f       


               1     1     8 2    E       2  1   


     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


