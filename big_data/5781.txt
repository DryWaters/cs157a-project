Terabyte-sized Image Computations on Hadoop Cluster Platforms Peter Bajcsy, Antoine Vandecreme, Julien Amelot, Phuong Nguyen, Joe Chalfoun, Mary Brady Software and Systems Division Information Technol ogy Laboratory National Institute of Standards and Technology Gaithersburg, MD e-mail: {peter.bajcsy antoine.vandecreme, julien amelot, phuong.nguyen, joe.ch alfoun, mary.brady}@nist.gov   Abstract We present a characterization of four basic Terabyte-sized image computations on a Hadoop cluster in terms of their relative effici ency according to the modified Amdahl’s law. The work is motivated by the lack of standard benchmarks and stress tests for big image processing operations on a Hadoop computer cluster platform. Our benchmark design and evaluati ons were performed on one of the three microscopy image sets each consisting of over one half Terabyte. All image proce ssing benchmarks executed on the NIST Raritan cluster with Hadoop were compared against baseline measurements, such as the Terasort/Teragen designed for Hadoop testing previously image processing executions on a multiprocessor desktop and on NIST Raritan cluster using Java Remote Method Invocation \(RMI\ with multiple configurations. By applying our methodology to assessing efficiencies of computations on computer cluster configurations, we could rank computation configurations and aid scientists in measuring the benefits of running image processing on a Hadoop cluster Keywords- Big Data Industry Standards, Big Data Open Platform, Big Data Applications and Infrastructure I   I NTRODUCTION  Our objective is to characterize Terabyte-sized image processing computations in terms of their computational scalability on Hadoop computer cluster platforms with multi-processor nodes.  The computations of interest include image background \(flat field\entation image feature extraction and pyramid building for Deep Zoom visualization. These computations range from computationally intensive to data intensive, and operate either on thousands of Mega-pixel images \(image tiles on hundreds of a half Giga-p ixel images \(stitched images From an image processing perspective, these operations are very typical in the image to kn owledge workflow consisting of intensity corrections visualization and information extraction over regions of interest In general, computational platforms for processing large size images can be categ orized as multi-processor desktop computers, computer clusters, high-performance computing \(HPC\ources with big shared memory, grid computing on loosely couple d and networked computers and computing on novel hardware architectures \(e.g Graphical Processing Units \(GPUs\, Field Programmable Gate Arrays \(FPGAs\tages and disadvantages of several above categories for life science applications can be found in Schadt et al. [1  O u r wo rk fo cu ses on co m p u t er clusters in order to support run time and configuration decisions for research centers and governmental organizations with security concerns about the use of external computational resources. Nevertheless, the fundamental problems in char acterizing Terabyte-sized image processing computations are similar to cloud platforms and are also tied to security \(access to physical and virtual storage and compute resources Our specific focus is on Hadoop clusters because Hadoop includes file system and computation solutions for utilizing distributed computationa l resources. For example the advantages of Hadoop Dist ributed File System \(HDFS are in data replication and collocation of data and computation [2  T h e Map and Reduce com putational paradigm of Hadoop has been shown very efficient especially for sorting computations    This work is  exploring image processing computations on Hadoop clusters with HDFS while running Map tasks The main goal is to establish benchmarks and stress tests for big image data processing operations on a Hadoop cluster platform. The computational benchmarks provide experimental characteristics of the above image processing computations and image partitions for \(a assessing the efficiency of a computer cluster configuration with respect to a given computation \(number of nodes number of tasks per node, RAM per node, job resource managers such as Hadoop middleware or Java Remote Method Invocation \(RMI\nd Portable Batch System \(PBS scripts\d \(b\icting run times for various input data distribution patterns. The stress tests are useful for understanding the system limits, and how to manage hardware failures and deal with cluster heterogeneity The application specific motivation of our work comes from live cell imaging applications of very large fields of view \(FOV\. Given the advances in microscopy imaging and its applicati ons in many bio-medical applications, a single microscope generates a large number of spatial image tiles with several measurements at each location over time. The image tiles can be stitched into a large FOV image with hundreds of Mega-pixels or tens of Giga-pixels. Multiple time slices of stitched images accumulate to Terabytes of image data. These image data cannot be analyzed without computations that calibrate segment and visualize image channels, as well as extract 2013 IEEE International Conference on Big Data 978-1-4799-1293-3/13/$31.00 ©2013 IEEE 729 


image features for further analyses. Such image processing computations are very common across many application domains and represent a sample subset for our work Our computer science motivation lies in the lack of benchmarks and stress tests for big image processing operations on a Hadoop computer cluster platform.  For instance, as biologists increase robustness of their conclusions by increasing the number of experimental replicas, their access to elastic computational resources and to benchmarks for optimal reconfiguration become apparent The image data type and the variety of image processing computations are different from the existing Hadoop benchmark tests that primarily focus on textual data and low level operations While the Apache Hadoop distribution includes several basic tests, it does not provide sufficient understanding about executions of image processing operations. The four common basic tests are MRBench NNBench, TestDFSIO, and Gridmi 5]. MRBenc h is  designed to check whether small job runs are running efficiently. NNBench introduces a high HDFS management stress on the NameNode by requesting a large number of files to be created, read, renamed and deleted. TestDFSIO writes into or reads from a user specified number of files Gridmix mimics a variety of data-access patterns seen in practice. There is also an interest in understanding the Hadoop performance as a functio n of processor and cluster hardware configurations [6  or agai nst relational databas e  management system  8    These Hadoop tests together with general networking performance tests \(Netperf  Iozone  or ot her  11   pr ovi de e x cellent pr obes i n to a Hadoop cluster for low level operations.  However, these tests are typically applied to many small-sized elements such as random bytes or words which are different from high-dimensional images with varying file sizes Furthermore, the differences between the above computations and the image processing computations are in the additional support of various image formats on each cluster node and in spatial dependency of image computations \(e.g., spatially local intensity changes versus spatially global image filtering or co-occurrence matrixbased feature extraction In order to design a suite of benchmark tests for image processing computations on Hadoop clusters, one has to sample commonly used image computations and their various image inputs and outputs.  The approach is similar conceptually to the design of the PUMA test su where the Hadoop tests are ex tended by additional types of text-based computations \(e.g.,  K-mean clustering, inverted index, adjacency list, self-joi n\ can also be related to Alme p aring a Hadoop cl uster and dual core personal computer run times. This case study considered a set of image format conversion, auto-contrast sharpening and resizing operations applied to 200 Landsat satellite images of 139 Mega pixels each \(total 83.3 GB of imagery\our case, the samp ling of computations tries to cover a wider range of input and output scenarios, and the run times are compared against a broader range of benchmarks on 0.6TB of imagery. The image input and output scenarios are summarized in Table 1. The details of each computation are provided in Section II Next, the designed suite of Hadoop cluster benchmark and stress tests for image processing are compared against baseline com putations. In our work, the baseline computations are either \(1\well-benchmarked non-image computation on a Hadoop cluster such as Teragen or Terasort, or \(2\e image processing operations \(a\a mu lticore desktop instead of a Hadoop cluster, and \(b\unn ing on a cluster without Hadoop middleware Terasort is one of the most popular text-based computations for sorting a Terabyte of numbers  T h e algorithm running on a Ha doop cluster was initially developed by Yahoo and later advanc ed by G o ogl e   Ideally, the well-unde rstood Terasort com putational performance on Hadoop clusters can be related to a priori unknown performance of image processing computations on Hadoop clusters for similar input file sizes. We reviewed the Hadoop Image Processing Interface \(HIPI\ibrary t  focused on filtering and b undling very small images Table 1: Summary of computations, and input and output image data files  Type of Image Processing Spatial Extent of Image Processing Input & Output File Characteristics Computational Complexity Data-Access Pattern During Computations Flat Field Correction Local Input & Output Tens of thousands of a few MB size files Low two subtractions and one division per pixel Medium accessing three files and creating one file data skew on two input files Segmentation based on convolution kernels Global with fixed kernel Input & Output Hundreds of a half GB size files Medium tens of subtractions multiplications, comparisons per pixel Low accessing one file and creating one file Feature Extraction  Global with mask defined kernel Input Co-located pairs of hundreds of a half GB files Output hundreds of KB size tables High several thousands of basic numerical operations per pixel Medium accessing two files and creating one file Deep Zoom Pyramid Building Global with fixed kernel Input Hundreds of a half GB size files Output Millions of KB size files Medium tens of additions and divisions per pixel High creating thousands of directories, accessing one input file and writing millions of pyramid tiles 730 


HipiImageBundle & CullMapper classes\ before running Hadoop. In our case, Terabyte collections of several megabyte files do not fit HI PI and the bundling was replaced by the Hadoop Sequence File and tar representations. Finally, two of the baselines are desktop and cluster configurations without Hadoop. The desktop has six cores of Intel Xeon @ 2GHz with 64GB of RAM and hyper threading activated. The cluster is the NIST Raritan cluster which is composed of more than 800 heterogeneous nodes controlled by Portable Batch System \(PBS\job resource manager The novelty of the work lies in designing a suite of benchmark and stress tests for image processing computations on Hadoop clusters that can complement the existing tests in the Apache Hadoop project. The infrastructure could be integrated into a larger image processing community effort to build an open source image processing library runnin g on a Hadoop cluster The paper is organized as follows. Section II introduces the image processing computations and their characteristics Section III presents experimental data, hardware and software platforms, exploratory and baseline benchmarks and relative efficiency evaluations. We conclude with a summary in Section IV II  I MAGE P ROCESSING C OMPUTATIONS  To address the algorithmic development of image processing, one approach is to take an existing wellestablished image processing libra ry and enable running its functionality on a Hadoop cluster. A candidate for such a widely used library is an instance of NIH ImageJ/Fiji ImageJDev or BioImageXD 18]. T h is approac h face s challenges in executing functi onalities in a headless mode and in dealing with a code base that has not been designed with Terabyte image sizes in mind. People have blogged about “parallel Im azon cl oud by adding a Hadoop InputFor mat to handle image types in HDFS and encapsulating ImageJ operations in map and reduce methods. However, the encapsulation has been performed for a very small number of operations in the ImageProcessor class of the I mageJ library. In addition, the developed source code is not publicly available, the ImageProcessor-based execution lacks process control \(not knowing when execution is done\ and has the overhead of starting ImageJ and loading plugins These considerations led us to a development of our own image processing functionality in Java for the following computations: flat field co rrection, segmentation, image feature extraction and pyramid building for Deep Zoom visualization. These computations range from computationally intensive to data intensive, and have to operate either on thousands of Mega-pixel images \(image tiles\or on hundreds of a half Giga-pixel images \(stitched images\Next, we characterize these four computations A  Flat Field Correction Flat field correction \(FFC\described mathematically below            RAW FFC I xy DIxy Ixy WI x y DI x y 1 where   FFC I xy is the flat-field corrected image intensity   DI x y is the dark image acquired by closing the camera shutter   RAW I xy is the raw uncorrect ed image intensity and   WI x y is the flat field intensity acquired without any object to correct primarily fo r spatial shading. This is the simplest computation that consists of two subtractions and one division per pixel. It needs the DI and WI images colocated from the distributed execution perspective B  Segmentation There are many segmentation methods applied to cell  We selected a segm entation method that consists of four linear workflow steps: \(1 Sobel-based image gradient co mputation, \(2\holding by a value equal to twice the intensity histogram mode, \(3 morphological opening \(dilation of erosion\to remove small holes and islands, and \(4\ectivity analysis to assign the same label to each contiguous group of 4-connected pixels The Sobel-based image gradient is defined over a 3×3 convolution kernel, and estimates the gradient in the column and row directions. The gradient image contains the magnitude of each estimate as shown below 2 2   xy Gxy  2 10 1 1 1 1 1 1 2 0 2  1    1 10 1  1 1  1  1 1 x Ix y Ix y Ix y Ixy Ixy Ixy Ix y Ix y Ix y         1 2 1 1 1 1 1 1 000 1  1 12 1 1 1 1 1 1 y Ix y Ix y Ix y Ixy Ixy Ixy Ix y Ix y Ix y         Sobel filtering has been explor ed on a Hadoop cluster by Alme an input to parallel gradient domain computing e g   c o m putations of the divergence of gradient using Message Passing Interface MPI\run time versus image file size e.g., in  Fig 2 im prov e general understanding of the performance benchmarks for various Hadoop cluster setups Another widely used computation of segmentation is morphological opening \(dilation of erosion\. During this operation, a thresholded binary image is convolved with the min and max operators over a 3×3 kernel. This computation has been explored in the pa st on distributed memory  m ultip le-instruction-multiple-data \(MIMD Intel Paragon and single-instr uction-multiple-data \(SIMD MasPar MP-1\ng advantages of data partitioning 731 


C  Feature Extraction Similar to segmentation, there is a large number of feature extraction methods designed for cell microscopy images The number ranges from hundreds [2  24 to th ousa n ds   n put to im age characterization a nd classification. We have categorized features according to their type as intensity, shape and texture descriptors. Each feature type is represented by sample representatives listed in Table 2  Table 2. Description of extracted features Feature Type Feature Name Mathematical Model Intensity Basic central moments according to statistics: Mode, Mean, Standard Deviation, Skewness, Kurtosis, Fifth and Sixth moments Integral computations Shape Hu’s moment invariants are computed   perimeter, circularity, aspect ratio extend, orientation, eccentricity Integral computations Texture The Gray Level Co-Occurrence matrix is com following four features are extracted Correlation, Energy, Contrast and Homogeneity Directional counting  The features of intensity and shape types are extracted by implementing integral equations over a masked region, and hence can be denoted as integral image computations. The extraction of texture features is also viewed as a directional counting computation of all intens ity pairs within a masked region. In general, the directional counting computation has higher requirements on CPU RAM than the integral computations because there are many more counting cooccurrence bins than the central moment accumulator variables. In terms of computation, the counting is less demanding than computing the higher moment powers.  We have evaluated both integral and directional counting computations together because most microscopy image analyses do not have a priori knowledge about preferred sets of features  D  Pyramid Building One way to view images with very large pixel counts is to use the Deep Zoom image pyramid representation [29  the OpenSeadragon javascript library  T h e vis u alization of Terabyte-sized images is critical for many applications and requires building a multi resolution pyramid. This computation is I/O bound because it generates many small files for fast data transmission and rendering purposes. The pyramid building computation was explored by Kooper and  3 2] on a cl uster with 16 nodes, each node wit h  8 cores and 16GB RAM. It took 30 days to build a pyramid for a 79 Gigapixel image and ge nerated 1.6M tiles stored on redundant array of independent disks \(RAID-5\. A speed-up by a factor of 20 was achieved after custom image loading These previous benchmarks as well as the benchmarks with storage on NO RAID, RAID 0, and RAID 1 configurations contribute to a better understanding of the pyramid computation on a Hadoop cluster and serve as comparison benchmarks III  E XPERIMENTAL R ESULTS  We have executed about one hundred runs of image processing computations with the data set described in Section III.A  and with the Terasort generated data. The hardware and software specifications are provided in Section III.B as well as observations about our computational reliability in Section III.E. Sections III.D and III.E. document the relative efficiency of each computation and the suitability of individual image processing operations based on these benchmarks A  Characteristics of Image Data Sets We experimented with three data sets where each raw image data set is about 0.6TB. For benchmarking and stress testing, we have selected a data set that consists of 18×14 252 image tiles covering approximately 180 square millimeters of a stem cell colony dish, over five days under both phase contrast and green fluorescence channels, with images acquired every 15 minutes. This example data set is composed of 195,552 images at 2.8 MB/image equal to 0.527 TB of data \(388 time samples of 252 images with 2 color channels acquired over 97 hours every 15 minutes The H9 human embryonic stem cell line was engineered to produce green fluorescent protein \(GFP\ the influence of Oct4 promoter and cells we re cultured under feeder-free conditions on Matrigel TM After correcting the GFP image tiles, all tiles were stitched and yielded 388 stitched time frames stored in a TIFF file format \(0.527 TB  GFP only 0.264 TB\Table 3 summarizes the data with respect to image processing computations. Note that the “Size per File” refers to compressed files via pack bits of the TIFF format that become much larger in RAM after loading  Table 3. A summary of inputs and outputs for each benchmarked computation Type of Image Processing  Input Data in TIFF File Format  Output Data  mostly in TIFF File Format  Total Files Per-File Size Total Files Per-File Size Flat Field Correction 97,776 GFP channel corrected tiles~264 GB 2 bytes per pixel 2.83 MB 97,776 GFP channel corrected tiles~527G B 4 bytes per pixel 5.6MB Segmentation based on convolution kernels 388 frames of stitched images 219GB 2 bytes per pixel 593 MB 388 frames of mask images 86GB 2 bytes per pixel 71MB331MB Feature Extraction 388 frames of stitched and segmented images 219GB 80.7GB 2 bytes per pixel 593 MB  66MB336MB 388 files 40MB CSV file format 100KB 732 


Deep Zoom Pyramid Building 388 frames of stitched images 219GB 2 bytes per pixel 593 MB 6,596 folders 2,476,683 files; 151 GB JPG file format 2-18KB per file B  Computer Hardware and Software Characteristics We ran the benchmarks on the NIST Raritan cluster and on a desktop computer. Table 4 summarizes the cluster and desktop hardware and software specifications. The cluster nodes differ in terms of CPU speed and RAM, and are allocated to jobs based on the requested resources in a Portable Batch System \(PBS script. We installed Hadoop and Java 1.7 on the cluster to support Java code execution and Java Remote Method Invocation \(RMI\he desktop computer had similar software configuration to the cluster  Table 4. NIST Raritan cluster and test desktop characteristics  Specs Cluster Desktop Hardware Cluster Nodes 800 computer nodes having from 2 to 16 virtual processors with 4 to 32GB of RAM Intel Xeon @ 2GHz 6 cores, 64GB of RAM and hyper threading activated Networking 1Gbit/second  Software Java Virtual Machine Java version 1.7.0_17 Java\(TM\E Runtime Environment \(build 1.7.0_17-b02 Java HotSpot\(TM 64-Bit Server VM build 23.7-b01 mixed mode Java version 1.7.0_15 Java\(TM\E Runtime Environment \(build 1.7.0_15-b03 Java HotSpot\(TM 64-Bit Server VM build 23.7-b01 mixed mode Hadoop hadoop-2.0.3-alpha Operating System CentOS 5.9 Linux 2.6.18274.3.1.el5 x86_64 Ubuntu 12.10 Linux 3.5.0-28generic x86_64 File System Lustre parallel distributed file system ext4 on top of Logical Volume Manager \(LVM  C  Characteristics of Image Processing Benchmarking Software All four computations were implemented as independent Java libraries and used on the desktop, Java RMI and Hadoop cluster platforms. Thus the same image processing code is invoked regardless of the platform. The desktop implementation is a Java program starting a fixed number of threads \(specified on the command line\ using a fixed thread pool. One image is processed by each thread. As soon as a thread has finished processing an image, it starts processing a new image The Java RMI Application Programming Interface API\nvokes methods on remote computers \(i.e., on the cluster nodes\We build a simple job scheduler with a client-server architecture using the Java RMI API. The Java RMI-based server provides a method to fetch the next image to process. When a client node has finished processing an image, it notifies the server with another RMI call and fetches the next image to pro cess. Similar to the desktop implementation, the client node s can run multiple threads one per image In order to invoke the four image processing computations from Hadoop, we implemented a new Hadoop FileInputFormat named ImageInputFormat. This class reads an image file from HDFS and submits it as a map task input The map tasks then retr ieve a BufferedImage from the ImageInputFormat and pass it to the processing implementations. Once the processing is done, the result is saved back to HDFS directly from the map task. Thus, the jobs do not have any reduce task. The design of our Hadoop implementation leverages Hadoop data and computation colocation as summarized in Table 5. The key differences among the three platform-specifi c implementations lie in the computational elasticity associated with each platform and in the locality of the data and computation. We have observed on average 98.4% of tasks getting input data from local node for image segmentation and 99.8% of tasks for flat field correction computations except accessing two common files from HDFS Table 5.  Characteristics of co mputational elasticity and data computation collocation for the three experimental platforms Computational Platform Computational Elasticity Data & Compute Collocation Desktop Low limited by the RAM and CPU of the executing computer Yes: all data are on a local disk Java Remote Method Invocation RMI\/Raritan Cluster High nodes can be requested as needed No: all data are transferred over network to the computing node Hadoop/Raritan Cluster High nodes can be requested as needed Yes with high probability: After pushing 3 data replicas to HDFS Hadoop launches computations where the data are D  Benchmark and Baseline Runs We have documented the following samples in the space of image processing computations, and hardware and software configurations applied to the selected data set  Four image processing computations described in Table 1 and the Terasort computation from the Apache Hadoop test suite [4  Raritan hardware cluster configurations used for computations \(8 virtual processors @2.5 GHz available per node with local storage:  >100GB o  Number of client nodes: 5, 10, 20, 30, 40, 50 60 o  RAM on cluster nodes: 16GB \(Segmentation Java RMI\, 24GB \(Feature extraction\ 32GB RAM \(all other computations  Raritan software cluster configurations used for computations 733 


o  Cluster computation management: Simple Java RMI server started with PBS script or Hadoop middleware o  Number of map tasks per node: 1 \(all computations\ 2 \( Pyramid building  Desktop software configurations o  Number of threads: 1 \(all computations Feature extraction\5,10,12,16 \(Pyramid The numerical results are shown in Figure 1 through 6 Figure 1 sets our desktop baseline and represents run times of processing a Terabyte volume on a desktop with a 6-core CPU and 64GB of RAM  Figure 2 compares flat field correction with Teragen on the Hadoop Raritan cluster since both image and text operations are very I/O intensive. Figure 3 shows image segmentation and Terasort computations executed on the Raritan cluster for the same input file sizes and file numbers. The image spatial filtering \(kernel multip lications of an image matrix and sorting \(number/word comp arisons key discriminators between image and text processing Figure 4 illustrates the comparis on of Hadoop and Java RMI based management of the Raritan cluster for image pyramid building in terms of the numb er of parallel processes. The map tasks in Hadoop and the number of threads per node in Java RMI show similar performance with Hadoop outperforming Java RMI for larger number of nodes. Figure 5 and Figure 6 focus on run time decomposition of pyramid building and feature extraction computations to understand the overheads and gains for various cluster configurations The overhead of pushing data to HDFS in Hadoop could be minimized by running multiple computations on the same data over many nodes since the Hadoop-based computation is only slightly outperforming the Java RMI based computation \(~9% at 60 nodes, feature extraction By way of an aside, the numerical results in the graphs are reported for the number of client nodes that were allocated by the Raritan cluster. Thus, the number of requested nodes was higher than the number shown in the figures below. Computations that deal with a large number of small input files \(flat fiel d correction\ or output files pyramid building\ were programmed to package the files by either tarring them or creating a serialized Hadoop Sequence File for better I/O efficiency  Figure 1: Pyramid and feature extraction computations executed on a 6 core desktop with various number of threads. Feature extraction with more than 2 threads is limited by the desktop RAM size equal to 65GB \(each feature extraction job requires more than 24GB of RAM  Figure 2: Flat field correction \(FFC\mputation executed on the Hadoop Raritan cluster and compared to Teragen that generates the same number of files as FFC  Figure 3: Image segmentation and Terasort computations executed on Raritan cluster for the same input file sizes and file numbers. The Hadoop execution \(blue\ outperforms RMI execution \(red\more than 10 nodes and is faster than the Terasort computation. All power approximations have a residual R 2 higher than 0.949 indicating a very good fit 734 


 Figure 4: Pyramid computation executed on the Hadoop Raritan cluster with one or two map tasks per node and on the Java RMI cluster with one or two threads per node. Hadoop with K map tasks outperforms Java RMI with the same number of K threads for more than 25 nodes \(K=1 nodes \(K=2   Figure 5 Pyramid computation executed on the Hadoop Raritan cluster with one or two map tasks per node and presented with 5 contributions to each run time The key benefit of launching multiple map tasks is the reduction of the pyramid building time green color bars  Figure 6: Feature extraction executed on the Raritan cluster using Hadoop solid green line\and Java RMI \(dashed orange line using Hadoop is slightly faster than using Java RMI if the overhead of pushing data to HDFS is viewed as a one-time overhead.  All power approximations have a residual R 2 higher than 0.9778 indicating a very good fit E  Computational Reliability In our case, running benchmarks for image processing operations has also served the purpose of stress testing the Raritan Hadoop cluster.  By default, Hadoop middleware has a built-in mechanism for handling possible hardware failures or straggler tasks through speculative execution  have used t h e defaul t speculative execution setting and documented the errors of failed executions. The NIST Raritan cluster is quite hetero geneous. Although we did not observe single straggler tasks, we have encountered task failures due to the entire system overload such as many tasks failing at the same time, SSH connection closed as a function of possible network overload, socket timeout exception from Hadoop or sometimes failing to push data to HDFS without a notification. Future stress tests for image processing computations can be designed for the users running image processing on Hadoop clusters similar to the analyses reported based on text 35  F  Relative Efficiency of Image Processing Computations As the number of nodes in the cluster increases computations can be run in sh orter amounts of time, but the efficiency of computation naturally decreases. We adopted Am e asuri ng parallel efficiency and computed the relative efficiency of cluster computations elapsed time for 1 worker/client divided by the multiplication of M workers and elapsed time for M workers\ Our modification is introduced on the right side of Equation \(3\wo reasons. First, benchmarking of image processing computations is very time consuming when using only one node of a cluster. For all practical purposes, users should be able to compute the parallel efficiency based on the elapsed run time on any small number of cluster nodes. Second, each benchmark has an inherent temporal uncertainty due to variable system loads no matter how many times the execution is re-run. Relative efficiency values larger than one do not have practical meaning and can be avoided by considering the minimum sample for the numerator of the following equation  1 L RELATIVE M M TLT E M TMT 3 Here 1 T is the run time on a single cluster node M T is the runtime on M nodes where M is in the sample set of working nodes of size NumSamples 1  NumSamples ii Mw  and L T is the runtime on the number of L cluster nodes for which min LM M LT M T Ideally, the parallel efficiency would be always one for any number of nodes. The efficiency formula can also be extended in the future to include power considerations  M o dified relative efficiency coefficients for all four image processing and Terasort computations are reported in Figure 7 735 


 Figure 7: Modified relative efficiency computed for all sample points  Based on the results in Figure 7, we computed a cluster computing suitability score S per computation configuration according to Equation \(4\is an average of deviations from the ideal relative efficiency \(equal to one  1 1 1 1 NumSamples CONFIG CONFIG RELATIVE i SEi NumSamples 4 According to the scores shown in Figure 8, the majority of image processing operations executed on a Hadoop cluster outperformed their corresponding benchmarks run on Java RMI clusters or compared against number/text sorting computations for the same input file sizes IV  S UMMARY  There is a lack of algorithms for processing Giga to Terabyte-sized images that can leverage very powerful parallel and distributed hardw are architectures such as Hadoop clusters. We have researched four image processing algorithms and the corresponding Hadoop infrastructure for running these algorithms on Hadoop clusters. The software enabled \(a e characterization of a Hadoop cluster against several baseline measurements, and \(b\ent of relative efficiency of image processing computation running on a cluster and their suitability for Hadoop platforms. The research also serves as a potential contribution to the suite of be nchmark and stress tests for the Apache Hadoop project and to a future development of a standard for big image data processing on distributed platforms.  Similar results to those showed in Figure 8 could be developed as reference measurements to aid cluster performance ‘calibration’ across smaller groups and larger institutions or to assess benefits of cluster configurations for various computation types We concluded that image processing operations benefit from scalability on a Hadoop cluster because of the computational elasticity of cluster platforms and the collocation of data and computation in Hadoop. The spatial extent of image processing operations defined the data access pattern during many computations. It was a factor for the RAM requirements per node since a half GB size files had to be loaded without subdividing the input images. On the other side, close to one hundred images as inputs \(flat field correction\4 million images as outputs \(pyramid pose challenges on efficient data transmission to the cluster nodes, and handling I/O operations. Image processing on a Hadoop cluster would not be efficient without additional considerations of RAM requirements, data transmission packaging and I/O tasks, and therefore a Hadoop infrastructure for benchmarking image analyses becomes important.  To illustrate this point, although the total runtime of all reported benchmarks was about 95 hours, the actual runtime was about three times as much due to test replicates of runs, configuration efficiency experimentations and failed computations In the future, we plan to design specific stress tests for image processing computations running on Hadoop clusters. Our aim is to make the transitions from a desktop solution for image processing to a solution running on Hadoop cluster hardware architectures much easier for all scientists   Figure 8. Computation and configuration ranking according to the cluster computation efficiency score. Lower scores imply better cluster computing suitability V  A CKNOWLEDGMENT  This work was sponsored by NIST as a part of the Computational Science in Biol ogical Metrology project. We would like to acknowledge all project team members for their contributions VI  D ISCLAIMER  Commercial products are identified in this document in order to specify the experime ntal procedure adequately Such identification is not intended to imply recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the products identified are necessa rily the best available for the purpose VII  R EFERENCES    E E. Schadt, M. D. Linder m an J. Sorenson, L. Lee, and G. P Nolan, “Computational solutions to large-scale data management 736 


and analysis Nature reviews. Genetics vol. 11, no. 9, pp. 647 57, Sep. 2010   Xy ratex, “Using Lustre with Apache Hadoop,” 2010. [O nline Available http://wiki.lustre.org/images/1/1b/Hadoop_wp_v0.4.2.pdf Accessed: 31-May  O. O. Malley TeraBy t e So rt on Apache Hadoop,” 200 8  m a rk.org/Y a hooHadoop.p df Accessed: 31-May  O O Malley  Hadoop Benchm ar king in Workshop on Big Data Benchmarking 2012, no. May, pp. 1–13   Apache, “Hadoop Bench m arking Code Grepcode 2013 Online]. Available http://grepcode.com/file/repository.cloudera.com/content/reposito ries/releases/org.apache.hadoop/hadoop-test/0.20.2cdh3u0/org/apache/hadoop/mapred. [Accessed: 31-Ma   I N TEL, “Optim izing Hadoop  Deploym ents,” 2010. [Online  Available http://software.intel.com/sites/default/files/m/f/4/3/2/f/31124Optimizing_Hadoop_2010_final.pdf. [Accessed: 31-May   M. Stonebreake r  D. Abadi D. J  De witt, S  Madden, E  Paulson A. Pavlo, and A. Rasin, “MapReduce and Parallel DBMSs  Friends or Foes  Communications of the ACM vol. 53, no. 1 2010   P. Kent, “Hadoop Bench m arki ng from a SAS Perspective,” in Workshop on Big Data Benchmarking 2012, pp. 1–9   R. Jones, “NetPerf Bench m ark, Computer Program,” 2013  erf.org/netperf/. [Accessed 31-May    W  D Nor c ott and D. Capps Iozone Filesystem Benchmark 2013. [Onlin ailable http://www.iozone.org/docs/IOzone_msword_98.pdf. [Accessed 31-May    D. Heger, “Hadoop Perform ance Tu ning - A Prag m a tic  Iterative Available http://www.cmg.org/measureit/issues/mit97/m_97_3.pdf Accessed: 31-May  F. Ah m a d  S. Le e M Tothenthodi, and T. N. Vijay ku m a r PUMA Purdue MapReduce Benchmarks Suite, Tech. Report 2012. [Onlin ailable http://web.ics.purdue.edu/~fahmad/benchmarks.htm. [Accessed 31-May    M. H. Al m e er Cloud Hadoo p Map Reduce For Rem o te Sensing Image Analysis Journal of Emerging Trends in Computing and Information Sciences vol. 3, no. 4, pp. 637–644, 2012   C. Ny berg M. Shah, and N. Govindar a ju, “Sor t Benchm ar k W e b  http://sortbench m ark.org Accessed: 15-May  Google, “Google I/O conference Google’s developer conference 2012. [Online]. Available http://www.engadget.com/event/googleio2012/articles Accessed: 05-May  J. Lawrence, S. Arietta, C Sweeney  and L. Liu, “Hadoop Im age Processing Interface, Computer Program Available: http://hipi.cs.virginia.edu/about.html. [Accessed: 31May    W  Rasband, “Im a geJ Fiji & I m ag eJA & I m ageJ2, C o m puter Program ble  http://rsbweb.nih.gov/ij Accessed: 15-May  Bioim ageXD, Com puter Pr ogr am  2013. [Onli n e Av ailable http://www.bioimagexd.net   T. Plunkett M A Sick, and J. Su, “Cloud Analy tics To ols by  Serene Software Inc SBIR DOD/OSD 2010. [Online Available: http://www.sbir.gov/sbirsearch/detail/13117 Accessed: 15May  A. A  Di m a  J T Elliott, J J. Fillibe n M Halte r A. P e skin, J Bernal, M. Kociolek, M. C. Brady, H. C. Tang, and A. L. Plant Comparison of segmentation algorithms for fluorescence microscopy images of cells Cytometry. Part A: the journal of the International Society for Analytical Cytology vol. 79, no. 7 pp. 545–59, Jul. 2011   S. Philip, B Su mm a  P. B r e m er an d V. Pascucci  P a r allel Gradient Domain Processing of Massive Images,” in Eurographics Symposium on Parallel Graphics and Visualization 2011 2011, p. 9   M. D   Theys R M  Born M. D  Al le m a ng, and H J. Si ege Morphological Image Processing on Three Parallel Machines in Frontiers of Massively Parallel Computing, Sixth Symposium  1996, pp. 327–334   C. Bakal, J. Aach G. Church, and N   Perri m on Q uantitative morphological signatures define local signaling networks regulating cell morphology Science \(New York, N.Y vol. 316 no. 5832, pp. 1753–6, Jun. 2007   K. Huang and R. F  Murphy F ro m q u antitative m icrosc opy to automated image understanding Journal of biomedical optics  vol. 9, no. 5, pp. 893–912, 2004   N. Or lov, L. Sham ir  T M acur a J. Johnston, D. M   Eckley and I   G. Goldberg, “WND-CHARM: Multi-purpose image classification using compound image transforms Pattern Recognition Letters vol. 29, no. 11, pp. 1684–1693, Aug. 2008   M.-K Hu, “Visual Pattern Recognition by Mo m e nt Invariants IRE Transactions on Information Theory pp. 179–188, 1962   R. M. Haralick M. Shan m ugan, and I. Dinstein, “Textural features for Image Classification IEEE Transactions on Systems, Man and Cybernetics vol. SMC-3, no. 6, pp. 610–621 1973   Y. Sae y s I Inza a nd P Larrañaga, “A review of feature selection techniques in bioinformatics Bioinformatics \(Oxford England vol. 23, no. 19, pp. 2507–17, Oct. 2007   Microsoft, “Deep Zoo m Silverlight  Microsoft Developer Network \(MSDN 2010. [Onlin ailable http://msdn.microsoft.com/en-us/library/cc645050\(v=vs.95 Accessed: 27-May Open Seadragon  Open Seadragon project 2013. [On Available: http://openseadragon.codeplex.com/. [Accessed: 15May    R. Kooper and P Bajcsy   C o m putat ional Scalability of Large  Size Image Dissemination,” in IS&T/SPIE Electronic Imaging  2011, pp. 7872–23   R. Kooper, P  Bajc sy, and N   M Hernández S titching Giga Pixel Images Using Parallel Computing,” in IS&T/SPIE Electronic Imaging 2011, pp. 7872–17   Yahoo Hadoop Tutor i al fr o m  Yahoo  On-Line Tutorial  2013. [Onlin ailable http://developer.yahoo.com/hadoop/tutorial/module4.html Accessed: 14-May  R. Dudko, A. Sharm a and J. Tedesco, “Effective Failure Prediction in Hadoop Clusters,” 2012 l e https://wiki.engr.illinois.edu/download/attachments/195766887/J AR-2nd.pdf?version=3&modi ficationDate=1333424381000 Accessed: 31-May  E. Bor tnikov, A Fr ank, K Tivon, and E. Hillel, “Predicting Execution Bottlenecks in Map-Reduce Clusters Available https://www.usenix.org/system/files/conference/hotcloud12/hotcl oud12-final50.pdf. [Accessed: 31-May  I Foster Designing and Building Parallel Programs Chicago IL ADDISON WESLEY Publishing Company Incorporated 1995, p. 381   D H  W oo and H  S Lee Extending Am dahl s  Law for Ener gyEfficient Computing in the Many-Core Era IEEE Computer pp 24–32, 2008    737 


  10  te m p e r a tu r e  s e n s o r  pol ynom i a l  i s  0 9977  w hi l e  t he  R 2   fo r  t h e  ba s e pl a t e  t e m pe r a t ur e  s e ns or  pol ynom i a l  i s  0 9984   Ta b l e  8  pr e s e nt s  t he  pol ynom i a l  c oe f f i c i e nt s     Fi g u r e  14  q ue nc y  H z    Te m p e r a t u r e  d  Ce l s i u s   f o r  A u x i l i a r y  O s c i l l a t o r  a n d  Ba s e p l a t e   Ta b l e  8  Co e f f i c i e n t s  fo r  F r e q u e n c y  v s  T e m p e r a t u r e  Po l y n o m i a l s    Al t h o u g h  t h e  h i g h  R 2  va l ue s  s e e m  t o pr ovi de  r e a s on f or  co n f i d en ce i n  t h es e t em p er at u r e p r ed i ct i o n s   t h e E D L  th e r m a l e n v ir o n m e n t d if f e r s  n o ta b ly  f r o m  th a t in  c r u is e  St e a d y  t e m p e r a t u r e s  a n d  t h e r m a l  e q u i l i b r i u m  c h a r a c t e r i z e  th e  c r u is e  th e r m a l e n v ir o n m e n t  T h e  E D L  th e r m a l i ro n m e n t  i s  c h a ra c t e ri z e d  b y  ra p i d  h e a t i n g    A s  a  re s u l t   th e  S D S T  a n d  its  in te r n a l c o m p o n e n ts  w e r e  n o t in  th e r m a l eq u i l i b r i u m  d u r i n g  E D L   In  t h e rm a l  e q ui l i br i um   a s  s how n  Fi g u r e  15  be l ow   t he  A U X  O S C  i s  nor m a l l y s om e w ha t  wa r m e r  t h a n  t h e  b a s e p l a t e    Ho we v e r   a s  Fi g u r e  15   sh o w s  d u r i n g  E D L   t h e  b a se p l a t e  t e m p e r a t u r e  r o se  re l a t i v e l y  q u i c k l y  a n d  ra p i d l y  o v e rs h o t  t h e  A U X  O S C  te m p e r a tu r e   A lth o u g h  A U X  O S C  te m p e r a tu r e  r o s e  s lo w ly  to   c a tc h  u p   w ith  b a s e p la te  te m p e r a tu r e  th e  S D S T  w a s  n o t in  th e r m a l e q u ilib r iu m  d u r in g  E D L   T h is  im p a c ts  th e  accu r acy  o f  an y  f r eq u en cy  p r ed i ct i o n s  b as ed  o n  b as ep l at e te m p e r a tu r e     15  MS L  E D L  S D S T  T e m p e r a t u r e s  S 34 S 45 34 m y  a  As  a  b a c k up t o t he  pr i m e  70 m a n t e n n a   D S S 43   t he  si g n a l s f r o m  a b eam  w av eg u i d e  34 m a n t e n n a s   D S S 34    a h i g h  ef f i ci en cy  3 4 m a n t e n n a   S 45  w e r e  co m b i n ed  an d  r eco r d ed    k up  da t a  w a s  not  ne e de d in r e a l  d  wa s  a n a l y z e d  i n  p o s t ng  Di r e c t to h   re c e i v e d  b y  t h e  D S S S y dur i ng M S L  E D L  i s  s how n i n  Fi g u r e  16   Av e r a g e  c er to noi s e  pow e r   P c N o  p l o t t e d  i n  l i g h t  b l u e  m e a s u re d  by th e  E D A  us i ng t he  D S S S 45 a r r a y dur i ng E D L  w a s  dB     Fi g u r e  16  MS L  E D L  P c N o P e N o a n d R e s id u a l Fr e q u e n c y  w i t h  D SS S y  An t e n n a  se n si t i v i t y  i s m e a su r e d  b y  T  w h e r e  G  is   an t en n a g ai n   an d  T  is  th e  s y s te m   te m p e r a tu r e   G   fu n c t i o n  o f w a v e l e n g t h     phys i c a l  a pe r t ur e  a r e a   A p   ap er t u r e ef f i ci en cy     as  s h o w n  b el o w                    6    Ba s e d  s o l e l y  o n   ap er t u r e  ea  A p    T   a  34 m a n t e n n a  is  a b o u t 1 7  0  1 7  0  3 5  0  3 5    6   T   0   Ho we v e r   t h e  N 70 m a n t e n n a  al s o  h as  b et t er  ap er t u r e ef f i ci en cy     a n d  a  lo w e r  s y s te m  noi s e  t e m pe r a t ur e   T  th a n  th e  D S N  3 4 e    th e s e  f a c to r s  c o n s id e r e d  th e  T  of  a  D S N  34 m a n t e n n a  i s  ab o u t  1 8   t h e T  of  a  D S N  70 m a n t e n n a  8    pr e di c t e d di f f e r e nc e  i n a r r a y ga i n be t w e e n t he  70  an d  t w o  ar r ay ed  3 4 m a n t e n n a s  i s  10 g 10   1 0   18  18    4 44  dB   a s s um i ng no c om bi ni ng l os s   P c N o  is  p r o p o r tio n a l  T   Th e  m e a s u r e d  d i f f e r e n c e  i n  P c N o   S S S n  17   Th e  m e a n  m e a s u r e d  d i f f e r e n c e  f r o m  E 645 s e c onds  t o E 2 9 9  s e c o n d s  w a s  4  2 6  d B   Th e r e f o r e  m e a n  c o m b i n i n g  lo s s d u r in g th is tim e w a s  ab o u t  0  1 8                      0 1 23 45''67626 84  3 6 9 2 7 7    6593 9\(:6 562\(6\(2'47   0        0 1 5'695\(95 87 7 3 5   4 7 5 2  97599 562\(6\(&765 


  11   Fi g u r e  17  e re n c e  i n  D S S 43 a nd D S S S P c N o   Af t e r  E DL   E v e n t  R e c o r d s   E VR s   t h a t  l o g g e d  e a c h  t o n e  is s u e d  d u r in g  E D L  w e r e  obt a i ne d f r om  M S L   Th e s e  l o g s  we r e  c o m p a r e d  wi t h  t h e  r e a l tim e  r e s u lts  p r o v id e d  b y  th e  ED A  t o  d e t e r m i n e  p e r f o r m a n c e    Th e  D TE c o m m u n i c a t i o n s  sy st e m  r e c e i v e d  a n d  c o r r e c t l y  i d e n t i f i e d  1 0 0   o f  ra d i a t e d   i n r e a l tim e  d u r in g  M S L  E D L   Th e s e  r e s u l t s  a r e  co n s i s t en t  w i t h  t h e t h eo r et i cal  pr oba bi l i t i e s  of  c a r r i e r  acq u i s i t i o n  t r ack i n g  an d  d at a t o n e d et ect i o n  co m p u t ed  i n  Se c t i o n  3   5   C ON   Th e  D i r e c t to Ea r t h  X ba nd c om m uni c a t i ons  s ys t e m  ut i l i z e d dur i ng M S L  E D L  s uc c e s s f ul l y de t e c t e d a l l  ra d i a t e d   de s pi t e  c ha l l e ngi ng s i gna l  dyna m i cs  w i t h  l ar g e u n k n o w n  ch an g es  i n  D o p p l er  f r eq u en cy   r at e  an d  accel er at i o n   Fu t u r e  m i s s i o n s  w i t h  p e r i o d s  o f  r a p i d  a n d  u n k n o w n  s i g n a l  dyna m i c s  s uc h a s  Ma r s  o r  i c y  m o o n  la n d e r s  can  l ev er ag e fr o m  t h e  M S L  de s i gn f or  D T E  c om m uni c a t i ons    6   A CK NO W L E DG E M E NT S   T he  a ut hor s  w oul d l i ke  t o a c know l e dge  t he  c ont r i but i on s   Ja n  T a r sa l a   te s tin g  o f  th e  E D A  p r io r  to  M S L  E D L  us i ng a  P R S R  a nd M S L  t e s t be d    Th e  a u t h o r s  w o u l d   th a n k  J e r e m y  S r  fo r  p r o v i d i n g  6 D O F  s i m u l a t i o n  d a t a  th a t w a s  v a lu a b le  in  c o n f ig u r in g  th e  E D A   Th e  a u t h o r s  wo u l d  a l s o  l i k e  t o  t h a n k  t h e  C DS C C  s t a t i o n  p e r s o n n e l  f o r  th e ir  e x c e lle n t s u p p o r t a n d  ope r a t i ons  of  t he  D S N  eq u i p m en t  an d  t h e F u l l  S p ect r u m  P r o ces s o r  A r r ay  i n  u   Th i s  r e s e a r c h  w a s  c a r r i e d  o u t  a t  t h e  J e t  P r o p u l s i o n  La b o r a t o r y   C a l i f o r n i a  I n s t i t u t e  o f  Te c h n o l o g y    Co p y r i g h t  2012 C a l i f or ni a  I ns t i t ut e  of  T e c hnol ogy  Go v e r n m e n t  sp o n so r sh i p  a c k n o w l e d g e d     


  12  R EF ER EN C ES   1  E  S a t o r i u s   P   Es t a b r o o k   J   W i l s o n   D   F o rt    D i re c t to  Ea r t h  c o m m u n i c a t i o n s  a n d  s i g n a l  p r o c e s s i n g  f o r  M a r s  ex p l o r at i o n  r o v er  en t r y   d es cen t  an d  l an d i n g   T h e In t e rp l a n e t a ry  N e t w o rk  P ro g re s s  R e p o rt   IP N  P ro g re s s  Re p o r t  4 2 2003  2 A n d re  J o n g e l i n g  an d  S u s an  F i n l ey     M ar s  S ci en ce La b o r a t o r y  Te l e c o m  S y s t e m  En g i n e e r i n g  P r e  Re v i e w   E D L  D a t a  A n a l y s i s  S i m u l a t i o n s  Re s u l t s     A p r i l  24  2007   3 W   J   H u rd   P   E s t a b ro o k   C   S   R a c h o   a n d  E   S a t o ri u s   C r i t i cal  sp acecr af t to ear t h  co m m u n i cat i o n s   ex p l o r at i o n  r o v er   M E R   en t r y   d es cen t  an d  l an d i n g   Pr o c   I E E E  A e r o s p a c e  C o n f e r e n c e   v o l  3   p p   1 2 8 3  MT   Ma r c h  2 0 0 2    4 M  S o r i a n o   S   F i n l e y   A   J o n g e l i n g   D   F o r t   C   G o o d h a r t   D  R o g s t a d   R   Na v a r r o    Sp a c e c r a f t to Ea r t h  Co m m u n i c a t i o n s  fo r J u n o  a n d  M a rs  S c i e n c e  L a b o ra t o ry  Cr i t i c a l  E v e n t s   P r o c  I E E E  A e r o sp a c e  C o n f e r e n c e   M T   2   5 A   M a k o v s k y   P   Il l o t t   J   T a y l o r    M a rs  S c i e n c e  La b o r a t o r y  Te l e c o m m u n i c a t i o n s  S y s t e m  D e s i g n    D e e p  Sp a c e  C o m m u n i c a t i o n s  a n d  N a v i g a t i o n  Sy s t e m s  C e  of  E xc e l l e nc e  D e s i gn a nd P e r f or m a nc e  S um m a r y S e r i e s   No v e m b e r  2 0 0 9   6 M   S o ri a n o  a n d  P   E s t a b ro o k    M S L  E D L  S i m u l a t i o n s   i n t e rn a l  d o c u m e n t   J e t  P ro p u l s i o n  L a b o ra t o ry   P a s a d e n a   CA   M a y  7   2 0 1 2   7  Sa t o r i u s  R e v i s e d  T h r e s h o l d s  f o r  E D L    i n t e r n  doc um e nt    J e t  P r opul s i on L a bor a t or y  P a s a de na   C A   Ja n u a r y  1 4   2 0 0 3   8 A   K w o k     M o d u l e  2 0 6  Te l e m e t r y  G e n e r a l  In fo rm a t i o n    i n DS N  T e l e c o mmu n i c a t i o n s  L i n k  De s i g n  k B   D S N  N o  8 1 0 005   P a s a de na  Ca l i f o r n i a   J P L   Oc t o b e r  3 1   2 0 0 9  ht t p   e i s  j pl  na s a g o v d e e p s p a c e d s n d o c s 8 1 0 005     


  13  M el i s s a  S o r i a n o  ff f tw a r e  e n g in e e r  in  th e  T r a c k in g  Sy s t e m s  and A ppl i c at i ons  Se c t i on at  t he  J e t  P r opul s i on L abor at or y    She  has  de v e l ope d r e al  so f t w a re  f o r t To  co m m u n i ca t i o n s  w i t h  M a r s  Sc i e nc e  L abor at or y  dur i ng E nt r y   De s c e n t   a n d  L a n d i n g   th e  L o n g  W a v e le n g th  Ar r a y   N AS A s  Br e a d b o a r d  Ar r a y   a n d  t h e  W i d e b a n d  VL BI  S c i e n c e  Re c e i v e r  u s e d  i n  t h e  D e e p  S p a c e  N e t w o r k   Me l i s s a  i s  a l s o  cu r r en t l y t h e s o f t w a r e co g n i z a n t  en g i n eer  f o r  t h e D S C C  Do w n l i n k  A r r a y   She  has  a B  S   fr o m  C a lte c h  d o u b le  m a jo r  in  E le c tr ic a l a n d  C o m p ut e r  E ngi ne e r i ng and B us i ne s s  Ec o n o m i c s  a n d  M a n a g e m e n t    S h e  a l s o  h a s  a n  M  S    Co m p u t e r  S c i e n c e  fr o m G e o r g e M a so n  U n i v e rsi t y   Sus a n F i nl e y  is  a  k e y  s ta ff me mb e r  i n  t h e  P r o c e s s o r  S y s t e ms  De v e l o p me n t  Gr o u p  a t  J P L     is  th e  s u b s y s te m  e n g in e e r  fo r  th e  Fu ll S p e c tr u m  P r o c e s s o r  su b sy st e m  d e p l o y e d  i n  N A S A  s De e p  S p a c e  N e t w o r k     exp er i en ce i n cl u d es  t h e o p er a t i o n  of  t he  E D A  f or  bot h of  t he  M E R  l andi ngs  on M ar s  as  w e l l  as  th e  o p e r a tio n  o f th e  R a d io  S c ie n c e  R e c e iv e r  fo r  th e  la n d in g  o f th e  H u y g e n s  P r o b e  o n  T it an and f or  t he  P hoe ni x  l andi ng on s    Da v i d  t  re c e i v e d  a  B  A  S c  i n  En g i n e e r i n g  Ph y s i c s  a n d  M  S c  i n  As t r o n o m y  f r o m  t h e  U n i v e r s i t y  o f  To r o n t o  a n d  a n  M S c   a n d  P h  D   i n  Ra d i o  As t r o n o m y  f r o m  t h e  U n i v e r s i t y  of  M anc he s t e r    H e  j oi ne d N R C  C a n a d a  i n  1 9 7 2  a n d  w o r k e d  o n  a l l  as pe c t s  of  V L B I  unt i l  1987   H e  su b se q u e n t l y  j o i n e d  J P L  i n  se c t i o n  3 3 5  a n d  w o rk e d  o n  a  num be r  of  har dw ar e  and s of t w ar e  pr oj e c t s  f or  t he   be c am e  s upe r v i s or  of  t he  P r oc e s s or  Sy s t e m s  de v e l opm e nt  Gr o u p  f o r  t h e  t w o  y e a r s  p r i o r  t o  r e t u r n i n g  t o  N R C  i n  2 0 0 2   Un t i l  h i s  r e t i r e me n t  i n  2 0 1 0  h e  w o r k e d  o n   Co r r e l a t o r  P r o j e c t    No w a   G u e s t  W o r k e r    h e  h e l p s  o u t  wi t h  t h e  E V L A  a s  i t  b e c o m e s f u l l y  o p e ra t i o n a l  a n d  w i t h  oc c as i onal  que s t i ons  f r om  J P L    Br i a n  S c h r a t z  is  th e  le a d  e n g in e e r  fo r  th e  E D L  te le c o m m u n ic a tio n s  o n  th e  Ma r s  S c i e n c e  L a b o r a t o r y  m i s s i o n  a n d  a m e m be r  of  J P L  s  C om m uni c at i ons  Sy s t e m s  and O pe r at i ons  gr oup      jo in e d  J P L  th r e e  y e a r s  a g o   B S  E E  a n d  M  S  E E   Pe nns y l v ani a St at e  U ni v e r s i t y    Pe t e r  I l o t t  is  th e  te le c o m m u n ic a tio n s  sy st e m  l e a d  f o r t h e  M S L  m i ssi o n   H e  has  w or k e d on s pac e c r af t  te le c o m m u n ic a tio n s  s y s te m  d e s ig n  fo r  2 5  y e a r s  1 1  y e a r s  o n  c o m m e r c ia l sp a c e c ra f t   a n d  si n c e  2 0 0 0  a t  J P L    wo r k e d  o n  M E R   P h o e a te le c o m m u n ic a tio n s  s y s te m  e n g in e e r  Pe t e r  w o r k e d  o n  a l l  t h e  M a r s  ED L   e n t r y  and la n d in g   e ffo r ts  s in c e  M E R  a n d  in  b e tw e e n  M a r s  m is s io n s  he l pe d out  on t he  D e e p I m pac t  and C l oudat  mi s s i o n s  a t  J P L   He  c u r r e n t l y  s u p p o r t s  t h e  M S L  s u r f a c e  mi s s i o n  p h a s e   a n d  i s  th e  te le c o m m u n ic a tio n s  le a d  fo r  th e  E u r o p a  m is s io n  cu r r en t l y u n d er  s t u d y  I l o t t  h o l d s  B S c  M S c  a n d  P h D  de gr e e s  i n phy s i c s  and e l e c t r i c al  en g i n eer i n g  f r o m  M cG i l l  i st y  o f  M o n t re a l    i  re c e i v e d  t h e  B  S  E  E   and t he  M  S E  E   i n 1997 and t he  Ph  D   i n  El e c t r i c a l  En g i n e e r i n g  i n  2003  al l  f r om  U C L A    He  h a s  b e e n  em p l o yed  a t  t h e Jet  P r o p u l s i o n  La b o r a t o r y  a s  a  Te l e c o m m u n i c  en g i n eer  s i n ce 1 9 9 9  a n d  h a s  s er ved  on t he  M ar s  E x pl or at i on R ov e r   DA W N   C a s s i n i   J u n o   a n d  M a r s  Sc i e nc e  L abor at or y  pr oj e c t s     Po l l y  E s t a b r o o k  is  th e  d e p u ty  ma n a g e r  o f  t h e  C o mmu n i c a t i o n  Ar c h i t e c t u r e s  a n d  Re s e a r c h  S e c t i o n  at  J P L     She  i s  a m e m be r  o f N A S A  s  Spac e  C om m uni c at i on and Na v i g a t i o n  P r o g r a m  s u p p o r t i n g  t h e  de f i ni t i on of  t he  N A SA  s  f ut ur e  In t e g r a t e d  C o m m u n i c a t i o n  a n d  Na v i g a t i o n  Ne t wo r k  a n d  i s  a  m e m b e r  o f  t h e  I n t e g r a t e d  Sy s t e m  E ngi ne e r i ng t e am  f or  t he  M ar s  Sc i e nc e  L abor at or y  r   Fr o m  2 0 0 5  t o 2010  s he  l e d s e v e r al  c om m uni c at i on sy st e m  d e si g n  t e a m s w i t h  t h e  g o a l  o f  d e f i n i n g  t h e  mo d i f i c a t i o n s  t o  N A S A  s  S p a c e  C o mmu n i c a t i o n  a n d  Na v i g a t i o n  i n f r a s t r u c t u r e  n e e d e d  t o  s u p p o r t  t h e  p l a n n e d  hum an m i s s i ons  t o t he  M oon and M ar s   F r om  2000 t o 2004 sh e  w a s t he  l e ad t e l e c om  s y s t e m  e ngi ne e r  f or  t he  M ar s  Ex p l o r a t i o n  Pr o j e c t   r e s p o n s i b l e  f o r  t h e  p e r f o r m a n c e  o f  t h e  en t r y d es cen t  a n d  l a n d i n g  t el eco m m u n i ca t i o n s  s ys t em  a n d  fo r  th e  o v e r a ll d e s ig n  a n d  p e r fo r m a n c e  o f th e  D ir e c t to  Ea r t h  a n d  r e l a y  c o m m u n i c a t i o n s  s y s t e m s   In  2 0 0 4   D r   Es t a b r o o k  r e c e i v e d  t h e  N AS A Ex c e p t i o n a l  Ac h i e v e m e n t  Me d a l  f o r  h e r  w o r k  o n  t h e  Ma r s  E x p l o r a t i o n  R o v e r  T e l e c o m  Sy s t e m   She  has  w r i t t e n ov e r  35 t e c hni c al  pape r s  and ch a i r ed  n u m er o u s  I E E E  a n d  A I A A  co n f er en ce S es s i o n s    Po l l y  Es t a b r o o k  r e c e i v e d  h e r B  A   i n  e n g i n e e ri n g  p h y si c s fr o m  th e  U n iv e r s ity  o f C a lifo r n ia  B e r k e le y  a n d  M S  a n d  Ph  D   d e g r e e s  i n  e l e c t r i c a l  e n g i n e e r i n g  f r o m  S t a n f o r d  Un i v e r s i t y   S t a n f o r d   C A      


  14  Ka m a l  O u d r h i r i  is  a  s e n io r  r  in  th e  R a d io  S c ie n c e  Sy s t e m s  G r oup at  NA S A  s  J e t  Pr o p u l s i o n  L a b o r a t o r y   As  a co n t r a ct  t ech n i ca l  m a n a g er   Ou d r h i r i  lti di s c i pl i nar y  te a m s  th r o u g h  th e  de s i gn  im p le m e n ta tio n  a n d  d e liv e r y  of  flig h t h a r d w ar e  t o t he  r adi o sc i e n c e  c o m m u n i t y   Ov e r  t h e  l a s t  d e c a d e   Ou d r h i r i  se rv e d  i n  key r o l es  o n  m u l t i p l e N A S A  mi s s i o n s   T h e  M a r s  E x p l o r a t i o n  s  M E R   t h e  In t e r n a t i o n a l  C a s s i n i  m i s s i o n  t o  Sat ur n T he  GR A I L  l u n a r  mi s s i o n  a n d  T h e  M a r s  S c i e n c e  La b o r a t o r y     Da n i e l  K a h a n  is  a s e ni or  m e m be r  of  S ci en ce S ys t em s  G r o u p  at  NA S A  s  J e t  Pr o p u l s i o n  La b o r a t o r y   Ov e r  t h e  l a s t  ei g h t  yea r s   h e h a s  pr ov i de d e ngi ne e r i ng s uppor t  f or  t he  i o s c i e nc e  c om m uni t y   NA S A  m i s s i o n s   i n c l u d i n g  M a r s  G l o b a l  Sur v e y or   M ar s  R e c onnai s s anc e  Or b i t e r   th e  G R A I L  lu n a r  m is s io n  th e  In t e r n a t i o n a l  C a s s i n i  mi s s i o n  t o  S a t u r n   a n d  Ma r s  S c i e n c e  La b o r a t o r y   Ed g a r  H   S a t o r i u s  is  a  p r in c ip a l me mb e r  o f  t h e  t e c h n i c a l  s t a f f  i n  th e  F lig h t C o m m u n ic a tio n s  Sy s t e m s  Se c t i on of  t he  J e t  Pr o p u l s i o n  L a b   H e  p e r f o r m s  sy st e m s a n a l y si s i n   de v e l opm e nt  of  di gi t al  s i gnal  e ssi n g  a n d  c o m m u n i c a t i o n s sy st e m s w i t h  sp e c i f i c  a p p l i c a t i o n s t o  b l i n d  d e m o d u l a t i o n   di gi t al  di r e c t i on f i ndi ng and di gi t al  r e c e i v e r s   H e  has  publ i s he d ov e r  90 ar t i c l e s  and hol ds  t w o pat e nt s  i n t he  f i e l d of  di gi t al  s i gnal  pr oc e s s i ng and i t s  appl i c at i ons   I n a ddi t i on  he  i s  an A dj unc t  A s s oc i at e  P r of e s s or  at  t he  U ni v e r s i t y  of  Sout he r n C al i f or ni a w he r e  he  t e ac he s  di gi t al  s i gnal  pr oc e s s i ng c our s e s   H e  r e c e i v e d hi s  B  Sc   i n e ngi ne e r i ng fr o m  th e  U n iv e r s ity  o f C a lifo r n ia  L o s  A n g e le s  a n d  th e  M S  and P h D   de gr e e s  i n  el ect r i ca l  en g i n eer i n g  f r o m  t h e Ca l i f o r n i a  I n s t i t u t e  o f  T e c h n o l o g y   P a s a d e n a   Ca l i f o r n i a   


Example 2: Identifying Causal Factors CAUSAL FACTORS  Indicators – Data Element Methods – Identifying Context and Causes  Time of day  Date range \(month day  Aircraft type  Fix or coordinates  Originating airport  Destination airport  Weather notes We pinpoint the sequencing of flights on the IAH Junction Seven departure \(at CUZZZ\the specified wind conditions to find cases wher e a B737-900 at 20,000 feet precedes by 11 miles a B737-800 at 17,000 feet  Search related data sets including ASDI flight tracks, local traffic and congestion  Weather conditions for alter native causes \(winds aloft shear and convecti ve activity  Airline specific informati on \(repeated occurrence of event in aircraft type Big data gives us visibility into contextual factors even if specific data points are missing such as a specific date or route Big-data analytics gives us insight into unreported factors as well 


Example 3: Correlating Utilization and Delays  60 65 70 75 80 85 90 95 100 7 9 11 13 ONTIME DEPARTURE PERFORMANCE HOURS OF DAILY UTILIZATION 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Narrowbodies By Day of Week 60.0 70.0 80.0 90.0 100.0 7.0 9.0 11.0 13.0 Widebodies by Day of Week Daily Utilization vs. On-time Departures January 2013 System Operations Correlation Coefficient -0.53 Includes AA, AC, AS B6 F9, FL, NK, UA, US VX and WN SOURCE masFlight \(masflight.com COMPARING OTP AND UTILIZATION 


 6.2 6.0 5.8 5.8 5.2 4.9 LGB JFK BOS MCO DCA FLL JetBlue Focus Average Daily Deps per Gate Used UTILIZATION BY HUB Example 4: Daily Utilization of Gates, by Hub Big-data analysis of different carriers daily departures per gate used SOURCE masFlight \(masflight.com June 1 through August 31, 2012 Gates with minimum 1x daily use 7.7 7.4 7.2 6.2 6.1 5.8 3.8 3.6 ORD LAX SFO EWR DEN IAH IAD CLE United Airlines Hubs Average Daily Deps per Gate Used 7.8 6.4 5.5 5.4 5.3 4.4 4.3 4.0 SEA SAN PDX ANC SFO GEG LAX SJC Alaska Airlines Hubs Average Daily Deps per Gate Used 7.2 6.9 6.8 6.4 5.0 2.7 ORD DFW LAX LGA MIA JFK American Hubs Average Daily Deps per Gate Used 7.2 6.9 6.6 4.9 4.2 CLT DCA PHL PHX BOS US Airways Hubs Average Daily Deps per Gate Used 6.6 5.9 5.5 4.7 MCO BWI ATL MKE AirTran Hubs Average Daily Deps per Gate Used ne pe 


Conclusions for Big Data in Aviation  Big-data transforms operational and commercial problems that were practically unsolvable using discrete data and on-premises hardware  Big data offers new insight into existing data by centralizing data acquisition and consolidation in the cloud and mining data sets efficiently  There is a rich portfolio of information that can feed aviation data analytics  Flight position, schedules, airport/gate, weather and government data sets offer incredible insight into the underlying causes of aviation inefficiency  Excessive size of each set forces analysts to consider cloud based architectures to store, link and mine the underlying information  When structured, validated and linked these data sources become significantly more compelling for applied research than they are individually  Today’s cloud based technologies offer a solution CONCLUSIONS 


Conclusions:  Our Approach  masFlight’s data warehouse and analysis methods provide a valuable example for others attempting to solve cloud based analytics of aviation data sets  masFlight’s hybrid architecture, consolidating secure data feeds in on-premises server installations and feeding structured data into the cloud for distribution, addresses the unique format, security and scale requirements of the industry  masFlight’s method is well suited for airline performance review competitive benchmarking, airport operations and schedule design and has demonstrated value in addressing real-world problems in airline and airport operations as well as government applications CONCLUSIONS 





