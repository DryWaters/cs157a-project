An Online Algorithm for Segmenting Time Series Eamonn Keogh Selina Chu David Hart Michael Pazzani Department of Information and Computer Science University of California Imine California 92697 USA eamonn, selina, dhart pazzani 0 ics uci.edu Abstract In recent years there has been an explosion of interest in mining time series databases As with most computer science problems representation of the data is the key to efficient and effective solutions One of the most commonly used representations is piecewise linear approximation This 
representation has been used by various researchers to support clustering classification indexing and association rule mining of time series data A variety of algorithms have been proposed to obtain this representation with several algorithms having been independently rediscovered several times In this paper we undertake the first extensive review and empirical comparison of all proposed iechniques We show that all these algorithms have fatal flaws from a data mining perspective We introduce a novel algorithm that we empirically show to be superior to all others in the literature 1 Introduction In 
recent years, there has been an explosion of interest in mining time series databases As with most computer science problems representation of the data is the key to efficient and effective solutions Several high level representations of time series have been proposed including Fourier Transforms  1,131 Wavelets 4 Symbolic Mappings 2 5 241 and Piecewise Linear Representation PLR In this work we confine our attention to PLR perhaps the most frequently used representation 8 10 12 14 15 16 17 18 20 21 22 25, 27 28 30 311 
Intuitively Piecewise Linear Representation refers to the approximation of a time series T of length n with K straight lines. Figure 1 contains two examples Because K is typically much smaller that ti this representation makes the storage transmission and computation of the data more efficient. Specifically in the context of data mining the piecewise linear representation has been used to Support fast exact similarly search  131 Support novel distance measures for time 
series including 223fuzzy queries\224 27 281 weighted queries  151 multiresolution queries 3 1 181 dynamic time warping 22 and relevance feedback  141 Support concurrent mining of text and time series  171 0 Surprisingly in spite of the ubiquity of this representation with the exception of  there has been little attempt to understand and compare the algorithms that produce it Indeed there does not even appear to be 
a consensus on what to call such an algorithm For clarity we will refer to these types of algorithm which input a time series and return a piecewise linear representation as segmentation algorithms The segmentation problem can be framed in several ways Given a time series T produce the best representation using only K segments Given a time series T produce the best representation such that the maximum error for any segment does not exceed some user-specified threshold max-error  
0 Given a time series T produce the best representation such that the combined error of all segments is less than some user-specified threshold total-max-error As we shall see in later sections not all algorithms can support all these specifications Segmentation algorithms can also be classified as batch or online This is an important distinction because many data mining problems are inherently dynamic 30 121 Data mining researchers who needed to produce a piecewise linear approximation have typically either independently rediscovered an algorithm or used an approach 
suggested in related literature For example from the fields of cartography or computer graphics 6 9 261 Support novel clustering and classification algorithms Support change point detection 29 81 51 1 Figure 1 Two time series and their piecewise linear representation A Space Shuttle Telemetry B Electrocardiogram ECG In this paper we review the three major segmentation approaches in the literature and provide an extensive empirical evaluation on a very heterogeneous collection of 0-7695-1 119-8/01 17.00 0 2001 IEEE 289 


datasets from finance medicine manufacturing and science The major result of these experiments is that only online algorithm in the literature produces very poor approximations of the data and that the only algorithm that consistently produces high quality results and scales linearly in the size of the data is a batch algorithm These results motivated us to introduce a new online algorithm that scales linearly in the size of the data set, is online, and produces high quality approximations The rest of the paper is organized as follows In Section 2 we provide an extensive review of the algorithms in the literature We explain the basic approaches and the various modifications and extensions by data miners In Section 3 we provide a detailed empirical comparison of all the algorithms We will show that the most popular algorithms used by data miners can in fact produce very poor approximations of the data The results will be used to motivate the need for a new algorithm that we will introduce and validate in Section 4 Section 5 offers conclusions and directions for future work 2 Background and Related Work In this section we describe the three major approaches to time series segmentation in detail Almost all the algorithms have 2 and 3 dimensional analogues which ironically seem to be better understood A discussion of the higher dimensional cases is beyond the scope of this paper We\222 refer the interested reader to 9 which contains an excellent survey Although appearing under different names and with slightly different implementation details most time series segmentation algorithms can be grouped into one of the following three categories Sliding Windows A segment is grown until it exceeds some error bound The process repeats with the next data point not included in the newly approximated segment Top-Down The time series is recursively partitioned until some stopping criteria is met Bottom-Up Starting from the finest possible approximation segments are merged until some stopping criteria is met Table 1 T T[a:b Seg_TS create-segment\(T The notation used in this paper A time series in the form tlrt2 _ t The subsection of T from a to b t,s,+i 9\222 222 221 Jb A piecewise linear approximation of a time series of length n with K segments Individual segments can be addressed with Seg-TS\(i A function which takes in a time series and returns a linear segment approximation of it A function which takes in a time series and returns the approximation error of the linear segment approximation of it Given that we are going to approximate a time series with straight lines there are at least two ways we can find the approximating line Linear Interpolation Here the approximating line for the subsequence T[a:b is simply the line connecting t and th This can be obtained in constant time Linear Regression Here the approximating line for the subsequence T[a:b is taken to be the best fitting line in the least squares sense 27 This can be obtained in time linear in the length of segment The two techniques are illustrated in Figure 2 Linear interpolation tends to closely align the endpoint of consecutive segments, giving the piecewise approximation a 223smooth\224 look In contrast, piecewise linear regression can produce a very disjointed look on some datasets The aesthetic superiority of linear interpolation, together with its low computational complexity has made it the technique of choice in computer graphic applications 9 However the quality of the approximating line in terms of Euclidean distance is generally inferior to the regression approach Figure 2 Two IO-segment approximations of electrocardiogram data The approximation created using linear interpolation has a smooth aesthetically appealing appearance because all the endpoints of the segments are aligned Linear regression in contrast produces a slightly disjointed appearance but a tighter approximation in terms of residual error All segmentation algorithms also need some method to evaluate the quality of fit for a potential segment A measure commonly used in conjunction with linear regression is the sum of squares or the residual error This is calculated by taking all the vertical differences between the best-fit line and the actual data points, squaring them and then summing them together. Another commonly used measure of goodness of fit is the distance between the best fit line and the data point furthest away in the vertical direction i.e the L norm between the line and the data As before we have kept our descriptions of the algorithms general enough to encompass any error measure In particular the pseudocode function calculate-error T can be imagined as using any sum of squares, furthest point or any other measure 2.1 The sliding window algorithm The Sliding Window algorithm works by anchoring the left point of a potential segment at the first data point of a time series then attempting to approximate the data to the right with increasing longer segments At some point i the error for the potential segment is greater than the user specified threshold so the subsequence from the anchor to i-1 is transformed into a segment The anchor is moved to 290 


location i and the process repeats until the entire time But on real world datasets with any amount of noise the series has been transformed into a piecewise linear approximation is greatly overfragmented approximation The pseudocode for the algorithm is Variations on the Sliding Window algorithm are shown in Table 2 particularly popular with the medical community \(where it end Table 2 The generic Sliding Window algorithm is known as FAN or SAPA since patient monitoring is inherently an online task ll 12 19 301 Algorithm Seg-TS  Sliding-Window\(T  max-error p\222skdocode for the algorithm is shown in Table 3 anchor  1 while not finished segmenting time series i  2 while calculate-error\(T[anchor anchor  i I  max-error i=i+l end Seg-TS concat\(Seg~TS,create~segment\(T[anchor:anchor+\(i-l anchor  anchor  i The Sliding Window algorithm is attractive because of its great simplicity intuitiveness and particularly the fact that it is an online algorithm Several variations and optimizations Of the basic algorithm have been proposed Koski et al noted that on ECG data it is possible to speed up the algorithm by incrementing the variable i by 223leaps of length k\224 instead of 1 For k  15 at 400Hz the algorithm is 15 times faster with little effect on the output 121 Depending on the error measure used there may be other optimizations possible Vullings et al noted that since the residual error is monotonically non-decreasing with the addition of more data points one does not have to test every value of i from 2 to the final chosen value 30 They suggest initially setting i to s where s is the mean length of the previous segments If the guess was pessimistic the measured error is still less than max-error then the algorithm continues to increment i 2.2 The top-down algorithm The Top-Down algorithm works by considering every possible partitioning of the times series and splitting it at the best location. Both subsections are then tested to see if their approximation error is below some user-specified threshold If not the algorithm recursively continues to split the subsequences until all the segments have atmroximation errors below the threshold The Seg-TS  Top-mm\(T 222 best-so-far  inf for i  2 to length\(T 2  Find best place to divide improvement-in-approximation  mrovement-splittinq-here\(T,i if improvement-in-approximation  best-so-far breakpoint  i best-so-far  improvement-in-approximation end end if calculate-error\(T[l:brealrpointl  error end  Recursively split the left segment if necessary Seg-TS  Top-mm\(T[l breakpintl  Recursively split the right segment if necessary if calculate-error TLbreakpoint  l:length\(T   max-error Seg-TS  Top-mm\(T[breakpoint  1 length\(T a 29 1 


world data sets with any amount of noise the approximation is greatly overfragmented Lavrenko et al uses the Top-Down algorithm to support the concurrent mining of text and time series 17 They attempt to discover the influence of news stories on financial markets Their algorithm contains some interesting modifications including a novel stopping criteria based on the t-test Finally Smyth and Ge use the algorithm to produce a representation which can support a Hidden Markov Model approach to both change point detection and pattern matching 8 2.3 The bottom-up algorithm The Bottom-Up algorithm is the natural complement to the Top-Down algorithm The algorithm begins by creating the finest possible approximation of the time series so that n/2 segments are used to approximate the n length time series Next the cost of merging each pair of adjacent segments is calculated and the algorithm begins to iteratively merge the lowest cost pair until a stopping criteria is met When the pair of adjacent segments i and i+l are merged the algorithm needs to perform some bookkeeping First the cost of merging the new segment with its right neighbor must be calculated In addition the cost of merging the i-1 segments with its new larger neighbor must be recalculated The pseudocode for the algorithm is shown in Table 4 Algorithm I Usercanspecify TopDown E, ME K BonomUp E, ME K Sliding E Window Algorithm Seg-TS  Bottm_Up\(T  max-error for i  1  2  length\(T  Create initial fine a&proximtion ScTS  concat\(SeQTS create-segrent\(T[i i  1 I for i  1  length\(!3z-g-TS  1  Find the cost of merging   _._ eachpair of segrents merge-cost\(i  mrge\(seeTs\(i seg_Ts\(i+l 1  end dle min\(merge-cost  max-error  While not finished i  min\(merge-cost  ses_r;\(i  merge\(-TS\(i SeeTS\(i+l  Merge than delete\(-TS\(i+l   merge-cost\(i calculate-e~or\(merge\(SeeTS\(i TS\(i+l   merge-cost\(i-l calculate-error\(merge\(SgtTS\(i-l mTS\(i  Find cheapest pair to merge  upaate records  Online Complexity Used by No O\(h IO 14,15,16 Yes 00 1 I 12 19,25,30 No O\(n'K 6,7,8 18,22 17 31,27 Two and three-dimensional analogues of this algorithm are common in the field of computer graphics where they are called decimation methods 9 In data mining the algorithm has been used extensively by two of the current authors to support a variety of time series data mining tasks 14 15 161 In medicine the algorithm was used by Hunter and McIntosh to provide the high level representation for their medical pattern matching system IO The properties of the various algorithms are summarized in Table 5 In this section we will provide an extensive empirical comparison of the three major algorithms It is possible to create artificial datasets that allow one of the algorithms to achieve zero error by any measure but forces the other two approaches to produce arbitrarily poor approximations In contrast testing on purely random data forces the all algorithms to produce essentially the same results To overcome the potential for biased results we tested the algorithms on a very diverse collection of datasets These datasets where chosen to represent the extremes along the following dimensions stationary/non stationary noisy/smooth cyclicalhon-cyclical symmetric asymmetric etc In addition the data sets represent the diverse areas in which data miners apply their algorithms including finance medicine manufacturing and science Figure 3 illustrates the 10 datasets used in the experiments 3   I r A 10  L Figure 3 The ten datasets used in the experiments 1 Radio Waves 2 Exchange Rates 3 Tickwise II 4 Tickwise I 5 Water Level 6 Manufacturing 7 ECG 8 Noisy Sine Cubed 9 Sine Cube 10 Space Shuttle 3.1 Experimental methodology For simplicity and brevity we only include the linear regression versions of the algorithms in our study Since linear regression minimizes the sum of squares error it also minimizes the Euclidean distance the Euclidean distance is just the square root of the sum of squares 292 


Euclidean distance or some measure derived from it is by far the most common metric used in data mining of time series I 2 4 5 13 14 15 16 25 311 The linear interpolation versions of the algorithms by definition will always have a greater sum of squares error The performance of the algorithms depends on the value of max-error As max-error goes to zero all the algorithms have the same performance since they would produce nl2 segments with no error At the opposite end as max-error becomes very large the algorithms once again will all have the same performance since they all simply approximate T with a single best-fit line Instead we must test the relative performance for some reasonable value of max-error a value that achieves a good trade off between compression and fidelity Because this reasonable value is subjective and dependent on the data mining application and the data itself we did the following We chose what we considered a reasonable value of max-error for each dataset then we bracketed it with 6 values separated by powers of two The lowest of these values tends to produce an over fragmented approximation and the highest tends to produce a very coarse approximation So in general the performance in the mid-range of the 6 values should be consider most important Figure 4 illustrates this idea Since we are only interested in the relative performance of the algorithms for each setting of max-error on each data set we normalized the performance of the 3 algorithms by dividing by the error of the worst performing approach 123456 max-error max-error max-error  2  _I max-error  Too coarse an approximation E*2 Figure 4 We are most interested in comparing the segmentation algorithms at the setting of the user-defined threshold max-error that produces an intuitively correct level of approximation. Since this setting is subjective we chose a value for E such that max-error  E*2 i  1 to 6 brackets the range of reasonable approximations 3.2 Experimental results The experimental results are summarized in Figure 5 The most obvious result is the generally poor quality of the Sliding Windows algorithm With a few exceptions it is the worse performing algorithm usually by a large amount Top-Down does occasionally beat Bottom-Up but only by small amount. On the other hand Bottom-Up often significantly out performs Top-Down especially on the ECG Manufacturing and Water Level data sets OB Ob 01 02 123456 23 456 OI 06 04 02 123456 123456 06 I 02 fCC Y..YI.S1"II.O wot I Figure 5 A comparison of the three major times series segmentation algorithms on ten diverse datasets 293 


4 A new approach Given the noted shortcomings of the major segmentation algorithms we investigated alternative techniques The main problem with the Sliding Windows algorithm is its inability to look ahead lacking the global view of its offline batch counterparts The Bottom-Up and the Top-Down approaches produce better results but are offline and require the scanning of the entire data set This is impractical or may even be unfeasible in a data mining context where the data are in the order of terabytes or arrive in continuous streams We therefore introduce a novel approach in which we capture the online nature of Sliding Windows and yet retain the superiority of Bottom-Up We call our new algorithm SWAB Sliding Window and Bottom-up 4.1 The SWAB segmentation algorithm The SWAB algorithm keeps a small buffer The buffer size should initially be chosen so that there is enough data to create about 5 or 6 segments Bottom Up is applied to the data in the buffer and the leftmost segment is reported The data corresponding to the reported segment is removed from the buffer and more datapoints are read in The number of datapoints read in depends on the structure of the incoming data This process is performed by the Best-Line function which is basically just classic Sliding Windows These points are incorporated into the buffer and Bottom-Up is applied again This process of applying Bottom-Up to the buffer reporting the leftmost segment and reading in the next 223best fit\224 subsequence is repeated as long as data arrives potentially forever The intuition behind the algorithm is this The Best-Line function finds data corresponding to a single segment using the relatively poor Sliding Windows and gives it to the buffer As the data moves through the buffer the relatively good Bottom-Up algorithm is given a chance to refine the segmentation because it has a 223semi-global\224 view of the data By the time the data is ejected from the buffer the segmentation breakpoints are usually the same as the ones the batch version of Bottom-Up would have chosen The pseudocode for the algorithm is shown in Table 6 Using the buffer allows us to gain a 223semi-global\224 view of the data set for Bottom-Up However it important to impose upper and lower bounds on the size of the window A buffer that is allowed to grow arbitrarily large will revert our algorithm to pure Bottom-Up but a small buffer will deteriorate it to Sliding Windows allowing excessive fragmentation may occur In our algorithm we used an upper and lower bound of twice and half\of the initial buffer Our algorithm can be seen as operating on a continuum between the two extremes of Sliding Windows and Bottom-Up The surprising result demonstrated below\is that by allowing the buffer to contain just 5 or 6 times the data normally contained by is a single segment the algorithm produces essentially the same results as Bottom Up yet is able process a never-ending stream of data Our new algorithm requires only a small, constant amount of memory and the time complexity is a small constant factor worse than that of the standard Bottom-Up algorithm Table 6 The SWAB \(Sliding Window and Bottom-up algorithm Algorithm Seg-TS  SWAB\(max-error seg-nun  seg-nun is integer about 5 or 6 read in data points to fill w  w is the buffer  Eslough to approximate seg-nun of segments lower-bound  size of w  2 upper-bound  2 size of w while data at input T  Bottom-Up\(w, max-error  Call the classic Bottom-Up algorithm Seg-TS  CONCAT\(SEG-TS, T\(1  Sliding window to the right w  TAKEOUT\(w w\222   Deletes w\222 points in T\(1 from w if data at input  Add points from BEST-LINE0 to w w  CONCAT\(W BEST-LINE\(max_error  Check upper and lower bound adjust if necessary else  Flush approximated segments from buffer Seg-TS  CONCAT\(SEG-TS T  T\(1 end end Function S  BEST-LINE\(max-error returns S points while error S max-error  next potential segment read in one additional data point d into S S  CONCAT\(S d error  approx-segment\(S and while\221 ret S 4.2 Experimental Validation We repeated the experiments in Section 3 this time comparing the new algorithm with pure batch Bottom Up and classic Sliding Windows The result summarized in Figure 6 is that the new algorithm produces results that are essentiality identical to Bottom-Up 5 Conclusions We have carried out the first extensive review and empirical comparison of time series segmentation algorithms from a data mining perspective We have shown the most popular approach Sliding Windows generally produces very poor results and that while the second most popular approach Top-Down can produce reasonable results it does not scale well In contrast the least well known Bottom-Up approach produces excellent results and scales linearly with the size of the dataset In addition we have introduced SWAB a new online algorithm which scales linearly with the size of the dataset requires only constant space and produces high quality approximations of the data 294 


1 08 Ob 04 02 123456 1 08 06 04 02 n 123456 123456 I 08 06 04 02 n I23456 123456 I 1 08 06 04 02 n 123456 I 1 08 Ob 04 02 0 123456 6l 12345 I 08 Ob 04 02 123456 1 08 06 04 02 n 123456 123456 Space Shuttle Sine cM Noisy Sine cubed ECG Manufacturing Water Level Tickwise 1 Tkkwise 2 Exchange Rate Radio Waver Figure 6 A comparison of the SWAB algorithm with pure batch Bottom-Up and classic Sliding Windows on ten diverse datasets, over a range in parameters Each experimental result ie. a triplet of histogram bars\is normalized by dividing by the performance of the worst algorithm on that experiment Reproducible Results Statement In the interests of competitive scientific inquiry all datasets and code used in this work are available together with a spreadsheet detailing the original unnormalized results by emailing the first author 6 References l Agrawal R Faloutsos C  Swami A 1993 Efficient similarity search in sequence databases Proceedings of the 4lh Conference on Foundations of Data Organization and Algorithms 2J Agrawal R Lin K I Sawhney H S  Shim K 1995 Fast similarity search in the presence of noise scaling and translation in times-series databases Proceedings of 21\221\224 International Conference on Very Large Data Bases pp 490-50 3 Agrawal R Psaila G Wimmers E L  Zait M 1995 Querying shapes of histories Proceedings of the 21\224\221 International Conference on Very Large Databases 4 Chan K  Fu W 1999 Efficient time series matching by wavelets Proceedings of the Ifh IEEE International Conference on Data Engineering 5 Das G Lin K Mannila H Renganathan G  Smyth P 1998 Rule discovery from time series Proceedings of 295 


the 3 International Conference of Knowledge Discovery and Data Mining pp 16-22 6 Douglas D H  Peucker T K.\(1973\Algorithms for the Reduction of the Number of Points Required to Represent a Digitized Line or Its Caricature Canadian Cartographer Vol IO No 2 December pp 112-122 7 Duda, R 0 and Hart, P. E 1973. Pattern Classification and Scene Analysis. Wiley, New York 8 Ge X  Smyth P 2001 Segmental Semi-Markov Models for Endpoint Detection in Plasma Etching To appear in IEEE Transactions on Semiconductor Engineering 9 Heckbert P S  Garland M 1997 Survey of polygonal surface simplification algorithms Multiresolution Surface Modeling Course Proceedings of the 24 International Conference on Computer Graphics and Interactive Techniques IO Hunter J  McIntosh N 1999 Knowledge-based event detection in complex time series data Artificial Intelligence in Medicine pp. 27 1-280 Springer I I Ishijima M et al 1983 Scan-Along Polygonal Approximation for Data Compression of Electrocardiograms IEEE Transactions on Biomedical Engineering BME I21 Koski A Juhola M  Meriste M. \(1995 Syntactic Recognition of ECG Signals By Attributed Finite Automata Pattern Recognition 28 I 2 pp 1927- 1940 I31 Keogh E Chakrabarti K Pazzani M  Mehrotra 2000 Dimensionality reduction for fast similarity search in large time series databases Journal of Knowledge and Information Systems I41 Keogh E  Pazzani M 1999 Relevance feedback retrieval of time series data Proceedings of the 221h Annual International ACM-SIGIR Conference on Research and Development in Information Ret rie va 1 15 Keogh E  Pazzani M 1998 An enhanced representation of time series which allows fast and accurate classification clustering and relevance feedback Proceedings of the 41h International Conference of Knowledge Discovery and Data Mining pp 239-241, AAA1 Press I61 Keogh E  Smyth P 1997 A probabilistic approach to fast pattern matching in time series databases Proceedings of the 3 International Conference of Knowledge DiscoveT and Data Mining pp 24-20 I71 Lavrenko V Schmill M Lawrie D Ogilvie P Jensen D  Allan J 2000 Mining of Concurent Text and Time Series Proceedings of the 61h International Conference on Knowledge Discovery and Data Mining pp 37-44 18 Li C Yu P  Castelli V.\(1998 MALM A framework for mining sequence database at multiple abstraction levels Proceedings of the 9 International Conference on Information and Knowledge Management pp 267-272 30 1 1 I91 McKee J.J Evans N.E  Owens F.J 1994 Efficient implementation of the FadSAPA-2 algorithm using fixed point arithmetic Auromedica Vol 16 pp 109-1 17 20 Osaki R Shimada M  Uehara K 1999 Extraction of Primitive Motion for Human Motion Recognition The 2"d International Conference on Discovery Science pp.35 1-352 21 Park S Kim S W  Chu W W. \(2001\Segment Based Approach for Subsequence Searches in Sequence Databases To appear in Proceedings of the I6Ih ACM Symposium on Applied Computing 22 Park S  Lee, D  Chu W W 1999\Fast Retrieval of Similar Subsequences in Long Sequence Databases Proceedings of the 3 IEEE Knowledge and Data Engineering Exchange Workshop  Pavlidis T 1976 Waveform segmentation through functional approximation IEEE Transactions on Computers 24 Perng C Wang H Zhang S  Parker S 2000 Landmarks a new model for similarity-based pattern querying in time series databases Proceedings of 16'h International Conference on Data Engineering 25 Qu Y Wang C  Wang S 1998 Supporting fast search in time series for movement patterns in multiples scales Proceedings of the 7Ih International Conference on Information and Knowledge Management  Ramer U 1972 An iterative procedure for the polygonal approximation of planar curves Computer Graphics and Image Processing 1  pp 244-256 27 Shatkay H 1995 Approximate Queries and Representations for Large Data Sequences Technical Report cs-95-03 Department of Computer Science Brown University 28 Shatkay H  Zdonik S 1996 Approximate queries and representations for large data sequences Proceedings of the 12 IEEE International Conference on Data Engineering pp 546-553 29 Sugiura N  Ogden R T 1994\Testing Change points with Linear Trend Communications in Statistics B Simulation and Computation 23 287-322 30 Vullings H.J.L.M Verhaegen M.H.G  Verbruggen H.B 1997 ECG Segmentation Using Time Warping Proceedings of the 2"d International Symposium on Intelligent Data Analysis 31 Wang C  Wang S 2000 Supporting content based searches on time Series via approximation Proceedings of the 12th International Conference on Scientific and Statistical Database Management 296 


Category Manual Automatic No of associations 63 30 No of rules 330 44 Max association size 6 4 Avg support 0.45 0.43 Avg rule con\256dence 0.80 0.82 Table 1 Manual versus automatic image content mining 4.2 Quality of results We should mention that there were no false association rules It did not happen that an object was incorrectly identi\256ed and then a rule was generated with the incorrect identi\256er In general when we found a match between two objects they were the same shape All the incorrect matches are 256ltered out by the support parameter and then the association rules are generated for objects correctly ideinti\256ed Also some redundant matches happened b ecause of the blobs that represented several shapes but these matches are 256ltered out by the rule support In Table 1 we present a summary of our experimental results with 100 hundred images We compare the results obtained by manually identifying objects in each image and then generating association rules from such identi\256ers Manual Column against the results obtained by our current implementation Automatic Column Ideally our image mining algorithm should produce the same results as the manual process So the table gives a standpoint to assess the quality of our experimental results For these 100 images unwanted matches either incorrect or involving many objects happened in at most 4 images and therefore their support was well below the minimum support frequency which was at 30 These experiments were run using the same parameters for object identi\256cation as in our small example with 10 images The parameters for object identi\256cation had the following values We set color standard deviation to 0.5 contrast standard deviation to 0.5 and anisotropy also to 0.5 The similarity threshold as needed by the similarity function was set to 0.6 We tuned these parameters after several experiments These parameters maximized the number of associations and decreased the errors in unwanted matches The association rule program was set to look for rules with a 30 support and 70 con\256dence The background represents an object itself Since association rules with the background were not interesting for our purposes it was eliminated from consideration by the object identi\256cation step It is important to note that this is done after objects have been identi\256ed We tuned the object identi\256cation step to 256nd similar objects changing values for several parameters in the following manner The most important features used from each object were color and contrast We allowed some variance for color 0.5 and the maximum allowed variance for contrast 0.5 The anisotropy helped eliminate matches involving several geometric shapes We ignored shape b ecause objects could be partially hidden and rotated Position was considered unimportant because objects could be anywhere in each image Anisotropy and polarity were i gnored because almost all our shapes had uniform texture Area was given no weight because objects could be overlapping and thus their area diminished this can be useful to make perfect matches when objects are apart from each other A few rules had high support One problem that arose during our experiments was that the same shape could have two different blob descriptors and these blob descriptors could not be matched with two other descriptors for the same shape in another image This caused two problems First a rule could be repeated because it related the same shapes Second a rule did not have enough support and/or con\256dence and therefore was discarded So the rules found were correct and in many cases had an actual higher support and also higher con\256dence To our surprise in some cases there were no object matches because an object was very close to another one or was located in a corner of the image When two or more objects were overlapping or very close they were identi\256ed as a single object This changed the features stored in the blob The problem was due to the ellipsoidal shape of the blobs and the fact that when a geometric shape was located in a corner thta changed its anysotropy and polarity descriptors Given a blob for an object very close to one corner means determining an adequate radius for the blob i.e ellipse Regular shapes such as the triangle square and hexagon were easily matched across images This is a direct consequence of the circular blob representation produced when the image is segmented In this case neither position nor rotation affect the mining process at all It was surprising that in some cases there were no matches for the circle in these cases it was in a corner or some other shape was very close or overlapping Another important aspect about shape is that we do not use it as a parameter to mine images but shape plays an important role during the segmentation step So shape does affect the image mining results quality The rectangle and the ellipse are the next shapes that are easily matched even though we did not use the shape feature The most complicated shape was the L In this case a number of factors affected matches When this shape was overlapped with other shapes a few matches were found b ecause a big blob was generated Also orientation changed dominant 


ofimages 50 100 150 200 1 feature 50292 80777 127038 185080 2 obj identif 210 338 547 856 3 aux image 3847 6911 10756 13732 4 assoc rules 6 3 6 4 Table 2 Measured times in seconds for each Image Mining step with different image set sizes colors and contrast When the L was close to another shape its colors were merged making it dissimilar to other L shaped objects This suggests that irregular shapes in general make image mining dif\256cult We worked with color images but it is also possible to use black and white images Color and texture were important in mining the geometric shapes we created However we ignored shape as mentioned above Shape may be more important for black and white images but more accurate shape descriptors are needed than those provided by the blobs 4.3 Performance evaluation We ran our experiments on a Sun Multiprocessor forge.cc.gatech.edu computer with 4 processors each running at 100 MHz and 128 MB of RAM The image mining program was written in Matlab and C The 256rst three steps are performed in Matlab The feature extraction process is done in Matlab by the software we obtained from UCB Object identi\256cation and record creation were also done in Matlab by a program developed by us An html page is created in Matlab to interpret results The association rules were obtained by a program written in C In this section we examine the performance of the various components of the image mining process as shown in Table 2 for several image set sizes These times were obtained by averaging the ellapsed times of executing the image mining program 256ve times 4.4 Running time analysis Feature extraction although linear in the number of images is slow and there are several reasons for this If image size increases performance should degrade considerably since feature extraction is quadratic in image size Nevertheless this step is done only once and does not have to be repeated to run the image mining algorithm several times Object identi\256cation is fast This is because the algorithm only compares unmatched objects and the number of objects per image is bounded For our experimental results time for this step scales up well Auxiliary image creation is relatively slow but its time grows linearly since it is done on a per image basis The time it takes to 256nd rules is the lowest among all steps If the image mining program is run several times over the same image set only the times for the second and the fourth step should be considered since image features already exist and auxiliary images have already been created 5 Application Image mining could have an application with real images The current implementation could be used with a set of images having the following characteristics 017 Homogeneous The images should have the same type of image content For instance the program can give useless results if some images are landscapes other images contain only people and the remaining images have only cars 017 Simple image content If the images are complex they will produce blobs dif\256cult to match Also the association rules obtained will be harder to interpret A high number of colors blurred boundaries between objects large number of objects signi\256cant difference in object size make the image mining process more prone to errors 017 A few objects per image If the number of objects per image is greater than 10 then our current implementation would not give accurate results since Blobworld in most cases generates at most 12 blobs per image 017 New information The image itself should should give information not already known If all the information about the image is contained in associated alphanumeric data then that data could be mined directly 6 Future Work Results obtained so far look promising but we need to improve several aspects in our research effort We are currently working on the following tasks We also need to analyze images with repeated geometric shapes If we want to obtain simple association rules this can make our program more general This can be done without further modi\256cation to what is working However if we want to mine for more speci\256c rules then we would need to modify our algorithm For instance we could try to 


produce rules like the following if there are two rectangles and one square then we are likely to 256nd three triangles The issues are the combinatorial growth of all the possibilities to mine and also a more complex type of condition We will also study more deeply the problem of mining images with more complex shapes such as the irregular one similar to the letter L We need a systematic approach to determine an optimal similarity threshold or at least a close one A very high threshold means only perfect matches are accepted On the other hand a very low similarity threshold may mean any object is similar to any other object Finding the right similarity threshold for each image type l ooks like an interesting problem Right now it is provided by the user but it can be changed to be tuned by the algorithm itself Also there are many ways to tune the eleven parameters to match blobs and the optimal tuning may be speci\256c to image type There also exists the possibility of using other segmentation algorithms that could perform faster or better feature extraction It is important to note that these algorithms should give a means to compare segmented regions and provide suitable parameters to perform object matching in order to be useful for image mining From our experimental results it is clear that this step is a bottleneck for the overall performance of image mining We can change the object identi\256cation algorithms to generate overlapping object associations using more features Our algorithm currently generates partititons of objects that is if one object is considered similar To another one the latter one will not be compared again By generating overlapping associations we can 256nd even more rules For instance a red rectangular object may be considered similar to another rectangular object and at the same time be similar to another red object Mining by position is also possible for instance two objects in a certain position may imply another object to be in some other position Since the software we are using for feature extraction produces eleven parameters to describe blobs we have 2 11 possibilites to match objects 7 Conclusions We presented a new algorithm to perform data mining on images and an initial experimental and performance study The positive points about our algorithm to 256nd association rules in images and its implementation include the following It does not use domain knowledge it is reasonably fast it does not produce meaningless or false rules it is automated for the most part The negative points include some valid rules are discarded because of low s upport there are repeated rules because of different object id's unwanted matches because of blobs representing several objects slow feature extraction step a careful tuning of several parameters is needed it does not work well with complex images We studied this problem in the context of data mining for databases Our image mining algorithm has 4 major steps feature extraction object identi\256cation auxiliary image creation and identi\256ed object mining The slowest part of image mining is the feature extraction step which is really a part of the process of storing images in a CBIR system and is done only once The next slowest operation is creating the auxiliary blob images which is also done once Object identi\256cation and association rule 256nding are fairly fast and scale up well with image set size We also presented several improvements to our initial approach of image mining Our experimental results are promising and show some potential for future study Rules referring to speci\256c objects are obtained regardless of object position object orientation and even object shape when one object is partially hidden Image mining is feasible to obtain simple rules from not complex images with a few simple objects Nevertheless it requires human intervention and some domain knowledge to obtain better results Images contain a great deal of information and thus the amount of knowledge that we can extract from them is enormous This work is an attempt to combine association rules with automatically identi\256ed objects obtained from a matching process on segmented images Although our experimental results are far from perfect we show that it is better to discover some reliable knowledge automatically than not discovering any new knowledge at all Acknowledgments We thank Chad Carson from the University of California at Berkeley for helping us setup the Blobworld system We also thank Sham Navathe and Norberto Ezquerra for their comments to improve the presentation of this paper References 1 R  A g r a w a l  T  I m i e lin s k i a n d A  S w a m i  M in in g a s s o ciation rules between sets of items in large databases In Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data  pages 207\261 216 Washington DC May 26-28 1993  R  A gra w a l a n d R  S ri ka nt  F a s t a l gori t h m s for m i n i n g association rules in large databases In Proceedings of the 20th International Conference on Very Large Data Bases  Santiago Chile August 29-September 1 1994  S  B e l ongi e  C Ca rs on H  G r e e n s p a n  a nd J  Ma lik Recognition of images in large databases using a learning framework Technical Report TR 97-939 U.C Berkeley CS Division 1997 


 C  C a r s on S  Be l ongi e  H  G r e e n s p a n  a nd J  Ma l i k  Region-based image querying In IEEE Workshop on Content-Based Access of Image and Video Libraries  1997 5 G  D u n n a n d B  S  E v e r itt An Introduction to Mathematical Taxonomy  Cambridge University Press New York 1982  U  F a yya d  D  H a u s s l e r  a nd P  S t orol t z  M i n i n g s c i e n ti\256c data Communications of the ACM  39\(11\51\26157 November 1996  U  F a yya d G  P i a t e t s k y-S h a p i r o a n d P  S m y t h  T he kdd process for extracting useful knowledge from volumes of data Communications of the ACM  39\(11\:27\261 34 November 1996 8 D  F o r s y t h J M a l i k  M F l e c k H G r e e n s p a n  T L e ung S Belongie C Carson and C Bregler Finding pictures of objects in large collections of images Technical report U.C Berkeley CS Division 1997  W  J  F ra wl e y  G  P i a t e t s k y S ha pi ro a nd C J  Ma t h e u s  Knowledge Discovery in Databases  chapter Knowledge Discovery in Databases An Overview pages 1 261 27 MIT Press 1991  V  G udi v a da a n d V  R a gha v a n Cont e n t ba s e d i m age retrieval systems IEEE Computer  28\(9\18\26122 September 1995 11 R  H a n s o n  J  S t u t z an d P  C h ees eman  B ay es i a n c l a s si\256cation theory Technical Report FIA-90-12-7-01 Arti\256cial Intelligence Research Branch NASA Ames Research Center Moffet Field CA 94035 1990  M H o l s he i m e r a n d A  S i e be s  D a t a m i ni ng T h e search for knowledge in databases Technical Report CS-R9406 CWI Amsterdam The Netherlands 1993  M H out s m a a nd A  S w a m i  S e t ori e nt e d m i ni ng of association rules Technical Report RJ 9567 IBM October 1993  C O r done z a nd E  O m i e c i ns ki  I m a ge m i ni ng A new approach for data mining Technical Report GITCC-98-12 Georgia Institute of Technology College of Computing 1998  J  R Q u i n l a n Induc t i o n o f d e c i s i on t r e e s  Machine Learning  1\(1\81\261106 1986  A  S a v a s e re  E  O m i e c i ns ki  a nd S  N a v a t h e  A n e f 256 cient algorithm for mining association rules In Proceedings of the VLDB Conference  pages 432 261 444 Zurich Switzerland September 1995  O  R Z a i a ne  J  H a n  Z  N  L i  J  Y  Chi a ng a n d S Chee Multimedia-miner A system prototype for multimedia data mining In Proc 1998 ACM-SIGMOD Conf on Management of Data  June 1998 


Database 1 proc 2 procs 4 procs 8 procs T5.I2.D100K 20 17 12 10 T10.I4.D100K 96 70 51 39 T15.I4.D100K 236 168 111 78 T20.I6.D100K 513 360 238 166 T10.I6.D400K 372 261 165 105 T10.I6.D800K 637 435 267 163 T10.I6.D1600K 1272 860 529 307 Table 3 Naive Parallelization of Apriori seconds   0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2    0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of Processors CCPD : With Reading Time Ideal  T5.I2.D100K.t2   T10.I4.D100K.t2   T15.I4.D100K.t2   T20.I6.D100K.t2   T10.I6.D400K.t2   T10.I6.D800K.t2   T10.I6.D1600K.t2  Figure 4 CCPD Speed-up a without reading time b with reading time 13 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


Reading  f Total Time Database Time P 000 1 P 000 2 P 000 4 P 000 8 P 000 12 T5.I2.D100K 9.1s 39.9 43.8 52.6 56.8 59.0 T10.I4.D100K 13.7s 15.6 22.2 29.3 36.6 39.8 T15.I4.D100K 18.9s 8.9 14.0 21.6 29.2 32.8 T20.I6.D100K 24.1s 4.9 8.1 12.8 18.6 22.4 T10.I6.D400K 55.2s 16.8 24.7 36.4 48.0 53.8 T10.I6.D800K 109.0s 19.0 29.8 43.0 56.0 62.9 T10.I6.D1600K 222.0s 19.4 28.6 44.9 59.4 66.4 Table 4 Database Reading Time in section 4 320 computation balancing hash tree balancing and short-circuited subset checking The 336gure on the left presents the speed-up without taking the initial database reading time into account We observe that as the number of transactions increase we get increasing speed-up with a speed-up of more than 8 n 2 processors for the largest database T10.I6.D1600K with 1.6 million transactions r if we were to account for the database reading time then we get speed-up of only 4 n 2 processors The lack of linear speedup can be attributed to false and true sharing for the heap nodes when updating the subset counts and to some extent during the heap generation phase Furthermore since variable length transactions are allowed and the data is distributed along transaction boundaries the workload is not be uniformly balanced Other factors like s contention and i/o contention further reduce the speedup Table 4 shows the total time spent reading the database and the percentage of total time this constitutes on different number of processors The results indicate that on 12 processors up to 60 of the time can be spent just on I/O This suggest a great need for parallel I/O techniques for effective parallelization of data mining applications since by its very nature data mining algorithms must operate on large amounts of data 5.3 Computation and Hash Tree Balancing Figure 5 shows the improvement in the performance obtained by applying the computation balancing optimization discussed in section 3.1.2 and the hash tree balancing optimization described in section 4.1 The 336gure shows the  improvement r a run on the same number of processors without any optimizations see Table 3 Results are presented for different databases and on different number of processors We 336rst consider only the computation balancing optimization COMP using the multiple equivalence classes algorithm As expected this doesn\325t improve the execution time for the uni-processor case as there is nothing to balance r it is very effective on multiple processors We get an improvement of around 20 on 8 processors The second column for all processors shows the bene\336t of just balancing the hash tree TREE using our bitonic hashing the unoptimized version uses the simple mod d hash function Hash tree balancing by itself is an extremely effective optimization It s the performance by about 30 n n uni-processors On smaller databases and 8 processors r t s not as 14 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I4.D100K T15.I4.D100K T10.I6.D400K T10.I6.D800K T10.I6.D1600K Optimizations across Databases 5 0 5 10 15 20 25 30 35 40 45 Improvement COMP TREE COMP-TREE 1 2 4 8 1 2 4 8 1 2 4 8 2 4 8 2 4 8 1 2 4 8 Processors Databases Figure 5 Effect of Computation and Hash Tree Balancing good as the COMP optimization The reason that the hash tree balancing is not suf\336cient to offset inherent load imbalance in the candidate generation in this case The most effective approach is to apply both optimizations at the same time COMP-TREE The combined effect is suf\336cient to push the improvements in the 40 range in the multiple-processor case On 1 processor only hash tree balancing is bene\336cial since computation balancing only adds extra cost 5.4 Short-circuited Subset Checking Figure 6 shows the improvement due to the short-circuited subset checking optimization with respect to the unoptimized version The unoptimized version is the Apriori algorithm due to Agrawal et al 5 The results are presented for dif ferent number of processors across dif ferent databases The results indicate that while there is some improvement for databases with small transaction sizes the optimization is most effective when the transaction size is large In this case we get improvements of around 25 r the unoptimized version To gain further insight into this optimization consider 336gure 7 It shows the percentage improvement obtained per iteration on applying this optimization on the T20.I6.D100K database It shows results only for the uni-processor case r similar results were obtained on more processors We observe that as the iteration k increases there is more opportunity for shortcircuiting the subset checking and we get increasing bene\336ts of up to 60 The improvements start to fall off t the high end where the number of candidates becomes small resulting in a small hash tree and less opportunity for short-circuiting It becomes clear that is an extremely effective 15 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 T5.I2.D100K T10.I6.D800K T15.I4.D100K T20.I6.D100K procs across Databases 0 5 10 15 20 25 Improvement 1 2 4 8 Figure 6 Effect of Short-circuited Subset Checking 23456789101112 Iterations 0 10 20 30 40 50 60 improvement T20.I6.D100K Figure 7  Improvement per Iteration  proc   16 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


optimization for larger transaction sizes and in cases where there are large number of candidate k itemsets 6 Conclusions In this paper e presented a parallel implementation of the Apriori algorithm on the SGI Power Challenge shared memory multi-processor We also discussed a set of optimizations which include optimized join and pruning computation balancing for candidate generation hash tree balancing and short-circuited subset checking We then presented experimental results on each of these Improvements of more than 40 were obtained for the computation and hash tree balancing The short-circuiting optimization was found to be extremely effective for databases with large transaction sizes Finally we reported the parallel performance of the algorithm While we d good speed-up we observed a need for parallel I/O techniques for further performance gains References  R Agra wal T  Imielinski and A Swami Database mining A performance perspecti v e  I n IEEE Trans on Knowledge and Data Engg  pages 5\(6 1993  R Agra wal T  Imielinski and A Swami Mining association rules between sets of items in lar ge databases In Proc M SIGMOD Intl Conf Management of Data  May 1993  R Agra wal H Mannila R Srikant H T o i v onen and A I V erkamo F ast disco v ery of association rules In U F et al editor Advances in Knowledge Discovery and Data Mining  MIT Press 1996  R Agra wal and J Shafer  P arallel mining of association rules design implementation and e xperience Technical Report RJ10004 IBM Almaden Research Center San Jose CA 95120 Jan 1996  R Agra wal and R Srikant F ast algorithms for mining association rules In Proc 20th VLDB Conf  Sept 1994  M Cierniak W  Li and M J Zaki Loop scheduling for heterogeneity  I n 4th IEEE Intl Symposium on High-Performance Distributed Computing also as URCS-TR 540 CS Dept Univ f Rochester  Aug 1995  M Holsheimer  M  K ersten H Mannila and H T o i v onen A perspecti v e on databases and data mining In 1st Intl Conf Knowledge Discovery and Data Mining  Aug 1995  M Houtsma and A Swami Set-oriented mining of association rules In RJ 9567  IBM Almaden Oct 1993  H Mannila H T o i v onen and I V erkamo Ef 336cient algorithms for disco v ering association rules In AAAI Wkshp Knowledge Discovery in Databases  July 1994  J S P ark M Chen and P  S Y u  A n e f fecti v e hash based algorithm for mining association rules In Proc M SIGMOD Intl Conf Management of Data  May 1995 17 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


 J S P ark M Chen and P  S Y u  E f 336cient parallel data mining for association rules T echnical Report RC20156 IBM T J Watson Research Center Aug 1995  G Piatetsk y-Shapiro Disco v ery  presentation and analysis of strong rules In G P S et al editor  KDD  AAAI Press 1991  A Sa v asere E Omiecinski and S Na v athe An ef 336cient algorithm for mining association rules in large databases In Proc 21st VLDB Conf  1995  M J Zaki M Ogihara S P arthasarathy  and W  Li P arallel data mining for association rules on shared-memory multi-processors Technical Report 618 Department of Computer Science University of Rochester 618 1996 18 Proceedings of the 1996 ACM/IEEE Conference on Supercomputing \(SC\22296 0-89791-854-1/96 $ 10.00 ACM 


