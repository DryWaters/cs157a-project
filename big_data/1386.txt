DERIVING HIGH-LEVEL CONCEPTS USING FUZZY-ID3 DECISION TREE FOR IMAGE RETRIEVAL Ying Liu 1  Dengsheng Zhang 1  Guojun Lu 1  Wei-Ying Ma 2 1 Gippsland School of Computing and Information Technology Monash University, Vic, 3842, Australia ying.liu, dengsheng.zhang, guojun.lu}@infotech.monash.edu.au 2 Microsoft Research Asia, No. 49 ZhiChun Road, Beijing, 100080, China wyma@microsoft.com ABSTRACT To improve the retrieval accuracy of content-based image 
retrieval, an important task is to reduce the èsemantic gap between low-level image features and the richness of human semantics. In this paper, we present a region-based image retrieval system with high-level semantic concepts used. The contribution of the paper is two-fold. First, salient low-level features are extracted from arbitrary-shaped regions. Second, a fuzzy-ID3 decision tree learning method is proposed to derive association rules which map low-level image features to highlevel concepts. Experimental results prove that by reducing the semantic gapê, the proposed system not only improves the retrieval accuracy, but also supports users in query-by-keyword 1. INTRODUCTION 
Content-based image retrieval \(CBIR\ introduced in the early 1990ês for large image database retrieval. CBIR systems index images by their own visual contents, such as color, shape texture. Although many sophisticated algorithms have been designed for color, shape, and texture description, the performance of conventional CBIR systems is still far from satisfaction. This is due to the èsemantic gapê between the limited descriptive power of low-level image features and the richness of user semantic  3 Literature survey shows that the methods developed for narrowing down the èsemantic gapê to improve retrieval accuracy include: 1\achine learning or data mining techniques to associate low-level image features with high-level 
concepts [2  2 Introducing R e lev a nce f e edba ck \(RF  the  retrieval loop for continuous learning through on-line interaction with users As us ers are of ten int ere st ed i n spec if ic reg ions  rather than the entire image m o s t c u rren t sy s t em s are region based In this paper, we develop a region-based image retrieval RBIR\system with high-level concepts used. Most work in this area focus on high-level semantics without much effort made in low-level region features. This paper makes the following contributions. First, salient low-level features are extracted from arbitrary-shaped regions. Second, we propose a fuzzy-ID3 decision tree method based on the work in  to de riv e a se t of  association rules which maps low-level image features to high 
level concepts. Experimental results confirm the substantial performance of the system The remaining of the paper is organized as follows. Section 2 includes the related work and the research focus. Section 3 explains the proposed system in details. Experimental results are given in Section 4. Finally, Section 5 concludes this paper 2. RELATED WORK AND RESEARCH FOCUS In our system, each region is described by its color, texture and spatial location. We do not consider shape feature as it is not important for regions in natural scenes Instead of using traditional color features such as color moments or color histograms  define a p e rce p tua l  color for each region by its dominant color in HSV space which is 
more natural in vision Texture features commonly used in CBIR include spectral features such as Gabor features[6 a t isti cal f e a t ur es  such a s  coarseness, contrast, directionality [7 a t isti cal  t e x t ur e f eatur es though work well in classifying Brodatz textures, are less effective when applied to natural scene im Ou r experimental results also prove this to be true. In addition, there are two problems to be considered. 1\any systems, texture features are obtained during segmentation from pixels or small  8]. Such features m a y not we ll represent the property  of an entire region. In some other systems, segmentation does not produce texture features. Hence, it is necessary to study 
texture feature extraction from the whole region after segmentation. 2\Transforms such as Gabor filtering require the input image to be rectangle. An instinctive way is to obtain an inner rectangle \(IR\ from a region on which filtering can be performed. This works when the size of the filtering mask is much smaller than the size of the IR. But many regions in RBIR systems are small, and the coefficients obtained canêt well describe the region. To solve these problems, we present a extended rectangle\(ER\ texture feature extraction algorithm. By initial padding, this algorithm extends an arbitrary-shaped region into a larger rectangle onto which Gabor filtering is applied Then a set of coefficients best describing the region is obtained 
from which texture features can be extracted Besides color and texture features, we define spatial location of a region as èbottom, middle, topê in an image Another focus in this paper is to derive high-level concepts from low-level features. Supervised learning such as Support Vector Machine \(SVM y e sian c l a s s i f i er  2 can b e used  for this purpose. The problem with such learning algorithms is that a large amount of labelled training data is required, but it is tedious and error-prone to provide such data. Another problem II - 501 0-7803-8874-7/05/$20.00 ©2005 IEEE ICASSP 2005 


with SVM which is good at binary-decision making is that its performance degrades when applied to multiple concepts classification. Decision tree learning methods are mathematically less complex and have been used to deduce association rules associating low-level image features with high-level concepts  In this paper, we present a èfuzzyê-ID3 decision tree method  3. SYSTEM DESCRIPTION In this work, we use to s e gm ent each  datab as e  im age into regions homogeneous in color and texture 3.1 Low-level features 3.1.1 Color feature HSV is the most natural color space in visual. As the regions are color homogeneous, it is possible to use the average HSV value of all pixels as its perceptual color \(èAve-clê\. However, in some cases, due to the inaccuracy in image segmentation, pixels not belonging to the interested region might be included. Hence, we use the dominant color as the perceptual color of a region. For this, we first calculate the HSV space color histogram \(10*4*4 bins\ion and select the bin with maximum size. The average HSV value of all the pixels in the selected bin is used as the dominant color and referred to as èDm-clê. Figure 1 gives two examples     1\(a\           1\(b\      1\(c\          2\(a\            2\(b\      2\(c Figure 1. Average color and dominant color a\iginal region \(b\cl \(c\ Dm-cl 3.1.2 Texture feature Our texture feature extraction algorithm includes two parts Firstly, an arbitrary-shaped region of M pixels is extended into a rectangle \(ER\of N  M N y padding some values outside its boundary. We use Zero padding due to its simplicity in implementation. Then a band of Gabor filters with 4 scales and 6 orientations are applied to the ER and we select the M largest coefficients from the N coefficients in each subband From these selected coefficients, mean and variance are calculated as texture features. This is the simplified version of our algorithm in in which  an it er at ive loop  is us ed to f i nd the set of coefficients best describing the original region. We observed slight performance degrade using the simplified version To compare the performance of different texture features, we implemented texture feature extraction from IR as well. Due to the various possible region shapes available, it is difficult to obtain maximum size IR. We design a simple method which can find a reasonable large size IR, though may not necessarily the maximum. For each point P along the region boundary and within the region, we find the largest IR with its left-top corner being P Among all the IRs obtained, we chose the one with maximum size. Figure 2 gives a few examples Note that for fast Gabor filtering, the height and width of ER and IR must be power of two 3.1.3 Spatial location Besides color and texture features, spatial location is also useful in region classification. For example sky and sea could have similar color and texture features, but their spatial locations are different with sky usually appears at the top of an image and sea at the bottom We define 3 simple spatial locations as ètop, middle, bottom First, we define the spatial center    Y X S of a region as in \(1 N is the number of pixels in a region and    y x p m represents the x y coordinate of a pixel. Then the spatial location of a region is defined as ètopê \(middle, bottom    Y X S is located in the top middle, bottom\ 1/3 of an image   N m m y x p N Y X S 1    1    1 Finally, each region is described by a 3-d color feature, 48-d texture feature, and its spatial location. Each dimension of the color and texture features is normalized to [0 3.2. Decision tree ID3 is a decision tree method based on Shannonês information theory. Given a sample data set described by a set of attributes and an outcome, ID3 produces a decision tree which can classify the outcome value based on the values of the given attributes Based on our image data, we define 10 classes \(concepts grass forest, sky, sea, sand, firework, sunset, flower, tiger, fur Here we only consider black ape fur as included in our image data set In our case, the attributes are the low-level image features and the outcome is one of the 10 concepts. We collect 400 sample data with 40 for each class At each level of the ID3 decision tree, the attribute with smallest entropy is selected from those attributes not yet used as the most significant for decision making. Spatial location is obviously less significant than color and texture features. We need to determine between color and texture which is more significant. We define the entropy of color feature c I as below 1\ For each class, we calculate the mean color feature of all the 40 regions as its representative color feature 1 0  1     i c c c C v i s i h i R i and we initialize the array counting the number of correct classification as Correct_Num 0 2 For the th j sample region in class i we calculate its Euclidean distance to the representative color feature of each class 1 0  1     m d m i j and find the minimum value 0    m i j d If i m 0  then we consider this region correctly classified based on color feature, and Correct_Num[i  3\ Repeat 2\for all the sample date. Thus, the probability of correct classification is P Correct_Num i]/40 i=1,É,10 4\ The entropy of color feature is calculated as  10 1 2   log 10 1 i c i P I 2 In our results 134  0 c I Similarly, we obtain the entropy of texture feature as 19 2  0 T I This tells that color feature is more significant and should be used at the first level of the tree to split the data into subsets. The original ID3 method requires the real region features to be discretized as input. In our method, we compare region feature to the representative feature of each class to obtain the corresponding class type of each region. In this way the complex feature discretization procedure is avoided. We refer to our method as èfuzzyê-ID3. We consider the classification successful if 0.7 Finally, we obtain a decision tree as shown in Figure 3. For example, if the dominant color of a region is most close to the II - 502 


representative color of either concept grass or forest and its spatial position is ètopê, then we classify it as forest Results show that color feature can successfully distinguish sunset, sand and fur from others Grass and forest are classified into same class as their colors are similar, the same to sky and sea Texture features can successfully recognize flower, tiger, firework regions. To separate sky from sea  grass from forest spatial position is used. Statistics from our sample date set show that no forest region appears at the bottom of an image while 90% of the grass regions are. No sky region is at the bottom while 92% of the sea regions are Using testing sets of 250, 320, 400 data to test the performance of the decision tree, we obtain the average correct classification probability of each class as 0.85, 0.8, 0.76, 0.72 0.86, 0.81, 0.93, 0.72, 0.80, 0.77 an overall average of 0.802 3.3 Retrieval with high-level concepts The user can either submit a keyword \(one of the 10 concepts\as query, or specify an interested region from a query image. In this work, we specify an interested region from the query image and provide the corresponding keyword. The system first finds all database images containing region\(s\ same concept. Then, all these candidate images are further ranked according to their  d i st an ces t o the quer y  im age. W e r e f e r  to th is pr oc e s s as retrieval with concepts  4. EXPERIMENTAL RESULTS We use 5,000 Corel images \(pre-classified into different categories each containing 100 images\ as test data. èJSEGê segmentation produces 29187 regions \(5.84 regions per image on average\with size no less than 3% of the original image. We ignore very small regions considering that regions should be large enough for us to study their texture features. Precision \(Pr\and Recall \(Re\ are used to measure retrieval performance. Let K be the number of images retrieved, with K 10,20,É100, we calculate the average Pr and Re of all the queries performed, and obtain the Pr~Re curve 4.1 Different texture features We extracted texture features from ER, IR. We also have the three most significant Tamura features-coarseness, contrast directionality. To compare their performance, we performed 30 queries. For each query image submitted, the system ranks the database images according to their EMD distances to the query Both color and texture features are used in distance calculation This is referred to as retrieval without concepts The results in Figure 4 show that ER outperforms the other two 4.2 Retrieval with concepts We selected 30 query images from the database, containing interested regions of different concepts. A few query images and the specified regions are given in Figure 5. We compare the performances of retrieval with/without high-level concepts  Figure 6 displays that using high-level concepts, the retrieval accuracy is improved. The retrieval results for query 1 and 2 with keywords sea  flower are given in Figure 7 as examples Table 1 lists the number of relevant images retrieved with different total number of retrieved images K or query 1, 2, 3  Table 1. Number of relevant images retrieved Q\\K 10 20 50 80 100  1 7 9 20 25 29 2 10 18 38 45 48 3 8 12 18 24 28 5. CONCLUSIONS In this paper, we present a region-based image retrieval system with high-level concepts obtained from low-level region features using a fuzzy-ID3 decision tree. By reducing the èsemantic gap the proposed system improves image retrieval accuracy. In addition, it can support not only query-by-sample, but also query-by-keyword which is more user-friendly In this work, we specify only one interested region in the retrieval process. We expect the retrieval performance to be further improved by considering multiple regions/concepts. In addition, the decision tree can be more robust by including a complete set of the available concepts in the database, and by learning through a larger sample dataset 6. REFERENCES 1  M j  Li L. Zh ang, HJ Zhang B Zhan g L earn ing in  Region-based Image Retrievalé, Proc. Inter. Conf. on Image and Video Retrieval\(CIVR2003 2 lay a M.AT Figueir ed o, A.K Jain H  J. Zhang  Im age Classification for Content-based Indexing IEEE Trans. on Image Processing 10\(1\130, 2001 3  Sethi, Io ana L.Co m a n, çM ining  As soc iation Ru les  Between Low-Level Image Features and High-Level Concepts,é Proc. of SPIE Data Mining and Knowledge Discovery, III, pp.279-290, 2001 4 l a n   Discovering Rules by Induction from Large Collections of Examples In Expert Systems in the MicroElectronic Age. Edinburgh University Press, 1979 5 Z.W ang J Li, G   W i eder h o ld S I M PLI city Sem antics Sentitive Integrated Matching for Picture Libraries IEEE Trans. Pattern and Mach.Intell Vol 23, no.9, pp947-963, 2001 6  Manjuna th A Texture The s aur us  f o r Brows ing  Large Aerial Photographs J. of the American Society for Information Science vol.49, \(no.7\.633-48, May 1998 7 ura S. Mori  and T  Yam awa k i, çText ure  Fe atur e s  Corresponding to Visual Perception IEEE Trans. On Systems, Man and Cybernetics vol.8, No.6, pp460-473,1978 8 Leow S.La i  Sca le  an d Orien tat i on-In var ian t Textur e  Matching for Image Retrieval World Scientific 2000 9  S.Manjun ath a nd H.Shin Color Im age Segmentation," Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR '99, Fort Collins, CO, vol.2, pp.446-51, June 1999 1  E xt raction of Texture Fe ature s from Arbitrary-Shaped Regions for Image Retrieval,é Inter Conf. on Multimedia and Expo \(ICME04\aipei, June 2004    T o m a si, C., a nd Guiba s   L A Metric f o r Distributions with Applications to Image Databases,é Proc. of IEEE Inter. Conf. on Computer Vision, Jan. 1998 II - 503 


  1 2                            3 Figure 2. Regions and their IR marked by red line Figure 5.  Query image/region examples  Figure 3. Decision tree Red circles contain concepts which have not been successfully classified yet. Green boxes contain concepts which are successfully classified. Italic terms are the attributes used in decision making   0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0 0.05 0.1 0.15 0.2 0.25 Recall Precision Gabor\(IR Gabor\(ER Tamura 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0 0.05 0.1 0.15 0.2 0.25 0.3 Recall Precision With-Concepts Without-Concepts Figure 4. Using different texture features                  Figure 6. Retrieval with/without concepts   T               T             T                                 T                               T               T T   T             T               T                T            T                T                T         T                T T Figure 7. Retrieval with concepts sea, flower Tê refers to the relevant images selected sea sky grass forest Color feature flower, firework tiger sunset, sand fur tiger firework flower Texture feature grass forest sea sky Spatial location regions II - 504 


each file:  S = T/A, where S is the çorder score value, T is the total simulation time for a user and A the fileês access time. Different features are extracted for each file considering either the average of these values or the \(unique earliest/latest access time. 118 features per user were extracted from the raw log data. Table 1 depicts the contextual and time structure of features Table 1: Structure of features   Duration Frequency Order Sum Files Activity 33 33 44 110 Overall Activity 4 4 - 8 37 37 44 118 3.3. Principal Components Analysis  The number of features designed is inevitably high. Some of them are possibly redundant adding more noise or delay to the process, making the need for data reduction imperative. PCA is a classical method which has been used mainly for data reduction purposes. This study involves PCA as a step of transforming the input of the neural network \(NN\oid NN overfitting or noise errors. The original features will be selected or discarded using a NN boot-strapping method  3.4. The NN Bootstrapping Approach  NN cross-validation and bootstrapping is considered to be well performing in terms of defining the generalisation error and best model selection    In t h i s st u d y   NN  bootstrapping is used as the core method to  1 Perform a Sensitivity Analysis of inputs which can be the original features or the Principal Components Scores variables  2 Remove the redundant features from the system and keep only those which appear to have the greatest sensitivity to output giving simultaneously the minimum generalisation error  3 Select the best model that minimises the classification error and gives maximum generalisation value  3.4.1. Description of the NN Bootstrapping In fig. 1, the NN Bootstrapping method is depicted There is one hidden layer with 4 nodes and the training error is defined as .05.  The classification target vector takes 2 values which are 0 for selection of packages type A and 1 for type B Type A = software packages with moderate or low service support, type B packages with high service support\The maximum number of the models is 1500. In each model a data validation subset is defined at random. Consequently 23 users are treated as the training set and 3 as the validation set.  The target is to find 100 models with low generalisation error, which is taken to be equal to the lowest error of the first 10 models.  In case that the instances of this error are fewer than 100, we select the 100 best models out of 1500     Figure 1: NN Bootstrapping  Furthermore a random variable/feature is added which can be used as a metric of sensitivity. For each successful model we run a simulation where for each variable of an input vector we assign 10 values from 0 to 1 increasing by step s = 0.1. Then the variance of output is calculated regarding the 10 variablesê values. For each variable \(including the random variable\he procedure is repeated for all the input vectors of the data and we calculate the variance mean of all outputs  The final sensitivity of each variable is calculated as the average of all 100 models as S V/M where   V  =  The average of variance of all modelsê and M  =  The average of all models simulated outputs means  It should be mentioned that the variance is divided by the overall mean of the outputs to smooth any distortions that can occur due to 0-7695-2268-8/05/$20.00 \(C Proceedings of the 38th Hawaii International Conference on System Sciences - 2005 5 


different magnitude of the output values. The variables with lower sensitivity than random are excluded from further analysis. The same happens when the variables have sensitivity less than 110 of the random gauge  3.4.2 The feature selection algorithm The method involves PCA and 3 stages of sensitivity analysis \(see fig.3\hese are  1. PCA and extraction of Principal Components \(PC\ Scores 2 NN Sensitivity Analysis of  PC Scores 3 NN Sensitivity Analysis of  Selected Features 4 NN Sensitivity Analysis to define the best Model  3.4.3  Principal Component Analysis \(PCA Initially the Principal Components \(PCs\which contribute to 95% of the covariance are computed This number is 18. As a second step we carry out Varimax on th e P C loadin g s T h is i s  a necessary step to achieve maximisation of variance within each loading and therefore a bigger differentiation of components in terms of variablesê significance within each loading Finally the PC Scores are calculated.  PC Scores indicate the score of user on each component and they can be expressed as S = A  L where A Standardised data and L = PC Loadings  3.4.4. Using PC Scores as Input to the Bootstrapping NN NN Overffiting is a critical issue which should be tackled if adequate generalisation and less output variance are to be achievec. Causes of overfitting include large number of input variables as well as multicollinearity problems [21 2 2    Since the number of original variables is large \(in respect to the training cases\, PC Scores can be used as inputs to the NN instead of the original variables and overcome respective problems. The task in this phase is to assign a degree of sensitivity to PC Scores. The number of selected significant components with the greater sensitivity is 7 PCs are then converted into features again through loadings after being weighted according to the result of the PC Scoresê sensitivity. In this way, the system runs an intermediate process offering more reliable results and avoiding overfitting. 80 \(out of 118\eatures with the greater sensitivity \(over 0.1\re selected The selected features from the sensitivity analysis process feed a similar NN. Following the same NN procedure, 13 features are finally extracted \(Table 2 Table 2: Final features  N o Description File Time Character istic Sensitivity  1 9 Idle time between file opening Package Functionality Info Duration  11.35 2 6 Idle time after file process Budget Duration 9.84 4 6 Times that file was accessed Package Performance Info Frequency  5.48 6 4 Number of different queries Supplierês Offers Frequency 5.22 6 6 Number of different queries Supplierês pricing discounts price policy etc Frequency 5.71 6 8 Number of different queries Package Performance Info Frequency 5.54 6 9 Number of different queries Package Functionality Info Frequency 4.55 7 0 Number of different queries  Package Service Info Reports Frequency 7.38 7 4 Number of different queries  Logistics Department Files Frequency 5.67 7 6 First Time Access Order Score Budget Order 6.8 7 8 First Time Access Order Score Supplierês Stability/Relia bility Information Order  11.35 7 9 First Time Access Order Score Package Performance Info Order 4.91 8 0 First Time Access Order Score Package Functionality Info Order 6.4 1 0 1 Average Order Scoreé of Access Package Performance Info Order  4.71 1 1 2 Average Order Scoreé of Access End Package Performance Info Order  5.2  0-7695-2268-8/05/$20.00 \(C Proceedings of the 38th Hawaii International Conference on System Sciences - 2005 6 


3.5. Assessing the Generalisation Performance Ö Best Model Selection  The NN process is repeated without the generalisation restriction. A cross-validation procedure with 100 random samples is carried out and the overall performance of the 100 models is calculated. Finally the one with maximum performance in terms of classification and generalization is selected  The criteria for selecting the best model out of 100 are \(in order of importance  1 The lowest total \(training and validation error for both usersê classes 2 The lowest validation error for both usersê classes 3 The most equal representation of both classes in the validation set  The selection of best model is determined mainly by the low generalisation \(or validation error for the two classes, since the training misclassifications are equal to zero most of the times. The third criterion concerns the equal prediction capability for both classes Table 4 shows the average performance of the 100 models for 3 different combinations of features. Comparing this performance to that of the 1 st row which is to do with the 1 st  Bootstrapping stage where all features are included \(via components\n that the contribution of data selection/reduction stage to performance is extremely important. It can be noted that the average performance of the data with the final set of features outperformed significantly the original representation of the data Table 3: Average Performance \(100 models   Features Class 0 Class1 Overall All features included \(118 47% 46% 46 Selected features with extreme values impact \(15 81% 71% 76 Selected Features without extreme values impact \(15 90% 87% 88  The last row of table 4 shows the average performance of the 3 rd final bootstrapping.  It can be concluded that there is actually a pattern in usersê behaviour, which can validate and give an interpretation of the usersê cognitive styles and interest 3.6. Discussion As is shown in Table 2, feature 70 related to the Package Service Info Reportsé file appears to have a significant level of sensitivity \(or discrimination value\garding the users classification. The value of this feature depends on the frequency of queries activities within the file This makes sense if we consider that the more queries the users use the more interest they have for the software support after purchase Apart from this obvious observation, salient attributes can be considered on features pertinent to çPackage performance Infoé file. That probably indicates that the interest of decision makers for the packageês technical performance appears to have a real information value regarding the users final judgment. This is a çsalient effecté which since the presentation format of the respective queriesê menu do not have any distinct characteristics in the prototype interface Ö can be considered as a genuine information effect  4. Conclusion The proposed method can be considered as a hybrid of data reduction and classification algorithm techniques. The main advantage of the method is that the users can be modelled using only the most significant variables which give the maximum prediction value Abnormal behaviour detection techniques can be based on the selected features with the highest sensitivity to the decision made. Identification of features non-compatible with the task could reveal bias or deviations from normal procedures Moreover, misclassification of a new user by the NN model could indicate a userês different behaviour which should be examined and further analysed. Thus the method can assist in verifying that the organisational procedures are followed and compliance is met. Also it could have the form of an intelligent agent possibly embedded in 1 Executive Information System \(EIS 2 Auditing software \(Computer Assisted Auditing Tools and Techniques CATTS 3 Control self assessment tools and 4 Decision aid tools  Contribution to task performance is also relevant, since decision making bias or information effects k acc u r ac y ca n be identified. Problems pertinent to the complexity of the contextual environment can be resolved by removing redundant information 0-7695-2268-8/05/$20.00 \(C Proceedings of the 38th Hawaii International Conference on System Sciences - 2005 7 


The method can have equal success in analyzing multidimensional accounting information that includes human and non-human economic\mponents mixed together Regardless the accounting/auditing context, the proposed algorithm can be a significant contribution to supervised classification, nonlinear regression and data reduction techniques. It represents a procedure for variable elimination according to generalisation and non-linear regression criteria, which is ideal for analyzing human behaviour due to the complexities of \(a information load \(too many variables\d \(b nature of human cognition  Acknowledgements One of the authors \(SF gratefully acknowledges financial support from the State Scholarships Foundation \(I.K.Y.\of Greece   5. References   W e b b  G  L    P azzan i  M  J  and D Bi l l s u s  M ach i n e  Learning for User Modeling, User Modeling and UserAdapted Interactioné, Vol. 11, 2001, 19-29  OêL ear y  D E  In tru s io n-Detectio n S y ste m s  Journal of Information Systems Spring\, 1992, 63-74 3 D e nning D  E A n intrus ionde te c tion M ode l   IEEE Trans. Soft. Eng., SE-13 1987, 222-232 4 B e lk a oui, A A cc ounti ng the ory  2nd Inte rna tio na l  Edition, Harcout Brace Jovanovich, Inc, 1985 5 R e port of the C o m m itte e o n B e ha v i oura l Sc ie nc e  Content of the Accounting Curriculum The Accounting Review supplement, 1971, p.247  Brun sw i c k E   T h e Co n cep t u al F r am e w o r k o f  Psychologyé, Chicago: University of Chicago Press 1952  Ko C-E  and T h eod o r e J M o ck Beh avi o ural  Accounting Research: A Critical Analysisé, Kenneth Ferris, Century Publishing Co., Chapter 8, 1987, pg 171-201 8 D i c k s on, G  W Se n J  A  a nd N  L  C h e r v a ny   Research in Management Information Systems: The Minnesota experiments Management Science May 1977, 913-923 9 Stoc k  D  a nd C  J  W a ts on H um a n J udgm e n t  Accuracy, Multidimensional Graphics, and Humans Versus Models Journal of Accounting Research 22 1 1984, 192-206 10 De rm e r J.D C og nitiv e  Cha ra c t e r istic s a nd the  Perceived Importance of Information The Accounting Review July 1973 511-519 11 B a rne s M C us tom iz a tion of ER P a pps re quire s  development skillsé, Michael Barnes. Informationweek Manhasset, 722, 1999, pg. 9A 12 Be s t  P   S A P R/3 A udit T r a il A n a l y s is 4th  Annual SAP Asia Pacific Institute of Higher Learning Forum, SAPHIRE 2000 13 C h a m be r s A  D  a nd J  M. C our t C om pute r  Auditingé, London, Pitman Publishing, 1991 1 In za I  L a rraa ga P   an d B  S i erra F eatu r e S u b s et Selection by Bayesian Networks: A Comparison with Genetic and Sequential Algorithms International Journal of Approximate Reasoning 27\(2\, 2001, 143164 15 L a m  K  Y H u i, L a nd S.L Chu ng   Multivariate Data Analysis Software for Enhancing System Securityé, J. Systems Software, 31, 1995, 267275 16 C h e n H  M. a n d M.D C o o p e r  U s i ng C l us te ring  Techniques to Detect Usage Patterns in a Web-based information System J. Am. Soc. Info. Sci. Tech., 52  2001, 888-904 17 B a x t W  G  a nd H  W h ite  B oots tra ppi ng  Confidence Intervals for Clinical Input Variable Effects in a Network Trained to Identify the Presence of Acute Myocardial Infarction Neural Computation, 7 1995 624-638 1  E f ro n  B T h e Jack kn i f e t h e Bo ot st rap and Ot h e r  Resampling Plansé, Philadelphia: SIAM, 1982 19 Ef ro n  B E stim atin g t h e Erro r Rate o f  a Prediction Rule: Improvement on Cross-validation J of the American Statistical Association, 78 1983, 316331 20 Kaiser, H. F., çT h e V a ri m a x Criterio n f o r A n al y t i c  Rotation in Factor Analysis Psychometrica, 23 1958 187-200 21 Hu rv ich C  M. an d Ch i h L in g T s ai, çRe g res sio n  and Time Series Model Selection in Small Samples Biometrika, 76 2\, 1989, 297-307 22 Sa rle  W  S S to ppe d T r a i ning a nd O t he r Remedies for Overfitting", Proceedings of the 27th Symposium on the Interface of Computing Science and Statistics, 1995, 352-360 2 Hay n es C M   S  J Kach el meier  T h e E f f e cts o f  Accounting Contexts on Accounting Decisions: A Synthesis of Cognitive and economic Perspectives in Accounting Experimentation Journal of Accounting Literature  Gainesville  17 1998, 97        0-7695-2268-8/05/$20.00 \(C Proceedings of the 38th Hawaii International Conference on System Sciences - 2005 8 


  n?1 2 j HT  n?3 exp  n?1 2 j 2 \(?2 j HT  2 j HT n?1  n?1 2 n?1 ds2?j 41 PDET    n?1 2 j HL  n?3 exp  n?1 2 j 2 \(?2 j HL  2 j HL n?1  n?1 2 n?1 ds2?j 42 where  2 n? 1 2?j |HT 2?j |HL 2?j |HL 2?j |HT 43  ln    2?j |HL 2?j |HT  n?1 np   Thus all four probabilities are governed by two equations in four unknowns: n, ?np, \(?2?j |HT 2?j |HL 


in four unknowns: n, ?np, \(?2?j |HT 2?j |HL The Neyman-Pearson decision rule is said to maximize the probability of detecting the track-lost regime, PDET for a given n, subject to the constraint that PFA is less than or equal to some user-de?ned probability. [6]. However, it is convenient in this context to instead ?x both PDET and PFA at some desired values, and use \(41 43 the corresponding minimum \(integer n that must be used in order to meet these goals. Typically there is a range of values of ?np that will achieve them Appropriate values of n and ?np can be determined for each pe element ?j using \(41 43 determinations for each element can then be made using 38 39 for each pe element; if a single overall track-loss metric is desired, Wishart distributed versions of \(33 35 also be used in the development of the Neyman-Pearson decision rule. However, because of the approximate nature of the distribution in \(35 additional covariance information is useful in the matrix likelihood test [5]. Further, a Wishart implementation has the disadvantage of being more computationally intensive than the implementation outlined above D. Application to the PDAF In order to implement the Neyman-Pearson rule, \(15 be rewritten x  k|k  k|k ? 1 k k 44 where, for the PDAF e?\(k  mk i=1 i zi\(k 1? ?0  k|k ? 1   45 This  effective  measurement innovation is then the single lter prediction error for purposes of calculating the sample variances s2?j . The j-th diagonal of the innovations covariance for the tracking regime, ST?sirf , and track-lost regime SL?sirf , can then be used for \(?2?j |HT 2?j |HL respectively Some method of handling the case of no gated measurements must be implemented. As suggested earlier, it is at least mathematically consistent in this case to de?ne the measurement innovations as zero. However, when there is a signi?cant probability of zero measurements gating the assumption that the prediction errors have a Gaussian distribution will be a poor one; in practice, this probability is likely to be signi?cant both when PD &lt; 1 and when in the track-lost regime. So the goals for PDET and PFA will not generally be met unless the innovations from timesteps when no measurements gate are excluded, and it is necessary to use the previous nC timesteps to ?nd n nonzero innovations with which to compute s2?j . The average number of timesteps n  T used when the ?lter is operating in the tracking regime is approximately n/\(PDPG average number of timesteps n  L used to when the ?lter is in the track-lost regime is problem speci?c E. Extensions to Other Data Association Methods This strategy is suitable for other data association methods. For the NN ?lter, the KF measurement innovations are the prediction errors. For MAP and maximum likelihood ML determined by similarly deriving a relationship between the estimated state, predicted state, and Kalman gain IV. EXAMPLE: PDAF TRACK REGIME TEST In this section, an example system using the PDAF is constructed. The SIRF approximation steady-state innovations covariances for the tracking and track-lost regimes are calculated and compared to simulation results. An example 


calculated and compared to simulation results. An example Neyman-Pearson decision rule is determined. Theoretical and simulation results for the probabilities of detection of the track-lost regime, PDET, and of false alarm while in the tracking regime, PFA, are given. An estimate of the number of timesteps required to detect track-loss is also provided 4320 A. Dynamics The kinematic model system \(with timestep x\(k + 1  1 0 1  x\(k  2/2   w\(k 46 y\(k  1 0  x\(k 47 is the standard zero-order hold discrete approximation to a continuous double-integrator system. For ? = 0.1 x\(k + 1  1 0.1 0 1  x\(k  0.005 0.1  w\(k 48 B. Kalman Filter System For Q = Q = 1000, R = R = 0.1 P  kf  0.3000 1.9998 1.9998 19.9965  Skf = 0.4000 are the steady-state covariances C. Clutter and Gating Though the example system is dependent on Q,R and ?, it can be described in the tracking regime using just three independent parameters [12]: the probability of detecting the truth measurement, PD, the normalized target acceleration, \(NTA sity, \(NCD the target is maneuvering, and NCD is a measure of how dif?cult it is to localize the target from the measurements For the example system, NTA = 1. Choosing PD = 1 0.02 results in NCD = 0.002, and an average track-lifetime of approximately 500 timesteps using the PDAF [8], [12 this is a tracking problem of  moderate  dif?culty for the example dynamics Using a four standard deviation gate \(? = 16 PG = 0.99994. For the example system, nz = 1, cnz = 2 and Vk = 8 |S\(k D. Experimental Tracking and Track-lost Regimes The following experimental setup allows for  controlled  track-loss and is used to verify operation of the trackloss detector. Until timestep 1000, the ?lter is tracking in the sense that PD = 1 \(the truth measurement is always 


available to the gating test 2000, PD = 0 \(the truth measurement is never available to the gating test sense that the truth measurement is never gated. PFA can then be calculated using measurement innovations from the rst 1000 timesteps, and PDET from using measurement innovations from the second 1000 timesteps, with roughly 1000 values of s2?j being tested by the decision rule in each regime. Note that PDET is thus calculated using both data points from the transient of  controlled  track-loss as well as from the  steady-state  operation of the ?lter in the tracklost regime E. PDAF SIRF Tracking Approximation For the example system with tracking regime assumptions: PG ? PD = 1, q1 ? PD = 1 [3], and \(25 T\(?Vk cnz 2  mk=1 exp\(??Vk Vk mk ? 1   nz nz/2  I2\(mk 49 Figure 1 shows the resulting function for ?T\(?Vk 0 1 2 3 4 5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Vk   T  L Fig. 1. ?T and ?L as functions of ?Vk for example system ating \(4 6 7 11 24 using \(49 T\(?Vk  T = 0.9602, ?V  k 0.1047, and P  T?sirf  0.3286 2.1122 2.1122 20.5492  ST?sirf = 0.4286 Simulating and then averaging P\(k|k? 1 k example PDAF tracking regime over 1000 timesteps gives P  T?sim  0.3210 2.0831 2.0831 20.3698  ST?sim = 0.4210 Thus ST?sirf has only 1.7% error when compared to the more time-consuming simulation results F. PDAF SIRF Track-lost Approximation Figure 1 shows the function for ?L\(?Vk system with track-lost regime assumptions. Iterating \(4 6 


7 11 30 31  L = 0.3264 V  k = 0.4594, and P  L?sirf  8.1431 15.8765 15.8765 56.2960  SL?sirf = 8.2431 Simulating and then averaging P\(k|k? 1 k example PDAF track-lost regime over 1000 timesteps gives P  L?sim  9.1253 17.3318 17.3318 54.3980  SL?sim = 9.2253 SL?sirf has 10.8% error when compared to the more time-consuming simulation results. In general, it has been observed that the approximation for the tracking regime is more accurate than for the track-lost regime when compared to the simulation results 4321 G. Neyman-Pearson Decision Rule PDET ? 0.99 and PFA ? 0.01 are reasonable targets for the decision rule. The number of effective innovations n necessary to meet these goals can be calculated by incrementing n until, for some value\(s and PFA ? 0.01 \(see Figure 2 the slope of the curve is ?np 0 0.005 0.01 0.015 0.02 0.975 0.98 0.985 0.99 0.995 1 target PD &amp; PFA PFA P D n=6 n=7 n=8 Fig. 2. Neyman-Pearson Threshold Curves As can be seen from the ?gure, the goals require n ? 7 timesteps. A good value for ?np \(one that exceeds the goals for both PDET and PFA 41 43 the means of 25 trials of the example experimental setup and the decision rule \(38 39 ST?sirf = 0.4286, and SL?sirf = 8.2431, the experimental results were PDET = 0.9997 and PFA = 0.0132. Though the actual process of track-loss is dif?cult to quantify, the experimental results n  T = 7.0 and n  L = 24.9 do provide some insight into the number of timesteps necessary for track-loss detection, as the average number of timesteps necessary, n  Loss, should be bounded by n  T ? n  Loss ? n  L V. SIMULATION RESULTS Table 1 provides a comparison of experimental values for PDET, PFA, n  T, and n  F across large ranges of NTA and NCD for PDAF simulation data when PD = 1. For each NTA-NCD combination, n and ?np have been chosen such that, theoretically, PDET ? 0.99 and PFA ? 0.01, with n as small as possible. The mean track-lifetime of all NTA-NCD combinations is approximately 100 timesteps where NCD is labeled  High  1000 timesteps where NCD is labeled  Medium  and 10,000 timesteps where NCD is labeled  Low  12]. The experimental values of PDET, and PFA were calculated using the means of 25 trials of the example experimental setup; trials were selected from realizations where track-loss did not occur until forced at timestep 1001 


where track-loss did not occur until forced at timestep 1001 Having the tracking and track-lost variances of the innovations spaced well apart is the condition for small n As Table 1 shows, this condition is met less often for low values of NTA In the track-lost regime, the assumption that the validated prediction errors are Gaussian distributed is usually quite conservative since the sample variances are generally larger than for a Gaussian distribution. This contributes to generally exceeding the PDET goals. Conversely, the sample variances in the tracking regime, while more Gaussian are still somewhat larger than would be expected from a Gaussian distribution, meaning that the PFA goals may not always be met. However, because n is restricted to integer values, the theoretical values of both PDET and PFA often exceed the desired values signi?cantly for low values of n Unlike the tracking regime, the track-lost regime cannot be described solely in terms of PD , NTA, and NCD. SL is non-linearly dependent on ?, so n, PDET, and PFA vary with ? as well. This can be seen by comparing the Medium NTA results in Table 1 with those in Table 2, where identical values of PD , NTA, and NCD constructed from different values of Q, R, and ? yield different results for n In general, lowering PD increases the tracking regime innovations variance ST?sirf , reducing the separation from SL?sirf and thus having the tendency to raise the required n to meet the goals for PDET and PFA. This can be seen by comparing the Low NTA data in Table 1 \(PD = 1 Table 3 \(PD = 0.9 Over the parameter space explored, the test \(decision rule VI. CONCLUSION AND ONGOING WORK A strategy has been laid out for creating a two-class decision rule to determine the regime of operation for the PDAF in the absence of truth data. Scalar information reduction factors can be used in an iterative scheme to predict the steady-state innovations covariance for both the tracking and track-lost regimes, which results in lower computational burden when compared to Monte Carlo simulation. Then a distribution can be assumed for the sample variances of the prediction errors. Together, these pieces of information constitute a model around which a Neyman-Pearson decision rule can be constructed, where the con?dences in both the probability of track-loss detection and of false alarms are explicitly chosen. Good performance of the test as a trackloss detector was demonstrated for an example system over a large range of tracking dif?culties Ongoing work includes modeling the effective innovations \(45 theoretical distributions for the prediction errors used in the decision rule \(38 desirable to more accurately model the sample distribution of the innovations variance, and it has been shown that the prediction errors of many data association algorithms can be well approximated by a Gaussian mixture [13 REFERENCES 1] Y. Bar-Shalom and T. Fortmann, Tracking and Data Association, Academic Press Inc., 1988 2] T. Fortmann, Y. Bar-Shalom, and Y. Scheffe  Sonar Tracking of Multiple Targets Using Joint Probabilistic Data Association  IEEE J. of Oceanic Engineering, July 1983 4322 TABLE I SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.152e3 1.153e3 1.073e4 5.963e3 11 11.0 336 0.591 0.9900 0.0081 1.0000 0.0099 5e-5 1e-4 0.1 Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 


Low Medium 1.152e3 1.153e3 5.509e3 3.741e3 20 20.0 393 0.647 0.9900 0.0092 1.0000 0.0093 R=1000 1e-4 0.2 High 1.153e3 1.154e3 3.180e3 2.698e3 45 45.0 597 0.214 0.9900 0.0071 1.0000 0.0162 2e-4 2e-4 Low 0.4017 0.4024 109.291 71.994 4 4.0 39.7 0.996 0.9960 0.0075 0.9995 0.0012 2e-3 1.0 1e-3 Medium Medium 0.4128 0.4129 17.488 19.216 6 6.0 29.6 0.992 0.9938 0.0018 0.9987 0.0034 R=0.1 0.01 4e-3 High 0.4612 0.4722 4.1029 5.410e3 12 12.0 28.0 0.992 0.9902 0.0045 1.0000 0.0132 0.04 1e-8 Low 0.0250 0.0272 253.917 155.263 3 3.0 38.0 1.002 0.9991 0.0001 0.9994 0.0008 1e-3 1e4 1e-7 High Medium 0.0338 0.0290 17.303 12.837 3 3.0 14.5 0.332 0.9900 0.0058 0.9951 0.0045 R=1e-5 1e-2 1e-6 High 0.0918 0.0557 1.6071 1.225e5 8 8.0 9.9 0.992 0.9906 0.0034 0.9994 0.0142 1e-3 TABLE II SIMULATION RESULTS FOR: TRACKING REGIME PD=1, Q=100, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 2e-4 Low 0.0408 0.0407 2.9255 2.0060 5 5.0 30.3 0.996 0.9933 0.0016 0.9998 0.0020 0.02 1.0 1e-3 Medium Medium 0.0446 0.0446 0.5143 333.138 10 10.0 27.8 0.996 0.9900 0.0042 0.9995 0.0093 R=0.01 0.1 4e-3 High 0.0763 0.1062 0.1580 3.402e3 84 84.0 90.9 0.793 0.9900 0.0099 1.0000 0.0334 0.4 TABLE III SIMULATION RESULTS FOR: TRACKING REGIME PD=0.9, Q=1000, ?=0.1 PDET PFA PDET PFA NTA NCD ST?sirf ST?sim SL?sirf SL?sim n n  T n  L ?np Theory Theory Exper. Exper 0.05 Low 1.166e3 1.167e3 1.073e4 3.357e4 11 12.2 257 0.542 0.9900 0.0089 1.000 0.0073 5e-5 1e-4 0.1 Low Medium 1.167e3 1.167e3 5.509e3 1.481e4 21 23.2 309 0.858 0.9900 0.0067 1.0000 0.0052 R=1000 1e-4 0.2 High 1.168e3 1.170e3 3.180e3 6.979e3 46 50.6 475 0.798 0.9900 0.0089 1.000 0.0022 2e-4 3] T. Fortmann, Y. Bar-Shalom, Y. Scheffe, and S. B. Gelfand  Detection Thresholds for Tracking in Clutter- A Connection Between Estimation and Signal Processing  IEEE Trans Auto. Ctrl., Mar 1985 4] S. B. Gelfand, T. Fortmann, and Y. Bar-Shalom  Adaptive Threshold Detection Optimization for Tracking in Clutter  IEEE Trans. Aero. &amp; Elec. Sys., April 1996 5] Ch. M. Gadzhiev  Testing the Covariance Matrix of a Renovating Sequence Under Operating Control of the Kalman Filter  IEEE Auto. &amp; Remote Ctrl., July 1996 6] L. C. Ludeman, Random Processes: Filtering, Estimation and Detection, Wiley, 2003 7] L. Y. Pao and W. Khawsuk  Determining Track Loss Without Truth Information for Distributed Target Tracking Applications  Proc. Amer. Ctrl. Conf., June 2000 8] L. Y. Pao and R. M. Powers  A Comparison of Several Different Approaches for Target Tracking in Clutter  Proc Amer. Ctrl. Conf., June 2003 9] X. R. Li and Y. Bar-Shalom  Stability Evaluation and Track Life of the PDAF Tracking in Clutter  IEEE Trans. Auto Ctrl., May 1991 10] X. R. Li and Y. Bar-Shalom  Performance Prediction of 


10] X. R. Li and Y. Bar-Shalom  Performance Prediction of Tracking in Clutter with Nearest Neighbor Filters  SPIE Signal and Data Processing of Small Targets, July 1994 11] X. R. Li and Y. Bar-Shalom  Detection Threshold Selection for Tracking Performance Optimization  IEEE Trans. on Aero. &amp; Elect. Sys., July 1994 12] D. Salmond  Mixture Reduction Algorithms for Target Tracking in Clutter  SPIE Signal and Data Processing of Small Targets, Oct. 1990 13] L. Trailovic and L. Y. Pao  Position Error Modeling Using Gaussian Mixture Distributions with Application to Comparison of Tracking Algorithms  Proc. Amer. Ctrl. Conf., June 2003 4323 pre></body></html 


dense data sets for evaluating pattern mining algorithms These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Recently, theMAFIA algorithm [6] was proposed to ef?ciently discover maximal frequent patterns. As shown in their paper, MAFIA can be several orders better than some alternatives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the ?rst BFS phase Experimental Platform We implemented the MHP algorithms using C++ and all experiments were performed Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE on a Pentium III    MHz PC machine with    megabytes main memory, running Linux Redhat 6.1 operating system 5.2. A Performance Comparison 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C he ck ed P at te rn s Support threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 


Figure 4. The Number of Checked Patterns on the Pumsb* Data Set 10 100 1000 10000 100000 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 R un T im e s ec  Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 5. The RunTime Comparison on the Pumsb* Data Set Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the number of checked patterns is increased with the decrease of the h-con?dence threshold. However, the number of checked patterns of MHP can be signi?cantly smaller than that of MAFIA even if a low h-con?dence threshold is speci?ed To check a pattern, we need to count the support of the patterns. Counting the support of a pattern is the most timeconsuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Ma?a, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more ef?cient if smaller number of patterns need to be checked 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N 


um be r o f C he ck ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 6. The Number of Checked Patterns on the Pumsb Data Set 10 100 1000 10000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 R un T im e s ec  Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 7. The RunTime Comparison on the Pumsb Data Set The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the ?gure, we can observe that the runtime of MHP can be signi?cantly reduced with the increase of h-con?dence thresholds. Also the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-con?dence threshold is as low as 0.3. The reason is that the number of checked patterns of MHP is sign?cantly smaller than that of MAFIA Similar results are also obtained from the pumsb data set as shown in Figure 6 and Figure 7. For the pumsb data set the number of checked patterns of MHP is much smaller 


than that of MAFIA and the runtime of MHP can be significantly less than that of MAFIA 5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns Figure 8 and Figure 9 show the number of maximal patterns identi?ed byMHP andMAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of maximal hyperclique patterns identi?ed by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identi?ed by MAFIA. In other words, the number Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 100 1000 10000 100000 1e+06 1e+07 1e+08 1e+09 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0 Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set 100 1000 10000 100000 1e+06 


1e+07 1e+08 0.1 0.2 0.3 0.4 0.5 0.6 0.7 N um be r o f C ou nt ed P at te rn s Support Threshold Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9 Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in realworld applications, it is dif?cult to interpret several million maximal frequent patterns. However, it is possible to interpret the results of maximal hyperclique pattern mining In addition, due to the memory limitation, we cannot extract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyperclique patterns when the support threshold is 0.1, if we set the h-con?dence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be dif?cult to identify for MAFIA. Hence, MHP can better explore the pattern space and ?nd interesting patterns at low levels of support 6. Conclusions and Future Work In this paper, we present a two-phase Maximal Hyperclique Pattern \(MHP best features of both the BFS strategy and the DFS strategy. More speci?cally, we adapted DFS pruning methods such as equivalence pruning, to an apriori-like approach In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algorithms and has the ability to identify patterns at extremely low levels of support in dense data sets There are several directions for future work. First, in 


this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the ?rst phase is stopped at a deeper level Also, the projection is a very ef?cient method for ?nding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an ef?cient parallel algorithm for mining maximal hyperclique patterns References 1] R. Agarwal, C. Aggarwal, and V. Prasad. A Tree Projection Algorithm For Generation of Frequent Itemsets. pages 350 371, Feb 2001 2] R. Agrawal, T. Imielinski, and A. Swami. Mining Association Rules between Sets of Items in Large Databases. In Proc. of the ACM SIGMOD Conference on Management of Data, pages 207ñ216,May 1993 3] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Proc. of the 20th Intíl Conference on Very LargeData Bases, 1994 4] R. Bayardo. Ef?ciently mining long patterns from databases In Proc. of the ACM SIGMOD Conference, 1998 5] R. Bayardo and R. Agrawal. Mining the Most Interesting Rules. In Proc. of the ACM SIGKDD Conference, 1999 6] D. Burdick, M. Calimlim, and J. Gehrke. Ma?a: AMaximal Frequent Itemset Algorithm for Transactional Databases. In Proc. of IEEE Conf. on Data Engineering, 2001 7] Y. Huang, H. Xiong, W. Wu, and Z. Zhang. A Hybrid Approach for Mining Maximal Hyperclique Patterns. In In Technical Report UTDCS-34-04, Department of computer science, University of Texas - Dallas, 2004 8] J.Han, J.Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of the ACM SIGMOD International Conference on Management of Data, 2000 9] M.J.Zaki and C.Hsiao. ChARM: An ef?cient algorithm for closed itemset mining. In Proc. of 2nd SIAM International Conference on Data Mining, 2002 10] R.Rymon. Search through Systematic Set Enumeration. In Proc. Third Intíl Conference on Principles of Knowledge Representation and Reasoning, 1992 11] H. Xiong, M. Steinbach, P.-N. Tan, and V. Kumar. HICAP: Hierarchial Clustering with Pattern Preservation. In Proc. of 2004 SIAM International Conference on Data Mining \(SDM 12] H. Xiong, P.-N. Tan, and V. Kumar. Mining Strong Af?nity Association Patterns in Data Set with Skewed Support. In Proc. of the Third IEEE International Conference on Data Mining \(ICDM Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence \(ICTAI 2004 1082-3409/04 $20.00 © 2004 IEEE 





