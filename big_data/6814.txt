Automatic Objects Removal for Scene Completion Jianjun Yang Department of Computer Science University of North Georgia Oakwood GA 30566 USA Email jianjun.yang@ung.edu Kun Hua Department of Electrical and Computer Engineering Lawrence Technological University Southìeld Michigan 48075 USA Email khua@ltu.edu Yin Wang Department of Mathematics and Computer Science Lawrence Technological University Southìeld Michigan 48075 USA Email ywang12@ltu.edu Wei Wang Department of Electrical Engineering and Computer Science South Dakota State University USA Email wei.wang@ieee.org Honggang Wang Department of Electrical and Computer Engineering University Massachusetts Dartmouth USA Email hwang1@umassd.edu Ju Shen Department of Computer Science University of Kentucky Lexington KY 40506 USA Email jushen.tom@uky.edu Abstract With the explosive growth of web-based cameras and mobile devices billions of photographs are uploaded to the internet We can trivially collect a huge number of photo streams for various goals such as 3D scene reconstruction and other big data applications However this is not an easy task due to the fact the retrieved photos are neither aligned nor calibrated Furthermore with the occlusion of unexpected foreground objects like people vehicles it is even more challenging to nd feature correspondences and reconstruct realistic scenes In this paper we propose a structurebased image completion algorithm for object removal that produces visually plausible content with consistent structure and scene texture We use an edge matching technique to infer the potential structure of the unknown region Driven by the estimated structure texture synthesis is performed automatically along the estimated curves We evaluate the proposed method on different types of images from highly structured indoor environment to the natural scenes Our experimental results demonstrate satisfactory performance that can be potentially used for subsequent big data processing 3D scene reconstruction and location recognition Keywords Image Completion Texture Synthesis Online Photos Scene Reconstruction Object Removal I I NTRODUCTION In the past few years the massive collections of imagery on the Internet have inspired a wave of work on many interesting big data topics scene reconstruction location recognition and online sharing of personal photo streams  3 4 F o r e xample one can easily do wnload a huge number of photo streams associated with a particular place By using features e.g SIFT it is possible to automatically estimate correspondence information and reconstruct 3D geometry for the scene 6 Imagine b uilding a w orldscale location recognition engine from all of the geotagged images from online photo collections such as Flickr and street view databases from Google and Microsoft However it is a challenging task as the photo streams are neither aligned nor calibrated since they are taken in different temporal spatial and personal perspectives Furthermore with the occlusion of unexpected foreground objects it is even more difìcult to recover the whole scene or accurately identify overlapping regions between different photos To resolve the above issue image in-painting is an effective solution In this paper we propose an automatic object removal algorithm for scene completion which beneìts subsequent large imagery processing The core of our method is based on the structure and texture consistency Our proposed approach has two major contributions First we develop a curve estimation approach to infer the potential structure of the occluded region on the image Second an orientated patch matching algorithm is designed for texture propagation Our work has a broad range of applications including image localization 8 pri v a c y protection 9  11 and other netw ork based applications 12 13  15 16 II R ELATED W ORKS In the literature image completion or in-painting has been intensively studied in Efros and Leung used a onepass greedy algorithm to render unknown pixels based on the assumption that the probability distribution of the pixelês brightness is independent to the rest of the image when the spatial neighborhood is given In the authors proposed an example-based approach to ll in the missing regions It worked well in lling in small gaps but not in large ones The weakness of such approach is that it fails to preserve the potential structures Jia et al  designed 2014 IEEE INFOCOM Workshops: 2014 IEEE INFOCOM Workshop on Security and Privacy in Big Data U.S. Government work not protected by U.S. copyright 553 


2 a b c d e f Fig 1 Scene recovery by removing speciìed foreground object a Original Image b Our result c Contour detection by using OWT-UCM method  d Edge e xtraction e Structure generation in the occlusion re gion by identifying corresponding edge pairs f Some denotations in our algorit hm an image in-painting method based on texture-segmentation and tensor-voting that created smooth linking structures in the occluded regions This method sometimes introduces noticeable artifact due to the texture inconsistency Criminisi et al  made an impro v ement by assigning in-painting orders based on the edge strength levels Their algorithm used a conìdence map and the image edges to determine the patch completion priority However the structures in the resulting images are not well preserved The method in produced a better result via structure propagation while this approach requires more interaction The completion results largely depend on the animatorês individual technique Some other existing work also explored in 23 24 III O UR A PPROACH The process of our framework is for a given image users specify the object for removal by drawing a closed contour around it The enclosure is considered an unknown region that is inferred and replaced by the remaining region of the image Figure 1\(a shows an example the red car is selected as the removing object In the resulting image Figure 1\(b the occluded region is automatically recovered based on the surrounding environment First let us deìne a set of notations for the rest of our paper For an image I  the target region for in-painting is denoted as   the remaining part of the image is denoted as   I     which is also known as source region The boundary contour along  is denoted as   A pixelês value is represented by p  I  x  y   where x and y are the 2D coordinates on the image The surrounding neighborhood centered at  x y  is often called as a patch denoted as  p  The coordinates of pixels inside the patch  p should be in the range  x  x y  y   These concepts are illustrated in Figure 1\(f In our framework there are three phases involved to achieve the scene recovery:structure estimation structure propagation and remaining part lling A Structure Estimation In this phase we estimate the potential structure in  by nding all the possible edges This procedure can be further decomposed into two steps Contour Detection in  and Curve Generation in   1 Contour Detection in   We rst segment the region  by using gPb Contour Detector  It is based on the idea of computing the oriented gradient signal G  x y   on the four channels of its transformed image brightness color a  color b and texture channel G  x y   is the gradient signal where  x y  indicates the center location of the circle mask that is drawn on the image and  indicates the orientation The gPb Detector is composed of two important components mPb Edge Detector and sPb Spectral Detector  W e apply linear combination on mPb and sPb factored by  and 012  according to the gradient ascent on F-measure gPb  x y     mPb  x y   012  sPb  x y   1 Thus a set of edges in  can be retrieved via gPb  However these edges are not in close form and have classiìcation 2014 IEEE INFOCOM Workshops: 2014 IEEE INFOCOM Workshop on Security and Privacy in Big Data 554 


3 ambiguities To solve this problem we use the Oriented Watershed Transform  and Ultrametric Contour Map   OWT-UCM  algorithm to nd the potential contours by segmenting the image into different regions The output of OWT-UCM is a set of different contours  C i  and their corresponding boundary strength levels  L i  as Figure 1\(c shows 2 Curve Generation in   After obtaining the contours  C i  from the above procedure salient boundaries in  can be found by traversing  C i   Our method for generating the curves in  is based on the assumption for the edges on the boundary in  that intersects with the   it either ends inside  or passes through the missing region  and exits at another point of   Below is our algorithm for identifying the curve segments in    Algorithm III.1 Identifying curve segments in  Require Construct curve segments in   Ensure The generated curves have smooth transition between known edges 1 Initial t  1.0 2 For t  t   t 3 if  e   C   E      4 Insert e into  E  5 End if t  T 6 Set t  t 0  retrieve all the contours in  C i  with L i  t 7 Obtain  x1  x2  for each E x 8 DP on   01  02    11  12    to nd optimal pairs from the list 9 According to the optimal pairs retrieve all the corresponding edge-pairs   E x1 E x 2    E x 3 E x 4       10 Compute a transition curve C st for each  E s E t   In algorithm III.1 it has three main parts a collect all potential edges  E x  in  that hits   b identify optimal edge pairs   E s E t   from  E x   c construct a curve C st for each edge pair  E s E t   Edges Collection The output of OWT-UCM are contours sets  C i  and their corresponding boundary strength levels  L i   Given different thresholds t  one can remove those contours C with weak L  Motivated by this we use the Region-Split scheme to gradually demerge the whole  into multiple sub-regions and extract those salient curves This process is carried out on lines 1-9 at the beginning the whole region  is considered as one contour then iteratively decrease t to let potential sub-contours  C i  faint out according the boundary strength Every time when any edges e from the newly emerged contours  C  were detected of intersecting with   they are put into the set  E   Optimal Edge Pairs the reason of identifying edge pairs is based on the assumption if an edge is broken up by   there must exist a pair of corresponding contour edges in  that intersect with   To nd the potential pairs   E s E t   from the edge list  E x   we measure the corresponding enclosed regions similarities The neighboring regions  x1  x2  which is partitioned by the edge E s are used to compare with the corresponding regions of another edge E t  This procedure is described on lines 7  9 of the algorithm III.1 Each neighboring region is obtained by lowing down the threshold value t to faint out more detailed contours as Figure 1\(d shows To compute the similarity between regions we use the Jensen-Shannon divergence  method that w orks on the color histograms d  H 1 H 2  n  i  1  H i 1  log 2  H i 1 H i 1  H i 2  H i 2  log 2  H i 2 H i 2  H i 1  2 where H 1 and H 2 are the histograms of the two regions  1  2  i indicates the index of histogram bin For any two edge  E s E t   the similarity between them can be expressed as M  E s E t   L s  L t  L max  min  d  H si H ti  d  H sj H tj   3 i and j are the exclusive numbers in  1 2   where 1 and 2 represent the indices of the two neighboring regions in  around a particular edge The L max is the max value of the two comparing edges strength levels The rst multiplier is a penalty term for big difference between the strength levels of the two edges To nd the optimal pairs among the edge list dynamic programming is used to minimize the global distance  s,t M  E s E t   where s   t and s t   0 1  size   E i     To enhance the accuracy a maximum constraint is used to limit the regions difference d  H 1 H 2   H  If the individual distance is bigger than the pre-speciìed threshold  H  the corresponding region matching is not considered In this way it ensures if there are no similar edges existed no matching pairs would be identiìed Generate Curves for each  E s E t   we adopt the idea of tting the clothoid segments with polyline stoke data rst before generating a curve Initially  a series of discrete points along the two edges E s and E t are selected denoted as  p s0 p s1   p sn p t0 p t1   p tm   These points have a distance with each other by a pre-speciìed value  d For any three adjacent points  p i  1 p i p i  1   the corresponding curvature k i could be computed according to k i  2  det  p i  p i  1 p i  1  p i   p i  p i  1    p i  1  p i    p i  1  p i  1  4 Combining the above curvature factors a sequence of polyline are used to t these points The polylines are expected to have a possibly small number of line segments while preserving the minimal distance against the original data Dynamic programming is used to nd the 2014 IEEE INFOCOM Workshops: 2014 IEEE INFOCOM Workshop on Security and Privacy in Big Data 555 


4 most satisìed polyline sequence by giving a penalty for each additional line segment A set of clothoid segments can be derived corresponding to each line segment After a series rotations and translations over the clothoid a nal curve C is obtained by connecting each adjacent pair with G 2 continuity Figure 1\(e demonstrates the curv e generation result B Structure Propagation After the potential curves are generated in   a set of texture patches denoted as   0  1     need to be found from the remaining region  and placed along the estimated curves by overlapping with each other with a certain proportion Similar to an ener gy minimization based method is proposed in a Belief Propagation BP framework However we have different deìnitions for the energy and message passing functions The details are in the algorithm III.2  Algorithm III.2 BP Propagation Algorithm Require Render the texture for each patch  i in  along the estimated structures Ensure Find the best matching patches while ensuring the global coherence and consistency 1 For each curve C in   deìne a series of anchor points on it  a i   i  1  n   2 Collect exemplar-texture patches    t i  in   where t i   1 m  3 Setup a factor graph G   V  E  based on  C  and  a i  4 Deìning the energy function E for each a i  E i  t i   where t i is the index in  1 M   5 Deìning the message function M ij for each edge E in G  with initial value M ij  0 6 Iteratively update all the messages M ij passed between  a i  7 M ij  min a i  E i  t i  E ij  t i t j   k N  i  k   j M ki  8 end until  M ij    i j by Convergence 9 Assign the best matching texture patch from    t  for each a i that arg min  T,R    i V E i  t i    i,j  E E ij  t i t j    Here T is an n dimensional vector  t 1 t 2   t n   where i   1 n   R is also an n dimensional vector  r 1 r 2   r n  with each element representing the orientation of source patch   t i  In the algorithm the anchor points are evenly distributed along the curves with an equal distance from each other d  These points represent the center where the patches   i   l  l  are synthesized as shown in Figure 1\(f In practice we deìne d  1 4  l  The    t  is the source texture patches in   They are chosen on from the neighborhood around   For the factor graph building we consider each a i as a vertex V i and E ij  a i a j  where i  j are the two adjacent points In previous works 20 each  i have the same orientation as   t i  which limits the varieties in the source texture Noticing that different patch orientations could produce different results we introduce a scheme called Adaptive Patch by deìning a new formulas for E and M in the structure propagation Traditionally the node energy E i  t i  is deìned as the Sum of Square Difference SSD by comparing the known pixels in each patch  i with the candidate corresponding portion in   t i  But this method limits the salient structure directions Instead of using SSD on the two patches a series of rotations are performed on the candidate patch before computing the similarity Mathematically E i  t i  can be formulated as E i  t i     P     i   R       t i  2  5 Where  R represents different rotations on the patch   t i  Since the size of a patch is usually small the rotation can be speciìed with an arbitrary number of angles In our experiment it is speciìed as    0   4    2 015   The parameter  represents the number of known pixels in  i that overlap with the rotated patch   t i  P is a penalty term the more number of overlapping pixels the higher of similarity is assigned So we use P to discourage the patches with smaller number of sharing pixels Here the percentage is expressed as P   l 2  l is the length of     is the corresponding normalized scalar Thus the best matching patch   is represented by two factors index t i and rotation R i  In a similar way the energy E ij  t i t j  on each edge E ij can be expressed as E ij  t i t j     P     i  t i  t i   j  t j  t j   2  6 Here i and j are the indices of the two adjacent patches in   A penalty scheme is applied to the similarity comparison The two parameters for  i indicate the index and rotation for the source patches in    t i   The messages propagation is derived from the results of the above energy functions We adopt a similar method as where the message M ij passes by patches  i is deìned as M ij  E i  t i  E ij  t i t j  7 Through iterative updating on the BP graph an optimal decision of  t i  for the patches in   i  is made by minimizing the nodes energy This principle can be formulated in the deìnition below  t i  arg min t i  E i  t i   k M ki  8 Where k is one of the neighbors of the patch  i  k  N  i    t i is the optimal index for the matching patch To achieve minimum global energy cost dynamic programming is used Each assignment for  i or a i is considered as a stage In each stage the choices of   t i represent different states The edge E ij represents the transit cost from state   t i at stage i to state   t j at stage j  Starting from i  0 an 2014 IEEE INFOCOM Workshops: 2014 IEEE INFOCOM Workshop on Security and Privacy in Big Data 556 


5 optimal solution is achieved by minimizing the total energy  i  t i  from last step  i  t i  E i  t i  min  E ij  t i t j   i  1  t i  1   9 where  i  t i  represents a set of different total energy values at current stage i  In the situation of multiple intersections among curves C  we adopted the idea in where readers can refer for further details C Remaining Part Filling After the curves are generated in   we ll the remaining regions by using the exemplar-based approach in The  is getting smaller and smaller by spreading out the known pixels  in a certain order To enhance the accuracy all the pixels in the above generate patches along the estimated curves are assigned with a pre-computed conìdence value based on the conìdence updating rule in 20 IV E XPERIMENTS a b c d Fig 2 Kanizsa Triangle Experiment a The original Image b\e reconstruction for the missing region  c Result by Criminisiês method d Our result In our experiments we rst evaluate our proposed approach in terms of structure coherence by comparing our result with the one in that w orks on the well-kno wn Kanizsa triangle  As shown in Figure 2\(a the white triangle in the front is considered as the occluded region  that needs to be removed First a structure propagation is carried out based on the detected edges along   The dash lines in Figure 2\(b indicate the estimated potential structures in   Texture propagation is applied to the rest of the image based on the conìdence and isophote terms One can notice both the triangle and the circles are well completed in our result Figure 2\(d comparing with Criminisiês method in Figure 2\(c To further demonstrate the performance a set of images are used for scene recovery ranging from indoor environment to natural scenes Figure 3\(e shows an indoor case where highly structured patterns often present such as the furniture windows walls The green bottle on the ofìce partition is successfully removed while preserving the remaining structure In this example three pairs of edges are identiìed and connected by the corresponding curves that are generated in the occluded region   Figure 3\(g and 3\(f show the results of removing trees in the nature scenes Several curves are inferred by matching the broken edges along  and maximizing the continuity We can notice the three layers of the scene sky background trees and grass land are well completed In Figure 3\(h it shows a case that a perching bird is removed from the tree Our structure estimation successfully completes the tree branch with smooth geometric and texture transitions V C ONCLUSION In this paper we present a novel approach for foreground objects removal while ensuring structure coherence and texture consistency The core of our approach is using structure as a guidance to complete the remaining scene This work would beneìt a wide range of applications especially for the online massive collections of imagery such as photo localization and scene reconstructions Moreover this work is applied to privacy protection by removing people from the scene R EFERENCES  P ablo Arbel  aez Boundary extraction in natural images using ultrametric contour maps Conference on Computer Vision and Pattern Recognition Workshop 2006 CVPRW 06  pp 182  190 Jun 2006  Y u Su Y i W ang Gagan Agra w al and Rajkumar K ettimuthu Sdquery dsi integrating data management support with a wide area data transfer protocol in Proceedings of SC13 International Conference for High Performance Computing Networking Storage and Analysis  ACM 2013 p 47  Y i W ang Y u Su and Gagan Agra w al Supporting a light-weight data management layer over hdf5 in Cluster Cloud and Grid Computing CCGrid 2013 13th IEEE/ACM International Symposium on  IEEE 2013 pp 335Ö342  Y i W ang W e i Jiang and Gagan Agra w al Scimate A n o v el mapreduce-like framework for multiple scientiìc data formats in Cluster Cloud and Grid Computing CCGrid 2012 12th IEEE/ACM International Symposium on  IEEE 2012 pp 443Ö450  Qingquan Sun Peng W u  Y eqing W u  Mengcheng Guo and Jiang Lu Unsupervised multi-level non-negative matrix factorization model Binary data case Journal of Information Security  vol 3 no 4 2012  Fei Hu Qi Hao Marcin Luk o wiak Qingquan Sun K yle W ilhelm Stanislaw Radziszowski and Yao Wu Trustworthy data collection from implantable medical devices via high-speed security implementation based on ieee 1363 Information Technology in Biomedicine IEEE Transactions on  vol 14 no 6 pp 1397Ö1404 2010 2014 IEEE INFOCOM Workshops: 2014 IEEE INFOCOM Workshop on Security and Privacy in Big Data 557 


6 a b c d e f g h Fig 3 Scene reconstruction results on different settings the rst row shows the original images the second row shows the corresponding result ima ges  Jianjun Y ang and Zongming Fei HD AR Hole detection and adaptive geographic routing for ad hoc networks in Computer Communications and Networks ICCCN 2010 Proceedings of 19th International Conference on  August 2010  Jianjun Y ang and Zongming Fei Broadcasting with prediction and selective forwarding in vehicular networks in International Journal of Distributed Sensor Networks  2013  Ju Shen Anusha Raghunathan S.S Cheung and Rita P atel  Automatic content generation for video self modeling in Multimedia and Expo ICME 2011 IEEE International Conference on  2011 pp 1Ö6  Ju Shen Changpeng T i  S.-C.S Cheung and R.R P atel  Automatic lip-synchronized video-self-modeling intervention for voice disorders in e-Health Networking Applications and Services Healthcom 2012 IEEE 14th International Conference on  2012 pp 244Ö249  Miao P an Xiao yan Zhu and Y uguang F ang Using homomorphic encryption to secure the combinatorial spectrum auction without the trustworthy auctioneer Wireless Networks  vol 18 no 2 pp 113 128 2012  Jianjun Y ang and Zongming Fei Bipartite graph based dynamic spectrum allocation for wireless mesh networks in Distributed Computing Systems Workshops 2008 ICDCSê08 28th International Conference on  IEEE 2008 pp 96Ö101  Jianjun Y ang and Zongming Fei Statistical ltering based broadcast protocol for vehicular networks in Proceedings of IEEE ICCCN  2011 vol 11 pp 1Ö6  K unjie Xu Da vid T ipper  Prashant Krishnamurthy  and Y i Qian An efìcient hybrid model and dynamic performance analysis for multihop wireless networks in Computing Networking and Communications ICNC 2013 International Conference on  IEEE 2013 pp 1090Ö1096  K unjie Xu and Mu Zhou Ener gy balanced chain in ieee 802.15 4 low rate wpan in Computing Networking and Communications ICNC 2013 International Conference on  IEEE 2013 pp 1010 1015  K unjie Xu Siriluck T ipmongk onsilp Da vid T ipper  Y i Qian and Prashant Krishnamurthy A time dependent performance model for multi-hop wireless networks with cbr tra fìc in 29th IEEE International Performance Computing and Communications Conference  2010  A Efros and T  Leung T e xture synthesis by non-parametric sampling Proc Int Conf Computer Vision  pp 1033Ö1038 September 1999  A Efros and W  T  Freeman Image quilting for te xture synthesis and transfer Proc ACM Conf Comp Graphics SIGGRAPH  pp 341Ö346 August 2001  J Jia and C.-K T ang Image repairing Rob ust image synthesis by adaptive nd tensor voting Proc Conf Comp Vision Pattern Rec  2003  A Criminisi P  Prez and K T o yama Re gion lling and object removal by exemplar-based inpainting IEEE Trans Image Process  vol 13 no 9 pp 1200Ö1212 Sep 2004  Jian Sun Lu Y uan Jiaya Jia and Heung-Y eung Shum Image completion with structure propagation ACM Transactions on Graphics TOG Proceedings of ACM SIGGRAPH  2005  J Shen PC Su SC Cheung and J Zhao V irtual mirror rendering with stationary rgb-d cameras and stored 3d background IEEE transactions on image processing a publication of the IEEE Signal Processing Society  2013  Ju Shen and S.-C.S Cheung Layer depth denoising and completion for structured-light rgb-d cameras in Computer Vision and Pattern Recognition CVPR 2013 IEEE Conference on  2013 pp 1187 1194  Ju Shen and W a i tian T an Image-based indoor place-ìnder using image to plane matching in Multimedia and Expo ICME 2013 IEEE International Conference on  2013 pp 1Ö6  P ablo Arbel  aez Michael Maire Charless Fowlkes and Jitendra Malik Contour detection and hierarchical image segmentation IEEE Transactions on Pattern Analysis and Machine Intelligence  vol 33 pp 898Ö916 May 2011  P  W  Lamberti A P  Majte y  A Borras M Casas and A Plastino Metric character of the quantum jensen-shannon divergence Phys Rev A  vol 77 pp 052311 May 2008  James McCrae and Karan Singh Sk etching piece wise clothoid curves Computers and Graphics  vol 33 pp 452  461 Aug 2009  Mulline x G and Robinson ST   F airing point sets using curv ature  Computer Aided Design  vol 39 pp 27Ö34 2007 2014 IEEE INFOCOM Workshops: 2014 IEEE INFOCOM Workshop on Security and Privacy in Big Data 558 


       


   





Manufacture B Performance of Real-Time Querying IncreQuerying Baseline Baseline IncreQuerying IncreQuerying Baseline Baseline Baseline Baseline mft IncreQuerying Baseline shipdate Lineitem shipdate return\224ag linestatus IncreQuerying Baseline IncreQuerying Baseline IncreQuerying y y IncreQuerying 11 In this experiment we investigate the performance of realtime querying First we compare the Fig 4 Throughput of Real-Time Data Cube Maintenance Algorithm                                                              Fig 5 Performance of Data Cube Refresh    Fig 6 Scalability real-time data cube with the data streams from HBase-R Thus the latency of periodically refreshing the data cube in HBaseR equals to the time of writing the real-time data cube into HBase-R This time is related to the the size of the data cube and does not change as the number of updates increases We also evaluate the scalability of R-Store In this experiment the number of nodes and the data size increase with the same ratio The percentage of updates is set to 1 for different scalability settings As can be seen in Figure 6 the running time of both re-computation the brown line and incremental update blue line do not change much as the number of nodes increase which demonstrates the scalability of R-Store 1 algorithm which optimizes the real-time query using the data cube with the algorithm implemented with the operation The cluster settings are the same as those of Figure 5 except that we 223x the number of updates to 8,000 million and vary the percentage of the keys updated Figure 7\(a shows the processing time of both algorithms for a typical data cube slice query algorithm consists of two parts the black rectangle ReCompScan is the time to scan the real-time table and the yellow rectangle ReCompExe is the execution time of the MapReduce job after the scan phase In contrast the processing time of consists of three parts the red rectangle CubeScan is the time to scan the data cube the blue rectangle UpdateScan is the time to scan the performs much better than  It outperforms the approach for two reasons 1 by using adaptive incremental scan it scans fewer data in HBase-R and shuf\224es fewer data to MapReduce 2 its MapReduce job processes fewer data than that of re-computation However as the percentage of updated keys increases more data are shuf\224ed from HBaseR to MapReduce Thus both the scan time and the execution time increase In contrast for  since the always shuf\224es one version for each key to MapReduce the amount of data shuf\224ed from HBase-R is constant As a result the running time of is almost constant Due to the existence of the 223ltering condition on attribute  most tuples of the table are 223ltered and fewer data are sorted and shuf\224ed during the execution of the MapReduce job As a result the difference between the execution times is not so signi\223cant In general algorithm outperforms algorithm when the percentage of keys being updated is low In addition to the data cube slice query we also evaluate TPC-H Q1 Figure 7\(b with the same experimental settings We did not illustrate other benchmark queries as they involve multiple tables which will not be able to illustrate as clearly the effectiveness of the basic operators supported in R-Store The parameter of TPC-H Q1 is set to 215365\216 days and only about 15 of the tuples are 223ltered  table since we only build the data cube on attributes  and  the data cube is much smaller than the real-time table and the time to scan the data cube is around 20 seconds Overall Figure 7\(b demonstrates that the performance of is signi\223cantly better than that of  To select the better querying method among the two we use the cost model Section V-C to estimate the number of I/Os Figure 8 shows the running time of  and the I/Os estimated for both and algorithms The axis on the left is the processing time of the query while the axis on the right is the estimated I/Os The estimated number of I/Os for the blue line increases linearly with almost the same slope the histogram as the processing time of the query while the estimated number of I/Os for the Baseline the brown line is constant which is around 2.52 10 FROM part WHERE mft 0 2,000 4,000 6,000 8,000 10,000 ReComp Update ReComp Update ReComp Update ReComp Update ReComp Update Processing time \(s Number of Updates 8M 400M 800M 1,200M 1,600M Update ReCompExe ReCompScan 100    200    300    400    500  10  20  30  40  50  60  70  0    1000    2000    3000    4000    5000    6000    7000    8000  50  75  100  125  145  IncrementalUpdate                            0    Updates Per Second \(K Number of Nodes Throughput          Processing Time \(s Number of Nodes ReComputation              The processing time of the table in HBase-R and the grey rectangle UpdateExe is the execution time of the MapReduce job after the scan phase When only a small range of keys are updated larger than 21512-01-1998\216 Thus the execution time of the MapReduce job after the scan phase is longer than that of Figure 7\(a For the  This result hence veri\223es the accuracy of our cost model Compared to querying only the data cube RTOLAP queries require two additional steps which incur additional cost scanning the real-time data from HBase-R and merging the real-time data with the data cube on demand in MapReduce SELECT sum   327 215 002\002    49 FullScan FullScan part shipdate prices GROU P BY brand type size container 


method increases slightly which is due to two reasons 1 the data before In this experiment we investigate the performance of OLTP queries when OLAP queries are running The workload is update-only and the keys being updated are uniformly distributed We launch ten clients to concurrently submit the updates when the system is deployed on 100 nodes Each client starts ten threads each of which submits one million updates 100 updates in batch Another client is launched to submit the data cube slice query That is one OLAP query and approximately 50,000 updates are concurrently processed in R-Store The system reaches its maximum usage in this setting based on our observation When the system is deployed on other number of nodes the number of clients submitting updates is adjusted accordingly Figure 11\(a shows the throughput of the system The throughput increases as the number of nodes increases which demonstrates the scalability of the system However when OLAP queries are running the update performance is lower than running only OLTP queries This result is expected because the OLAP queries compete for resources with the OLTP queries We also evaluate the latency of updates when the system is approximately fully used As shown in Figure 11\(b the aggregated response time for 1000 updates are similar with respect to varying scales VII C ONCLUSION MapReduce is a parallel execution framework which has been widely adopted due to its scalability and suitability in 0    500    1000    1500    2000  0  10  20  30  40  50  60  70  80  90  100  IncreQueryScan             IncreQueryExe              DC DC DC  Q i i i i T part  Q a Data Cube Slice Query                                                                                                b TPC-H Q1 Fig 7 Performance of Querying    Fig 8 Accuracy of Cost Model    Fig 9 Performance vs Freshness On each HBase-R node the key/values are stored in format Though only one or two versions of the same key are returned to MapReduce HBase-R has to scan all the of the table Since the is materialized to HDFS when it is full these 223les are sorted by time Thus instead of scanning all the and between  only the between   are scanned The value of decides the freshness of the result There is a trade-off between the performance of the query and the freshness of the result the smaller is the fewer real-time data are scanned Figure 9 shows the query processing time with different freshness ratios which is de\223ned as the percentage of the real-time data we have to scan for the query In this experiment  1600 million and 800 million updates on 1 distinct keys are submitted to HBase-R When the freshness ratio is 0 the input of the query is only the data cube Thus the cost of scanning the real-time data is 0 When the freshness ratio increases to 10 the cost of scanning the real-time data is around 1500 seconds because the cost of scanning the real-time table dominates the OLAP query As the freshness ratio increases the running time of and  and when it is not  and  We submit 800 million updates to the server each day and the percentage of keys updated is 223xed to 1 The data cube is refreshed at the beginning of each day and the OLAP query is submitted to the server at the end of the day Since the data are compacted after the data cube refresh the amount of data stored in the real-time table are almost the same at the same time of each day The processing time of and are thus almost constant In contrast when the compaction scheme is turned off HBase-R stores much more data and the cost of locally scanning these data becomes larger than the cost of shuf\224ing the data to MapReduce As a result the processing time of and increases over time and and a user speci\223ed timestamp still need to be scanned and 2 the amount of data shuf\224ed to mappers are roughly the same with different ratios Figure 10 depicts the effectiveness of our compaction scheme In this experiment we measure the processing time of the data cube slice query when the compaction scheme is applied  0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan Processing Time \(s I/Os \(X10 11  Percentage of Keys Updated CubeScan        Processing Time \(s Freshness Ratio CubeScan                                                                                                            50 0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan store\223le store\223les part memstore store\223les memstore store\223les IncreQuerying Baseline IncreQuerying Baseline-NC IncreQuerying-NC Baseline IncreQuerying Baseline-NC IncreQuerying-NC C Performance of OLTP 0    1200    2400    3600    4800    6000  1  5  10  15  20  25  0  0.8  1.6  2.4  3.2  4  IncreQueryScan        IncreQueryExe        I/Os estimated for IncreQuery                               I/Os estimated for  Baseline                 T T T T T T T T 


3000    6000    9000    12000  1  2  3  4  5  6  7  IncreQuerying                                   Baseline-NC                   IncreQuerying-NC                       51 002 Fig 10 Effectiveness of Compaction    a Throughput    b Latency Fig 11 Performance of OLTP Queries a large scale distributed environment However most existing works only focus on optimizing the OLAP queries and assume that the data scanned by MapReduce are unchanged during the execution of a MapReduce job In reality the real-time results from the most recently updated data are more meaningful for decision making In this paper we propose R-Store for supporting real-time OLAP on MapReduce R-Store leverages stable technology HBase and HStreaming and extends them to achieve high performance and scalability The storage system of R-Store adopts multi-version concurrency control to support real-time OLAP To reduce the storage requirement it periodically materializes the real-time data into a data cube and compacts the historical versions into one version During query processing the proposed adaptive incremental scan operation shuf\224es the real-time data to MapReduce ef\223ciently The data cube and the newly updated data are combined in MapReduce to return the real-time results In addition based on our proposed cost model the more ef\223cient query processing method is selected To evaluate the performance of R-Store we have conducted extensive experimental study using the TPCH data The experimental results show that our system can support real-time OLAP queries much more ef\223ciently than the baseline methods Though the performance of OLTP degrades slightly due to the competition for resources with OLAP the response time and throughput remain good and acceptable A CKNOWLEDGMENT The work described in this paper was in part supported by the Singapore Ministry of Education Grant No R-252000-454-112 under the epiC project and M.T 250 Ozsu\220s work was partially supported by Natural Sciences and Engineering Research Council NSERC of Canada We would also like to thank the anonymous reviewers for their insightful comments R EFERENCES  http://hbase.apache.or g  http://hstreaming.com  http://www comp.nus.edu.sg epic  M Athanassoulis S Chen A Ailamaki P  B Gibbons and R Stoica Masm ef\223cient online updates in data warehouses In  pages 865\205876 2011  Y  Cao C Chen F  Guo D Jiang Y  Lin B C Ooi H T  V o S W u and Q Xu Es2 A cloud data storage system for supporting both oltp and olap ICDE pages 291\205302 2011  S Ceri and J W idom Deri ving production rules for incremental vie w maintenance In  pages 577\205589 1991  T  Condie N Conw ay  P  Alv aro J M Hellerstein K Elmelee gy  and R Sears Mapreduce online In  pages 313\205328 2010  J Dean S Ghema w at and G Inc Mapreduce simpli\223ed data processing on large clusters In  pages 137\205150 2004  L Golab T  Johnson and V  Shkapen yuk Scheduling updates in a real-time stream warehouse ICDE pages 1207\2051210 2009  M Grund J Kr 250 uger H Plattner A Zeier P Cudre-Mauroux and S Madden Hyrise a main memory hybrid storage engine  4\(2 Nov 2010  A Gupta I S Mumick and V  S Subrahmanian Maintaining vie ws incrementally extended abstract In  pages 157\205166 1993  S H 264 eman M Zukowski N J Nes L Sidirourgos and P Boncz Positional update handling in column stores In  pages 543\205 554 2010  D Jiang G Chen B C Ooi and K.-L T an epic an e xtensible and scalable system for processing big data 2014  D Jiang B C Ooi L Shi and S W u The performance of mapreduce an in-depth study  3\(1-2 Sept 2010  D M Kane J Nelson and D P  W oodruf f An optimal algorithm for the distinct elements problem PODS 22010 pages 41\20552  A K emper  T  Neumann F  F  Informatik T  U Mnchen and DGarching Hyper A hybrid oltp&olap main memory database system based on virtual memory snapshots In  2011  T W  K uo Y T  Kao and C.-F  K uo T w o-v ersion based concurrenc y control and recovery in real-time client/server databases  52\(4 Apr 2003  K Y  Lee and M H Kim Ef 223cient incremental maintenance of data cubes In  pages 823\205833 2006  F  Li B C Ooi M T  250 Ozsu and S Wu Distributed data management using mapreduce In  2014  I S Mumick D Quass and B S Mumick Maintenance of data cubes and summary tables in a warehouse In  pages 100\205111 1997  A Nandi C Y u P  Bohannon and R Ramakrishnan Distrib uted cube materialization on holistic measures In  pages 183\205194 2011  L Neume yer  B Robbins A Nair  and A K esari S4 Distrib uted stream computing platform In  pages 170\205177 2010  C Olston B Reed U Sri v asta v a R K umar  and A T omkins Pig latin a not-so-foreign language for data processing In  pages 1099\2051110 2008  K Ser ge y and K Y ury  Applying map-reduce paradigm for parallel closed cube computation In  pages 62\20567 2009  M Stonebrak er  D J Abadi A Batkin X Chen M Cherniack M Ferreira E Lau A Lin S Madden E O\220Neil P O\220Neil A Rasin N Tran and S Zdonik C-store a column-oriented dbms In  pages 553\205564 2005  A Thusoo J S Sarma N Jain Z Shao P  Chakka S Anthon y  H Liu P Wyckoff and R Murthy Hive a warehousing solution over a mapreduce framework  2\(2 2009  P  V assiliadis and A Simitsis Near real time ETL In  volume 3 pages 1\20531 2009  C White Intelligent b usiness strate gies Real-time data w arehousing heats up  2012 SIGMOD VLDB NSDI OSDI SIGMOD SIGMOD Proc VLDB Endow In ICDE IEEE Trans Comput VLDB ACM Computing Survey SIGMOD ICDE ICDMW SIGMOD DBKDA VLDB PVLDB Annals of Information Systems DM Review 0    Processing Time \(s Time since the Creation of Data Cube \(day Baseline                  Updates Per Second \(K Number of Nodes Updates only                  Response Time for 1000 Updates\(s Number of Nodes Updates only                  0    20    40    60    80    100  10  20  30  40  50  60  70  Updates + OLAP                                    0    2    4    6    8    10  10  20  30  40  50  60  70  Updates + OLAP                                    Proc VLDB Endow 


  13    1  2   3   4   5   6   7   8  9  10  11   


and aeronautical engineering with degrees from Universitat Politecnica de Catalunya in Barcelona Spain and Supaero in Toulouse France He is a 2007 la Caixa fellow and received the Nortel Networks prize for academic excellence in 2002 Dr Bruce Cameron is a Lecturer in Engineering Systems at MIT and a consultant on platform strategies At MIT Dr Cameron ran the MIT Commonality study a 16 002rm investigation of platforming returns Dr Cameron's current clients include Fortune 500 002rms in high tech aerospace transportation and consumer goods Prior to MIT Bruce worked as an engagement manager at a management consultancy and as a system engineer at MDA Space Systems and has built hardware currently in orbit Dr Cameron received his undergraduate degree from the University of Toronto and graduate degrees from MIT Dr Edward F Crawley received an Sc.D in Aerospace Structures from MIT in 1981 His early research interests centered on structural dynamics aeroelasticity and the development of actively controlled and intelligent structures Recently Dr Crawleys research has focused on the domain of the architecture and design of complex systems From 1996 to 2003 he served as the Department Head of Aeronautics and Astronautics at MIT leading the strategic realignment of the department Dr Crawley is a Fellow of the AIAA and the Royal Aeronautical Society 050UK\051 and is a member of three national academies of engineering He is the author of numerous journal publications in the AIAA Journal the ASME Journal the Journal of Composite Materials and Acta Astronautica He received the NASA Public Service Medal Recently Prof Crawley was one of the ten members of the presidential committee led by Norman Augustine to study the future of human space\003ight in the US Bernard D Seery is the Assistant Director for Advanced Concepts in the Of\002ce of the Director at NASA's Goddard Space Flight Center 050GSFC\051 Responsibilities include assisting the Deputy Director for Science and Technology with development of new mission and measurement concepts strategic analysis strategy development and investment resources prioritization Prior assignments at NASA Headquarters included Deputy for Advanced Planning and Director of the Advanced Planning and Integration Of\002ce 050APIO\051 Division Director for Studies and Analysis in the Program Analysis and Evaluation 050PA&E\051 of\002ce and Deputy Associate Administrator 050DAA\051 in NASA's Code U Of\002ce of Biological and Physical Research 050OBPR\051 Previously Bernie was the Deputy Director of the Sciences and Exploration Directorate Code 600 at 050GSFC\051 Bernie graduated from Fair\002eld University in Connecticut in 1975 with a bachelors of science in physics with emphasis in nuclear physics He then attended the University of Arizona's School of Optical Sciences and obtained a masters degree in Optical Sciences specializing in nonlinear optical approaches to automated alignment and wavefront control of a large electrically-pumped CO2 laser fusion driver He completed all the course work for a PhD in Optical Sciences in 1979 with emphasis in laser physics and spectroscopy He has been a staff member in the Laser Fusion Division 050L1\051 at the Los Alamos National Laboratories 050LANL\051 managed by the University of California for the Department of Energy working on innovative infrared laser auto-alignment systems and infrared interferometry for target alignment for the HELIOS 10 kilojoule eight-beam carbon dioxide laser fusion system In 1979 he joined TRW's Space and Defense organization in Redondo Beach CA and designed and developed several high-power space lasers and sophisticated spacecraft electro-optics payloads He received the TRW Principal Investigators award for 8 consecutive years Dr Antonios A Seas is a Study Manager at the Advanced Concept and Formulation Of\002ce 050ACFO\051 of the NASA's Goddard Space Flight Center Prior to this assignment he was a member of the Lasers and Electro-Optics branch where he focused on optical communications and the development of laser systems for space applications Prior to joining NASA in 2005 he spent several years in the telecommunication industry developing long haul submarine 002ber optics systems and as an Assistant Professor at the Bronx Community College Antonios received his undergraduate and graduate degrees from the City College of New York and his doctoral degree from the Graduate Center of the City University of New York He is also a certi\002ed Project Management Professional 14 


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


