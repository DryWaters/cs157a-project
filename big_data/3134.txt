Research on Data Mining Algorithms for Automotive Customers’ Behavior Prediction Problem          Supported by the National High technology Development 863 program of china \(Grant No. 2007AA04Z10 HUANG Lan College of Computer Jilin University Changchun, P.R.C huanglan@jlu.edu.cn  ZHOU Chun-guang College of Computer Jilin University Changchun, P.R.C cgzhou@jlu.edu.cn  ZHOU Yu-qin College of Computer Jilin University Changchun, P.R.C zhouyuqin1987@163 com WANG Zhe College of Computer Jilin University Changchun, P.R.C wz2000@jlu.edu.cn   Abstract  This paper extracts automotive marketing information, constructs data warehouse, adopts an improved ID3 decision tree model and an association rule model to do data mining, and then obtains prediction information of automotive customers behavior. Experimental and comparative results verify the validity and accuracy of the prediction results   1. Introduction  With the rapid development of automobile industry the automotive marketing competition becomes more and more vigorous. Automotive types are numerous and customers’ needs are completely different due to different region, cultural etc. Therefore, how to obtain the information of customers’ needs quickly, to predict the purchase intention accurately, to lower marketing cost, and to estimate marketing trends, is the key problem for an automotive enterprise to handle well To raise marketing success rate and customers satisfaction, the core problems are analyzing the customer group, locating the target customer and making an active marketing to the target customer That is to say, we should try to discover which customers more likely to purchase which types of car and the customer who purchases a certain car type has which common characteristics In this paper, the research was divided into the following three parts. Firstly, we collect the valid customers’ purchase records from history data set, and clean up, converse and induct these data, and then using SQL Server construct data warehouse to withdraw and handle the data. Secondly, we make use of two models which are more appropriate for prediction of customers’ purchase behavior, that is, the decision tree and the association rule. Here, we propose an improved ID3 algorithm and analyze it Thirdly, we test the validity of the predicting results  2. Construct of Data Warehouse  What the enterprise policymaker concern is the information of market developmental trends customers’ distribution and market prediction etc., but not those concrete trivial data. Therefore, the data warehouse should be designed according to needs. We can adopt subject-oriented data warehouse to store the analysis data of different use. Taking the data warehouse with automobile marketing subject as an example, we can adopt the simplest star model to predict market trends. The advantages of star model are convenient to construct, easy for comprehension, valid to analyze data from several dimension While constructing data warehouse, we need to select data from the car sale system real-timely However, while analyzing the customer group and locating the target customer, not all data are all valid to us. We need to select sample data from the data warehouse according to the decision analytical purpose. Generally, in order to avoid system being occupied to handle the confused data, data needs to be simplified. Here for the sake of comparison conveniently, we adopt same sample data as reference   T h e attribu t es of C U S T OMER m a i n l y i n cl u d e SEX, AGE, EDU and INCOME. Supposing the Automobile enterprise mainly sales SANTANA AUDIA6 and BULK these three brands. We select 16 customers’ purchase records from history data sets, and 
2008 Seventh International Conference on Machine Learning and Applications 978-0-7695-3495-4/08 $25.00 © 2008 IEEE DOI 10.1109/ICMLA.2008.23 677 


clean up, converse and induct these data before restore into the data warehouse. The converted sample data is shown as Table 1  Table 1. 16 Converted sample data NO NAME SEX AGE EDU INCOME CAR_TYPE 1 Cheng L M young Bachelor low SANTANA 2 Zhang F M midlife Master high AUDIA6 3 Li W M young Master average1 BULK 4 Zhao YJ M old Master average2 AUDIA6 5 Lin Y M midlife Bachelor low SANTANA 6 Zhang LY M young Master average2 BULK 7 Li FK M midlife Master average2 BULK 8 Zhao Q F midlife Bachelor average1 SANTANA 9 Yao WY M young Master average2 AUDIA6 10 Li WW F midlife Master average1 SANTANA 11 Cheng K M young Bachelor average1 SANTANA 12 Jin W M midlife Master high BULK 13 Jiang C F old Bachelor high AUDIA6 14 Zhao GP M midlife Bachelor average2 AUDIA6 15 Jiang L F midlife Bachelor low BULK 16 Zhang XW F old Bachelor high BULK  3. Models of Customers’ behavior Prediction  The techniques or methods of data mining which used in prediction are numerous, and their mutations are also same. The common methods include regression prediction, decision tree, neural network cluster, neighbors prediction, rules-oriented, etc. [2-5 Considering the advantages and shortcomings of these methods and the characteristics of automotive customers’ behavior prediction, we select decision tree and associate rule to construct prediction models  3.1 Decision tree  The decision tree is to classify the history data of car-sales. The purpose is to resolve the problem which customers more likely to purchase which types of car The decision tree is essentially a greedy method. It induces a reasonable decision tree by top-down recursive partitioning strategy. Here we propose a new ID3 algorithm which improved the judgment of noise node of ID3 Input: a set of sample data \(INFO\d a set of candidate attributes \(ATTRIBUTE\, including SEX AGE, EDU, and INCOME Output: a decision tree PROCEDURE  CreateTree\(INFO, ATTRIBUTE CreateNode\(INFO Create node N using the set of sample data INFO IF \(all CAR_TYPE values of sample data in INFO are the same THEN   Return N as a leaf node ELSE FOR \(each attribute in ATTIBUTE IF \(the attribute of the node hasn’t been used to be a classification  attribute before THEN Compute the information gain of  the attribute of the node  IF \( the attribute whose information gain is the biggest \(>0\ is marked as ATTIBUTE THEN { Mark this node as the node  which needs to be divided next step according to  ATTIBUTE Divide N into N i and generate each branch of the node N   ELSE  { Mark the node as a noise node Return the node as a leaf node  FOR \(each branch N i  CreateTree N i ATTIBUTE Recursively calling this process   Supposing the attribute CAR_TYPE includes m  values \(namely, there are m car types totally s 1  s m  mean the number of cars of each type respectively in the data set, the S means the total amount of cars. For the attribute attr whose gain we try to find, 1 n mean the corresponding number of the n kind of attr s possibility value. So s 1 j  s mj  mean the number of cars of each type respectively in the data set while attr is j  The gain can be obtained through formulas \(1\~\(4 shown as follows, where m i   1and n j   1   j s ij s ij p  1    m i ij p ij p mj s j s j s I 1   2 log    2  1  2    2  1  1 2 1   mj s j s j s I n j S mj s j s j s attr E       3 
678 


     2  1    attr E m s s s I attr Gain   4 According to the above-mentioned algorithm and the formulas, we can get a decision tree as Figure 1 shown   Figure 1. Produced decision tree   From Figure 1 we can see, the data set is divided to 10 nodes at the end, among them node 1 and node 5 are noise nodes. Therefore, there are 8 normal nodes which are produced to act as the prediction nodes According to these 8 normal crunodes, we can get 8 decision rules which can support automobile marketing prediction 1\The customer whose INCOME is low and SEX is M purchases SANTANA generally. \(node 2 2\The customer whose INCOME is low and SEX is F purchases BUIK generally. \(node 3 3\The customer whose AGE is midlife and INCOME is average1 purchases SANTANA generally node 4 4\The customer whose AGE is old and INCOME is average2 purchases AUDIA6 generally. \(node 6 5\The customer whose AGE is young, EDU is Bachelor and INCOME is avarage1 purchases SANTANA generally. \(node 7 6\The customer whose AGE is young, EDU is Master and INCOME is avarage1 purchases BUIK generally. \(node 8 7\The customer whose AGE is midlife, EDU is Bachelor and INCOME is avarage2 purchases AUDIA6 generally. \(node 9 8\The customer whose AGE is midlife, EDU is Master and INCOME is avarage2 purchases BUIK generally. \(node 10  3.2 Association rule  In data mining, association rule algorithm is a popular and well researched method for discovering interesting relations between variables in large databases. This algorithm can effectively discover the inside relations between the customers’ purchasing behavior and car’s attributes. It can assist making decisions of automobile marketing, mainly resolving the problem that the customer who purchases a certain car type has which common characteristics The rules produced by association analysis have credibility and support. The credibility measures the accuracy of association rule. The support measures the usage scope of the association rule, and means the appearing possibility of the associate rule in all transactions. The mining problem of association rule can convert into find out the rules which is larger or equal to the minimum support \(min_sup\d minimum credibility \(min_cred\ which user provided in the data set. So looking for association rule information, can be resolved to as following two steps   Step 1: Find out all frequent item set A in the transaction database, namely the support of the item set A sup min_   S  Step 2: For each frequent item set A, if    B A B and the credibility of B A   cred min_   C then B A  is an association rule To explain easily, we will generalize the data of the customers’ attributes and the marketing information. In this example, the data of customer’s attributes includes AGE, EDU INCOME and SEX, which are named with the set of item AG, ED, IN, SE respectively.  And the CAR_TYPE is named with CT The values of AGE include {young, midlife, old so the item set of AG is represented by AG AG 1 AG 2 AG 3  The values of EDU include {Bachelor, Master}, so the item set of ED is represented by ED = {ED 1 ED 2  The values of INCOME include {low, average1 average2, high}, so the item set of IN is represented by IN = {IN 1 IN 2 IN 3 IN 4  The values of SEX include {M, F}, so the item set of SE is represented by SE = {SE 1 SE 2  The values of CAR_TYPE include {SANTANA AUDIA6, BUIK}, so the item set of CT is represented by CT={CT 1 CT 2 CT 3  We take for example the research on the relations of the customers’ age, education, income and sex who purchases SANTANA as CAR_TYPE, and promising that the support of association rule must be larger or equal to 30%, the credibility must be larger or equal to 90%. We can extract the transaction set W from the item set shown as Table 1 W={\(SE 1 AG 1 SE 1 ED 1 SE 1 IN 1 AG 1 ED 1  AG 1 IN 1  ED 1 IN 1 SE 1 AG 1 ED 1  SE 1 AG 1 IN 1 SE 1 ED 1 IN 1 AG 1 ED 1 IN 1  SE 1 AG 1 ED 1 IN 1 SE 1 AG 2 AG 2 ED 1  AG 2 IN 1 SE 1 AG 2 ED 1 SE 1 AG 2 IN 1  AG 2 ED 1 IN 1 SE 1 AG 2 ED 1 IN 1  
679 


SE 2 AG 2 SE 2 ED 1 SE 2 IN 2 AG 2 IN 2  ED 1 IN 2 SE 2 AG 2 ED 1 SE 2 AG 2 IN 2  SE 2 ED 1 IN 2 AG 2 ED 1 IN 2  SE 2 AG 2 ED 1 IN 2 SE 2 ED 2 AG 2 ED 2  ED 2 IN 2 SE 2 AG 2 ED 2 SE 2 ED 2 IN 2  AG 2 ED 2 IN 2 SE 2 AG 2 ED 2 IN 2 SE 1 IN 2  AG 1 IN 2 SE 1 AG 1 IN 2 SE 1 ED 1 IN 2  AG 1 ED 1 IN 2 SE 1 AG 1 ED 1 IN 2   W is the item combination of all maybe frequent set in the item set with SANTANA as CAR_TYPE. After processing the transaction set W to find out all frequent set, we will take out the rules which satisfy the credibility of B A  being larger or equal to min_cred in the frequent set. According to the definition of the credibility: credibility B A   P\(B|A\, we can get 14 rules which satisfy the support and credibility in the customers who purchase SANTANA as CAR_TYPE 1\G 1  ED 1 2\G 1  SE 1  3 1  ED 1 4\E 1  ED 1  5 1  SE 1 AG 1 ED 1   SE 1  AG 1 SE 1   ED 1 ED 1 IN 1   SE 1  9 IN 1 SE 1   ED 1 10\E 2  AG 2  11\E 2  IN 2 12\\(AG 2 IN 2   SE 2  AG 2 SE 2   IN 2 14\ \(IN 2 SE 2   AG 2 We can get the information from the rule \(1\ the customers who purchase SANTANA, if his AGE is young, then his EDU is Bachelor generally. With the above-mentioned methods, we can mine associate rules to assist automobile marketing decision better. Finally we will get 30 associate rules which respectively correspond to the three car types. Among them, there are 14 rules corresponding to SANTANA, 4 rules corresponding to AUDIA6, and 12 rules corresponding to BUIK. The characteristics of associate rule algorithm make the set of these 30 rules not a minimum set, there are redundancy among the different k-item sets  4. Verification of prediction results  4.1 Verification of the decision tree  With the same input data set, the algorithm adopted in referen ced th e clas s i f i cat ion decis i o n  tree shown as Figure 2. Contrasting the decision trees produced in referen n d th is paper \(s h o w n as Figure 1\e different places marked with ovals and labels for convenient to descript   Figure 2. Decision tree produced in reference [1 and differences with this paper  Label  is a careless mistake in referen EDU” should be replaced by “INCOME”. Label shows we find the noise node earlier than referen In Label , the attribute of the second layer is “EDU and the attribute of the third layer is “AGE”, but the results of this paper just are on the contrary. It is because that the gains of the attribute “EDU” and AGE” are equal in this layer, and which is used firstly to classify is also permitted After contrasting, we can get following verification conclusions: 10 nodes were created totally as shown as Figure 1. The CAR_TYPE of each node is same as the Figure 2 if we look the nodes  as a noise node. The results of this paper is completely correct, and higher efficient since we find the noise node earlier than the original ID3  4.2 Verification of the association rule  We adopt the same idea of the association rule model as referen h rou g h co n t ras t i n g t h e ou tp u t  results, we verified the validity and the accuracy of the prediction results Now we contrast the results of these two algorithms introduced in this paper. Comparing the output prediction rules, we can see, the problem that the decision tree model tries to solve is “which customers more likely to purchase which types of car”, and the problem of the association rule model tries to solve is the customer who purchases a certain car type has which common characteristics Take the first rule produced by the decision tree results: The customer whose INCOME is low and SEX is M purchase SANTANA generally. And we can find easily the 5th rule in the rules produced by the association rule model: In the customers who purchase SANTANA, if his INCOME is low, then his SEX is M generally. Though these two decision rules have different credibility because of probabilistic problem the essences of their description are consistent basically 
680 


If we converse the results of association rule slightly, we can find the similar description in the decision tree results. Although the number of decision rules produced by the association rule is more than the decision tree, it doesn’t influence the horizontal comparison. Because we have mentioned there are redundancy among the different k-item sets. If we want to raise speed and efficiency of the associate rule, we need to improve the algorithm further  5. Conclusions  This paper constructed a data warehouse with automobile marketing subject by using SQL Server And on this foundation, we adopt the decision tree model and associate rule model to analyze the history data of the automobile marketing, get the automotive customer classification rules and customers’ attributes associate rules, and realize prediction of customers purchasing behavior to raise the core competitions of automotive enterprises  References    P a n Hen g  Research i n t o ap p l i cat i o n o f d eci si o n t r ee technology in car sale Journal of Chongqing Technology and Business University\(Natural Science Edition 2006 23\(4\, pp.1-4  2 T ia nShy u g Le e  C h ihC ho u C h iu,Y uC h a o C h o u C h iJ i e  Lu, “Mining the customer credit using classification and regression tree and multivariate adaptive regression splines Computational Statistics & Data Analysis 2006,50, pp.1113 1130  3 L i f e ng J i a  Zhe W a ng N a n L u  Xi uj ua n X u  D o ng bin  Zhou, Yan Wang, “RFIMiner: A regression-based algorithm for recently frequent patterns in multiple time granularity data streams Applied Mathematics and Computation Elsevier 2007, 185, pp.769-783   L i n g P i n g  Z h o u Chu n G u an g  P aram et eri zed S e m i supervised Classification Based on Support Vector for Multirelational Data”, ICNC&FSKD 2006, Springer-Verlag LNCS 4221, pp. 66-75    X u e H u i f en g Z h an g W e i y u  Ko u Xi ao do n g   Intelligent Data Mining Technology northwest industrial university press, 2005  6  P a n H e ng  T r a f f ic  f l o w a n a l y s is ba s e d on a s s o c i a tion  rule algorithm Journal of Chengdu University of Information Technology 2004, 19\(4\ pp.1-4   
681 


Perth, Australia, 29 November-1 December 1995, IEEE vol.2, 1995, pp. 759-764 9 J  L i u, J  K w ok  A n e x te nde d g e ne tic r u le induc ti on  algorithm, In:Proceedings of the Congress on Evolutionary Computation Conference, 16-19 July 2000, La Jolla, CA USA, vol. 1, 2000,pp. 458-463 10 J.H  H o lla n d A d a p ta tio n in N a tura l a n d A r ti  cial Systems, University of Michigan Press, Ann Arbor, 1975 1 J G o m e z D Dasgu p t a E v o l vi n g f u zz y cl as si  ers for intrusion detection, in: Proceedings of IEEE Works hop on Information Assurance, United States Military Academy West Point, New York, June 2001, pp. 68–75  12 D  S ong M.I  H e y w ood, A  N. Zinc ir H e y w ood, T r a i ning  genetic programming on half a million patterns: an example from anomaly detection, IEEE Transactions on Evolutionary Computation 9\(3\ \(2005\ 225–239, doi: 10.1109/TEVC. 2004 841683 13 G  Flor e z  S  M.B r i d g e s  R  B  Va ug hn, A n im pr ov e d  algorithm for fuzzy data mining for intrusion detection, in Proceedings of North American Fuzzy Information Processing Society Conference, NAFIPS 2000, New Orleans LA, June 2002, pp. 457–462 14 Y  J i n  W  v on Se e l e n B  Se ndh of f  O n g e ne r a ting FC 3 fuzzy rule systems with data using evolution strategies, IEEE Trans. Syst. Man Cybern.—Part B: Cybernetics 29 \(6\ \(1999 829–845 15 Y  J i n Fuz z y  m ode ling of hig h d im e n s i ona l s y s t em s   complexity reduction and interpretability improvement, IEEE Trans. Fuzzy Syst. 8 \(2\ \(2000\ 212–221 1 H Rou b o s  M  S e t n es Co m p act an d t r an sp aren t f u zz y models and classi  ers through interactive complexity reduction, IEEE Trans. Fuzzy Syst. 9 \(4\ \(2001\ 516–524 1 KDD Cu p  1 9 9 9 I n t r u s i o n  d e t ect i o n d a t a set   http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html  
919 


                                                                                                                 
456 


Time Complexity and Speed We now evaluate scalability and speed with large high dimensional data sets to only compute the models as shown in Figure 7 The plotted times include the time to store models on disk but exclude the time to mine frequent itemsets We experimentally prove 1 Time complexity to compute models is linear on data set size 2 Sparse vector and matrix computations yield efÞcient algorithms whose accuracy was studied before 3 Dimensionality has minimal impact on speed assuming average transaction size T is small Transactions are clu stered with Incremental Kmeans 26 introduced on Sectio n 3 Large transaction les were created with the IBM synthetic data generator 3 ha ving defaults n 1 M T=10 I=4 Figure 7 shows time complexity to compute the clustering model The rst plot on the left shows time growth to build the clustering with a data set with one million records T10I4D1M As can be seen times grow linearly as n increases highlighting the algorithms efÞciency On the other hand notice d has marginal impact on time when it is increased 10-fold on both models due to optimized sparse matrix computations The second plot on the right in Figure 7 shows time complexity to compute clustering models increasing k on T10I4D100k Remember k is the main parameter to control support estimation accuracy In a similar manner to the previous experiments times are plotted for two high dimensionalities d  100 and d 1  000 As can be seen time complexity is linear on k  whereas time is practically independent from d  Therefore our methods are competitive both on accuracy and time performance 4.5 Summary The clustering model provides several advantages It is a descriptive model of the data set It enables support estimation and it can be processed in main memory It requires the user to specify the number of clusters as main input parameter but it does not require support thresholds More importantly clusters can help discovering long itemsets appearing at very low support levels We now discuss accuracy In general the number of clusters is the most important model characteristic to improve accuracy A higher number of clusters generally produces tighter bounds and therefore more accurate support estimations The clustering model quality has a direct relationship to support estimation error We introduced a parameter to improve accuracy wh en mining frequent itemsets from the model this parameter eliminate spurious itemsets unlikely to be frequent The clustering model is reasonably accurate on a wide spectrum of support values but accuracy decreases as support decreases We conclude with a summary on time complexity and efÞciency When the clustering model is available it is a signiÞcantly faster mechanis m than the A-priori algorithm to search for frequent itemsets Decreasing support impacts performance due to the rapid co mbinatorial growth on the number of itemsets In general the clustering model is much smaller than a large data set O  dk  O  dn  A clustering model can be computed in linear time with respect to data set size In typical transaction data sets dimensionality has marginal impact on time 5 Related Work There is a lot of research work on scalable clustering 1 30 28 a nd ef  c i e nt as s o ci at i o n m i n i n g 24  1 6  40  but little has been done nding relations hips between association rules and other data mining techniques SufÞcient statistics are essential to accelerate clustering 7 30 28  Clustering binary data is related to clustering categorical data and binary streams 26 The k modes algorithm is proposed in 19  t hi s a l gori t h m i s a v a ri ant o f K means  but using only frequency counting on 1/1 matches ROCK is an algorithm that groups points according to their common neighbors links in a hierarchical manner 14 C A C TUS is a graph-based algorithm that clusters frequent categorical values using point summaries These approaches are different from ours since they are not distance-based Also ROCK is a hierarchical algorithm One interesting aspect discussed in 14 i s t he error p ropagat i o n w hen u s i ng a distance-based algorithm to cluster binary data in a hierarchical manner Nevertheless K-means is not hierarchical Using improved computations for text clustering given the sparse nature of matrices has been used before 6 There is criticism on using distance similarity metrics for binary data 12  b ut i n our cas e w e h a v e p ro v e n K means can provide reasonable results by ltering out most itemsets which are probably infrequent Research on association rules is extensive 15 Mos t approaches concentrate on speed ing up the association generation phase 16 S ome o f t hem u s e dat a s t ruct ures t h at can help frequency counting for itemsets like the hash-tree the FP-tree 16 or heaps  18  Others res o rt to s t atis tical techniques like sampling 38 s t at i s t i cal pruni ng 24   I n  34  global association support is bounded and approximated for data streams with the support of recent and old itemsets this approach relies on discrete algorithms for efÞcient frequency computation instead of using machine learning models like our proposal Our intent is not to beat those more efÞcient algorithms but to show association rules can be mined from a clustering model instead of the transaction data set In 5 i t i s s ho wn that according t o s e v eral proposed interest metrics the most interesting rules tend to be close to a support/conÞdence border Reference 43 p ro v e s several instances of mining maximal frequent itemsets a 
616 
616 


constrained frequent itemset search are NP-hard and they are at least P-hard meaning t hey will remain intractable even if P=NP This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial In 13 t h e a ut hors d eri v e a bound on the number of candidate itemsets given the current set of frequent itemsets when using a level-wise algorithm Covers and bases 37 21 a re an alternati v e t o s ummarize association rules using a comb inatorial approach instead of a model Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset The model represents an approximate cover for all potential associations We now discuss closely related work on establishing relationships between association rules and other data mining techniques Preliminary results on using clusters to get lower and upper bounds for support is given in 27  I n g eneral there is a tradeoff between rules with high support and rules with high conÞdence 33 t h i s w o rk propos es an al gorithm that mines the best rules under a Bayesian model There has been work on clustering transactions from itemsets 41  H o w e v er  t hi s a pproach goes i n t he oppos i t e di rection it rst mines associations and from them tries to get clusters Clustering association rules rather than transactions once they are mined is analyzed in 22  T he out put is a summary of association rules The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set In 42 the idea of mining frequent itemsets with error tolerance is introduced This approach is related to ours since the error is somewhat similar to the bounds we propose Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity In 39 t he aut hors explore the idea of building approximate models for associations to see how they change over time 6 Conclusions This article proposed to use clusters on binary data sets to bound and estimate association rule support and conÞdence The sufÞcient statistics for clustering binary data are simpler than those required for numeric data sets and consist only of the sum of binary points transactions Each cluster represents a long itemset from which shorter itemsets can be easily derived The clustering model on high dimensional binary data sets is computed with efÞcient operations on sparse matrices skipping zeroes We rst presented lower and upper bounds on support whose average estimates actual support Model-based support metrics obey the well-known downward closure property Experiments measured accuracy focusing on relative error in support estimations and efÞciency with real and synthetic data sets A clustering model is accurate to estimate support when using a sufÞciently high number of clusters When the number of clusters increases accuracy increases On the other hand as the minimum support threshold decreases accuracy also decreases but at a different rate depending on the data set The error on support estimation slowly increases as itemset length increases The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels Clustering is faster than A-priori to mine frequent itemsets without considering the time to compute the model Adding the time to compute the model clustering is slower than Apriori at high support levels but faster at low support levels The clustering model can be built in linear time on data size Sparse matrix operations enable fast computation with high dimensional transaction data sets There exist important research issues We want to analytically understand the relationship between the clustering model and the error on support estimation We need to determine an optimal number of clusters given a maximum error level Correlation analysis and PCA represent a next step after the clustering model but the challenges are updating much larger matrices and dealing with numerical issues We plan to incorporate constraints based on domain knowledge into the search process Our algorithm can be optimized to discover and periodically refresh a set of association rules on streaming data sets References 1 C  A ggar w al and P  Y u F i ndi ng gener a l i zed pr oj ect ed cl usters in high dimensional spaces In ACM SIGMOD Conference  pages 70Ð81 2000 2 R  A g r a w a l  T  I mie lin sk i a n d A  S w a mi M in in g a sso c i a tion rules between sets of items in large databases In ACM SIGMOD Conference  pages 207Ð216 1993 3 R  A gr a w al and R  S r i kant  F ast a l gor i t h ms f o r m i n i n g a ssociation rules in large databases In VLDB Conference  pages 487Ð499 1994 4 A  A su n c io n a n d D Ne wman  UCI Machine Learning Repository  University of California Irvine School of Inf and Comp Sci http://www.ics.uci.edu 002 mlearn/MLRepository.html 2007 5 R  B a y a r d o a n d R  A g r a w a l  M in in g t h e mo st in te re stin g rules In ACM KDD Conference  pages 145Ð154 1999 6 R  B ekk e r m an R  E l Y a ni v  Y  W i nt er  a nd N  T i shby  O n feature distributional clustering for text categorization In ACM SIGIR  pages 146Ð153 2001 7 P  B r a dl e y  U  F ayyad and C  R ei na S cal i n g c l u st er i n g a l gorithms to large databases In ACM KDD Conference  pages 9Ð15 1998  A  B yk o w sk y a nd C Rigotti A c ondensed representation t o nd frequent patterns In ACM PODS Conference  2001 9 C  C r e i ght on and S  H anash Mi ni ng gene e xpr essi on databases for association rules Bioinformatics  19\(1\:79 86 2003 
617 
617 


 L  C r i s t o f o r a nd D  S i mo vi ci  G ener at i n g a n i nf or mat i v e cover for association rules In ICDM  pages 597Ð600 2002  W  D i ng C  E i ck J  W ang and X  Y uan A f r a me w o r k f o r regional association rule mining in spatial datasets In IEEE ICDM  2006  R  D uda and P  H ar t  Pattern ClassiÞcation and Scene Analysis  J Wiley and Sons New York 1973  F  G eer t s  B  G oet h al s and J  d en B u ssche A t i ght upper bound on the number of candidate patterns In ICDM Conference  pages 155Ð162 2001  S  G uha R  R ast ogi  a nd K  S h i m  R O C K  A r ob ust c l u stering algorithm for categorical attributes In ICDE Conference  pages 512Ð521 1999  J H a n a nd M K a mber  Data Mining Concepts and Techniques  Morgan Kaufmann San Francisco 1st edition 2001  J H a n J P e i  and Y  Y i n  M i n i n g f r e quent pat t e r n s w i t hout candidate generation In ACM SIGMOD Conference  pages 1Ð12 2000 17 T  Ha stie  R  T ib sh ira n i a n d J  F rie d ma n  The Elements of Statistical Learning  Springer New York 1st edition 2001  J H u ang S  C h en a nd H  K uo A n ef  c i e nt i n cr emental mining algorithm-QSD Intelligent Data Analysis  11\(3\:265Ð278 2007  Z  H u ang E x t e nsi ons t o t h e k m eans a l gor i t h m f or cl ust e r ing large data sets with categorical values Data Mining and Knowledge Discovery  2\(3\:283Ð304 1998  M K r yszki e w i cz Mi ni ng w i t h co v e r a nd e x t e nsi o n oper a tors In PKDD  pages 476Ð482 2000  M K r yszki e w i cz R e duci n g bor der s of kdi sj unct i o n f r e e representations of frequent patterns In ACM SAC Conference  pages 559Ð563 2004  B  L e nt  A  S w a mi  a nd J W i dom C l u st er i n g a ssoci at i o n rules In IEEE ICDE Conference  pages 220Ð231 1997 23 T  M itc h e ll Machine Learning  Mac-Graw Hill New York 1997  S  Mori shi t a and J  S ese T r a v ersi ng i t e mset s l at t i ces wi t h statistical pruning In ACM PODS Conference  2000  R  N g  L  L akshmanan J H a n and A  P ang E xpl or at or y mining and pruning optimizations of constrained association rules In ACM SIGMOD  pages 13Ð24 1998  C  O r donez C l ust e r i ng bi nar y dat a st r eams w i t h K means In ACM DMKD Workshop  pages 10Ð17 2003  C  O r donez A m odel f or associ at i o n r ul es based o n c l u st er ing In ACM SAC Conference  pages 549Ð550 2005 28 C Ord o n e z  In te g r a tin g K me a n s c lu ste r in g w ith a r e l a tio n a l DBMS using SQL IEEE Transactions on Knowledge and Data Engineering TKDE  18\(2\:188Ð201 2006  C  O r donez N  E z quer r a  a nd C  S a nt ana C onst r ai ni ng and summarizing association rules in medical data Knowledge and Information Systems KAIS  9\(3\:259Ð283 2006  C  O r donez a nd E  O m i eci nski  E f  ci ent d i s kbased K means clustering for relational databases IEEE Transactions on Knowledge and Data Engineering TKDE  16\(8\:909Ð921 2004 31 S Ro we is a n d Z  G h a h r a m a n i A u n i fy in g r e v ie w o f lin e a r Gaussian models Neural Computation  11:305Ð345 1999  A  S a v a ser e  E  O mi eci nski  a nd S  N a v a t h e A n ef  c i e nt al gorithm for mining association rules In VLDB Conference  pages 432Ð444 September 1995  T  S c hef f er  F i ndi ng associ at i o n r ul es t h at t r ade s uppor t optimally against conÞdence Intelligent Data Analysis  9\(4\:381Ð395 2005  C  S i l v est r i a nd S  O r l a ndo A ppr oxi mat e mi ni ng of f r e quent patterns on streams Intelligent Data Analysis  11\(1\:49Ð73 2007  R  S r i k ant a nd R  A g r a w a l  Mi ni ng gener a l i zed associ at i o n rules In VLDB Conference  pages 407Ð419 1995 36 R Srik a n t a n d R Ag ra w a l M i n i n g q u a n tita ti v e a sso c i a tio n rules in large relational tables In ACM SIGMOD Conference  pages 1Ð12 1996  R  T a oui l  N  Pasqui er  Y  B ast i d e and L  L akhal  Mi ni ng bases for association rules using closed sets In IEEE ICDE Conference  page 307 2000  H  T o i v onen S a mpl i n g l ar ge dat a bases f or associ at i o n r ul es In VLDB Conference  1996  A  V e l o so B  G usmao W  Mei r a M C a r v al o Par t hasar a t h i  and M Zaki EfÞciently mining approximate models of associations in evolving databases In PKDD Conference  2002  K  W a ng Y  H e  a nd J H a n P u shi n g s uppor t c onst r ai nt s into association rules mining IEEE TKDE  15\(3\:642Ð658 2003  K  W a ng C  X u  a nd B  L i u C l ust e r i ng t r ansact i ons usi n g large items In ACM CIKM Conference  pages 483Ð490 1999  C  Y a ng U  Fayyad and P  B r a dl e y  E f  ci ent d i s co v e r y of error-tolerant of frequent itemsets in high dimensions In ACM KDD Conference  pages 194Ð203 2001  G  Y a ng T h e c ompl e x i t y of mi ni ng maxi mal f r e quent i t e msets and maximal frequent patterns In ACM KDD Conference  pages 344Ð353 2004  T  Z h ang R  R a makr i s hnan and M  L i v n y  B I R C H  A n efÞcient data clustering method for very large databases In ACM SIGMOD Conference  pages 103Ð114 1996 
618 
618 


