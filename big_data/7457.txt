  1 Flow-Enablement of the NASA SensorWeb using RESTful and Secure Pat G. Cappelaere Vightel Corporation Ellicott City, MD 21043 443.283.0369 pat@cappelaere.com  Stuart W. Frye SGT Inc Greenbelt, MD 20771 stuart.frye@nasa.gov  Daniel Mandl NASA Goddard Space Flight Center Greenbelt, MD 20771 301 Daniel.J.Mandl@nasa.gov    Abstract 227Applied Remote Sensing to Disaster Management requires just-in-time delivery of custom data products to 
unsophisticated end-users such as fire fighters and first responders.  These requests are wide-ranging, from wild fire hot spots detection smoke, fire suppressi on and rehabilitation flood coverage to erupting volcanoes, and require coordination of many assets such as satellites, UAV\222s and ground sensors.  They also require predictive models to complete the SensorWeb Using Open Geospatial Consortium \(OGC\andards distributed web services from many organizations have been geo-enabled \(or sensor-web enabled\o process the data and distribute it to end-users using Web 2.0 technologies such as Atom feeds and KML The next challenge is to flow-enable these services to facilitate 
automated orchestration for on-demand requests coming from various communities in times of need.  With our desire to provide the ability to create quick mash-ups for our end-users using a simple web browser, we have implemented a RESTful 1 architecture and applied it to workflow management with the help of the Workflow Management Coalition WfMC t interoperability across our SensorWeb Community.  These workflows must operate on behalf of a wide range of user s and access services that may or may not be restricted.  At a minimum, data consumers and 
providers need to communicate over http using some level of authentication that can be eas ily implemented in a RESTful manner This paper also presents our Decision Support System used to manage our various Communities of Interests \(COI\d give 1  1 REST \226 Representational State Transfer them transparent access to the SensorWeb assets and relevant custom data products when the needs arise.  An Analytical Hierarchy Process \(AHP ed to automate the daily selection of requests to be scheduled for imaging and 
processing based on themes of interest 2  3  T ABLE OF C ONTENTS  1  I NTRODUCTION  1  2  F LOW E NABLEMENT AND ROA FOR S ENSOR W EB  3  3 
 RESTFUL  WORKFLOWS  3  4  SECURE  WORKFLOWS  4  5  INTEGRATING  MODEL  FORECASTS  AND  USER  REQUESTS  USING  SENSORWEB 4  6 
 CONCLUSION 6  7  REFERENCES  7  8  BIOGRAPHY  7  1   I NTRODUCTION  Many OGC standards have given a wider community of users access to remote sensing assets and data.  The Sensor Planning Service \(SPS Observation Service \(SOS Web Processing Service \(WPS\any others can now be orchestrated with workflows to provide just-in-time 
relevant data to end-users such as first responders or personnel from disaster management agencies.  KML files distributed via GeoRSS feeds allow those users to quickly visualize multiple layers of data and information on the desktop using Google Earth These users are not necessarily sufficiently trained to task a particular instrument onboard a satellite such as the 1  2 978-1-4244-2622-5/09 25.00 \2512009 IEEE 3 IEEEAC paper#1335, Versi on 4, Updated Dec 12, 2008 


  2 hyperspectral or multispectral imagers onboard EO-1.  A higher-level interface is necessa ry to allow them to post a request for a specific area-of-int erest and a theme. Disaster management themes include: flooding, fires, oil spills drought, earthquakes, tsunamis, and hurricanes following the NOAAWatch themes 4 Users are now disconnected from the sensors, instruments and data processing requirements.  Under the hood, an infrastructure powered by dynamic and/or concrete workflows manages the complexity transparently to the user For a specific example, we can go back to the recent flood in Kenya on Nov 13 th 2008.  The Regional Centre for Mapping of Resources for Development \(RCMRD\n Kenya posted a request for imagery in the flood area. The system picked up the request, tasked the EO-1 satellite acquired the data and processed it using various web processing services such as the Jet Propulsion Lab Web Processing Service \(WPS\for Hyperion data and the Vightel processing service for ALI data.  Some of the Level 2 products were generated a nd made available as atom feeds.  These feeds can be pub lished to an aggregator such as Feedburner \(www.feedburner.com\ and viewed with a standard web browser like FireFox with the Sage Feed reader Add-on or Google Reader.  An example feeds can be found here 5 This relieved the end-users in Kenya to run the data through ENVI and painfully generate the products themselves.  Feed entries contain links to the data and KMZ files that allow for direct ingest and visualization using Google Earth 2  4 http://www.noaawatch gov/themes/severe.php 5 http://feeds.feedbur ner.com EO1A1700602008318110KF   Figure 1 \226 Product Feed in Reader     


  3 2  F LOW E NABLEMENT AND ROA FOR S ENSOR W EB  Targeting a wider community, dubbed as Mass Market requires a gentler architecture th at can scale in a global and federated environment such as GEOSS.  A service-oriented architecture \(SOA\based on SOAP/WSDL is out of reach for most of our users and disadvantaged organizations in third world countries.  However it is critical to reach these users.  Quick mashups within a standard browser or Google Earth application on the desktop could make a difference in the field during emergencies OGC services originated circa 2000 in a REST world heavily influenced by RPC software technologies from ONC RPC \(1980\o DCE and CORBA \(1990-2000 Publishing service operations or methods was the de-facto norm but proved to be difficult to scale as different types of web services increased in numbers.  Over the years, a natural web service evolution caused a multiplication of services, which increased the number of operations to be mastered by the users and successfully implemented by developers.  The size of the specifications has become an issue and a formidable obstacle for new entrants.  This is unfortunately, not unlike the evolution of the Human genome 6 over the years In 2007, Leonard Richardson and Sam Ruby finally codified 8 RESTful Web Services.  Using a reduced operation set such as GET, POST, PUT and DELETE mapped to the traditional Cr eate/Read/Update/Delete or CRUD transactions on resources, ROA \(or Resource Oriented Architecture born with a solid framework that could scale.  There ar e additional services that complement REST 7 such as AtomPub, OpenSearch, Google GData, GeoRSS and KML. A new and consistent way of accessing web resources by casua l users finally emerged 3  RESTFUL  WORKFLOWS The Workflow Management Coalition \(WfMC\created in 1993 to focus on Business Process Management Orchestration of web services using workflows was enabled with the BPEL standard in 2003 and a generic API developed by the Workflow Management Coalition WfMC\fXML, which was SOAP-based In 2006, we developed WfXML-R, a RESTful version of that specification during the Open Geospatial Consortium interoperability demonstration #5.  Following a Resource Oriented Architecture Approach and the AtomPub standard resource collections are exposed via a service document that can be easily mined by the users 3  6 http://www.nature.com/embor journal/v7/n8/fu ll/7400756.html 7 Roy Fielding, PhD Dissertation \(2000 http://www.ics.uci.edu/~fieldi ng/pubs/dissertation/top.htm Far from being optimum, Workflow Chaining Service WfCS flow metadata information workflow definitions, participants, workitems, processes which are running instantiations of specific workflow definitions and so on.  From a user perspective, these workflow resources appear strangely similar to the RPC operations that have been exposed since the 1980\222s.  Users are not as interested in these high level operations or workflows.  They are more interested in the products that these workflows will create on th eir behalf.  These products that do not exist are virtual but, from the user\222s perspective as real as any other products that have already been generated.  Virtual products are the actual resources that need to be published to the external world.  Extended GeoRSS feeds are used for publication.  Users can instantiate them on-demand by following the links that will eventually trigger workflows with the parameters necessary to generate the requested products As workflows can orchestrate services on many sites on behalf of the users, secure access to the services and delegation of user authorization is becoming an important issue to address properly.  This capability has to be fairly easy to implement to be adopted by the various Communities of Interest Critical to a Workflow Chaining Service is the capability to include one or more users as participants to selected activities of the flow.  Workitems or user tasks can be created and queued to specific stores.  Users have to retrieve the workitems from their respective stores, complete the allocated task and return the workitem to the workflow to continue the process.  When workitems accumulate in a store, an RSS feed can be generated to warn the user monitoring that feed that work items are ready to process The user can follow one of the links within a specific entry to be redirected to the right store and start the processing of the workitem The main advantage of a WfCS is the inherent capability to support transactions: adding or removing workflows or definitions to the system.   A workflow can be viewed as a Web Processing Service \(WPS\ is a black box where inputs and outputs are specified.  A WPS can be viewed as a WfCS with one workflow.  A WPS-T or WPS with transactions is really a full-fledged WfCS.  Is a new specification needed for the WPS to support transactions A WfCS is a better way to design a WPS.  Beyond the inputs and outputs, the flow diagram acts as documentation for the process to be instantiated.  Standardization of the diagram notation is essential to allow analysts a better comprehension of the process.  The business Process Modeling Notation \(BPMN\is used to support technical and business users with an intuitive notation that can represent complex process semantics independently of the underlying execution language 


  4 This notation is similar to the musical notation that allows us to play musical pieces creat ed several hundred years ago Any musician can understand what the notation means This standard notation does not imply that any instrument can play the music on the sheet Your instrument may be range limited   Figure 2 \226 BPMN diagram example BPMN has the same goal and limitation.  Consideration to the targeted execution engine is key.  This is not quite a universal language.  The complementary standard to BPMN is XPDL.  XPDL is used for serialization of the diagram XDPL can be used as an interchange document format between the modeler and the execution engine.  Some engines can execute XPDL natively; others will translate XPDL to a native format.  This format allows for the separation of the modeler and the engine itself.  Users can pick what modeler works best for their own needs   4  SECURE  WORKFLOWS The last breakthrough came with new security protocol standards such as OpenID a nd OAuth for RESTful services The major issue that hinders acceptance of a SOAP/WSDL architecture is the difficulty to successfully implement security.  The WS-* stack layer requires a higher level of expertise than our neo-geographers have Working in a federated environment of trusted organizations, user authentication using OpenID is becoming a de-facto standard in the Web 2.0 world.  Single sign-on is a must for the SensorWeb to succeed and scale It would be unconceivable to require users to have accounts on every machine accessed during the various activities required to task sensors, process and distribute the data leveraging web services available across the country OpenID allows for authentica tion of users and services leveraging the support of a trusted network of Identity Providers.  The delegation of user authority to workflows was the final milestone reached last year during the OWS-5 interoperability demonstration.  Workflows that have been instantiated on behalf of a sp ecific user may need to access services that may require rest ricted access privilege.  User information and granted role may need to be accessible by one or more services in a flow.  OAuth has been devised as a simple secure API authoriza tion that allows services to exchange information in a trusted manner on behalf of a user.  Since users may not be available at run-time, a modification has been made to the protocol to allow for preapproved transactions.  Users can, off-line, grant or revoke specific services access to resources they may have privileged access to.  User and security information can be safely exchanged within the http header.  Recent demonstrations have shown that developers can successfully implement that protocol within a day or so.  This is a significant improvement over other more exotic methodologies, such as WS Using the OpenID of the requestor, the web service can easily request and obtain user profile information if granted by the user.  Profile information contains email organization and a fe w other pieces of information related to user identity.  In our case a critical piece of information is missing: the user role granted by the organization to the user.  Once the user role is known, access right can be inferred assuming that the role has not been tampered by the user and has been granted by a trusted organization.  The attribute exchange protocol can be used to exchange extended attributes outside the current scope of the protocol At the moment, custom OpenID providers must be used to exchange role information using a previously agreed upon type identifier A secured, flow-enabled SensorWeb is now in place to allow end users such as first responders to request custom data products just-in-time to meet their special needs in an area of interest without requiring a deep knowledge of remote imaging sensors, data acquisition and complex data processing provided by organizations spread across the entire nation 5  INTEGRATING  MODEL  FORECASTS  AND  USER  REQUESTS  USING  SENSORWEB In many cases, models can predict potential floods or other disasters.  This can give us enough notice to task imaging satellites and capture the event as it happens.  Many 


  5 organizations such as CATHALAC 8 RCMRD 9 and the International Federation of the Red Cross \(IFRC\ can now generate requests for imagery using a simple API to our Decision Support System or Campaign Manager.  The next challenge for us is to prioritize the incoming requests against our current mission science imagery requirements When an emergency task is gran ted, a science task has to be bumped.  This may cause some significant problems for a science investigator.  However, it would also address a major societal benefit that n eeds to be fairly weighted against the loss of a critical science task As a proof-of-concept, the EO-1 Mission Science Office agreed to give us up to two imaging slots per day \(out of 25 to support international emergencies or disaster relief efforts and to demonstrate the concept Ideally, the goal would be to achieve complete operations autonomy.  Long-term mission science goals are decided months in advance and inserted in the schedule once a week Emergency requests and other short-term missions are by nature unplanned and can potentially be requested up to 6 hours in advance.  The syst em needs to prioritize the incoming requests and generate daily replacement records for the scheduler to manage and upload to the spacecraft This is a multi-stage decision making process that must take into account many qualitative and quantitative criteria which can be weighted offlin e and other criteria such as predicted cloud forecast and number of images already acquired for that campaign that contribute to the decision score at the last minute.  Th e cloud forecast is obtained from NOAA\222s National Climatic Data Center \(NCEP\Global Forecasting Service \(GFS\ilar and complementary capability from the Air Force is being integrated as part of another Earth Science Technology Office grant effort A user can log on to the Campaign Manager using an OpenID.  The system can quick ly authenticate that user access the user profile \(if permission is granted\from the inferred Identity Provider to ge t access to the user email parent organization, grade/rank and role granted by the organization to that user \(future capability of attribute exchange called AX\e could be inferred from the user grade/rank if not specified in the profile The user creates a campaign for a particular theme and associates one or more imaging requests with that campaign at potentially different locations.  The system automatically generates the imaging feasibilities for the next seven days by accessing the asset Sensor Planning Service \(SPS GetFeasibilities operation 5  8 http://www.cathalac.org 9 Regional Center For Mapping of Resources for Development http://www.rcmrd.org At noon every day, the system prioritizes the requests for the next day and generates an email to the Review Board members.  The first two requests will be submitted unless a member vetoes a request for a specific reason that has to be documented online and revi ew-able for accountability Prioritizing the requests is derived from the Analytic Hierarchy Process, developed by Thomas L. Saaty in the 1970\222s to help people deal with complex decisions.  The problem is decomposed into a hierarchy of criteria and subcriteria that can be weighted with the objective of ranking alternatives submitted by the various communities of interest Using this method, request a lternatives ought to be ranked against applicable sub-criteria using a weighting method such as pair-wise comparison, direct weighting or value function. This was deemed to be too much work for the review board to be done on a daily basis Campaigns, however, can be scored at creation time using the logical framework of AHP.   The score is re-computed daily based on the predicted cloud coverage or other additional criteria \(e.g., how many images have we already taken of that area Remote sensing imagery is driven by NASA Strategic goal 3A: \223Advance scientific understanding and meets societal needs\224.  The long-term science goal ranking is not currently managed by our decision making process, just short-term goals for societal benefits.  Four criteria are currently defined in the decision hierarch y tree.  The first one is the submitter\222s organization.  Is it NASA or a NASA affiliated organization, DOD, a non-governmental organization or others?  The user grade or rank is also taken into account as a criterion.  A higher-ranking s ubmitter would certainly get a higher score than a lower ranking submitter would Societal benefits have been broken down in different themes: flood, fire, drought, volcanic, hurricane, tsunami oil spill, algal bloom and a recently added national theme for DOD customers  


  6 A final criterion has been added to rank national disasters based on potential scope or disaster category levels.  This criterion takes into account the potential population risk economic and infrastructure risk, environmental risk and finally a national risk for DOD interests A direct weighting tool is provided on the GeoBPMS portal to allow the allocation of weights to criteria and sub-criteria The review board is tasked to review this monthly.  A sensitivity analysis tool is also provided to assess the impact of a weight change to a potential decision reversal 6  CONCLUSION A RESTful Resource Oriented Architecture approach is quickly becoming a best practice to address the mass market needs.  User Authorization a nd web services authentication are now possible with open standards such as OpenID and OAuth.  Delegation of user authority to workflows can be successfully implemented in a short period of time by developers with very little experience in the security domain.  This is critical for integration and the scaling of SensorWeb to GEOSS Secure Workflows can now be easily used to orchestrate web services on behalf of the users and provide new products on demand.  Virtual products are the resources to be published for users to find and instantiate when necessary AHP is proving to be extremely helpful in automating the decision-making process for imagery campaigns.  This has been an area of intense conflicts between groups for many years.  A clear model allows users to understand how decisions are made or rejected.  This opens the door for automated negotiation between organizations once the ranking has been calculated As mentioned earlier, science goals are not ranked using this decision framework.  This has been requested by many organizations including JPL to allow for a fairer imaging allotment of science campaigns.  This would also allow a potentially increased number of societal benefit scenes if their rankings were high enough on a given day   Figure 3 \226 Analytic Hierarchy For EO-1 Campaign Manager  


  7 7  REFERENCES    223An Updat e d St at us of t h e Experi m e nt s wi t h  Sensor Webs and OGC Service Oriented Architectures to Enable Global Earth Observing System of Systems\224, Mandl D Frye S., Chien S., Sohlberg R., Cappelaere P., Derezinski L., Ungar S., Ames T., presented at Ground System Architecture Workshop 2007, Manhattan Beach CA, March 2007  eh, Nadine; Bam b acus, Myra & al. \223Leveraging Open Standard Interfaces in Providing Effici ent Discovery Retrieval and Integration of NASA-Sponsored Observations and Predictions, 2006 AGU Spri ng Assembly proceedings Baltimore, May 2006  eh, Nadine; Bam b acus Myra & al. \223NASA\222s Earth Science Gateway: A Platform fo r Interoperable Services in Support of the GEOSS Vision\224, 2006 IGARSS conference proceedings \(Denver, Aug. 2006   223GEOB L IKI  Geo-Dat a   B l og + W i ki An OGC Sensor Web Enabled Data Node\227A Data Publisher for Community Collaboration around Geo-Spatial Data\224 Cappelaere P., Frye S., Mandl D., Derezinski L., presented at the Free and Open Source Software and Geoinformatics FOSS4G2006\n Lausanne, Switzerland September 2006  223Open ID 2.0  How does i t work\224 URL:http://openid.net/about.bml[Cited 12 Ma   Hart Di ck \223Et ech 2006 -- W ho Is t h e Di ck on M y  Site?\224 URL http://identity20.com/media ETECH_2006/[Cited 12 March   7]  OpenID specifications URL http://openid.net/specs.bml[Cited 12 Ma   Leonard R i chardson, Sam R uby R E STful W e b Services O\222Reilly Media, Inc, 2007  8  BIOGRAPHY Pat Cappelaere  is a Software Architect working for NASA and other DOD agencies developing SensorWeb us ing many of the Open Geospatial Consortium Standards and Web 2.0 technologies.  Pat\222s current focus is on Resource Oriented Architecture for lower cost Mass Market applications.  As an OGC Member, Pat has been involved in many OGC st andards and developed a generic open source Sensor Web E nabled data node capability called GeoBliki with eo1.geobliki.com being one of the first instantiations for NASA.  This SensorWeb Architecture has received the 2008 R&D 100 award from the prestigious R&D magazine \(The \223Oscars of Invention\224\.  After delivering onboard satellite autom ation with Clementine EO-1 and FUSE, end-to-end secure automation with workflows and web 2.0 technologies is the next challenge Pat is also member of the OpenID foundation, Workflow Management Coalition and one of the developers of the WfXML-R standard used for web service orchestrations Pat has an Master\222s degree from HEI, France and an Executive MBA from Loyola College in Maryland  STU FRYE is a Systems Engineer working for NASA on satellite development, launch and on-orbit operations.  On the Earth Observing One \(EO-1\is responsible for implementing service oriented architecture approaches for autonomous satellite tasking a nd on-board processing using Sensor Web approaches.  Stu worked to convert the EO-1 mission from a NASA technology pathfinder spacecraft to an international hyperspectral remote sensing research testbed for developing Sensor Webs and autonomous systems in concert with unmanned aerial vehicles, robotic surface vessels, and in-situ sensors.  He was co-winner of the 2005 NASA Software of the Year award for his role in the Autonomous Sciencecraft Experiment on-board the EO1 satellite and won the R&D 100 Award in 2008 for the Sensor Web 2.0.  Stu has a Bachelor\222s in Mathematics from the University of California and an M.S. in Operations Research from the George Washington University  DAN MANDL  is the EO-1 Mission Manager and the Principle Investigator for the NASA Earth Science Technology Office research award on SensorWeb 2.0 which is a multi-NASA center effort.  The SensorWeb 2.0 team consists of the three authors of this paper, other team members who reside at NASA GSFC, NASA JPL, NASA Ames and the University of Maryland.  Furthermore, other collaborators on this effort span many other organizations internationally, such as Committee on  Earth Observing Satellites \(CEOS\, Group on Ea rth Observations \(GEO and the United Nations  Space-based Information Disaster Emergency Response\(UN-SPIDER\ group  Previously, Dan managed the development of ground components on ten other missions.  Dan has a MS degree in Engineering Management from the George Washington University  


one for the valve It one particular pump Mean and Standard Deviation of Pump Parameter B 100 F 1.05 0.9 Ba was found that each of these features uniquely correlated to was derived from the data-driven routines which indicated pump cavitation faults Table 1 summarizes the results of the PHM routine development 8 m 1.05 a a-1 0 E X 1 0 F-Cau im'e Feahtw I F eauwe 2 130 L 0.96 Baseline Gear Fault 0.98 1 Pump Parameter B 1.02 1.04 Figure 12 Gear fault results left and PDF of Pump Parameter B right The PDF of this second pump parameter at 1000 F is shown in Figure 12 right plot As the figure shows there is good separation between the probability density functions for the baseline and faulted cases Data-driven PHMResults As mentioned in an earlier section data-driven techniques were also developed to handle fault cases which are difficult to capture through the model-based approach In the current work the developed data-driven routines were used to detect and diagnose pump cavitation Cavitation is a classic example of a complex fault phenomenon with multiple causes and which further is too dynamic to capture using low-order model parameters Since cavitation manifests itself through unsteady pump performance it would be expected that variations in the delivered pressure would be observed The left half of Figure 13 presents pressure transducer data for 30 seconds of healthy baseline testing while the right half of the figure presents pressure data for faulted cavitation FSTB testing The mean value has been removed from the plots to eliminate the static pressure value It is clear from the time domain data that fluctuations in the pressure become evident during pump cavitation Such clear variations in the time domain are well suited to statistical data features A feature was developed to indicate an increase in low bandwidth energy since the fluctuations due to cavitation were noted to cycle around a frequency of one Hertz There were also several possibilities for features to be derived from the pump pressure signal and these features were evaluated based on their fault response The fault response for four of these features is illustrated in the bar plot in Figure 14 As seen three of the four features in the plot respond very well to the development of pressure value spikes in the time domain with the last feature responding the best d 3 T-4 I4 ai ii 1 L iL|jIi LAFl I C 0 Tv Figure 13 Baseline left and cavitation right mean removed pressure data 4 151 1 05 _Fe 4 Feahum 3 Feahume 4 Figure 14 Cavitation feature response Feature Results Summary The model-based routines yielded three features two for the pump and X 0.95 IL seline Leakage Gear Fault GF+Leakage Valve Fault VF+Leakage Case Gear Fault Gear Fault+Leakage Valve Fault l l lValve Fault+Leakage 5 10 15 0U 25 v 0 Data Serial Number 120 C 260 100 C E 80 0 a 60 0  40 Baseline Leakage or valve fault In addition a feature 0 20 


Table 1 Summary of PHM results False Alarm Missed Detection Parameter System Fault Fault Severity Rate  Rate  Pump Parameter A Pump Leakage Moderate 5 Negligible Pump Parameter B Pump Gear Fault Moderate 5 Negligible Valve Parameter A Valve Blockage Incipient 5 6.2 Data-driven Feature 4 Pump Cavitation Moderate to severe 5 Negligible Fault Classification and Failure Prognostics The authors then implemented the developed Fault Pattern Classifiers to separate the various classes of faults on the Fluid System Test Bench FSTB These classifiers operate on a set of features derived from real-time data from the FSTB This feature set is plotted in N-dimensional feature space and then compared to known fault regions The Euclidean distance between the set of current features and each fault region is then computed and the fault region with the minimum Euclidean distance from the current feature set indicates the type and severity of the faults on the setup As an example a feature space plot is shown in Figure 15 The figure shows the basic faults along the axes of the plot Since each feature is uniquely linked to a particular fault these fault regions fall on the primary axes of the plot For example the developed leakage fault parameter is a good indicator of leakage faults and none of the other faults simulated on the setup cause changes in this parameter Similarly the valve fault parameter indicates valve malfunction Thus combinations of these faults cause the Feal xlO 6  It I s ill I  t _M lid I|rL 0.04 Zsjp 0.06 Gi t z6t  4u 3 IO III 2 0Mil   Milo ture corresponding features to fall on the primary planes of the plot as indicated by the fault regions on the X-Y Y-Z and X-Z planes Figure 15 also shows a set of features derived from a simulated fault run at 100\260 F 10 psi and 6 GPM of flow represented with a teal circle wherein the faults simulated consisted of flow loss and valve malfunction As seen the Euclidean distance between this set of features and the various fault regions is the least for the fault region corresponding to a combination of a mild valve fault and mild flow loss due to leakage Thus the classifier successfully indicates the presence of both valve and leakage faults on the system The authors have also applied the developed Evolutionary Prognostics to model fault-to-failure progression and determine the time remaining before the current health state progresses to functional failure As indicated in an earlier section a Double Exponential Smoothing DXS algorithm was implemented to track and project the features across feature space Space Ces Figure 15 Feature Space Plot and Uncertainty Estimation for Fault Classification 9 


C  Setpoints Severity 1 00 F 10 Faults Valve Fault with Leakage QM.111614r Faults Leakage A _ S:Alarat pexom uFs  bgs e veri Ty I c x;mpov inw Faults Gear Fault with Leakage Aftn ir-in lr Gear Cavitation Valve Fault Figure 17 PHM validation 10    F 1 o  m fifi fifi i,,ii    l...l.....l i X jjj  MIllWllll!!li!"l!!ii-X11l  1  RMt 1       i iu!H   l ar lll Z      z xs0    rl    0 0 a x 1C o 0.75.r  0.5 0 en  psIl 6 GPM Confidence the setup and the results of the PHM routines in the form of the severities of the various faults were compared to the actual seeded faults Figure 17 displays some of these results at various temperature pressure and flow setpoints As the figure shows the seeded faults were successfully detected by the PHM algorithms As seen from the figure the severity of the leakage fault reduces with increasing flow This is because the valve was opened by a fixed amount to seed the leakage fault thus causing roughly the same amount of leakage flow each time Thus the leakage flow as a fraction of the flow through the test section is lower as the flow setpoint is increased Seeded Fault Progression Figure 16 Evolutionary Prognostics As Figure 16 shows the DXS algorithm begins tracking the path of the features through feature space as soon as observations of the feature values become available At any point of time the algorithm may be used to predict the subsequent path of the features This is indicated in Figure 16 by the blue cone marked DXS Forecast This cone may approach the functional failure regions for one or more faults on the system The time interval over which the current feature set evolves as predicted by the DXS algorithm into a feature set indicative of failure then indicates the time to failure as shown in Figure 16 PHM Validation The PHM routines were validated with additional experimental data from the FSTB Faults were seeded into Seeded Faults None 120 F 10 psi 6 GPM _ Confidence 120 F 40 0.75 X 0.5 l X0.25 Leakage Gear Cavitation Valve Leakage Fault Seeded 0.250L h  _ be veri Ty l  I U Leakage Gear Cavitation Valve Leakage Gear Cavitation Valve Fault Fault Seeded  l 111 llM 1 i ii iiI II1  I  o a 100poFnts2 SeG _ Cnverny 0 1 00 F 25 psi 12 GPM M Confidence  0.75075 2602 m 2m T U 


Real-time Implementation and Demonstration The authors have developed a software Graphical User Interface GUI to enable real-time implementation of the PHM analysis techniques described in this paper A screenshot of this GUI is shown in Figure 18 Figure 18 Screenshot of real-time implementation interface The interface allows the user to control the flow temperature and pressure setpoints to the setup The user may choose to view the feedback signals from the setup or as shown in Figure 18 to observe the results of the PHM analysis These PHM results are computed on-the-fly using real-time sensor data from the setup The interface allows the seeding of software faults such as gear and valve faults into the setup The PHM results shown in the figure include health indices and mean times to failure and confidences thereof for the pump and valve as well as the actual and model-predicted flow pressure and RPM signals The parameters obtained from the model-based analysis are also displayed along with the baseline values of these parameters In addition the interface has indicators to alert the user to certain critical faults in the setup such as gear damage and cavitation which might require immediate shutdown and maintenance on the setup 5 CONCLUSIONS Gas turbine engine accessory components such as pumps valves and actuators create a majority of the maintenance issues in aircraft gas turbine systems A combination of model-based and data-driven approaches was implemented to address the problem of predicting and forecasting the health of these accessory components Experimental data were collected on a test setup representative of aircraft fuel and lubrication systems and these data were used to train a developed model of the system as well as data-driven routines The model-based routines yielded features that uniquely correlated to pump and valve faults The datadriven routines yielded a feature which picked out pump cavitation There was a one-to-one correspondence between the developed features and the faults indicated by these features The routines were validated with additional experimental data from the FSTB setup The authors have also developed evolutionary prognostics routines based on the DXS algorithm for the purpose of trending and tracking the progress of fault features in feature space In addition a demonstration interface was developed to enable real-time implementation of the developed techniques These techniques constitute a significant initial step towards addressing the issue of on-board diagnostics and prognostics for gas turbine accessory components 6 FUTURE WORK Work will continue to streamline the developed PHM routines and to develop data fusion routines to merge the results of model-based and data-driven algorithms The applicability of the developed PHM algorithms may be extended to a variety of gas turbine engine fluid systems such as engine compressor and nozzle systems starting systems and Auxiliary Power Units APUs and fuel and lube systems Engine fluid system reliability data may be included to improve the fidelity of the PHM algorithms in predicting impending failures and Remaining Useful Life RUL given current health status and envisioned use In addition dedicated PHM system prototypes may be developed for fuel and lubrication system health assessment and these prototypes may be tested on OEM test cells 7 ACKNOWLEDGMENTS This work has significantly benefited from the invaluable support and technical input from the Wright Patterson Air Force Base The authors would also like to acknowledge financial support for this work provided through the Air Force Small Business Innovative Research SBIR program In addition we would like to recognize the contributions of our colleague Matthew Smith 8 REFERENCES 1 DUSD\(LMR CBM+Memorandum November 2002 Available at Defense Acquisition Guidebook 2 Department of Defense Web site DoD Instruction 5.2 3.,9,2,,asp3.9.2 3 Pratt and Whitney Web site ht pr 4 J Rhoades JSF System Development and Demonstration CodeOne Magazine 17 2002 Available on-line at 11 


5 U.S Air Force Web site 6 Jelali M and A Kroll Hydraulic Servo-systems Modeling Identification and Control London Springer-Verlag 2003 7 F.M White Fluid Mechanics 4th ed New York McGraw Hill 2000 8 Roemer M Ghiocel D A Probabilistic Approach to the Diagnosis of Gas Turbine Engine Faults 9 BIOGRAPHY Carl S Byington is a Professional Engineer and the Director of Systems Engineering at Impact Technologies and possesses over 17 years in the design analysis and testing experience with mechanical thermal and fluid power systems He currently performs as the Principal Investigator on the development of innovative prognostics and health management technologies for military and commercial customers and he has successfully lead numerous programs for the Army Navy Air Force DARPA and military OEMs Prior to joining Impact Carl worked at NASA Langley Research Center performing air breathing propulsion research and more recently the Penn State Applied Research Laboratory PSU ARL as the Department Head in Condition-Based Maintenance Carl is a lead and co-author on 2 patents related to PHM technology and he has published over 65 papers book chapters magazine and journal articles related to signal processing data fusion statistical analysis model-based prognostics andpredictive health management Matthew J Watson is a Manager of Dynamic Systems at Impact Technologies with 8 years experience in the design development and testing of diagnostic and prognostic systems He has participated in the design of advanced feature development fault class ifcation and dynamic systems modeling techniques for a variety of applications including gas turbine flight control power transmission drive train electrochemical fluid and hydraulic systems Prior to joining Impact Matt worked in the Condition-Based Maintenance department of PSU-ARL where he focused on model-based PHM development of electrochemical andfuel systems He has co-authored 23 papers related to advanced sensing techniques signal processing diagnostics and control model-based prognostics data fusion and machinery health management and is co-author on 2 patents Dr Sudarshan P Bharadwaj received his undergraduate degree in Mechanical Engineering from the Indian Institute of Technology Madras India as well as Masters and Ph.D degrees in M.E from the Pennsylvania State University He has published several papers in his field He has been involved with a number of projects at Im act related to the health management of gas turbine engines aircraft and ship systems and has considerable experience in developing diagnostics and prognostics routines for these systems 12 


17 3GPP TS 25.212 Multiplexing and channel coding FDD 18 IEEE Standard for Local and metropolitan area networks Part 16 Air Interface for Fixed and Mobile Broadband Wireless Access Systems Amendment 2 Physical and Medium Access Control Layers for Combined Fixed and Mobile Operation in Licensed Bands and Corrigendum 1 2006 pp 0_1 822 19 3GPP TS 25.211 Physical channels and mapping of transport channels onto physical channels FDD 20 Data sheet from Lyrtech Inc http available at htp c hwwkneopusff _.l/p.s/lrtehs _sr d21]D ateforomTdf 21 Data sheet from Texas Instruments http available at 22 Data sheet from Xilinx http available at httll/www.xilinx.com 23 L Hanzo W Webb and T Keller Singleand Multicarrier Quadrature Amplitude Modulation  Wiley New York 2000 titled Wireless and Satellite Communications  The research interests of Dr Sacchi are mainly focused on wideband mobile and satellite transmission systems based on space time andfrequency diversity multi-user receivers based on non conventional techniques neural networks genetic algorithms higher-order statistics-based receivers etc cross-layer PHY-MAC design and high-frequency broadband satellite communications He is currently local coordinator for University of Trento of research projects dealing with reconfigurable communication platforms based on MIMO techniques and space-time signal processing ICONA project funded by MIUR and with exploitation of W-band for broadband satellite communications WA VE programs funded by ASI Claudio Sacchi is author and co-author of more than 50 papers published in international journals and conferences and reviewer for international journals and magazines IEEE Transactions on Communications IEEE Transactions on Wireless Communications IEEE Communications Letters IEEE Transactions on Aerospace and Electronic Systems Electronics Letters Wireless Networks IEEE Communications Magazine etc Dr Sacchi is member of the Organizing Committees and Technical Program Committees of international conferences like ICIP ICC GLOBECOM ACM-MOBIMEDIA etc Claudio Sacchi is member of IEEE M'01 SM'07 BIOGRAPHIES Olga Zlydareva is a PhD student of the University of Trento Italy She obtained her Master degree in Design Electronics Systems with specialization in High Radio Frequency Devices from MATI Moscow State Aviation Technological University named after KE Tsiolkovsky Moscow Russia Her research interests have oriented on the Software Defined Radio Technology Wireless Technologies Cellular Technologies Tunable devices Multi-standard systems Multi-protocol systems Physical layer of mobile devices Reconfigurability and Reprogramming of mobile devices The recent research focuses on the development of the baseband level of multistandard mobile devices based on SDR technology Claudio Sacchi was born in Genoa Italy in 1965 He obtained the Laurea degree in Electronic Engineering and the Ph.D in Space Science and Engineering at the University of Genoa Italy Since August 2002 Dr Sacchi has been holding aposition as assistant professor at the Faculty of Engineering of the University of Trento Italy In 2004 he was appointed by the Department of Information and Communication Technology of the University of Trento as leader of the Research Program 13 


  14  Figure 5:  Site B1 Terrain horizon ma sk with 1 degree azimuth spacing  Figure 6:  Site B1 Terrain horizon mask with 1 de gree azimuth spacing, in e quatorial coordinates 


  15  Figure 7: Lunar South Pole Solar Illumination Yearly Average  Figure 8:  Lunar South Pole DTE Visibility Yearly Average 


  16  Figure 9: Lunar North Pole Sola r Illumination Yearly Average  Figure 10:  Lunar North Pole D TE Visibility Yearly Average 


  17  Figure 11: Site A1 Elevation Topography  Figure 12: Site A1 Yearly Average Solar Illumination and DTE visibility, Medium Resolution 


  18   Figure 13:  Site LB Te rrain Horizon Mask  Figure 14:  Theory and Computed values of Average Yearly Solar Illumination 


  19  Figure 15:  Theory and Computed values of Average Yearly DTE Communication  Figure 16:  Heliostat Mirror Design to Eliminate Cable Wrap 


  20 Angeles, where he also received a B.S. in Applied Mathematics  Eric Fetzer is a Senior Member of the Technical Staff at the Jet Propulsion Laboratory, Pasadena, California specializing in satellite observations of the atmosphere.  His scientific interests include planetary boundary layer processes, tropical phenomena, upper tropospheric variability, and climatologies of temperature, water vapor and clouds.  His technical interests include analysis of large data sets, and of multi-sensor observations. He has over 20 peer-reviewed publications and given numerous scientific presentations, public lectures and media interviews about climate science. Eric received a B.A. in Physics from the University of California Berkeley, and a Ph.D. in Astrophysical, Planetary and Atmospheric Sciences from the University of Colorado, Boulder   Amy Braverman is a Senior Statistician at the Jet Propulsion Laboratory, California Institute of Technology She holds a B.A. in Economics from Swarthmore College an M.A. in Mathematics from UCLA, and a Ph.D. in Statistics also from UCLA. Prior to her current position in JPL's Science Data Understanding Group, she was a Caltech Post-doctoral Scholar at the Jet Propulsion Laboratory, and a Scientist in the Flight Sciences Experiments Section of the Science Division. Dr Braverman conducts research on information-theoretic methods for the analysis of massive data sets and streams statistical data fusion, high-dimensional data analysis, and statistical analysis for climate model evaluation and diagnosis. She has published in both the statistics and geoscience literature, and is active in both communities She is a member of the Multi-angle Imaging SpectroRadiometer Science Team, and serves as a member of the Atmospheric Infrared Sounder Science Integration Team. Her responsibilities on both missions include designing data reduction algorithms for massive, remote sensing data sets. Dr. Braverman also holds an appointment in the Department of Statistics at UCLA as Adjunct Associate Professor, and is active in UCLA\222s Center for Environmental Statistics. She is member of the Committee on Applied and Theoretical Statistics of the US National Academy of Science. She has refereed for the Journal of the American Statistical Association, the Journal of Computational and Gr aphical Statistics, IEEE Transactions on Geoscience and Remote Sensing, and the Journal of Applied Meteorology and Climatology Seungwon Lee is a senior member of the High Capability Computing and Modeling Group at Jet Propulsion Laboratory. She is conducti ng research on comet gas dynamics, nonlinear dynamics control, climate model parameterization, Earth science data analysis, parallel computing, and advanced numerical algorithms. She received her Ph.D in Physics fr om the Ohio State University and her M.S. and B.S. in Physics from the Seoul National University, Korea  Matthew Henderson is software engineer in the High Capability Computing and Mode ling group at JPL. His current work includes Web Services and Instrument Data Level 2 subsetting. He received a B.S. Computer Science from CSU Pomona, and is currently pursuing M.S Computer Science  Steven J. Lewis is a member of the Information System and Computer Science staff member at the Jet Propulsion Laboratory.  He received a BS in Mathematics from the University of California, Los Angeles in June 2001, and the MS and Ph.D. Degree from Claremont Graduate University in May 2004 and May 2007, respectively.  He worked as a post doctoral fellow at Keck Graduate Institute from June 2007 until he joined JPL in March of 2008.  During his graduate and post doctoral work, his studies focused on applications of Bayesian methods to hidden Markov models with particular interest and application to protein sequencing.  His work at JPL has focused on integrating web services into various programming platforms for the purposes of accessing NASA satellite data, as well as developing object tracking so ftware and contributing to image enhancement and restoration efforts Van Dang is a member of the Science Data Understanding Group at the Jet Propulsion Laboratory. She was responsible for the NEWS Level 2 processing that generated the formal merged Level 2 data from multiple A-Train instruments  Manuel de la Torre is a Physicist from the Universidad Complutense at Madrid \(Spain\. After finishing his Ph.D work at the University of Bayreuth \(Germany\ on pattern formation in turbulent flows and a 7 1/2 year stunt as Ass and Assoc. Prof. at the Escuela T\351cnica Superior de Ingenieros Aeron\341uticos in Madrid \(Spain\, he came to the Jet Propulsion Laboratory on a 1-year Sabatical leave in 1997 wanting to apply fundamental concepts of nonlinear systems and geophysical fluid dynamics to something that might be directly useful to soci ety. He discovered the JPL as a great place to achieve that goal and extende d his stay a bit longer, becoming Technical staff and working on different aspects of remote sensing, validation of satellite instruments, and data analysis of atmospheric processes and climate  


