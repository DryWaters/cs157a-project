Distributed  Online  Similarity  Search  in  High  Dimensional  Space Baohui  Li School  of  Computer  Science Beijing  University  of  Posts  and  Telecommunications Beijing  China libh@nelmail.iie.ac.cn Hongtao  Xie National  Engineering  Laboratory  of  Information  Security Chinese  Academy  of  Science Beijing  China xiehongtao@iie.ac.cn Kefu  Xu National  Engineering  Laboratory  of  Information  Security Chinese  Academy  of  Science Beijing  China xukefu@iie.ac.cn Abstract 227 In  this  paper  we  consider  distributed  on-line  similarity  search  for  big  data  in  high  dimensional  spaces  for  which  Locality  Sensitive  Hashing  LSH  was  the  method  of  choice  But  LSH  scheme  needs  a  rather  large  number  of  hash  tables  and  optimal  parameters  So  it  is  difficult  for  LSH  to  deal  with  big  data  in  one  machine  To  reduce  the  size  of  big  data  we  divide  the  dataset  into  well  separated  clusters  with  bounded  aspect  ratios  locating  them  in  different  peers  in  ring  network  using  random  projection  tree\(RP-tree  To  limit  the  number  of  network  accesses  we  put  similar 
 subgroups  adjacent  to  each  other  Then  we  construct  one  LSH  hash  table  for  each  subgroup  using  optimal  parameters  It  is  shown  by  comprehensive  performance  evaluations  using  real  world  data  that  our  approach  decreases  the  network  cost  and  brings  major  performance  improvement   while  maintaining  a  good  load  balance  between  different  machines Keywords 227 distributed  similarrity  search  high  dimensional  locality  sensitive  search I I NTRODUCTION T h e  r a p i d  g r o w t h  o f  o n l i n e  i n f o r m a t i o n   t r i g g e r e d  b y  t h e  p o p u l a r i t y  o f  t h e  I n t e r n e t  a n d  t h e  h u g e  a m o u n t s  o f  u s e r g e n e r a t e d  c o n t e n t    c a l l s  f o r  e f f i c i e n t  m a n a g e m e n t  o f  t h i s  d a t a  t o  i m p r o v e  u s a b i l i t y  a n d  e n a b l e  e f f i c i e n t  a n d  a c c u r a t e  a c c e s s  t o  t h e  b i g  d a t a   F u r t h e r m o r e   t h e  u 
n d e r l y i n g  a p p l i c a t i o n s  u s e  f e a t u r e r i c h  d a t a  t o  a v o i d  l o s s  o f  i n f o r m a t i o n   r a n g i n g  f r o m  s i m p l e  t e x t  s n i p p e t s  t o   s e m i   s t r u c t u r e d  d o c u m e n t s  a n d  m u l t i m e d i a  c o n t e n t   w h i c h  a r e  t y p i c a l l y  r e p r e s e n t e d  a s  h i g h d i m e n s i o n a l  f e a t u r e  v e c t o r s  M o s t  c u r r e n t  e x a c t  a p p r o a c h e s  f o r  s i m i l a r i t y  s e a r c h  a r e  u n a b l e  t o  s a t i s f y  t h e  r u n t i m e  r e q u i r e m e n t s  f o r  h i g h d i m e n s i o n a l  d a t a s e t s   f o r  t h a t  221 c o n c e n t r a t i o n  e f f e c t 222  t e n d s  t o  a p p e a r  i n  h i g h d i m e n s i o n a l  d a t 
a s e t s   i  e   t h e  d i s t a n c e s  b e t w e e n  t h e  n e a r e s t  a n d  t o  t h e  f a r t h e s t  n e i g h b o r s  b e c o m e  i n d i s c e r n i b l e  w h e n  d a t a  d i m e n s i o n a l i t y  i n c r e a s e s  1    F o r  e x a m p l e   t r e e b a s e d  m e t h o d s  s u c h  a s  c o v e r t r e e  2    S R t r e e  3   c a n  c o m p u t e  a c c u r a t e  r e s u l t s   b u t  a r e  n o t  t i m e  e f f i c i e n t  f o r  h i g h  d i m e n s i o n a l  d a t a   T h i s  p r o b l e m  i s  w e l l  s t u d i e d  i n  l i t e r a t u r e   a n d  i s  r e g a r d e d  a s  a  c h a l l e n g i n g  p r o b l e m  d u e  t o  i t s  i n t r i n s i c  c o m p l e x i t y  a n d  t h e  q u a l i t y  o r  a c c u r a c y  i s s u e s  t h 
a t  a r i s e  i n  t e r m s  o f  c o m p u t i n g  t h e  a p p r o p r i a t e  n e a r e s t  n e i g h b o r s  4   5   6   A p p r o x i m a t e  n e a r e s t  n e i g h b o r  a l g o r i t h m s  t e n d  t o  c o m p u t e  n e i g h b o r s  t h a t  a r e  c l o s e  e n o u g h  t o  t h e  q u e r i e s  i n s t e a d  o f  t h e  e x a c t  k n e a r e s t  n e i g h b o r s   a n d  h a v e  a  l o w e r  r u n t i m e  a n d  m e m o r y  o v e r h e a d  t h a n  t h e  e x a c t  a l g o r i t h m s  7    F o r  h i g h d i m e n s i o n a l  k n e a r e s t  n e i g h b o r  s e a r c h   o n e  o f  t h e  w i d e l y  u s e d  a p p r o x i m a t e  m e t h o d s  i s  l o c a l i t y s e n s i t i v e  h a s h i n g   L S H    w h i c h  u s e s  a  f 
a m i l y  o f  h a s h  f u n c t i o n s  t o  g r o u p  o r  c o l l e c t  n e a r b y  i t e m s  i n t o  t h e  s a m e  b u c k e t  w i t h  a  h i g h  p r o b a b i l i t y  8    B u t   f o r  t h e  r e a s o n  t h a t  e v e r y  h a s h  t a b l e  h a s  t o  s t o r e  a l l  t h e  d a t a s e t   i t  r e q u i r e s  s i g n i f i c a n t  m e m o r y  s p a c e   u s u a l l y  h u n d r e d s  o f  h a s h i n g  t a b l e s  t o  p r o d u c e  g o o d  a p p r o x i m a t i o n   I t  i s  d i f f i c u l t  t o  s t o r e  a l l  t h e  h a s h  t a b l e s  s t a n d a l o n e  w h e n  f a c i n g  a  l a r g e  d a t a  s e t   I t s  s e a r c h  q u a l i t y  i s  s e n s i t i v e  t o  s e v e r a l  p a r a m e t e r s  t h a t  a r e  q u i t e  d a t 
a  d e p e n d e n t   t u n i n g  p a r a m e t e r s  f o r  a  g i v e n  d a t a s e t  r e m a i n s  a  t e d i o u s  p r o c e s s   A n d   i n  t h e  d i s t r i b u t e d  s e t t i n g   a  l a r g e  n e t w o r k  l o a d   a s  e a c h  h a s h  b u c k e t  l o o k  u p  r e q u i r e s  a  c o m m u n i c a t i o n  o v e r  t h e  n e t w o r k  T o  a d d r e s s  t h e  d e m a n d i n g  n e e d s  c a u s e d  b y  t h i s  r a p i d l y  g r o w i n g   l a r g e s c a l e   a n d  h i g h  d i m e n s i o n a l  d a t a  s p a c e   w e  p r o p o s e  i n  t h e  f o l l o w i n g  a n  d i s t r i b u t e d  o n l i n e  s i m i l a r i t y  s e a r c h  f o r  h i g h d i m e n s i o n a l  d a t a   T h e  r e s t  o f  t h e  p a p e r  i s  o r g a n i z 
e d  a s  f o l l o w s   W e  s u r v e y  t h e  b a c k g r o u n d  o f  L S H  a n d  R a n d o m  P r o j e c t i o n  T r e e  i n  S e c t i o n  2   S e c t i o n  3  g i v e s  a n  o v e r v i e w  o f  d i v i d i n g  l a r g e  d a t a  s e t  i n t o  w e l l  s e p a r a t e d  s u b g r o u p s   g e n e r a t i n g  o n e  L S H  t a b l e  f o r  e a c h  c l u s t e r s  a n d  a n a l y s i s  i t s  p r o p e r t i e s   I n  S e c t i o n  4   w e  g i v e  a  f r a m e w o r k  o f  d i s t r i b u t i n g  o n l i n e  s i m i l a r i t y  s e a r c h   W e  v a l i d a t e  o u r  m e t h o d  t h r o u g h  e x p e r i m e n t s  a n d  a n a l y s i s  i n  s e c t i o n  5   a n d  g i v e  a  c o n c l u s i o n  i n  S e c t i o n  6  204 


II R ELATED  W ORKS Similarity  search  often  reduces  to  an  instantiation  of  the  Nearest  Neighbor  search  problem  and  similarity  search  objects  are  characterized  as  points  in  a  high  dimensional  space  In  some  applications  objects  are  considered  in  a  metric  space  where  only  a  distance  function  is  defined  among  them  the  features  of  the  objects  are  unknown  One  popular  way  to  perform  similarity  search  on  these  points  is  via  an  exact  or  approximate  k nearest  neighbor  search  in  a  high-dimensional  feature  space  Given  a  collection  of  points  a  distance  function  between  them  and  q  query  point  d   the  goal  of  the  k Nearest  Neighbor   KNN   query  is  to  find  the  k  closest  in  terms  of  the  distance  function  points  to  it The  approximate  version  is  even  more  desirable  when  the  data  dimensionality  is  high  as  similarity  search  is  very  expensive  in  such  domains  Here  the  goal  is  to  find  K  objects  whose  distances  are  within  a  small  factor    of  the  true  K  nearest  neighbors\222  distances  The  quality  of  similarity  search  is  measured  by  the  number  of  returned  results  as  well  as  the  distances  to  the  query  for  the  K  points  returned  compared  to  the  corresponding  distances  of  the  true  K  nearest  objects  for  KNN  querie A Locality  Sensitive  Hashi D e f i n i t i o n  1   For  the  space  T  with  metric    given  distance  threshold  r  approximation  ratio  c>1  and  probabilities  p1>p2  a  family  of  hash  functions  H    h    T  U    is  said  to  be  a    r  cr  p 1  p 2    LSH  family  if  for  all  p,q T  If    then  Pr  h\(p\h\(q      P 1  If    cr  then  Pr  h\(p\h\(q      P 2  Hash  functions  drawn  from  H  have  the  property  that  near  points  with  distance  at  most  r   have  a  high  likelihood  at  least  p 1    of  being  hashed  to  the  same  value  while  far  away  points  with  distance  at  least  cr    are  less  likely  probability  at  most  p 2    to  be  hashed  to  the  same  value  hence  the  name  locality  sensitive We  amplify  the  gap  between  the  223high\224  probability  p 1  and  223low\224  probability  p 2  by  concatenating  several  functions  as  follows  First  for  an  integer  k   let  H 222     H    T  U k   be  a  family  of  hash  functions  in  which  any  H H\222  is  the  concatenation  of  k  functions  in  H   i.e  H      h 1  h 2  h k   where  h i   H  1   i   k    Then  for  an  integer  M   draw  M  hash  functions  from  H\222   independently  and  uniformly  at  random  and  use  them  to  construct  the  index  consisting  of  M  hash  tables  on  the  data  points  With  this  index  given  a  query  q   the  similarity  search  is  done  by  first  generating  the  set  of  all  data  points  mapping  to  the  same  bucket  as  q  in  at  least  one  hash  table  and  then  finding  the  closest  point  to  q  among  those  data  points  To  utilize  this  indexing  scheme  one  needs  an  LSH  family  H  to  start  with  Such  families  are  known  for  a  variety  of  metric  spaces  including  the  Hamming  distance  the  Earth  Mover  Distance  and  the  Jaccard  measure  Furthermore  Datar  et  al  proposed  LSH  families  for  l p  norms  with  0   p   2  using  p stable  distributi  For  any  W>0  they  consider  a  family  of  hash  functions  H W    h a,b   R d Z    such  that  h a,b  v   where  a R d  is  a  d  dimensional  vector  each  of  whose  en-tries  is  chosen  independently  from  a  p-stable  distribution  and  b R  is  chosen  uniformly  from  0     Further  improvements  have  been  obtained  in  various  special  settings  11  proposed  a  Multi-probe  method  to  improve  the  performance  of  LSH  This  scheme  seeks  to  make  better  use  of  a  smaller  number  of  hash  tables   L   To  accomplish  this  goal  it  not  only  considers  the  main  bucket  where  the  query  point  falls  but  also  examines  other  buckets  that  are  223close\224  to  the  main  bucket  In  this  paper  we  will  focus  on  the  most  widely  used  p stable  distribution  i.e  the  2-stable  Gaussian  distribution  and  used  the  Multi-probe  method  For  this  case  Indyk  and  Motwani  proved  the  following  theorem  T h e o r e m  1   W i t h  n  d a t a  p o i n t s   c h o o s i n g  k  O   l o g  n   a n d  M  O   n 1  c     t h e  L S H  i n d e x i n g  s c h e m e  a b o v e  s o l v e s  t h e    c   r   N N  p r o b l e m  w i t h  c o n s t a n t  p r o b a b i l i t y  B Random  Projection  Tree Dasgupta  and  Freund  proposed  space  partitioning  algorithms  that  adapt  to  the  intrinsic  dimensionality  of  data  and  do  not  assume  explicit  knowledge  of  this  paramete  Their  data  structures  are  akin  to  the  k d  tree  structure  and  offer  guaranteed  reduction  in  the  size  of  the  cells  after  a  bounded  number  of  levels  Both  k d[13  trees  and  random  projection  RP  trees  are  built  by  recursive  binary  splits  They  differ  only  in  the  nature  of  the  split  which  we  define  in  a  subroutine  ChooseRule   ChooseRule  chooses  a  direction  uniformly  at  random  from  the  unit  sphere  S   and  splits  the  data  into  two  roughly  equal-sized  sets  using  a  hyper  plane  orthogonal  to  this  direction  The  core  tree-building  algorithm  called  RandProTree   taking  as  input  a  data  set  S R D   is  as  follow Precedure  RandProTree\(S  If  S    MinSize  Then return  Leaf Else     S:Rule\(x\=true  S:Rule\(x\=false Return  Rule  LeftTree  RightTr   III D IVIDE  THE  L ARGE  D ATA  S ET  AND  G ENERATING  O NE  LSH  T ABLE  FOR  E ACH  C LUSTER In  this  section  we  present  the  details  of  our  framework  to  divide  the  large  data  set  and  generate  one  LSH  table  for  each  cluster  As  depicted  in  Figure  1  our  algorithm  contains  three  205 


steps   M a p p i n g  1 2 3 4                             1 4 3 2  G e n e r a t i n g  L S H  T a b l e 1 3 4 2     Fig. 1 An  example  with  4  peers A Divide  the  Dataset  into  Well-Seperated  Clusters  The  RP-tree  construction  algorithm  projects  the  data  onto  a  randomly  chosen  unit  direction  and  then  splits  the  set  into  two  roughly  equal-sized  sets  using  new  split  rules  which  are  unlike  K-d  tree's  method  to  choose  the  coordinate  with  the  largest  spread  and  splits  recursively  until  the  resulting  tree  has  a  desired  depth  Theorem  2  For  RP-tree  There  is  a  constant  c 2  with  the  following  property  For    with  probability  at  least    every  descendant  C\222  which  is  more  than  c 2 s*d*logd  levels  below  C  has  radius   RP-tree  has  many  good  properties  including  fast  convergence  speed  guaranteed  221roundness\222  of  leaf  nodes  et  c  comparing  with  other  methods  such  as  a  K-d  tree  or  Kmeans  algorithm  And  these  properties  are  useful  for  generating  compact  LSH  hash  code  reducing  the  algorithm\222s  performance/quality  variance  and  conducive  to  choose  optimal  parameters  of  LSH  for  each  leaf  node  So  we  choose  RPtree  to  partition  a  dataset  into  several  subsets  called  leaf  nodes  so  that  each  leaf  node  only  contains  similar  data  items  e.g  images  for  the  same  or  similar  objects Dasgupta  and  Freund  have  proposed  two  rules  for  RP-trees  RP-tree  max  and  RP-tree  mean  which  are  adaptive  to  intrinsic  dimension  In  practice  we  observe  that  RP-tree  mean  rule  would  compute  K-nearest  neighbor  results  with  better  quality  T o  a p p l y  R P t r e e  m e a n  r u l e   w e  u s e  t h e  i t e r a t i v e  m e t h o d  p r o p o s e d  b y  E g e c i o g l u  a n d  K a l a n t a r i  1 0    w h i c h  c o n v e r g e s  f a s t  t o  a  d i a m e t e r  a p p r o x i m a t i o n  w i t h  g o o d  p r e c i s i o n  t o  e f f i c i e n t l y  c o m p u t e   S    t h e  d i a m e t e r s  f o r  a  g i v e n  p o i n t  s e t  S  i n  a  h i g h d i m e n s i o n a l  s p a c e   T h e  d i a m e t e r  c o m p u t a t i o n  a l g o r i t h m  u s e s  a  s e r i e s  o f  m  v a l u e s  r 1        r m  t o  a p p r o x i m a t e  t h e  d i a m e t e r  f o r  a  p o i n t  s e t  S   w h e r e  m    S    I t  c a n  b e  p r o v e d  t h a t  r 1    r 2          r m    S    m i n  r 1    r m    1 0    I n  p r a c t i c e   w e  f i n d  5  S   e v e n  w h e n  m  i s  s m a l l   e  g   4 0    T h e  t i m e  c o m p l e x i t y  o f  t h i s  a p p r o x i m a t e  d i a m e t e r  a l g o r i t h m  i s  O  m  S    a n d  w e  c a n  c o n s t r u c t  e a c h  l e v e l  o f  t h e  R P t r e e  i n  t i m e  t h a t  i s  l i n e a r  i n  t h e  s i z e  o f  t h e  e n t i r e  d a t a s e t   A s  a  r e s u l t   t h e  o v e r a l l  c o m p l e x i t y  t o  p a r t i t i o n  t h e  d a t a s e t  i n t o  g  g r o u p s  i s  O  l o g  g  n   B Method  of  Locating  Clusters  to  Peers When  locate  clusters  to  peers  more  formally  the  mapping  should  satisfy  the  following  two  conditions  1  assign  clusters  likely  to  hold  similar  data  to  the  same  or  adjacent  peer  to  reduce  the  number  of  network  hops 2  if  there  is  a  network  access  it  should  be  constrained  in  a  small  range  instead  of  extending  to  the  entire  network-wide  to  aggravate  the  burden  of  the  whole  network  We  fix  such  a  pair  of  balls  calling  them  B1  andB2  and  the  data  has  doubling  dimension  d   A  split  is  said  to  be  good  with  respect  to  this  pair  if  it  sends  points  inside  B1  to  one  child  of  the  cell  and  points  inside  B2  to  the  other  bad  if  it  sends  points  from  both  balls  to  both  children  and  neutral  otherwise  See  Figure  2  We  have  the  following  properties  of  a  random  split       B 1 B 2 n e u t r a l  s p l i t B a d  s p l i t g o o d  s p l i t Fig. 2 Balls  B1  and  B2  are  of  radius  and  their  centers  are    apart Lemma  1  Let  B1  and  B2  be  a  pair  of  balls  as  described  above  contained  in  the  cell  C  that  contains  data  of  doubling  dimension  d  Then  a  random  split  of  the  cell  is  a  good  split  with  respect  to  this  pair  with  probability  at  least    and  a  bad  split  with  probability  at  least  16 That  is  to  say  the  probability  of  the  data  d 1  and  d 2  in  a  ball  B1  to  be  split  in  the  same  descendant  cell  is  at  least    and  in  the  different  cells  1  after  s  levels  in  other  words  the  probability  of  d 1  and  d 2  in  different  descendant  nodes  is  smaller  than  in  the  same  descendant  So  for  any  nodes  x  y  z  if  the  nearest  ancestor  of  x  and  z  has  the  lower  level  than  x  and  y  the  data  in  x  has  bigger  probability  in  the  same  ball  included  in  their  nearest  ancestor  with  y  than  with  z Theorem  3  Pick  any  three  nodes  in  RP-tree  x  y  and  z  if  x  and  y  has  the  lower  level  than  x  and  z  then  the  similarity  between  x  and  y  is  bigger  than  x  and  z Based  on  the  analysis  above  we  organize  the  peers  as  a  ring  as  depicted  in  Figure  3  For  a  RP-tree  we  denote  the  leaf  nodes  from  one  side  to  the  other  1-n   We  map  the  buckets  206 


generated  by  node  i  to  peer  j   and  make  sure  that  the  buckets  generated  by  node   i-1   mod  n  and   i+1   mod  n  mapping  to  peer   j-1   mod  m  and   j+1   mod  m  respectively  Figure  1  illustrate  the  mapping  for  4  nodes  among  4  peers  Inference  1  Pick  any  peer  peer_service   and  the  corresponding  leaf  node  serial  number  is  leaf   Suppose  that  peer_service 222s  neighbor  peer  is  peer1  and  peer2   and  their  corresponding  leaf  node  serial  number  is  leaf1  and  leaf2  respectively  If  the  nearest  ancestor  for  leaf  and  leaf1  is  bigger  than  leaf  and  leaf2   than  the  similarity  between  the  data  stored  in  peer_service  and  peer1  is  bigger  than  peer_service  and  peer2  Proof  It  is  easy  to  be  concluded  by  theorem  2 C Generating  Hash  Tables  for  Each  Cluster  with  Different  Parameters As  the  data  in  a  cluster  partitioned  by  RP-tree  have  homogeneous  properties  the  estimated  parameters  for  each  cluster  can  improve  the  runtime  performance  and  quality  of  the  LSH  scheme  as  compared  to  the  single  set  of  parameters  used  for  the  entire  dataset  We  choose  different  LSH  parameters  that  are  optimal  for  each  leaf  node  instead  of  choosing  a  single  set  of  parameters  for  the  entire  dataset  There  are  four  parameters  that  affect  the  performance  of  Multi-probe  LSH  algorithm  number  of  projections  per  hash  value   k   number  of  hash  tables   l   the  width  of  the  projection   w   and  the  length  of  the  probing  sequence t   The  value  k  and  l  represent  a  tradeoff  between  the  time  spent  in  computing  hash  values  and  time  spent  in  pruning  false  positives  i.e  computing  distances  between  the  query  and  candidates  a  bigger  value  increases  the  number  of  hash  computations  We  do  a  binary  search  over  a  large  range  to  find  the  optimal  value  This  binary  search  can  be  avoided  if  we  have  a  good  model  of  the  relative  times  of  hash  computations  to  distance  computations  for  the  application  at  hand  The  width  of  the  projection   w   has  great  effect  on  the  probability  of  collision  for  any  two  points  that  is  to  say  decreasing  the  width  means  decreases  the  probability  and  the  same  result  as  increasing  w   We  set  w  as  small  as  possible  and  in  this  way  decrease  the  number  of  projections  we  need  to  make  However  decreasing  w  below  a  certain  threshold  increases  the  quantity    thereby  requiring  us  to  increase  w   Thus  we  cannot  decrease  w  by  too  much  For  a  fixed  k   the  parameter  m  is  chosen  to  be  the  smallest  natural  number  satisfying  l  is  set  to  be  k  k 1\2  To  choose  k   we  use  the  method  proposed  by  Indyk  to  get  T g  and  T c   which  T g  represents  computing  the  l  functions  g i  for  the  query  point  q  as  well  as  retrieving  the  buckets  g i q  from  hash  tables  and  T c  for  computing  the  distance  to  all  points  encountered  in  the  retrieved  buckets  And  k  is  chosen  such  that  T c T g  is  minimal  where  T c   is  the  mean  of  the  times  T c  for  all  points  in  the  sample  query  set  S T h e  t  p a r a m e t e r  i s  r e l a t e d  t o  t h e  q u e r y  p r o c e s s i n g  a l g o r i t h m   a n d  w e  c h a n g e  i t  f r o m  q u e r y  t o  q u e r y  u s i n g  t h e  m e t h o d  o f  a d a p t i v e  p r o b i n g  p r o p o s e d  b y  W e i  D o n g   e t  c   m a i n t a i n i n g  a n  o n l i n e  p r e d i c t i o n  o f  t h e  e x p e c t e d  r e c a l l  o f  c u r r e n t  q u e r y   a n d  k e e p i n g  p r o b i n g  u n t i l  t h e  r e q u i r e d  v a l u e  i s  r e a c h e d  IV O NLINE  SIMILARITY  SEARCH  P ROCESSING Every  machine  in  the  network  can  process  query  data  maintaining  the  RandoProTree  when  the  hash  tables  created  the  mapping  table  table_id_server  between  the  leaf  node  serial  number  and  peers  and  the  neighbor  machines\222s  IP  address  Neighbor_server  Fig. 3 An  example  of  distributed  online  similarity  search Upon  receiving  a  query  data  d   the  server  will  calculate  leaf  node  serial  number  leaf  by  the  maintained  RandoProTree   then  search  the  table  table_id_server   with  the  mapping  between  the  leaf  node  serial  number  and  peers  address  to  obtain  the  address  of  peer_obj  stored  the  buckets  generated  by  leaf  Then  peer_obj  performs  Multi-probe  search  to  systematically  probe  those  buckets  that  are  closest  to  main  bucket  to  get  the  best  result  peer_obj_res   utilizing  the  parameters  for  the  peer  For  a  single  hash  table  let  d  be  the  query  point  and  H d   its  hash  respectively  Recall  that  H d   consists  of  the  concatenation  of  M  integral  values  each  produced  by  an  atomic  hash  function  on  d   Buckets  corresponding  to  hash  values  that  differ  from  H d   by  2611  in  one  or  several  components  are  also  likely  to  contain  points  near  the  original  query  point  d  for  that  buckets  corresponding  to  hash  values  that  differ  from  H d   by  more  than  1  in  certain  component  are  much  less  likely  to  contain  points  of  interest Then  the  neighboring  peer  which  has  the  nearest  ancestor  with  leaf  will  be  the  next  target  peer  peer_obj_next   peer_obj_next  will  issue  a  local  Multi-probe  search  compare  their  best  result  peer_obj_next_res  to  peer_obj_res   If  no  update  we  stop  forwarding  the  query  d  to  its  neighbor  or  we  will  go  on  searching  until  preset  conditions  full  filled  Figure  3  shows  an  example  of  doing  similarity  search  for  a  data  point  The  formal  description  for  generating  buckets  and  mapping  them  to  different  peers  is  as  follows Precedure  Multi-Peer-Search\(d    while  Flag==False  and  not  all  servers  visited Multi-probe  peer_obj If  no  update  for  peer_obj_res  207 


exit Else p e e r _ o b j _ n e x t f i n d _ n e x t  N e i g h b e r _ s e r v e r   R a n d o P r o T r e e  V E XPERIMENTS A Experimental  Setup  We  have  implemented  the  proposed  system  and  algorithms  using  C  language  running  on  eight  machines  connected  as  ring  model  Every  machine  has  eight  Intel\(R  Xeon\(R  CPU  of  2.33GHz  and  64  GB  RAM  We  use  Flickr  data  set  a  crawl  of  Flickr  consisting  of  1  000  000  images  represented  by  their  MPEG7  visual  descriptors  The  total  number  of  dimensions  per  image  is  282  and  contains  descriptors  such  as  Edge  Histogram  Type  and  Homogeneous  Texture  Type  We  implemented  the  following  methods  plus  the  mechanism  this  paper  prosed  as  comparison Random_method  the  baseline  algorithm  which  distributed  the  dataset  randomly  on  servers  At  query  time  the  query  data  was  also  delivered  to  the  random  selected  server  We  report  on  the  following  measures Number  of  Network  Hops  We  count  the  number  of  network  hops  during  the  query  execution  Network  hops  are  one  of  the  most  critical  parameters  and  the  dominating  factor  in  making  distributed  algorithms  work  in  large-scale  wide-area  networks  One  single  network  hop  in  a  wide-area  costs  in  average  around  100  ms  which  overrules  the  I/O  cost  induced  by  a  standard  hard  disk  with  approximately  8  ms  for  disk  seek  time  plus  rotation  latency  and  100  MB/s  transfer  rate  for  sequential  accesses  in  case  of  local  disk  access Recall  Ratio  For  the  effectiveness  metric  we  report  on  the  recall  ratio  i.e  the  percentage  of  the  actual  nearest  neighbors  N\(d  in  the  returned  results  I\(d       where  N\(d  can  be  computed  using  any  exact  k-nearest  neighbor  approach  and  serves  as  the  ground-truth  The  relevance  is  defined  by  the  full-scan  run  over  all  data  points  to  determine  points  within  knearest  neighbor  for  a  given  query  where  similarity  is  measured  based  on  l 2  B Experimental  Results Figure  4  illustrated the  recall  ration  and  the  number  of  network  hops  The  height  of  random  projection  tree  is  3  and  the  parameters  local  LSH  for  every  peer  is  generated  automatically  shown  as  follows  Table  1    Parameters  each  server  taken Number k t l w 1 20 2 595 4.0 2 26 2 2346 4.0 3 22 2 946 4.0 4 22 2 946 4.0 5 20 2 595 4.0 6 12 2 91 4.0 7 24 2 1485 4.0 8 24 2 1485 4.0 As  illustrated  in  figure  4  the  recall  ratio  of  our  method  is  better  than  random_method  especially  for  the  previous  servers  that  is  to  say  the  network  hops  will  reduced  greatly  when  not  required  the  whole  answers Fig. 4 Recall  ratio  versus  number  of  network  hops In  the  second  experiments  we  compare  the  number  of  answers  returned  by  random_method  and  the  mechanism  proposed  by  this  paper  for  15  query  data  in  3  network  hops  The  parameters  of  each  server  are  also  as  table  1 As  is  shown  in  figure  5  our  method\222s  performance  are  superior  to  random_method  with  recall  ratio  achieving  73  which  are  superior  than  random  method  37.4  obviously Fig. 5 Number  of  answers  comparison  among  exact  random  and  the  method  we  proposed  for  a  fixed  network  hops VI C ONCLUSION We  have  presented  a  robust  and  scalable  solution  to  the  online  distributed  nearest  neighbor  search  problem  over  high  dimensional  data  Considering  the  requirements  that  arise  in  distributed  settings  we  have  shown  how  to  partition  the  large  data  set  into  small  ones  which  are  similar  among  them  map  them  to  the  ring  ordered  set  of  peers  to  achieve  high  quality  search  results  through  LSH   balance  the  load  among  peers  and  limit  the  number  of  network  accesses  which  are  verified  by  experiments  There  are  many  avenues  for  future  work  We  hope  to  test  our  algorithm  on  more  real-world  datasets  including  images  textures  videos  etc A CKNOWLEDGMENT W e  w o u l d  l i k e  t o  t h a n k  t h e  a n o n y m o u s  r e v i e w e r s  f o r  t h e i r  t h o u g h t f u l  c o m m e n t s  a n d  s u g g e s t i o n s   T h i s  r e s e a r c h  i s  208 


s p o n s o r e d  b y  t h e  S p e c i f i c  S t r a t e g i c  P i l o t  o f  C h i n e s e  A c a d e m y  o f  S c i e n c e  X D A 0 6 0 3 0 6 0 2    N a t i o n a l  H i g h  T e c h n o l o g y  R e s e a r c h  a n d  D e v e l o p m e n t  P r o g r a m  2 0 1 1 A A 0 1 0 7 0 5     N a t i o n a l  N a t u r a l  S c i e n c e  F o u n d a t i o n  6 1 0 0 3 2 9 5   a n d  N a t i o n a l  N a t u r a l  S c i e n c e  F o u n d a t i o n  6 1 3 0 3 1 7 1   R EFERENCES 1 K  Beyer  J  Goldstein  R  Ramakrishnan  and  U  Shaft  When  is  224nearest  neighbor\224  meaningful  InInternational  Conference  on  Database  Theory  pages  217 226 235  1999 2 A  Beygelzimer  S  Kakade  and  J  Langford  Cover  trees  for  nearest  neighbor  In  International  Conference  on  Machine  Learning  pages  97 226 104  2006 3 N  Katayama  and  S  Satoh  The  sr-tree  an  index  structure  for  highdimensional  nearest  neighbor  queries  In  International  Conference  on  Management  of  Data  pages  369 226 380  1997 4 Q  Lv  W  Josephson  Z  Wang  M  Charikar  and  K  Li  Ferret  a  toolkit  for  content-based  similarity  search  of  feature-rich  data  InEuroSys  22206  Proceedings  of  the  ACM  SIGOPS/EuroSys  European  Conference  on  Computer  Systems  2006  pages  317 226 330  New  York  NY  USA  2006  ACM 5 Roger  Weber  Hans  J  Schek  and  Stephen  Blott  A  quantitative  analysis  and  performance  study  for  similarity-search  methods  in  highdimensional  spaces  Proceedings  of  the  24th  Int  Conf  Very  Large  Data  Bases  VLDB  1998 6 Piotr  Indyk  and  Rajeev  Motwani  Approximate  Nearest  Neighbors    Towards  Removing  the  Curse  of  Dimensionality  In30th  Annual  ACM  Symposium  on  Theory  of  Computing  pages  604 226 613  1998 7 J  M  Kleinberg  Two  algorithms  for  nearest-neighbor  search  in  high  dimensions  In  Symposium  on  Theory  of  Computing  pages  599 226 608  1997 8 Aristides  Gionis  Piotr  Indyk  Rajeev  motwani  Similarity  Search  in  High  Dimensions  via  Hashing  Proceedings  of  the  25th  VLDB  Conference  Edinburgh  Scotland  1999 9 P  Indyk  and  R  Motwani  Approximate  nearest  neighbors  towards  removing  the  curse  of  dimensionality  InSTOC  22298  Proceedings  of  the  thirtieth  annual  ACM  symposium  on  Theory  of  computing  pages  604 226 613  New  York  NY  USA  1998  ACM 10 M  Datar  N  Immorlica  P  Indyk  and  V  S  Mirrokni  Locality-sensitive  hashing  scheme  based  on  p-stable  distributions  InSCG  22204  Proceedings  of  the  twentieth  annual  symposium  on  Computational  geometry  pages  253 226 262  New  York  NY  USA  2004  ACM  Press 11 Q  Lv  W  Josephson  Z  Wang  M  Charikar  and  K  Li  Multi-probe  lsh  Efficient  indexing  for  high-dimensional  similarity  search  InVLDB  22207  Proceedings  of  the  24rd  International  Conference  on  Very  Large  Data  Bases  Vienna  Austria  2007 12 Sanjoy  Dasgupta  and  Yoav  Freund  Random  Projection  Trees  and  Low  dimensional  Manifolds  In40th  Annual  ACM  Symposium  on  Theory  of  Computing  pages  537 226 546  2008 13 J  Bentley  Multidimensional  binary  search  trees  used  for  associative  searching  Communications  of  the  ACM  18\(9\509 226 517  1975 14 Par  isa  Haghani  Sebastian  Michel  Philippe  Cudr  264e-Mauroux  Kar  l  Aberer  LSH  At  Large  Distributed  KNN  Search  in  High  Dimensions  Proceedin  gs  of  the  11th  In  ternati  on  al  Workshop  on  Web  and  Databases  WebDB  2008  Jun  e  13  2008  Van  couver  Can  ada 15 Jia  Pan  Dinesh  Manocha  Bi-level  Locality  Sensitive  Hashing  for  KNearest  Neighbor  Computation  VLDB  22110,September  13-17  2010  Singapore 16 Aman  Dhesi  Purushottam  Kar  Random  Projection  Trees  Revisited  NIPS  2010 209 


       


   





Manufacture B Performance of Real-Time Querying IncreQuerying Baseline Baseline IncreQuerying IncreQuerying Baseline Baseline Baseline Baseline mft IncreQuerying Baseline shipdate Lineitem shipdate return\224ag linestatus IncreQuerying Baseline IncreQuerying Baseline IncreQuerying y y IncreQuerying 11 In this experiment we investigate the performance of realtime querying First we compare the Fig 4 Throughput of Real-Time Data Cube Maintenance Algorithm                                                              Fig 5 Performance of Data Cube Refresh    Fig 6 Scalability real-time data cube with the data streams from HBase-R Thus the latency of periodically refreshing the data cube in HBaseR equals to the time of writing the real-time data cube into HBase-R This time is related to the the size of the data cube and does not change as the number of updates increases We also evaluate the scalability of R-Store In this experiment the number of nodes and the data size increase with the same ratio The percentage of updates is set to 1 for different scalability settings As can be seen in Figure 6 the running time of both re-computation the brown line and incremental update blue line do not change much as the number of nodes increase which demonstrates the scalability of R-Store 1 algorithm which optimizes the real-time query using the data cube with the algorithm implemented with the operation The cluster settings are the same as those of Figure 5 except that we 223x the number of updates to 8,000 million and vary the percentage of the keys updated Figure 7\(a shows the processing time of both algorithms for a typical data cube slice query algorithm consists of two parts the black rectangle ReCompScan is the time to scan the real-time table and the yellow rectangle ReCompExe is the execution time of the MapReduce job after the scan phase In contrast the processing time of consists of three parts the red rectangle CubeScan is the time to scan the data cube the blue rectangle UpdateScan is the time to scan the performs much better than  It outperforms the approach for two reasons 1 by using adaptive incremental scan it scans fewer data in HBase-R and shuf\224es fewer data to MapReduce 2 its MapReduce job processes fewer data than that of re-computation However as the percentage of updated keys increases more data are shuf\224ed from HBaseR to MapReduce Thus both the scan time and the execution time increase In contrast for  since the always shuf\224es one version for each key to MapReduce the amount of data shuf\224ed from HBase-R is constant As a result the running time of is almost constant Due to the existence of the 223ltering condition on attribute  most tuples of the table are 223ltered and fewer data are sorted and shuf\224ed during the execution of the MapReduce job As a result the difference between the execution times is not so signi\223cant In general algorithm outperforms algorithm when the percentage of keys being updated is low In addition to the data cube slice query we also evaluate TPC-H Q1 Figure 7\(b with the same experimental settings We did not illustrate other benchmark queries as they involve multiple tables which will not be able to illustrate as clearly the effectiveness of the basic operators supported in R-Store The parameter of TPC-H Q1 is set to 215365\216 days and only about 15 of the tuples are 223ltered  table since we only build the data cube on attributes  and  the data cube is much smaller than the real-time table and the time to scan the data cube is around 20 seconds Overall Figure 7\(b demonstrates that the performance of is signi\223cantly better than that of  To select the better querying method among the two we use the cost model Section V-C to estimate the number of I/Os Figure 8 shows the running time of  and the I/Os estimated for both and algorithms The axis on the left is the processing time of the query while the axis on the right is the estimated I/Os The estimated number of I/Os for the blue line increases linearly with almost the same slope the histogram as the processing time of the query while the estimated number of I/Os for the Baseline the brown line is constant which is around 2.52 10 FROM part WHERE mft 0 2,000 4,000 6,000 8,000 10,000 ReComp Update ReComp Update ReComp Update ReComp Update ReComp Update Processing time \(s Number of Updates 8M 400M 800M 1,200M 1,600M Update ReCompExe ReCompScan 100    200    300    400    500  10  20  30  40  50  60  70  0    1000    2000    3000    4000    5000    6000    7000    8000  50  75  100  125  145  IncrementalUpdate                            0    Updates Per Second \(K Number of Nodes Throughput          Processing Time \(s Number of Nodes ReComputation              The processing time of the table in HBase-R and the grey rectangle UpdateExe is the execution time of the MapReduce job after the scan phase When only a small range of keys are updated larger than 21512-01-1998\216 Thus the execution time of the MapReduce job after the scan phase is longer than that of Figure 7\(a For the  This result hence veri\223es the accuracy of our cost model Compared to querying only the data cube RTOLAP queries require two additional steps which incur additional cost scanning the real-time data from HBase-R and merging the real-time data with the data cube on demand in MapReduce SELECT sum   327 215 002\002    49 FullScan FullScan part shipdate prices GROU P BY brand type size container 


method increases slightly which is due to two reasons 1 the data before In this experiment we investigate the performance of OLTP queries when OLAP queries are running The workload is update-only and the keys being updated are uniformly distributed We launch ten clients to concurrently submit the updates when the system is deployed on 100 nodes Each client starts ten threads each of which submits one million updates 100 updates in batch Another client is launched to submit the data cube slice query That is one OLAP query and approximately 50,000 updates are concurrently processed in R-Store The system reaches its maximum usage in this setting based on our observation When the system is deployed on other number of nodes the number of clients submitting updates is adjusted accordingly Figure 11\(a shows the throughput of the system The throughput increases as the number of nodes increases which demonstrates the scalability of the system However when OLAP queries are running the update performance is lower than running only OLTP queries This result is expected because the OLAP queries compete for resources with the OLTP queries We also evaluate the latency of updates when the system is approximately fully used As shown in Figure 11\(b the aggregated response time for 1000 updates are similar with respect to varying scales VII C ONCLUSION MapReduce is a parallel execution framework which has been widely adopted due to its scalability and suitability in 0    500    1000    1500    2000  0  10  20  30  40  50  60  70  80  90  100  IncreQueryScan             IncreQueryExe              DC DC DC  Q i i i i T part  Q a Data Cube Slice Query                                                                                                b TPC-H Q1 Fig 7 Performance of Querying    Fig 8 Accuracy of Cost Model    Fig 9 Performance vs Freshness On each HBase-R node the key/values are stored in format Though only one or two versions of the same key are returned to MapReduce HBase-R has to scan all the of the table Since the is materialized to HDFS when it is full these 223les are sorted by time Thus instead of scanning all the and between  only the between   are scanned The value of decides the freshness of the result There is a trade-off between the performance of the query and the freshness of the result the smaller is the fewer real-time data are scanned Figure 9 shows the query processing time with different freshness ratios which is de\223ned as the percentage of the real-time data we have to scan for the query In this experiment  1600 million and 800 million updates on 1 distinct keys are submitted to HBase-R When the freshness ratio is 0 the input of the query is only the data cube Thus the cost of scanning the real-time data is 0 When the freshness ratio increases to 10 the cost of scanning the real-time data is around 1500 seconds because the cost of scanning the real-time table dominates the OLAP query As the freshness ratio increases the running time of and  and when it is not  and  We submit 800 million updates to the server each day and the percentage of keys updated is 223xed to 1 The data cube is refreshed at the beginning of each day and the OLAP query is submitted to the server at the end of the day Since the data are compacted after the data cube refresh the amount of data stored in the real-time table are almost the same at the same time of each day The processing time of and are thus almost constant In contrast when the compaction scheme is turned off HBase-R stores much more data and the cost of locally scanning these data becomes larger than the cost of shuf\224ing the data to MapReduce As a result the processing time of and increases over time and and a user speci\223ed timestamp still need to be scanned and 2 the amount of data shuf\224ed to mappers are roughly the same with different ratios Figure 10 depicts the effectiveness of our compaction scheme In this experiment we measure the processing time of the data cube slice query when the compaction scheme is applied  0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan Processing Time \(s I/Os \(X10 11  Percentage of Keys Updated CubeScan        Processing Time \(s Freshness Ratio CubeScan                                                                                                            50 0 1,000 2,000 3,000 4,000 5,000 Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Baseline IncreQuery Processing time \(s Percentage of keys being updated 1 5 10 15 20 25 IncreQueryExe IncreQueryScan CubeScan BaselineExe BaselineScan store\223le store\223les part memstore store\223les memstore store\223les IncreQuerying Baseline IncreQuerying Baseline-NC IncreQuerying-NC Baseline IncreQuerying Baseline-NC IncreQuerying-NC C Performance of OLTP 0    1200    2400    3600    4800    6000  1  5  10  15  20  25  0  0.8  1.6  2.4  3.2  4  IncreQueryScan        IncreQueryExe        I/Os estimated for IncreQuery                               I/Os estimated for  Baseline                 T T T T T T T T 


3000    6000    9000    12000  1  2  3  4  5  6  7  IncreQuerying                                   Baseline-NC                   IncreQuerying-NC                       51 002 Fig 10 Effectiveness of Compaction    a Throughput    b Latency Fig 11 Performance of OLTP Queries a large scale distributed environment However most existing works only focus on optimizing the OLAP queries and assume that the data scanned by MapReduce are unchanged during the execution of a MapReduce job In reality the real-time results from the most recently updated data are more meaningful for decision making In this paper we propose R-Store for supporting real-time OLAP on MapReduce R-Store leverages stable technology HBase and HStreaming and extends them to achieve high performance and scalability The storage system of R-Store adopts multi-version concurrency control to support real-time OLAP To reduce the storage requirement it periodically materializes the real-time data into a data cube and compacts the historical versions into one version During query processing the proposed adaptive incremental scan operation shuf\224es the real-time data to MapReduce ef\223ciently The data cube and the newly updated data are combined in MapReduce to return the real-time results In addition based on our proposed cost model the more ef\223cient query processing method is selected To evaluate the performance of R-Store we have conducted extensive experimental study using the TPCH data The experimental results show that our system can support real-time OLAP queries much more ef\223ciently than the baseline methods Though the performance of OLTP degrades slightly due to the competition for resources with OLAP the response time and throughput remain good and acceptable A CKNOWLEDGMENT The work described in this paper was in part supported by the Singapore Ministry of Education Grant No R-252000-454-112 under the epiC project and M.T 250 Ozsu\220s work was partially supported by Natural Sciences and Engineering Research Council NSERC of Canada We would also like to thank the anonymous reviewers for their insightful comments R EFERENCES  http://hbase.apache.or g  http://hstreaming.com  http://www comp.nus.edu.sg epic  M Athanassoulis S Chen A Ailamaki P  B Gibbons and R Stoica Masm ef\223cient online updates in data warehouses In  pages 865\205876 2011  Y  Cao C Chen F  Guo D Jiang Y  Lin B C Ooi H T  V o S W u and Q Xu Es2 A cloud data storage system for supporting both oltp and olap ICDE pages 291\205302 2011  S Ceri and J W idom Deri ving production rules for incremental vie w maintenance In  pages 577\205589 1991  T  Condie N Conw ay  P  Alv aro J M Hellerstein K Elmelee gy  and R Sears Mapreduce online In  pages 313\205328 2010  J Dean S Ghema w at and G Inc Mapreduce simpli\223ed data processing on large clusters In  pages 137\205150 2004  L Golab T  Johnson and V  Shkapen yuk Scheduling updates in a real-time stream warehouse ICDE pages 1207\2051210 2009  M Grund J Kr 250 uger H Plattner A Zeier P Cudre-Mauroux and S Madden Hyrise a main memory hybrid storage engine  4\(2 Nov 2010  A Gupta I S Mumick and V  S Subrahmanian Maintaining vie ws incrementally extended abstract In  pages 157\205166 1993  S H 264 eman M Zukowski N J Nes L Sidirourgos and P Boncz Positional update handling in column stores In  pages 543\205 554 2010  D Jiang G Chen B C Ooi and K.-L T an epic an e xtensible and scalable system for processing big data 2014  D Jiang B C Ooi L Shi and S W u The performance of mapreduce an in-depth study  3\(1-2 Sept 2010  D M Kane J Nelson and D P  W oodruf f An optimal algorithm for the distinct elements problem PODS 22010 pages 41\20552  A K emper  T  Neumann F  F  Informatik T  U Mnchen and DGarching Hyper A hybrid oltp&olap main memory database system based on virtual memory snapshots In  2011  T W  K uo Y T  Kao and C.-F  K uo T w o-v ersion based concurrenc y control and recovery in real-time client/server databases  52\(4 Apr 2003  K Y  Lee and M H Kim Ef 223cient incremental maintenance of data cubes In  pages 823\205833 2006  F  Li B C Ooi M T  250 Ozsu and S Wu Distributed data management using mapreduce In  2014  I S Mumick D Quass and B S Mumick Maintenance of data cubes and summary tables in a warehouse In  pages 100\205111 1997  A Nandi C Y u P  Bohannon and R Ramakrishnan Distrib uted cube materialization on holistic measures In  pages 183\205194 2011  L Neume yer  B Robbins A Nair  and A K esari S4 Distrib uted stream computing platform In  pages 170\205177 2010  C Olston B Reed U Sri v asta v a R K umar  and A T omkins Pig latin a not-so-foreign language for data processing In  pages 1099\2051110 2008  K Ser ge y and K Y ury  Applying map-reduce paradigm for parallel closed cube computation In  pages 62\20567 2009  M Stonebrak er  D J Abadi A Batkin X Chen M Cherniack M Ferreira E Lau A Lin S Madden E O\220Neil P O\220Neil A Rasin N Tran and S Zdonik C-store a column-oriented dbms In  pages 553\205564 2005  A Thusoo J S Sarma N Jain Z Shao P  Chakka S Anthon y  H Liu P Wyckoff and R Murthy Hive a warehousing solution over a mapreduce framework  2\(2 2009  P  V assiliadis and A Simitsis Near real time ETL In  volume 3 pages 1\20531 2009  C White Intelligent b usiness strate gies Real-time data w arehousing heats up  2012 SIGMOD VLDB NSDI OSDI SIGMOD SIGMOD Proc VLDB Endow In ICDE IEEE Trans Comput VLDB ACM Computing Survey SIGMOD ICDE ICDMW SIGMOD DBKDA VLDB PVLDB Annals of Information Systems DM Review 0    Processing Time \(s Time since the Creation of Data Cube \(day Baseline                  Updates Per Second \(K Number of Nodes Updates only                  Response Time for 1000 Updates\(s Number of Nodes Updates only                  0    20    40    60    80    100  10  20  30  40  50  60  70  Updates + OLAP                                    0    2    4    6    8    10  10  20  30  40  50  60  70  Updates + OLAP                                    Proc VLDB Endow 


  13    1  2   3   4   5   6   7   8  9  10  11   


and aeronautical engineering with degrees from Universitat Politecnica de Catalunya in Barcelona Spain and Supaero in Toulouse France He is a 2007 la Caixa fellow and received the Nortel Networks prize for academic excellence in 2002 Dr Bruce Cameron is a Lecturer in Engineering Systems at MIT and a consultant on platform strategies At MIT Dr Cameron ran the MIT Commonality study a 16 002rm investigation of platforming returns Dr Cameron's current clients include Fortune 500 002rms in high tech aerospace transportation and consumer goods Prior to MIT Bruce worked as an engagement manager at a management consultancy and as a system engineer at MDA Space Systems and has built hardware currently in orbit Dr Cameron received his undergraduate degree from the University of Toronto and graduate degrees from MIT Dr Edward F Crawley received an Sc.D in Aerospace Structures from MIT in 1981 His early research interests centered on structural dynamics aeroelasticity and the development of actively controlled and intelligent structures Recently Dr Crawleys research has focused on the domain of the architecture and design of complex systems From 1996 to 2003 he served as the Department Head of Aeronautics and Astronautics at MIT leading the strategic realignment of the department Dr Crawley is a Fellow of the AIAA and the Royal Aeronautical Society 050UK\051 and is a member of three national academies of engineering He is the author of numerous journal publications in the AIAA Journal the ASME Journal the Journal of Composite Materials and Acta Astronautica He received the NASA Public Service Medal Recently Prof Crawley was one of the ten members of the presidential committee led by Norman Augustine to study the future of human space\003ight in the US Bernard D Seery is the Assistant Director for Advanced Concepts in the Of\002ce of the Director at NASA's Goddard Space Flight Center 050GSFC\051 Responsibilities include assisting the Deputy Director for Science and Technology with development of new mission and measurement concepts strategic analysis strategy development and investment resources prioritization Prior assignments at NASA Headquarters included Deputy for Advanced Planning and Director of the Advanced Planning and Integration Of\002ce 050APIO\051 Division Director for Studies and Analysis in the Program Analysis and Evaluation 050PA&E\051 of\002ce and Deputy Associate Administrator 050DAA\051 in NASA's Code U Of\002ce of Biological and Physical Research 050OBPR\051 Previously Bernie was the Deputy Director of the Sciences and Exploration Directorate Code 600 at 050GSFC\051 Bernie graduated from Fair\002eld University in Connecticut in 1975 with a bachelors of science in physics with emphasis in nuclear physics He then attended the University of Arizona's School of Optical Sciences and obtained a masters degree in Optical Sciences specializing in nonlinear optical approaches to automated alignment and wavefront control of a large electrically-pumped CO2 laser fusion driver He completed all the course work for a PhD in Optical Sciences in 1979 with emphasis in laser physics and spectroscopy He has been a staff member in the Laser Fusion Division 050L1\051 at the Los Alamos National Laboratories 050LANL\051 managed by the University of California for the Department of Energy working on innovative infrared laser auto-alignment systems and infrared interferometry for target alignment for the HELIOS 10 kilojoule eight-beam carbon dioxide laser fusion system In 1979 he joined TRW's Space and Defense organization in Redondo Beach CA and designed and developed several high-power space lasers and sophisticated spacecraft electro-optics payloads He received the TRW Principal Investigators award for 8 consecutive years Dr Antonios A Seas is a Study Manager at the Advanced Concept and Formulation Of\002ce 050ACFO\051 of the NASA's Goddard Space Flight Center Prior to this assignment he was a member of the Lasers and Electro-Optics branch where he focused on optical communications and the development of laser systems for space applications Prior to joining NASA in 2005 he spent several years in the telecommunication industry developing long haul submarine 002ber optics systems and as an Assistant Professor at the Bronx Community College Antonios received his undergraduate and graduate degrees from the City College of New York and his doctoral degree from the Graduate Center of the City University of New York He is also a certi\002ed Project Management Professional 14 


 





 17  Jar r e n  A   B al d w i n  is  a  Ch i c a g o  n a t i v e  a n d  c u r r e n t l y  se r v e s a s t h e  l e a d  E l e c t r i c a l  En g i n e e r  a t  B a y  A r e a  s t a r t u p   Oc u l e v e  I n c   He  g r a d u a t e d  fr o m  t h e  U n i v e r s i t y  o f Il l i n o i s  wi t h  a  B  S   i n  2 0 0 9  an d  r ecei v ed  an  M  S   i n  El e c t r i c a l  En g i n e e r i n g  f r  St a n f o r d  U n i v e r s i t y  i n  2 0 1 2   Ja r r e n  d e v e l o p e d  h a r d w a r e  a n d  so f t w a r e  sy st e m s f o r  a  w i d e  ra n g e  o f  f i e l d s   i n c l u d i n g  s p a c e  s c i e n c e  s y s t e m s  a n d  m e d i c a l  de vi c e s  a s  a N A S A  A m es  i nt e r n i n t he  In t e l l i g e n t  S y s t e m s     1  2  3   4   5   6   7   8   9   10   11   12   13   


                        


                           


   












































     2 8    b 4      1 8             1 2     1  8 2  


1 9    8      2 1       1     2    8    2 3\f        


     8 D 4 9  F  \b 1 8 #J 9 1     2 1   2 #-@ 1   2 9  E 1   1   2 9      6  


    8  8   1  D 1         1 F  \b0         2 D    F  \b 1 8  9  


  1 9  1   E 1  2 9     1 1 F  \b       1    18   F   1    1 #-$+  \b  2 2  


1 D     1 #-$+.B- 0/:% .0             9 1      18    1 6     1 2  1  1  


1   6      2    1 2 E 8 D 1      1 2   1   1 #-4  #-@E     2  1  1  1       


 8     1          2 F    6   F  2   8    2 C<CC/C N\bO 5      


CD    b$44NO F P Q 6   2 b$$$ ,=\b\bA  A N,O 2 C C  b$$4N  92 2   f  9-\b$$4 B N?O  !-    91  2 1 HH111-18-N+O    -1 :3%   2     0-4 


     b N4O 2   2- \f  C b$@$ \b# >\b\b$3\b$N@O  f :.% 9 /9 \f    1  6  f 2  4   A254 


Advantages of Our M ethod Advantages of Our M ethod Exploit the memory v ertical data format utilizes slidin g windows to g form a much larger database to analyze  Flexibility in Choosing what to choose  Choosing what to choose to build the rules Computational and memory efficiency We have a team working only on this aspect 45 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 45 


Preliminary Results Intra transaction Relations Data rate simulator NH-134 Mb HOP PATH update \(Y/N Inference 11.5 Y Y 2 0.120 N N      Y   n 0.345 N NH134 Y/N   Inf 1 0.150 N N 2 0 120 Y N Inter transaction Relations 2 0  120 Y N       5 5.55 0.456 Y Relations  n 0.345 N N Nmap on DMRL nmap.org 


Anticipated Outcome Anticipated Outcome Develop algorithm capable of learning from a given heterogeneous diverse Develop algorithm capable of learning from a given heterogeneous diverse data ff Dynamic algorithmic f ramework designed to shi f t modalities and sampling rates based on complex logic Flexibility of integration into the Snort intrusion detection system 47 Associative IDS for NextGen Frameworks Dr S Dua LA Tech 47 


References References Aircraft Cockpit Image courtesy USAF http://www.faa.gov htt p   www.faa g ov  air traffic  technolo gy  p g  _ gy  Acharya R Dua S Du X Sree V Chua C K Automated Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features To appear in IEEE Transactions on Information Technology in Biomedicine  Du X Dua S 2011 Cancer Prognosis Using Support Vector Regression in Imaging  Modality World Journal of Clinical Oncology 2  1   44 49 Du X Dua S 2010 Salient Frame Extraction Using Support Vector Regression and Motion Features pp 5 Proc of the National Aerospace and Electronics Conference July 14 16 2010 D M P D S 2010 Di i i ti Ft d Cl ifi ti Mthd f D essaue r  M  P  D ua S  2010  Di scr i m i na ti ve F ea t ures an d Cl ass ifi ca ti on M e th o d s f or Accurate Classification 1st ed vol 7704 pp 14 Bellingham WA Proceedings of SPIE Dessauer M P Dua S 2010 Low Resolution Vehicle Tracking using Dense and Reduced Local Gradient Features Maps 1st ed vol 7694 pp 8 Bellingham WA Proceedings of SPIE SPIE 


Acknowledgements Acknowledgements Fundin g A g encies  US 4 1 Million direct fundin g g g 4 g LA BoR NIH NSF AFRL AFOSR and NASA Research Team Samuel Kasimalla Brandy McKnight Dr Pradeep Chowriappa Connor Johnson Vasanth Nair Mihir Chowriappa  Connor Johnson  Vasanth Nair  Mihir Karnik Mohit Jain and Swadheen Songmen Associative IDS for NextGen Frameworks Dr S Dua LA Tech 49 All the respective Logos belong to their owners 


Rf d Rdi R e f erence d R ea di ngs Copyright of cover pages held by respective publishers 


Thank You Questions Thank You  Questions Dr Sumeet Dua E mail sdua@latech.edu Web http://dmrl.latech.edu Associative IDS for NextGen Frameworks Frameworks Dr S Dua LA Tech 51 Image Source roadtrafficsigns.com 


